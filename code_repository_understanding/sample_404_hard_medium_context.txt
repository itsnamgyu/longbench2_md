######################################################################
#
# CMakeLists.txt for SUPERLU_DIST
#
######################################################################

# Required version
cmake_minimum_required(VERSION 3.18.1 FATAL_ERROR)

# Project version numbers
#project(SuperLU_DIST C CXX CUDA)
project(SuperLU_DIST C CXX)
set(VERSION_MAJOR "9")
set(VERSION_MINOR "0")
set(VERSION_BugFix "0")
set(PROJECT_VERSION ${VERSION_MAJOR}.${VERSION_MINOR}.${VERSION_BugFix})

list(APPEND CMAKE_MODULE_PATH "${PROJECT_SOURCE_DIR}/cmake")

# Set up options
option(enable_doc       "Build doxygen documentation" OFF)
option(enable_single    "Enable single precision library" ON)
option(enable_double    "Enable double precision library" ON)
option(enable_complex16 "Enable complex16 precision library" ON)
option(enable_tests  "Build tests" ON)
option(enable_examples  "Build examples" ON)
option(XSDK_ENABLE_Fortran "Enable Fortran" ON)

#-- BLAS
option(TPL_ENABLE_INTERNAL_BLASLIB  "Build the CBLAS library" ${enable_blaslib_DEFAULT})
option(TPL_BLAS_LIBRARIES "List of absolute paths to blas libraries [].")
#-- ParMETIS
option(TPL_ENABLE_PARMETISLIB   "Enable the ParMETIS library" ON)
option(TPL_PARMETIS_LIBRARIES "List of absolute paths to ParMETIS link libraries [].")
option(TPL_PARMETIS_INCLUDE_DIRS "List of absolute paths to ParMETIS include directories [].")
#-- COLAMD
option(TPL_ENABLE_COLAMDLIB   "Enable the COLAMD library" OFF)
option(TPL_COLAMD_LIBRARIES "List of absolute paths to COLAMD link libraries [].")
option(TPL_COLAMD_INCLUDE_DIRS "List of absolute paths to COLAMD include directories [].")
#-- LAPACK
option(TPL_ENABLE_LAPACKLIB "Enable LAPACK library" OFF)
option(TPL_LAPACK_LIBRARIES "List of absolute paths to LAPACK libraries [].")
#-- CombBLAS
option(TPL_ENABLE_COMBBLASLIB   "Build the CombBLAS library" OFF)
option(TPL_COMBBLAS_LIBRARIES "List of absolute paths to CombBLAS link libraries [].")
option(TPL_COMBBLAS_INCLUDE_DIRS "List of absolute paths to CombBLAS include directories [].")
#-- CUDA
option(TPL_ENABLE_CUDALIB   "Enable the CUDA libraries" OFF)
option(TPL_ENABLE_HIPLIB   "Enable the HIP libraries" OFF)
#-- NVSHMEM
option(TPL_ENABLE_NVSHMEM   "Enable the NVSHMEM libraries" OFF)
option(TPL_ENABLE_ROCSHMEM   "Enable the ROCSHMEM libraries" OFF)
#-- MAGMA 
option(TPL_ENABLE_MAGMALIB "Enable the MAGMA library" OFF)

######################################################################
#
# IDEAS: xSDK standards module
#MESSAGE("\nProcess XSDK defaults ...")
# SET(USE_XSDK_DEFAULTS_DEFAULT TRUE) # Set to false if desired
#INCLUDE("cmake/XSDKDefaults.cmake")
######################################################################

include(CTest)
include(CheckLanguage)

######################################################################
#
# Usual initialization stuff
#
######################################################################
set(CMAKE_CXX_STANDARD 11)
#set(CMAKE_CXX_STANDARD 14)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

#message("!!!! top: cxx_implicit='${CMAKE_CXX_IMPLICIT_LINK_LIBRARIES}'")

if (XSDK_ENABLE_Fortran)
  enable_language (Fortran)
  set(NOFORTRAN FALSE)
endif()

set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)    ## ????
set(CMAKE_INSTALL_NAME_DIR "${CMAKE_INSTALL_PREFIX}/lib")

#---- For shared library

# use, i.e. don't skip the full RPATH for the build tree
SET(CMAKE_SKIP_BUILD_RPATH  FALSE)

# when building, don't use the install RPATH already
# (but later on when installing)
SET(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE) 

# the RPATH to be used when installing
set(CMAKE_INSTALL_RPATH "${CMAKE_INSTALL_PREFIX}/lib")

# add the automatically determined parts of the RPATH
# which point to directories outside the build tree to the install RPATH
SET(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
#----

SET(BUILD_STATIC_LIBS TRUE CACHE BOOL "Include static libs when building shared")

if (BUILD_SHARED_LIBS)
  message("-- SuperLU_DIST will be built as a shared library.")
  # set the position independent code property on all targets, so that
  # -fPIC is added in compiler flag.
  set(CMAKE_POSITION_INDEPENDENT_CODE ON)

  set(PROJECT_NAME_LIB_EXPORT libsuperlu_dist.so)
  SET(CMAKE_EXE_LINKER_FLAGS
      "${CMAKE_EXE_LINKER_FLAGS} -Wl,-rpath,${CMAKE_INSTALL_PREFIX}/SRC")

  if (BUILD_STATIC_LIBS)
    message("-- SuperLU_DIST will also be built as a static library.")
  endif()
  set(SHARED_C_FLAGS_EXPORT ${CMAKE_SHARED_LIBRARY_C_FLAGS})
else()
  message("-- SuperLU_DIST will be built as a static library.")
  set(PROJECT_NAME_LIB_EXPORT libsuperlu_dist.a)
endif()

set(SUPERLU_VERSION "${PROJECT_VERSION}")
set(SUPERLU_REV "${PROJECT_REV}")

# The XSDK standard does not allow using internally built BLAS
if (USE_XSDK_DEFAULTS)
  set(enable_blaslib_DEFAULT OFF)
else()
  set(enable_blaslib_DEFAULT ON)
endif()

if (NOT CMAKE_INSTALL_PREFIX)
  set(CMAKE_INSTALL_PREFIX /usr/local)
endif()

if(NOT MSVC)
  include(GNUInstallDirs)
  set(default_install_inc_dir ${CMAKE_INSTALL_INCLUDEDIR})
  set(default_install_lib_dir ${CMAKE_INSTALL_LIBDIR})
  set(default_install_bin_dir ${CMAKE_INSTALL_BINDIR})
else() # for Windows
  set(default_install_inc_dir "include")
  set(default_install_lib_dir "lib")
  set(default_install_bin_dir "bin")
endif()

set(INSTALL_INC_DIR "${default_install_inc_dir}" CACHE STRING "The folder where headers will be installed.")
set(INSTALL_LIB_DIR "${default_install_lib_dir}" CACHE STRING "The folder where libraries will be installed.")
set(INSTALL_BIN_DIR "${default_install_bin_dir}" CACHE STRING "The folder where runtime files will be installed.")

# Set up required compiler defines and options.
## get_directory_property( DirDefs COMPILE_DEFINITIONS )

if(XSDK_INDEX_SIZE EQUAL 64)
    message("-- Using 64 bit integer for index size.")
endif()	
set(CMAKE_C_FLAGS_RELEASE "-O3 -g" CACHE STRING "")
set(CMAKE_CXX_FLAGS_RELEASE "-O3 -g" CACHE STRING "")
set(CMAKE_Fortran_FLAGS_RELEASE "-O3 -g" CACHE STRING "")

set(CMAKE_CXX_FLAGS_DEBUG "${CMAKE_CXX_FLAGS_DEBUG}  -O0")
set(CMAKE_C_FLAGS_DEBUG "${CMAKE_C_FLAGS_DEBUG} -O0")

######################################################################
#
# Find packages
#
######################################################################
#
#--------------------- MPI ---------------------
find_package(MPI REQUIRED)
if(MPI_C_FOUND)
    set(CMAKE_C_FLAGS "${MPI_C_COMPILE_FLAGS} ${CMAKE_C_FLAGS}")
    set(CMAKE_CXX_FLAGS "${MPI_CXX_COMPILE_FLAGS} ${CMAKE_CXX_FLAGS}")
    set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} ${MPI_C_LINK_FLAGS}" )
#    message("MPI_C_INCLUDE_DIRS: ${MPI_C_INCLUDE_DIRS}")
endif()
if (XSDK_ENABLE_Fortran)
  if(MPI_Fortran_FOUND)
    set(CMAKE_Fortran_FLAGS "${MPI_Fortran_COMPILE_FLAGS} ${CMAKE_Fortran_FLAGS}")
    set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} ${MPI_Fortran_LINK_FLAGS}")
    include_directories(${MPI_Fortran_INCLUDE_PATH})
  endif()
endif()  

#---- CUDA libraries
if (TPL_ENABLE_CUDALIB)   ## want to use cuda
  check_language(CUDA)
  if(CMAKE_CUDA_COMPILER)
    message("-- Enabled support for CUDA.")
    enable_language(CUDA)
    find_package(CUDA REQUIRED)
    if (CUDA_FOUND)
      set(CUDA_SEPARABLE_COMPILATION ON)  
      if(NOT CMAKE_CUDA_ARCHITECTURES)
        # if(${CMAKE_VERSION} VERSION_LESS_EQUAL "3.13.4")
        #   cuda_select_nvcc_arch_flags(ARCH_FLAGS "Auto") # optional argument for arch to add
        #   message("ARCH_FLAGS = ${ARCH_FLAGS}")
        #   string(REPLACE "-gencode;" "--generate-code=" ARCH_FLAGS "${ARCH_FLAGS}")
        #   string(APPEND CMAKE_CUDA_FLAGS "${ARCH_FLAGS}")
        # else()
        include(FindCUDA/select_compute_arch)
        CUDA_DETECT_INSTALLED_GPUS(INSTALLED_GPU_CCS_1)
        string(STRIP "${INSTALLED_GPU_CCS_1}" INSTALLED_GPU_CCS_2)
        string(REPLACE " " ";" INSTALLED_GPU_CCS_3 "${INSTALLED_GPU_CCS_2}")
        string(REPLACE "." "" CUDA_ARCH_LIST "${INSTALLED_GPU_CCS_3}")
        SET(CMAKE_CUDA_ARCHITECTURES ${CUDA_ARCH_LIST})
        set_property(GLOBAL PROPERTY CUDA_ARCHITECTURES "${CUDA_ARCH_LIST}")
      # endif()
      endif()
      set(HAVE_CUDA TRUE)
      if (TPL_ENABLE_NVSHMEM)
        set(CMAKE_CUDA_FLAGS_RELEASE "-O3 --expt-relaxed-constexpr -DNDEBUG -rdc=true" CACHE STRING "")
      else()
        set(CMAKE_CUDA_FLAGS_RELEASE "-O3 --expt-relaxed-constexpr -DNDEBUG" CACHE STRING "")
      endif()
      set(CMAKE_CUDA_FLAGS_DDEBUG "-O0 --expt-relaxed-constexpr -DDEBUG -g" CACHE STRING "")
    endif()

#     find_package(CUB REQUIRED)

    find_package(CUDAToolkit REQUIRED)
# The following appears SRC/CMakeLists.txt
#    if(CUDAToolkit_FOUND)
#      target_link_libraries(superlu_dist PUBLIC CUDA::cudart CUDA::cusolver CUDA::cublas)
#    endif()
    message("-- CUDAToolkit_LIBRARY_ROOT='${CUDAToolkit_LIBRARY_ROOT}'")
    if (NOT "${CUDAToolkit_LIBRARY_ROOT}" STREQUAL "")
       set(CUDA_LIBRARIES "${CUDAToolkit_LIBRARY_ROOT}/lib64/libcudart.so")
       # expose the following to make.inc, but location may not be reliable
       # set(CUDA_CUBLAS_LIBRARIES "${CUDAToolkit_LIBRARY_ROOT}/lib64/libcublas.so")
       set(CUDA_CUSPARSE_LIBRARIES "${CUDAToolkit_LIBRARY_ROOT}/lib64/libcusparse.so")
       set(CUDA_CUSOLVER_LIBRARIES "${CUDAToolkit_LIBRARY_ROOT}/lib64/libcusolver.so")
    else()
	message("-- CUDAToolkit_LIBRARY_ROOT empty, not setting CUDA_LIBRARIES")
    endif()

#     # The following make.inc exporting does not work
#     set(CUDA_LIB CUDA::cudart CUDA::cublas CUDA::cusolver)
# # fix up CUDA library names    
#     string (REPLACE ";" " " CUDA_LIB_STR "${CUDA_LIB}")
#     set(CUDA_LIB_EXPORT ${CUDA_LIB_STR}) 
#     set(HAVE_CUDA TRUE)
    # set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -DHAVE_CUDA")
    # set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -DHAVE_CUDA")
  else()
    message("-- CUDA libraries not found.")
  endif()
endif()


#--------------------- OpenMP ---------------------
if (NOT DEFINED enable_openmp)
  set(enable_openmp TRUE)
endif ()
if (enable_openmp)
  find_package(OpenMP)
## include(FindOpenMP)  # Strumpack uses this

  if(OPENMP_FOUND)
    set(CMAKE_C_FLAGS "${OpenMP_C_FLAGS} ${CMAKE_C_FLAGS}")
    set(CMAKE_CXX_FLAGS "${OpenMP_CXX_FLAGS} ${CMAKE_CXX_FLAGS}")
    set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_EXE_LINKER_FLAGS}")
# The following causes problem with cmake/3.20.+
# set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_C_FLAGS}")
    message("-- OpenMP_EXE_LINKER_FLAGS='${OpenMP_EXE_LINKER_FLAGS}'")
    message("-- CMAKE_EXE_LINKER_FLAGS='${CMAKE_EXE_LINKER_FLAGS}'")
  endif()  
endif()


include(CheckLanguage)

#--------------------- CUDA libraries ---------------------
if (TPL_ENABLE_CUDALIB)   ## want to use cuda
  check_language(CUDA)
  if(CMAKE_CUDA_COMPILER)
    message("-- Enabled support for CUDA.")
    enable_language(CUDA)
    find_package(CUDAToolkit REQUIRED)

    # make.inc exporting not yet tested
    set(CUDA_LIB CUDA::cudart CUDA::cublas CUDA::cusolver)
    string (REPLACE ";" " " CUDA_LIB_STR "${CUDA_LIB}")
    set(CUDA_LIB_EXPORT ${CUDA_LIB_STR}) 
    set(HAVE_CUDA TRUE)
    # set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -DHAVE_CUDA")
    # set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -DHAVE_CUDA")

    #--------------------- NVSHMEM libraries ---------------------
    if (TPL_ENABLE_NVSHMEM)   ## want to use nvshmem
        set(NVSHMEM_LIB ${TPL_NVSHMEM_LIBRARIES}) 
        message("-- Enabled support for NVSHMEM='${NVSHMEM_LIB}'")
        set(HAVE_NVSHMEM TRUE)
    endif()

  else()
    message("-- CUDA libraries not found.")
  endif()
endif()




#--------------------- HIP libraries ---------------------
if (TPL_ENABLE_HIPLIB)   ## want to use hip
  find_package(HIP MODULE)
  if(HIP_FOUND)
    message("-- Enabled support for HIP.")
    find_package(hipblas REQUIRED)
    find_package(rocsolver REQUIRED)
    find_package(rocblas REQUIRED)
    set(HIP_LIB roc::hipblas roc::rocblas roc::rocsolver)
    message("HIP_LIB ${HIP_LIB}")

    # make.inc exporting not yet tested
    string (REPLACE ";" " " HIP_LIB_STR "${HIP_LIB}")
    set(HIP_LIB_EXPORT ${HIP_LIB_STR}) 
    set(HAVE_HIP TRUE)
    # set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -DHAVE_HIP")
    # set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -DHAVE_HIP")    
    # set(HIP_HIPCC_FLAGS "${HIP_HIPCC_FLAGS} -DHAVE_HIP")    
    if (BUILD_SHARED_LIBS)
    set(HIP_HIPCC_FLAGS "${HIP_HIPCC_FLAGS} -fPIC")    
    endif()

    # if ("$ENV{HIP_PLATFORM}" STREQUAL "hcc")
    #   set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -DHAVE_HIP -D__HIP_PLATFORM_HCC__")
    #   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -DHAVE_HIP -D__HIP_PLATFORM_HCC__")
    # elseif ("$ENV{HIP_PLATFORM}" STREQUAL "nvcc")
    #   set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -DHAVE_HIP -D__HIP_PLATFORM_NVCC__")
    #   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -DHAVE_HIP -D__HIP_PLATFORM_NVCC__")
    # endif()

  else()
    message("-- HIP libraries not found.")
  endif()
endif()

#--------------------- BLAS ---------------------
if(NOT TPL_ENABLE_INTERNAL_BLASLIB)
#  set(TPL_BLAS_LIBRARIES "" CACHE FILEPATH
#    "Override of list of absolute path to libs for BLAS.")
  if(TPL_BLAS_LIBRARIES)
    set(BLAS_FOUND TRUE)
  else()
    find_package(BLAS)
    if(BLAS_FOUND)
      set(TPL_BLAS_LIBRARIES "${BLAS_LIBRARIES}" CACHE FILEPATH
        "Set from FindBLAS.cmake BLAS_LIBRARIES." FORCE)
    endif()
  endif()
endif()

if(BLAS_FOUND)
    message("-- Using TPL_BLAS_LIBRARIES='${TPL_BLAS_LIBRARIES}'")
    set(CMAKE_C_FLAGS "-DUSE_VENDOR_BLAS ${CMAKE_C_FLAGS}")
    set(CMAKE_CUDA_FLAGS "-DUSE_VENDOR_BLAS ${CMAKE_CUDA_FLAGS}")
    set(BLAS_LIB ${TPL_BLAS_LIBRARIES})
    # fix up BLAS library name
    string (REPLACE ";" " " BLAS_LIB_STR "${BLAS_LIB}")
    set(BLAS_LIB_EXPORT ${BLAS_LIB_STR})
else()
    message("-- Did not find or specify BLAS, so configure to build internal CBLAS ...")
    add_subdirectory(CBLAS)
    set(BLAS_LIB blas)
    if (BUILD_SHARED_LIBS)  # export to be referenced by downstream makefile
        set(BLAS_LIB_EXPORT ${CMAKE_INSTALL_PREFIX}/${INSTALL_LIB_DIR}/libblas.so)
    else()
        set(BLAS_LIB_EXPORT ${CMAKE_INSTALL_PREFIX}/${INSTALL_LIB_DIR}/libblas.a)
    endif()
endif()

#--------------------- LAPACK ---------------------
if(TPL_ENABLE_LAPACKLIB)  ## want to use LAPACK
  if(TPL_LAPACK_LIBRARIES)
    set(LAPACK_FOUND TRUE)
  else()
    find_package(LAPACK)
    if(LAPACK_FOUND)
      set(TPL_LAPACK_LIBRARIES "${LAPACK_LIBRARIES}" CACHE FILEPATH
        "Set from FindLAPACK.cmake LAPACK_LIBRARIES." FORCE)
    endif()
  endif()
else()  ## not to use LAPACK
  set(LAPACK_FOUND FALSE)
  set(SLU_HAVE_LAPACK FALSE)
endif()

if(LAPACK_FOUND)
    message("-- Using TPL_LAPACK_LIBRARIES='${TPL_LAPACK_LIBRARIES}'")
    set(LAPACK_LIB ${TPL_LAPACK_LIBRARIES})
    # fix up LAPACK library name
    string (REPLACE ";" " " LAPACK_LIB_STR "${LAPACK_LIB}")
    set(LAPACK_LIB_EXPORT ${LAPACK_LIB_STR})
    set(SLU_HAVE_LAPACK TRUE)
else()
    message("-- Will not link with LAPACK.")
endif()

#--------------------- MAGMA ---------------------
if(TPL_ENABLE_MAGMALIB)  ## want to use MAGMA
    set(MAGMA_FOUND TRUE)
endif()

if(MAGMA_FOUND)
    message("-- Using TPL_MAGMA_LIBRARIES='${TPL_MAGMA_LIBRARIES}'")
    set(MAGMA_LIB ${TPL_MAGMA_LIBRARIES})
    # fix up MAGMA library name
    string (REPLACE ";" " " MAGMA_LIB_STR "${MAGMA_LIB}")
    set(MAGMA_LIB_EXPORT ${MAGMA_LIB_STR})
    set(HAVE_MAGMA TRUE)
else()
    message("-- Will not link with MAGMA.")
endif()

#--------------------- ParMETIS ---------------------
if (TPL_ENABLE_PARMETISLIB)   ## want to use parmetis
  if (NOT TPL_PARMETIS_LIBRARIES)
    message(FATAL_ERROR "TPL_PARMETIS_LIBRARIES option should be set for PARMETIS support to be enabled.")
  endif()

  if (NOT TPL_PARMETIS_INCLUDE_DIRS)
    message(FATAL_ERROR "TPL_PARMETIS_INCLUDE_DIRS option be set for PARMETIS support to be enabled.")
  endif()
  foreach(dir ${TPL_PARMETIS_INCLUDE_DIRS})
    if (NOT MINGW AND NOT EXISTS ${dir})
      message(FATAL_ERROR "PARMETIS include directory not found: ${dir}")
    endif()
    set(CMAKE_C_FLAGS "-I${dir} ${CMAKE_C_FLAGS}")
  endforeach()

  message("-- Enabled support for PARMETIS.")
  set(PARMETIS_FOUND TRUE)

  set(PARMETIS_LIB ${TPL_PARMETIS_LIBRARIES})
  # fix up PARMETIS library names
  string (REPLACE ";" " " PARMETIS_LIB_STR "${PARMETIS_LIB}")
  set(PARMETIS_LIB_EXPORT ${PARMETIS_LIB_STR})
else()
  message("-- Will not link with ParMETIS.")
endif()

if(TPL_ENABLE_PARMETISLIB AND NOT PARMETIS_FOUND)
  find_package(ParMETIS)
  if(PARMETIS_FOUND)
    set(PARMETIS_LIB ParMETIS::ParMETIS)
    set(TPL_PARMETIS_INCLUDE_DIRS "")
  endif()
endif()

if(PARMETIS_FOUND)
  set(HAVE_PARMETIS TRUE)
endif()

#--------------------- COLAMD ---------------------
if (TPL_ENABLE_COLAMD)   ## want to use colamd
  if (NOT TPL_COLAMD_LIBRARIES)
    message(FATAL_ERROR "TPL_COLAMD_LIBRARIES option should be set for COLAMD support to be enabled.")
  endif()

  if (NOT TPL_COLAMD_INCLUDE_DIRS)
    message(FATAL_ERROR "TPL_COLAMD_INCLUDE_DIRS option be set for COLAMD support to be enabled.")
  endif()
  foreach(dir ${TPL_COLAMD_INCLUDE_DIRS})
    if (NOT MINGW AND NOT EXISTS ${dir})
      message(FATAL_ERROR "COLAMD include directory not found: ${dir}")
    endif()
    set(CMAKE_C_FLAGS "-I${dir} ${CMAKE_C_FLAGS}")
  endforeach()

  message("-- Enabled support for COLAMD.")
  set(COLAMD_FOUND TRUE)

  set(COLAMD_LIB ${TPL_COLAMD_LIBRARIES})
  # fix up COLAMD library names
  string (REPLACE ";" " " COLAMD_LIB_STR "${COLAMD_LIB}")
  set(COLAMD_LIB_EXPORT ${COLAMD_LIB_STR})
else()
  message("-- Will not link with COLAMD.")
endif()

if(COLAMD_FOUND)
  set(HAVE_COLAMD TRUE)
endif()

#--------------------- CombBLAS ---------------------
if (TPL_ENABLE_COMBBLASLIB)   ## want to use CombBLAS
  if (NOT TPL_COMBBLAS_LIBRARIES)
    message(FATAL_ERROR "TPL_COMBBLAS_LIBRARIES option should be set for COMBBLAS support to be enabled.")
  endif()

  if (NOT TPL_COMBBLAS_INCLUDE_DIRS)
    message(FATAL_ERROR "TPL_COMBBLAS_INCLUDE_DIRS option be set for COMBBLAS support to be enabled.")
  endif()
  foreach(dir ${TPL_COMBBLAS_INCLUDE_DIRS})
    if (NOT MINGW AND NOT EXISTS ${dir})
      message(FATAL_ERROR "COMBBLAS include directory not found: ${dir}")
    endif()
    set(CMAKE_CXX_FLAGS "-I${dir} ${CMAKE_CXX_FLAGS}")
  endforeach()

  message("-- Enabled support for COMBBLAS")
  set(COMBBLAS_FOUND TRUE)
  set(CMAKE_CXX_STANDARD 14)  # CombBLAS requires c++14

  set(COMBBLAS_LIB ${TPL_COMBBLAS_LIBRARIES})
  # fix up COMBBLAS library names
  string (REPLACE ";" " " COMBBLAS_LIB_STR "${COMBBLAS_LIB}")
  set(COMBBLAS_LIB_EXPORT ${COMBBLAS_LIB_STR})

else()
  message("-- Will not link with CombBLAS.")
endif()

if(COMBBLAS_FOUND)
  set(HAVE_COMBBLAS TRUE)
  set(LOADER $(CXX))
else()
  set(LOADER $(CC))
endif()

#---------------------- Additional C linker library ---------
SET(_c_libs ${CMAKE_C_IMPLICIT_LINK_LIBRARIES})
FOREACH(_lib ${_c_libs})
  set(EXTRA_LIB "-l${_lib} ${EXTRA_LIB}")
ENDFOREACH()
string (REPLACE ";" " " EXTRA_LIB_STR "${EXTRA_LIB}")
set(EXTRA_LIB_EXPORT ${EXTRA_LIB_STR}) 
message("-- EXTRA_LIB_EXPORT='${EXTRA_LIB_EXPORT}'")

#---------------------- Additional Fortran linker library ---------
if (XSDK_ENABLE_Fortran)
   SET(_f_libs ${CMAKE_Fortran_IMPLICIT_LINK_LIBRARIES})
   FOREACH(_lib ${_f_libs})
       set(EXTRA_FLIB "${EXTRA_FLIB} -l${_lib}")
   ENDFOREACH()
   string (REPLACE ";" " " EXTRA_FLIB_STR "${EXTRA_FLIB}")
   set(EXTRA_FLIB_EXPORT ${EXTRA_FLIB_STR})
   message("-- EXTRA_FLIB_EXPORT='${EXTRA_FLIB_EXPORT}'")
   
   if (BUILD_SHARED_LIBS)
      message("-- superlu_dist_fortran will be built as a dynamic library.")
      set(PROJECT_NAME_LIB_FORTRAN libsuperlu_dist_fortran.so)
      SET(CMAKE_EXE_LINKER_FLAGS
          "${CMAKE_EXE_LINKER_FLAGS} -Wl,-rpath,${CMAKE_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR}")
   else()
      message("-- superlu_dist_fortran will be built as a static library.")
      set(PROJECT_NAME_LIB_FORTRAN libsuperlu_dist_fortran.a)
   endif()
endif()

######################################################################
#
# Fortran-C name mangling 
#
######################################################################
if (XSDK_ENABLE_Fortran)
  include(FortranCInterface)
  FortranCInterface_HEADER(${SuperLU_DIST_SOURCE_DIR}/SRC/superlu_FortranCInterface.h
                            MACRO_NAMESPACE "FC_")
  FortranCInterface_VERIFY(CXX)
  SET(MPI_Fortran_LINK_FLAGS "${CMAKE_EXE_LINKER_FLAGS}")

  #---------------------------------------------------------------------
  # Set default fortran format to free and preprocess all files
  #---------------------------------------------------------------------
  set(CMAKE_Fortran_FORMAT FREE)
  set(CMAKE_Fortran_PREPROCESS ON)
endif()

######################################################################
#
# Include directories
#
######################################################################

include_directories(${CMAKE_BINARY_DIR}/SRC) # For superlu_dist_config.h
include_directories(${CMAKE_SOURCE_DIR}/SRC/include)
include_directories(${CMAKE_SOURCE_DIR}/SRC/cuda)
include_directories(${CMAKE_SOURCE_DIR}/SRC/hip)
if (TPL_PARMETIS_INCLUDE_DIRS)
  include_directories(${TPL_PARMETIS_INCLUDE_DIRS})  ## parmetis
endif ()
if (TPL_COMBBLAS_INCLUDE_DIRS)
  include_directories(${TPL_COMBBLAS_INCLUDE_DIRS})  ## CombBLAS
endif ()
if (TPL_MAGMA_INCLUDE_DIRS)
  include_directories(${TPL_MAGMA_INCLUDE_DIRS})  ## MAGMA
endif ()
include_directories(${MPI_C_INCLUDE_PATH})

######################################################################
#
# Add subdirectories
#
######################################################################

add_subdirectory(SRC)

if(enable_doc)
  message(FATAL_ERROR "Documentation build requested but not implemented.")
  #implement doxygen
endif()

if(enable_tests)
  enable_testing()
  add_subdirectory(TEST)
endif()

if(enable_examples)
  enable_testing()
  add_subdirectory(EXAMPLE)
endif()

if (XSDK_ENABLE_Fortran)
  add_subdirectory(FORTRAN)
endif()

# superlu_dist uses c++11. PUBLIC means that the other codes linking to it need c++11
#target_compile_features(SuperLU_DIST PUBLIC cxx_std_11)


# Generate various configure files with proper definitions

# Add pkg-config support
if(IS_ABSOLUTE ${CMAKE_INSTALL_LIBDIR})
  set(pkgconfig_libdir ${CMAKE_INSTALL_LIBDIR})
else()
  set(pkgconfig_libdir ${CMAKE_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR})
endif()
configure_file(${CMAKE_CURRENT_SOURCE_DIR}/superlu_dist.pc.in ${CMAKE_CURRENT_BINARY_DIR}/superlu_dist.pc @ONLY)
install(FILES ${CMAKE_CURRENT_BINARY_DIR}/superlu_dist.pc
	DESTINATION ${CMAKE_INSTALL_LIBDIR}/pkgconfig)


# configure_file(${CMAKE_SOURCE_DIR}/make.inc.in ${CMAKE_BINARY_DIR}/make.inc)
configure_file(${SuperLU_DIST_SOURCE_DIR}/make.inc.in ${SuperLU_DIST_SOURCE_DIR}/make.inc)

#configure_file(${SuperLU_DIST_SOURCE_DIR}/SRC/superlu_dist_config.h.in ${SuperLU_DIST_BINARY_DIR}/SRC/superlu_dist_config.h)
configure_file(${SuperLU_DIST_SOURCE_DIR}/SRC/superlu_dist_config.h.in ${SuperLU_DIST_SOURCE_DIR}/SRC/include/superlu_dist_config.h)

# Following is to configure a file for FORTRAN code
configure_file(${SuperLU_DIST_SOURCE_DIR}/SRC/superlu_dist_config.h.in ${SuperLU_DIST_BINARY_DIR}/FORTRAN/superlu_dist_config.h)

#message("MPI_Fortran_LINK_FLAGS '${MPI_Fortran_LINK_FLAGS}'")


# SuperLU_DIST (version 9.0.0)   <img align=center width="55" alt="superlu" src="https://user-images.githubusercontent.com/11741943/103982988-5a9a9d00-5139-11eb-9ac4-a55e80a79f8d.png">

[![Build Status](https://travis-ci.org/xiaoyeli/superlu_dist.svg?branch=master)](https://travis-ci.org/xiaoyeli/superlu_dist) 
[Nightly tests](http://my.cdash.org/index.php?project=superlu_dist)

SuperLU_DIST contains a set of subroutines to solve a sparse linear system 
A*X=B. It uses Gaussian elimination with static pivoting (GESP). 
Static pivoting is a technique that combines the numerical stability of
partial pivoting with the scalability of Cholesky (no pivoting),
to run accurately and efficiently on large numbers of processors. 

SuperLU_DIST is a parallel extension to the serial SuperLU library.
It is targeted for the distributed memory parallel machines.
SuperLU_DIST is implemented in ANSI C, with OpenMP for on-node parallelism
and MPI for off-node communications. We are actively developing multi-GPU
acceleration capabilities.
<!-- Currently, the LU factorization and triangular solution routines, -->
<!-- which are the most time-consuming part of the solution process,-->
<!-- are parallelized. The other routines, such as static pivoting and -->
<!-- column preordering for sparsity are performed sequentially. -->
<!-- This "alpha" release contains double-precision real and-->
<!-- double-precision complex data types.-->

Table of Contents
=================

* [SuperLU_DIST (version 9.0.0)   <a href="https://user-images.githubusercontent.com/11741943/103982988-5a9a9d00-5139-11eb-9ac4-a55e80a79f8d.png" target="_blank" rel="nofollow"><img align="center" width="55" alt="superlu" src="https://user-images.githubusercontent.com/11741943/103982988-5a9a9d00-5139-11eb-9ac4-a55e80a79f8d.png" style="max-width:100%;"></a>](#superlu_dist-version-81---)
* [Directory structure of the source code](#directory-structure-of-the-source-code)
* [Installation](#installation)
   * [Installation option 1: Using CMake build system.](#installation-option-1-using-cmake-build-system)
      * [Dependent external libraries: BLAS and ParMETIS](#dependent-external-libraries-blas-and-parmetis)
      * [Optional external libraries: CombBLAS, LAPACK](#optional-external-libraries-combblas-lapack)
      * [Use GPU](#use-gpu)
      * [Summary of the CMake definitions.](#summary-of-the-cmake-definitions)
   * [Installation option 2: Manual installation with makefile.](#installation-option-2-manual-installation-with-makefile)
      * [2.1 Edit the make.inc include file.](#21-edit-the-makeinc-include-file)
      * [2.2. The BLAS library.](#22-the-blas-library)
      * [2.3. External libraries.](#23-external-libraries)
         * [2.3.1 Metis and ParMetis.](#231-metis-and-parmetis)
         * [2.3.2 LAPACK.](#232-lapack)
         * [2.3.3 CombBLAS.](#233-combblas)
      * [2.4. C preprocessor definition CDEFS. (Replaced by cmake module FortranCInterface.)](#24-c-preprocessor-definition-cdefs-replaced-by-cmake-module-fortrancinterface)
      * [2.5. Multicore and GPU.](#25-multicore-and-gpu)
* [Summary of the environment variables.](#summary-of-the-environment-variables)
* [Windows Usage](#windows-usage)
* [Reading sparse matrix files](#reading-sparse-matrix-files)
* [REFERENCES](#references)
* [RELEASE VERSIONS](#release-versions)

Created by [gh-md-toc](https://github.com/ekalinin/github-markdown-toc)

# SuperLU_DIST (version 8.2)   <img align=center width="55" alt="superlu" src="https://user-images.githubusercontent.com/11741943/103982988-5a9a9d00-5139-11eb-9ac4-a55e80a79f8d.png">

[![Build Status](https://travis-ci.org/xiaoyeli/superlu_dist.svg?branch=master)](https://travis-ci.org/xiaoyeli/superlu_dist) 
[Nightly tests](http://my.cdash.org/index.php?project=superlu_dist)

SuperLU_DIST contains a set of subroutines to solve a sparse linear system 
A*X=B. It uses Gaussian elimination with static pivoting (GESP). 
Static pivoting is a technique that combines the numerical stability of
partial pivoting with the scalability of Cholesky (no pivoting),
to run accurately and efficiently on large numbers of processors. 

SuperLU_DIST is a parallel extension to the serial SuperLU library.
It is targeted for the distributed memory parallel machines.
SuperLU_DIST is implemented in ANSI C, with OpenMP for on-node parallelism
and MPI for off-node communications. We are actively developing GPU
acceleration capabilities.
<!-- Currently, the LU factorization and triangular solution routines, -->
<!-- which are the most time-consuming part of the solution process,-->
<!-- are parallelized. The other routines, such as static pivoting and -->
<!-- column preordering for sparsity are performed sequentially. -->
<!-- This "alpha" release contains double-precision real and-->
<!-- double-precision complex data types.-->


# Directory structure of the source code

```
SuperLU_DIST/README    instructions on installation
SuperLU_DIST/CBLAS/    needed BLAS routines in C, not necessarily fast
	 	       (NOTE: this version is single threaded. If you use the
		       library with multiple OpenMP threads, performance
		       relies on a good multithreaded BLAS implementation.)
SuperLU_DIST/DOC/      the Users' Guide
SuperLU_DIST/FORTRAN/  Fortran90 wrapper functions
SuperLU_DIST/EXAMPLE/  example programs
SuperLU_DIST/INSTALL/  test machine dependent parameters
SuperLU_DIST/SRC/      C source code, to be compiled into libsuperlu_dist.a
SuperLU_DIST/TEST/     testing code
SuperLU_DIST/lib/      contains library archive libsuperlu_dist.a
SuperLU_DIST/Makefile  top-level Makefile that does installation and testing
SuperLU_DIST/make.inc  compiler, compiler flags, library definitions and C
	               preprocessor definitions, included in all Makefiles.
	               (You may need to edit it to suit your system
	               before compiling the whole package.)
SuperLU_DIST/MAKE_INC/ sample machine-specific make.inc files
```

# Installation

There are two ways to install the package. The first method is to use
CMake automatic build system. The other method requires users to 
The procedures are described below.

## Installation option 1: Using CMake build system.
You will need to create a build tree from which to invoke CMake.

### Dependent external libraries: BLAS and ParMETIS
If you have a BLAS library on your machine, you can link with it
with the following cmake definition:
```
-DTPL_BLAS_LIBRARIES="<BLAS library name>"
```
Otherwise, the CBLAS/ subdirectory contains the part of the C BLAS
(single threaded) needed by SuperLU_DIST, but they are not optimized.
You can compile and use it with the following cmake definition:
```
-DTPL_ENABLE_INTERNAL_BLASLIB=ON
```

The default sparsity ordering is METIS. But, in order to use parallel
symbolic factorization function, you
need to install ParMETIS parallel ordering package and define the
two environment variables: PARMETIS_ROOT and PARMETIS_BUILD_DIR

(Note: ParMETIS library also contains serial METIS library.)

```
export PARMETIS_ROOT=<Prefix directory of the ParMETIS installation>
export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/build/Linux-x86_64
```

### Optional external libraries: CombBLAS, LAPACK

In order to use parallel weighted matching HWPM (Heavy Weight
Perfect Matching) for numerical pre-pivoting, you need to install 
CombBLAS and define the environment variable:

```
export COMBBLAS_ROOT=<Prefix directory of the CombBLAS installation>
export COMBBLAS_BUILD_DIR=${COMBBLAS_ROOT}/_build
```
Then, install with cmake option:
```
-DTPL_ENABLE_COMBBLASLIB=ON
```

By default, LAPACK is not needed. Only in triangular solve routine, we
may use LAPACK to explicitly invert the dense diagonal block to improve
speed. You can use it with the following cmake option:
```
-DTPL_ENABLE_LAPACKLIB=ON
```

### Use GPU
You can enable (NVIDIA) GPU with CUDA with the following cmake option:
```
-DTPL_ENABLE_CUDALIB=TRUE
```
You can enable (AMD) GPU with HIP with the following cmake option:
```
-DTPL_ENABLE_HIPLIB=TRUE
```

Once these needed third-party libraries are in place, the installation
can be done as follows at the top level directory:

For a simple installation with default setting, do:
(ParMETIS is needed, i.e., TPL_ENABLE_PARMETISLIB=ON)
```
mkdir build ; cd build;
cmake .. \
    -DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
    -DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a" \
```
For a more sophisticated installation including third-party libraries, do:
```
cmake .. \
    -DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
    -DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a" \
    -DTPL_ENABLE_COMBBLASLIB=ON \
    -DTPL_COMBBLAS_INCLUDE_DIRS="${COMBBLAS_ROOT}/_install/include;${COMBBLAS_R\
OOT}/Applications/BipartiteMatchings" \
    -DTPL_COMBBLAS_LIBRARIES="${COMBBLAS_BUILD_DIR}/libCombBLAS.a" \
    -DCMAKE_C_FLAGS="-std=c99 -g -DPRNTlevel=0 -DDEBUGlevel=0" \
    -DCMAKE_C_COMPILER=mpicc \
    -DCMAKE_CXX_COMPILER=mpicxx \
    -DCMAKE_CXX_FLAGS="-std=c++11" \
    -DTPL_ENABLE_BLASLIB=OFF \
    -DBUILD_SHARED_LIBS=OFF \
    -DCMAKE_INSTALL_PREFIX=.

( see example cmake script: run_cmake_build.sh )
```

You can disable CombBLAS or LAPACK with the following cmake options:
```
-DTPL_ENABLE_LAPACKLIB=FALSE
-DTPL_ENABLE_COMBBLASLIB=FALSE
```

To actually build (compile), type:
`make`

To install the libraries, type:
`make install`

To run the installation test, type:
`ctest`
(The outputs are in file: `build/Testing/Temporary/LastTest.log`)
or,
`ctest -D Experimental`
or,
`ctest -D Nightly`

**NOTE:**
The parallel execution in ctest is invoked by "mpiexec" command which is
from MPICH environment. If your MPI is not MPICH/mpiexec based, the test
execution may fail. You can pass the definition option "-DMPIEXEC_EXECUTABLE"
to cmake. For example on Cori at NERSC, you will need the following:
`-DMPIEXEC_EXECUTABLE=/usr/bin/srun`

Or, you can always go to TEST/ directory to perform testing manually.

### Summary of the CMake definitions.
The following list summarize the commonly used CMake definitions. In each case,
the first choice is the default setting. After running 'cmake' installation,
a configuration header file is generated in SRC/superlu_dist_config.h, which
contains the key CPP definitions used throughout the code.
```
    -TPL_ENABLE_PARMETISLIB=ON | OFF
    -DTPL_ENABLE_INTERNAL_BLASLIB=OFF | ON
    -DTPL_ENABLE_LAPACKLIB=OFF | ON
    -TPL_ENABLE_COMBBLASLIB=OFF | ON
    -DTPL_ENABLE_CUDALIB=OFF | ON
    -DTPL_ENABLE_HIPLIB=OFF | ON
    -Denable_complex16=OFF | ON
    -DXSDK_INDEX_SIZE=32 | 64

    -DBUILD_SHARED_LIBS= OFF | ON
    -DCMAKE_INSTALL_PREFIX=<...>.
    -DCMAKE_C_COMPILER=<MPI C compiler>
    -DCMAKE_C_FLAGS="..." 
    -DCMAKE_CXX_COMPILER=<MPI C++ compiler>
    -DMAKE_CXX_FLAGS="..."
    -DCMAKE_CUDA_FLAGS="..." 
    -DHIP_HIPCC_FLAGS="..." 
    -DXSDK_ENABLE_Fortran=OFF | ON
    -DCMAKE_Fortran_COMPILER=<MPI F90 compiler>
```

## Installation option 2: Manual installation with makefile.
Before installing the package, please examine the three things dependent 
on your system setup:

### 2.1 Edit the make.inc include file.

This make include file is referenced inside each of the Makefiles
in the various subdirectories. As a result, there is no need to 
edit the Makefiles in the subdirectories. All information that is
machine specific has been defined in this include file. 

Sample machine-specific make.inc are provided in the MAKE_INC/
directory for several platforms, such as Cray XT5, Linux, Mac-OS, and CUDA.
When you have selected the machine to which you wish to install
SuperLU_DIST, copy the appropriate sample include file 
(if one is present) into make.inc.

For example, if you wish to run SuperLU_DIST on a Cray XT5,  you can do
`cp MAKE_INC/make.xt5  make.inc`

For the systems other than listed above, some porting effort is needed
for parallel factorization routines. Please refer to the Users' Guide 
for detailed instructions on porting.

The following CPP definitions can be set in CFLAGS.
```
-DXSDK_INDEX_SIZE=64
use 64-bit integers for indexing sparse matrices. (default 32 bit)

-DPRNTlevel=[0,1,2,...]
printing level to show solver's execution details. (default 0)

-DDEBUGlevel=[0,1,2,...]
diagnostic printing level for debugging purpose. (default 0)
```      

### 2.2. The BLAS library.

The parallel routines in SuperLU_DIST use some BLAS routines on each MPI
process. Moreover, if you enable OpenMP with multiple threads, you need to
link with a multithreaded BLAS library. Otherwise performance will be poor.
A good public domain BLAS library is OpenBLAS (http://www.openblas.net),
which has OpenMP support.

If you have a BLAS library your machine, you may define the following in
the file make.inc:
```
BLASDEF = -DUSE_VENDOR_BLAS
BLASLIB = <BLAS library you wish to link with>
```
The CBLAS/ subdirectory contains the part of the C BLAS (single threaded) 
needed by SuperLU_DIST package. However, these codes are intended for use
only if there is no faster implementation of the BLAS already
available on your machine. In this case, you should go to the
top-level SuperLU_DIST/ directory and do the following:

1) In make.inc, undefine (comment out) BLASDEF, and define:
` BLASLIB = ../lib/libblas$(PLAT).a`

2) Type: `make blaslib`
to make the BLAS library from the routines in the
` CBLAS/ subdirectory.`

### 2.3. External libraries. 

  #### 2.3.1 Metis and ParMetis.

If you will use Metis or ParMetis for sparsity ordering, you will
need to install them yourself. Since ParMetis package already
contains the source code for the Metis library, you can just
download and compile ParMetis from:
[http://glaros.dtc.umn.edu/gkhome/metis/parmetis/download](http://glaros.dtc.umn.edu/gkhome/metis/parmetis/download)

After you have installed it, you should define the following in make.inc:
```
HAVE_PARMETIS = TRUE
METISLIB = -L<metis directory> -lmetis
PARMETISLIB = -L<parmetis directory> -lparmetis
I_PARMETIS = -I<parmetis directory>/include -I<parmetis directory>/metis/include
```
You can disable ParMetis with the following line in SRC/superlu_dist_config.h:
```
#undef HAVE_PARMETIS
```
  #### 2.3.2 LAPACK.
  Starting Version 6.0, the triangular solve routine can perform explicit
  inversion on the diagonal blocks, using LAPACK's xTRTRI inversion routine.
  To use this feature, you should define the following in make.inc:
```
SLU_HAVE_LAPACK = TRUE
LAPACKLIB = <lapack library you wish to link with>
```
You can disable LAPACK with the following line in SRC/superlu_dist_config.h:
```
#undef SLU_HAVE_LAPACK
```

 #### 2.3.3 CombBLAS.

You can use parallel approximate weight perfect matching (AWPM) algorithm
to perform numerical pre-pivoting for stability. The default pre-pivoting
is to use MC64 provided internally, which is an exact algorithm, but serial.
In order to use AWPM, you will need to install CombBLAS yourself, at the
download site:
[https://people.eecs.berkeley.edu/~aydin/CombBLAS/html/index.html](https://people.eecs.berkeley.edu/~aydin/CombBLAS/html/index.html)

After you have installed it, you should define the following in make.inc:
```
HAVE_COMBBLAS = TRUE
COMBBLASLIB = <combblas root>/_build/libCombBLAS.a
I_COMBBLAS=-I<combblas root>/_install/include -I<combblas root>/Applications/BipartiteMatchings
```
You can disable CombBLAS with the following line in SRC/superlu_dist_config.h:
```
#undef HAVE_COMBBLAS
```

### 2.4. C preprocessor definition CDEFS. (Replaced by cmake module FortranCInterface.)

In the header file SRC/superlu_FCnames.h, we use macros to determine how
C routines should be named so that they are callable by Fortran.
(Some vendor-supplied BLAS libraries do not have C interfaces. So the 
re-naming is needed in order for the SuperLU BLAS calls (in C) to 
interface with the Fortran-style BLAS.)
The possible options for CDEFS are:
```
-DAdd_: Fortran expects a C routine to have an underscore
  postfixed to the name;
  (This is set as the default)
-DNoChange: Fortran expects a C routine name to be identical to
      that compiled by C;
-DUpCase: Fortran expects a C routine name to be all uppercase.
```

### 2.5. Multicore and GPU.

To use OpenMP parallelism, need to link with an OpenMP library, and
set the number of threads you wish to use as follows (bash):

`export OMP_NUM_THREADS=<##>`

To enable NVIDIA GPU access, need to take the following step:
Add the CUDA library location in make.inc:
```
HAVE_CUDA=TRUE
INCS += -I<CUDA directory>/include
LIBS += -L<CUDA directory>/lib64 -lcublas -lcudart 
endif
```
A Makefile is provided in each subdirectory. The installation can be done
completely automatically by simply typing "make" at the top level.


# Summary of the environment variables.
A couple of environment variables affect parallel execution.
```
    export OMP_NUM_THREADS=<...>
    export SUPERLU_ACC_OFFLOAD=1  // this enables use of GPU. Default is 1.
```
Several integer blocking parameters may affect performance. Most of them can be
set by the user through environment variables. Oherwise the default values
are provided. Various SuperLU routines call an environment inquiry function
to obtain these parameters. This function is provided in the file SRC/sp_ienv.c.
Please consult that file for detailed description of the meanings.
```
    export NREL=<...>   // supernode relaxation parameter
    export NSUP=<...>   // maximum allowable supernode size, not to exceed 512
    export FILL=<...>   // estimated fill ratio of nonzeros(L+U)/nonzeros(A)
    export MAX_BUFFER_SIZE=<...>   // maximum buffer size on GPU for GEMM
```

# Windows Usage
Prerequisites: CMake, Visual Studio, Microsoft HPC Pack
This has been tested with Visual Studio 2017, without Parmetis,
without Fortran, and with OpenMP disabled. 

The cmake configuration line used was
```
'/winsame/contrib-vs2017/cmake-3.9.4-ser/bin/cmake' \
  -DCMAKE_INSTALL_PREFIX:PATH=C:/winsame/volatile-vs2017/superlu_dist-master.r147-parcomm \
  -DCMAKE_BUILD_TYPE:STRING=Release \
  -DCMAKE_COLOR_MAKEFILE:BOOL=FALSE \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=TRUE \
  -Denable_openmp:BOOL=FALSE \
  -DCMAKE_C_COMPILER:FILEPATH='C:/Program Files (x86)/Microsoft Visual Studio/2017/Professional/VC/Tools/MSVC/14.11.25503/bin/HostX64/x64/cl.exe' \
  -DCMAKE_C_FLAGS:STRING='/DWIN32 /D_WINDOWS /W3' \
  -DTPL_ENABLE_PARMETISLIB:BOOL=FALSE \
  -DXSDK_ENABLE_Fortran=OFF \
  -G 'NMake Makefiles JOM' \
  C:/path/to/superlu_dist
```

After configuring, simply do
```
  jom # or nmake
  jom install  # or nmake install
```

Libraries will be installed under
C:/winsame/volatile-vs2017/superlu_dist-master.r147-parcomm/lib
for the above configuration.

If you wish to test:
  `ctest`

# Reading sparse matrix files

The SRC/ directory contains the following routines to read different file 
formats, they all have the similar calling sequence.
```
$ ls -l dread*.c
dreadMM.c              : Matrix Market, files with suffix .mtx
dreadhb.c              : Harrell-Boeing, files with suffix .rua
dreadrb.c              : Rutherford-Boeing, files with suffix .rb
dreadtriple.c          : triplet, with header
dreadtriple_noheader.c : triplet, no header, which is also readable in Matlab
```

# REFERENCES

**[1]** X.S. Li and J.W. Demmel, "SuperLU_DIST: A Scalable Distributed-Memory
 Sparse Direct Solver for Unsymmetric Linear Systems", ACM Trans. on Math.
 Software, Vol. 29, No. 2, June 2003, pp. 110-140.  
**[2]** L. Grigori, J. Demmel and X.S. Li, "Parallel Symbolic Factorization
 for Sparse LU with Static Pivoting", SIAM J. Sci. Comp., Vol. 29, Issue 3,
 1289-1314, 2007.  
**[3]** P. Sao, R. Vuduc and X.S. Li, "A distributed CPU-GPU sparse direct
 solver", Proc. of EuroPar-2014 Parallel Processing, August 25-29, 2014.
 Porto, Portugal.  
**[4]** P. Sao, X.S. Li, R. Vuduc, “A Communication-Avoiding 3D Factorization
 for Sparse Matrices”, Proc. of IPDPS, May 21–25, 2018, Vancouver.   
**[5]** P. Sao, R. Vuduc, X. Li, "Communication-avoiding 3D algorithm for
 sparse LU factorization on heterogeneous systems", J. Parallel and
 Distributed Computing (JPDC), September 2019.     
**[6]** Y. Liu, M. Jacquelin, P. Ghysels and X.S. Li, “Highly scalable
 distributed-memory sparse triangular solution algorithms”, Proc. of
 SIAM workshop on Combinatorial Scientific Computing, June 6-8, 2018,
 Bergen, Norway.   
**[7]** N. Ding, S. Williams, Y. Liu, X.S. Li, "Leveraging One-Sided
 Communication for Sparse Triangular Solvers", Proc. of SIAM Conf. on
 Parallel Processing for Scientific Computing. Feb. 12-15, 2020.   
**[8]** A. Azad, A. Buluc, X.S. Li, X. Wang, and J. Langguth,
"A distributed-memory algorithm for computing a heavy-weight perfect matching 
on bipartite graphs", SIAM J. Sci. Comput., Vol. 42, No. 4, pp. C143-C168, 2020.   


**Xiaoye S. Li**, Lawrence Berkeley National Lab, [xsli@lbl.gov](xsli@lbl.gov)   
**Gustavo Chavez**, Lawrence Berkeley National Lab, [gichavez@lbl.gov](gichavez@lbl.gov)   
**Jim Demmel**, UC Berkeley, [demmel@cs.berkeley.edu](demmel@cs.berkeley.edu)   
**Nan Ding**, Lawrence Berkeley National Lab, [nanding@lbl.gov](nanding@lbl.gov)  
**John Gilbert**, UC Santa Barbara, [gilbert@cs.ucsb.edu](gilbert@cs.ucsb.edu)
**Laura Grigori**, INRIA, France, [laura.grigori@inria.fr](laura.grigori@inria.fr)  
**Paul Lin**, Lawrence Berkeley National Lab, [paullin@lbl.gov](paullin@lbl.gov)   
**Yang Liu**, Lawrence Berkeley National Lab, [liuyangzhuan@lbl.gov](liuyangzhuan@lbl.gov)   
**Piyush Sao**, Georgia Institute of Technology, [piyush.feynman@gmail.com](piyush.feynman@gmail.com)  
**Meiyue Shao**, Lawrence Berkeley National Lab, [myshao@lbl.gov](myshao@lbl.gov)   
**Ichitaro Yamazaki**, Univ. of Tennessee, [ic.yamazaki@gmail.com](ic.yamazaki@gmail.com)  


# RELEASE VERSIONS
```
October 15, 2003    Version 2.0  
October 1,  2007    Version 2.1  
Feburary 20, 2008   Version 2.2  
October 15, 2008    Version 2.3  
June 9, 2010        Version 2.4  
November 23, 2010   Version 2.5  
March 31, 2013      Version 3.3  
October 1, 2014     Version 4.0  
July 15, 2014       Version 4.1  
September 25, 2015  Version 4.2  
December 31, 2015   Version 4.3  
April 8, 2016       Version 5.0.0  
May 15, 2016        Version 5.1.0  
October 4, 2016     Version 5.1.1  
December 31, 2016   Version 5.1.3  
September 30, 2017  Version 5.2.0  
January 28, 2018    Version 5.3.0
June 1, 2018        Version 5.4.0
September 22, 2018  Version 6.0.0
December 9, 2018    Version 6.1.0
February 8, 2019    Version 6.1.1
November 12, 2019   Version 6.2.0
February 23, 2020   Version 6.3.0
October 23, 2020    Version 6.4.0
May 10, 2021        Version 7.0.0
October 5, 2021     Version 7.1.0
October 18, 2021    Version 7.1.1
December 12, 2021   Version 7.2.0
May 22, 2022        Version 8.0.0
July 5, 2022        Version 8.1.0
October 1, 2022     Version 8.1.1
Novembe 9, 2023     Version 8.2.0
Novembe 17, 2023    Version 8.2.1
May 8, 2024         Version 9.0.0
```



# Doxyfile 1.8.15

# This file describes the settings to be used by the documentation system
# doxygen (www.doxygen.org) for a project.
#
# All text after a double hash (##) is considered a comment and is placed in
# front of the TAG it is preceding.
#
# All text after a single hash (#) is considered a comment and will be ignored.
# The format is:
# TAG = value [value, ...]
# For lists, items can also be appended using:
# TAG += value [value, ...]
# Values that contain spaces should be placed between quotes (\" \").

#---------------------------------------------------------------------------
# Project related configuration options
#---------------------------------------------------------------------------

# This tag specifies the encoding used for all characters in the configuration
# file that follow. The default is UTF-8 which is also the encoding used for all
# text before the first occurrence of this tag. Doxygen uses libiconv (or the
# iconv built into libc) for the transcoding. See
# https://www.gnu.org/software/libiconv/ for the list of possible encodings.
# The default value is: UTF-8.

DOXYFILE_ENCODING      = UTF-8

# The PROJECT_NAME tag is a single word (or a sequence of words surrounded by
# double-quotes, unless you are using Doxywizard) that should identify the
# project for which the documentation is generated. This name is used in the
# title of most generated pages and in a few other places.
# The default value is: My Project.

PROJECT_NAME           = "SuperLU Distributed"

# The PROJECT_NUMBER tag can be used to enter a project or revision number. This
# could be handy for archiving the generated documentation or if some version
# control system is used.

PROJECT_NUMBER         = 9.0.0

# Using the PROJECT_BRIEF tag one can provide an optional one line description
# for a project that appears at the top of each page and should give viewer a
# quick idea about the purpose of the project. Keep the description short.

PROJECT_BRIEF          = "gpu3d"

# With the PROJECT_LOGO tag one can specify a logo or an icon that is included
# in the documentation. The maximum height of the logo should not exceed 55
# pixels and the maximum width should not exceed 200 pixels. Doxygen will copy
# the logo to the output directory.

PROJECT_LOGO           = 

# The OUTPUT_DIRECTORY tag is used to specify the (relative or absolute) path
# into which the generated documentation will be written. If a relative path is
# entered, it will be relative to the location where doxygen was started. If
# left blank the current directory will be used.

OUTPUT_DIRECTORY       = DOC

# If the CREATE_SUBDIRS tag is set to YES then doxygen will create 4096 sub-
# directories (in 2 levels) under the output directory of each output format and
# will distribute the generated files over these directories. Enabling this
# option can be useful when feeding doxygen a huge amount of source files, where
# putting all generated files in the same directory would otherwise causes
# performance problems for the file system.
# The default value is: NO.

CREATE_SUBDIRS         = NO

# If the ALLOW_UNICODE_NAMES tag is set to YES, doxygen will allow non-ASCII
# characters to appear in the names of generated files. If set to NO, non-ASCII
# characters will be escaped, for example _xE3_x81_x84 will be used for Unicode
# U+3044.
# The default value is: NO.

ALLOW_UNICODE_NAMES    = NO

# The OUTPUT_LANGUAGE tag is used to specify the language in which all
# documentation generated by doxygen is written. Doxygen will use this
# information to generate all constant output in the proper language.
# Possible values are: Afrikaans, Arabic, Armenian, Brazilian, Catalan, Chinese,
# Chinese-Traditional, Croatian, Czech, Danish, Dutch, English (United States),
# Esperanto, Farsi (Persian), Finnish, French, German, Greek, Hungarian,
# Indonesian, Italian, Japanese, Japanese-en (Japanese with English messages),
# Korean, Korean-en (Korean with English messages), Latvian, Lithuanian,
# Macedonian, Norwegian, Persian (Farsi), Polish, Portuguese, Romanian, Russian,
# Serbian, Serbian-Cyrillic, Slovak, Slovene, Spanish, Swedish, Turkish,
# Ukrainian and Vietnamese.
# The default value is: English.

OUTPUT_LANGUAGE        = English

# The OUTPUT_TEXT_DIRECTION tag is used to specify the direction in which all
# documentation generated by doxygen is written. Doxygen will use this
# information to generate all generated output in the proper direction.
# Possible values are: None, LTR, RTL and Context.
# The default value is: None.

OUTPUT_TEXT_DIRECTION  = None

# If the BRIEF_MEMBER_DESC tag is set to YES, doxygen will include brief member
# descriptions after the members that are listed in the file and class
# documentation (similar to Javadoc). Set to NO to disable this.
# The default value is: YES.

BRIEF_MEMBER_DESC      = YES

# If the REPEAT_BRIEF tag is set to YES, doxygen will prepend the brief
# description of a member or function before the detailed description
#
# Note: If both HIDE_UNDOC_MEMBERS and BRIEF_MEMBER_DESC are set to NO, the
# brief descriptions will be completely suppressed.
# The default value is: YES.

REPEAT_BRIEF           = YES

# This tag implements a quasi-intelligent brief description abbreviator that is
# used to form the text in various listings. Each string in this list, if found
# as the leading text of the brief description, will be stripped from the text
# and the result, after processing the whole list, is used as the annotated
# text. Otherwise, the brief description is used as-is. If left blank, the
# following values are used ($name is automatically replaced with the name of
# the entity):The $name class, The $name widget, The $name file, is, provides,
# specifies, contains, represents, a, an and the.

ABBREVIATE_BRIEF       = "The $name class" \
                         "The $name widget" \
                         "The $name file" \
                         is \
                         provides \
                         specifies \
                         contains \
                         represents \
                         a \
                         an \
                         the

# If the ALWAYS_DETAILED_SEC and REPEAT_BRIEF tags are both set to YES then
# doxygen will generate a detailed section even if there is only a brief
# description.
# The default value is: NO.

ALWAYS_DETAILED_SEC    = NO

# If the INLINE_INHERITED_MEMB tag is set to YES, doxygen will show all
# inherited members of a class in the documentation of that class as if those
# members were ordinary class members. Constructors, destructors and assignment
# operators of the base classes will not be shown.
# The default value is: NO.

INLINE_INHERITED_MEMB  = NO

# If the FULL_PATH_NAMES tag is set to YES, doxygen will prepend the full path
# before files name in the file list and in the header files. If set to NO the
# shortest path that makes the file name unique will be used
# The default value is: YES.

FULL_PATH_NAMES        = YES

# The STRIP_FROM_PATH tag can be used to strip a user-defined part of the path.
# Stripping is only done if one of the specified strings matches the left-hand
# part of the path. The tag can be used to show relative paths in the file list.
# If left blank the directory from which doxygen is run is used as the path to
# strip.
#
# Note that you can specify absolute paths here, but also relative paths, which
# will be relative from the directory where doxygen is started.
# This tag requires that the tag FULL_PATH_NAMES is set to YES.

STRIP_FROM_PATH        =

# The STRIP_FROM_INC_PATH tag can be used to strip a user-defined part of the
# path mentioned in the documentation of a class, which tells the reader which
# header file to include in order to use a class. If left blank only the name of
# the header file containing the class definition is used. Otherwise one should
# specify the list of include paths that are normally passed to the compiler
# using the -I flag.

STRIP_FROM_INC_PATH    =

# If the SHORT_NAMES tag is set to YES, doxygen will generate much shorter (but
# less readable) file names. This can be useful is your file systems doesn't
# support long names like on DOS, Mac, or CD-ROM.
# The default value is: NO.

SHORT_NAMES            = NO

# If the JAVADOC_AUTOBRIEF tag is set to YES then doxygen will interpret the
# first line (until the first dot) of a Javadoc-style comment as the brief
# description. If set to NO, the Javadoc-style will behave just like regular Qt-
# style comments (thus requiring an explicit @brief command for a brief
# description.)
# The default value is: NO.

JAVADOC_AUTOBRIEF      = NO

# If the QT_AUTOBRIEF tag is set to YES then doxygen will interpret the first
# line (until the first dot) of a Qt-style comment as the brief description. If
# set to NO, the Qt-style will behave just like regular Qt-style comments (thus
# requiring an explicit \brief command for a brief description.)
# The default value is: NO.

QT_AUTOBRIEF           = NO

# The MULTILINE_CPP_IS_BRIEF tag can be set to YES to make doxygen treat a
# multi-line C++ special comment block (i.e. a block of //! or /// comments) as
# a brief description. This used to be the default behavior. The new default is
# to treat a multi-line C++ comment block as a detailed description. Set this
# tag to YES if you prefer the old behavior instead.
#
# Note that setting this tag to YES also means that rational rose comments are
# not recognized any more.
# The default value is: NO.

MULTILINE_CPP_IS_BRIEF = NO

# If the INHERIT_DOCS tag is set to YES then an undocumented member inherits the
# documentation from any documented member that it re-implements.
# The default value is: YES.

INHERIT_DOCS           = YES

# If the SEPARATE_MEMBER_PAGES tag is set to YES then doxygen will produce a new
# page for each member. If set to NO, the documentation of a member will be part
# of the file/class/namespace that contains it.
# The default value is: NO.

SEPARATE_MEMBER_PAGES  = NO

# The TAB_SIZE tag can be used to set the number of spaces in a tab. Doxygen
# uses this value to replace tabs by spaces in code fragments.
# Minimum value: 1, maximum value: 16, default value: 4.

TAB_SIZE               = 4

# This tag can be used to specify a number of aliases that act as commands in
# the documentation. An alias has the form:
# name=value
# For example adding
# "sideeffect=@par Side Effects:\n"
# will allow you to put the command \sideeffect (or @sideeffect) in the
# documentation, which will result in a user-defined paragraph with heading
# "Side Effects:". You can put \n's in the value part of an alias to insert
# newlines (in the resulting output). You can put ^^ in the value part of an
# alias to insert a newline as if a physical newline was in the original file.
# When you need a literal { or } or , in the value part of an alias you have to
# escape them by means of a backslash (\), this can lead to conflicts with the
# commands \{ and \} for these it is advised to use the version @{ and @} or use
# a double escape (\\{ and \\})

ALIASES                =

# This tag can be used to specify a number of word-keyword mappings (TCL only).
# A mapping has the form "name=value". For example adding "class=itcl::class"
# will allow you to use the command class in the itcl::class meaning.

TCL_SUBST              =

# Set the OPTIMIZE_OUTPUT_FOR_C tag to YES if your project consists of C sources
# only. Doxygen will then generate output that is more tailored for C. For
# instance, some of the names that are used will be different. The list of all
# members will be omitted, etc.
# The default value is: NO.

OPTIMIZE_OUTPUT_FOR_C  = NO

# Set the OPTIMIZE_OUTPUT_JAVA tag to YES if your project consists of Java or
# Python sources only. Doxygen will then generate output that is more tailored
# for that language. For instance, namespaces will be presented as packages,
# qualified scopes will look different, etc.
# The default value is: NO.

OPTIMIZE_OUTPUT_JAVA   = NO

# Set the OPTIMIZE_FOR_FORTRAN tag to YES if your project consists of Fortran
# sources. Doxygen will then generate output that is tailored for Fortran.
# The default value is: NO.

OPTIMIZE_FOR_FORTRAN   = NO

# Set the OPTIMIZE_OUTPUT_VHDL tag to YES if your project consists of VHDL
# sources. Doxygen will then generate output that is tailored for VHDL.
# The default value is: NO.

OPTIMIZE_OUTPUT_VHDL   = NO

# Set the OPTIMIZE_OUTPUT_SLICE tag to YES if your project consists of Slice
# sources only. Doxygen will then generate output that is more tailored for that
# language. For instance, namespaces will be presented as modules, types will be
# separated into more groups, etc.
# The default value is: NO.

OPTIMIZE_OUTPUT_SLICE  = NO

# Doxygen selects the parser to use depending on the extension of the files it
# parses. With this tag you can assign which parser to use for a given
# extension. Doxygen has a built-in mapping, but you can override or extend it
# using this tag. The format is ext=language, where ext is a file extension, and
# language is one of the parsers supported by doxygen: IDL, Java, Javascript,
# Csharp (C#), C, C++, D, PHP, md (Markdown), Objective-C, Python, Slice,
# Fortran (fixed format Fortran: FortranFixed, free formatted Fortran:
# FortranFree, unknown formatted Fortran: Fortran. In the later case the parser
# tries to guess whether the code is fixed or free formatted code, this is the
# default for Fortran type files), VHDL, tcl. For instance to make doxygen treat
# .inc files as Fortran files (default is PHP), and .f files as C (default is
# Fortran), use: inc=Fortran f=C.
#
# Note: For files without extension you can use no_extension as a placeholder.
#
# Note that for custom extensions you also need to set FILE_PATTERNS otherwise
# the files are not read by doxygen.

EXTENSION_MAPPING      =

# If the MARKDOWN_SUPPORT tag is enabled then doxygen pre-processes all comments
# according to the Markdown format, which allows for more readable
# documentation. See https://daringfireball.net/projects/markdown/ for details.
# The output of markdown processing is further processed by doxygen, so you can
# mix doxygen, HTML, and XML commands with Markdown formatting. Disable only in
# case of backward compatibilities issues.
# The default value is: YES.

MARKDOWN_SUPPORT       = YES

# When the TOC_INCLUDE_HEADINGS tag is set to a non-zero value, all headings up
# to that level are automatically included in the table of contents, even if
# they do not have an id attribute.
# Note: This feature currently applies only to Markdown headings.
# Minimum value: 0, maximum value: 99, default value: 0.
# This tag requires that the tag MARKDOWN_SUPPORT is set to YES.

TOC_INCLUDE_HEADINGS   = 0

# When enabled doxygen tries to link words that correspond to documented
# classes, or namespaces to their corresponding documentation. Such a link can
# be prevented in individual cases by putting a % sign in front of the word or
# globally by setting AUTOLINK_SUPPORT to NO.
# The default value is: YES.

AUTOLINK_SUPPORT       = YES

# If you use STL classes (i.e. std::string, std::vector, etc.) but do not want
# to include (a tag file for) the STL sources as input, then you should set this
# tag to YES in order to let doxygen match functions declarations and
# definitions whose arguments contain STL classes (e.g. func(std::string);
# versus func(std::string) {}). This also make the inheritance and collaboration
# diagrams that involve STL classes more complete and accurate.
# The default value is: NO.

BUILTIN_STL_SUPPORT    = NO

# If you use Microsoft's C++/CLI language, you should set this option to YES to
# enable parsing support.
# The default value is: NO.

CPP_CLI_SUPPORT        = NO

# Set the SIP_SUPPORT tag to YES if your project consists of sip (see:
# https://www.riverbankcomputing.com/software/sip/intro) sources only. Doxygen
# will parse them like normal C++ but will assume all classes use public instead
# of private inheritance when no explicit protection keyword is present.
# The default value is: NO.

SIP_SUPPORT            = NO

# For Microsoft's IDL there are propget and propput attributes to indicate
# getter and setter methods for a property. Setting this option to YES will make
# doxygen to replace the get and set methods by a property in the documentation.
# This will only work if the methods are indeed getting or setting a simple
# type. If this is not the case, or you want to show the methods anyway, you
# should set this option to NO.
# The default value is: YES.

IDL_PROPERTY_SUPPORT   = YES

# If member grouping is used in the documentation and the DISTRIBUTE_GROUP_DOC
# tag is set to YES then doxygen will reuse the documentation of the first
# member in the group (if any) for the other members of the group. By default
# all members of a group must be documented explicitly.
# The default value is: NO.

DISTRIBUTE_GROUP_DOC   = NO

# If one adds a struct or class to a group and this option is enabled, then also
# any nested class or struct is added to the same group. By default this option
# is disabled and one has to add nested compounds explicitly via \ingroup.
# The default value is: NO.

GROUP_NESTED_COMPOUNDS = NO

# Set the SUBGROUPING tag to YES to allow class member groups of the same type
# (for instance a group of public functions) to be put as a subgroup of that
# type (e.g. under the Public Functions section). Set it to NO to prevent
# subgrouping. Alternatively, this can be done per class using the
# \nosubgrouping command.
# The default value is: YES.

SUBGROUPING            = YES

# When the INLINE_GROUPED_CLASSES tag is set to YES, classes, structs and unions
# are shown inside the group in which they are included (e.g. using \ingroup)
# instead of on a separate page (for HTML and Man pages) or section (for LaTeX
# and RTF).
#
# Note that this feature does not work in combination with
# SEPARATE_MEMBER_PAGES.
# The default value is: NO.

INLINE_GROUPED_CLASSES = NO

# When the INLINE_SIMPLE_STRUCTS tag is set to YES, structs, classes, and unions
# with only public data fields or simple typedef fields will be shown inline in
# the documentation of the scope in which they are defined (i.e. file,
# namespace, or group documentation), provided this scope is documented. If set
# to NO, structs, classes, and unions are shown on a separate page (for HTML and
# Man pages) or section (for LaTeX and RTF).
# The default value is: NO.

INLINE_SIMPLE_STRUCTS  = NO

# When TYPEDEF_HIDES_STRUCT tag is enabled, a typedef of a struct, union, or
# enum is documented as struct, union, or enum with the name of the typedef. So
# typedef struct TypeS {} TypeT, will appear in the documentation as a struct
# with name TypeT. When disabled the typedef will appear as a member of a file,
# namespace, or class. And the struct will be named TypeS. This can typically be
# useful for C code in case the coding convention dictates that all compound
# types are typedef'ed and only the typedef is referenced, never the tag name.
# The default value is: NO.

TYPEDEF_HIDES_STRUCT   = NO

# The size of the symbol lookup cache can be set using LOOKUP_CACHE_SIZE. This
# cache is used to resolve symbols given their name and scope. Since this can be
# an expensive process and often the same symbol appears multiple times in the
# code, doxygen keeps a cache of pre-resolved symbols. If the cache is too small
# doxygen will become slower. If the cache is too large, memory is wasted. The
# cache size is given by this formula: 2^(16+LOOKUP_CACHE_SIZE). The valid range
# is 0..9, the default is 0, corresponding to a cache size of 2^16=65536
# symbols. At the end of a run doxygen will report the cache usage and suggest
# the optimal cache size from a speed point of view.
# Minimum value: 0, maximum value: 9, default value: 0.

LOOKUP_CACHE_SIZE      = 0

#---------------------------------------------------------------------------
# Build related configuration options
#---------------------------------------------------------------------------

# If the EXTRACT_ALL tag is set to YES, doxygen will assume all entities in
# documentation are documented, even if no documentation was available. Private
# class members and static file members will be hidden unless the
# EXTRACT_PRIVATE respectively EXTRACT_STATIC tags are set to YES.
# Note: This will also disable the warnings about undocumented members that are
# normally produced when WARNINGS is set to YES.
# The default value is: NO.

EXTRACT_ALL            = YES

# If the EXTRACT_PRIVATE tag is set to YES, all private members of a class will
# be included in the documentation.
# The default value is: NO.

EXTRACT_PRIVATE        = YES

# If the EXTRACT_PACKAGE tag is set to YES, all members with package or internal
# scope will be included in the documentation.
# The default value is: NO.

EXTRACT_PACKAGE        = YES

# If the EXTRACT_STATIC tag is set to YES, all static members of a file will be
# included in the documentation.
# The default value is: NO.

EXTRACT_STATIC         = YES

# If the EXTRACT_LOCAL_CLASSES tag is set to YES, classes (and structs) defined
# locally in source files will be included in the documentation. If set to NO,
# only classes defined in header files are included. Does not have any effect
# for Java sources.
# The default value is: YES.

EXTRACT_LOCAL_CLASSES  = YES

# This flag is only useful for Objective-C code. If set to YES, local methods,
# which are defined in the implementation section but not in the interface are
# included in the documentation. If set to NO, only methods in the interface are
# included.
# The default value is: NO.

EXTRACT_LOCAL_METHODS  = NO

# If this flag is set to YES, the members of anonymous namespaces will be
# extracted and appear in the documentation as a namespace called
# 'anonymous_namespace{file}', where file will be replaced with the base name of
# the file that contains the anonymous namespace. By default anonymous namespace
# are hidden.
# The default value is: NO.

EXTRACT_ANON_NSPACES   = NO

# If the HIDE_UNDOC_MEMBERS tag is set to YES, doxygen will hide all
# undocumented members inside documented classes or files. If set to NO these
# members will be included in the various overviews, but no documentation
# section is generated. This option has no effect if EXTRACT_ALL is enabled.
# The default value is: NO.

HIDE_UNDOC_MEMBERS     = NO

# If the HIDE_UNDOC_CLASSES tag is set to YES, doxygen will hide all
# undocumented classes that are normally visible in the class hierarchy. If set
# to NO, these classes will be included in the various overviews. This option
# has no effect if EXTRACT_ALL is enabled.
# The default value is: NO.

HIDE_UNDOC_CLASSES     = NO

# If the HIDE_FRIEND_COMPOUNDS tag is set to YES, doxygen will hide all friend
# (class|struct|union) declarations. If set to NO, these declarations will be
# included in the documentation.
# The default value is: NO.

HIDE_FRIEND_COMPOUNDS  = NO

# If the HIDE_IN_BODY_DOCS tag is set to YES, doxygen will hide any
# documentation blocks found inside the body of a function. If set to NO, these
# blocks will be appended to the function's detailed documentation block.
# The default value is: NO.

HIDE_IN_BODY_DOCS      = NO

# The INTERNAL_DOCS tag determines if documentation that is typed after a
# \internal command is included. If the tag is set to NO then the documentation
# will be excluded. Set it to YES to include the internal documentation.
# The default value is: NO.

INTERNAL_DOCS          = NO

# If the CASE_SENSE_NAMES tag is set to NO then doxygen will only generate file
# names in lower-case letters. If set to YES, upper-case letters are also
# allowed. This is useful if you have classes or files whose names only differ
# in case and if your file system supports case sensitive file names. Windows
# and Mac users are advised to set this option to NO.
# The default value is: system dependent.

CASE_SENSE_NAMES       = YES

# If the HIDE_SCOPE_NAMES tag is set to NO then doxygen will show members with
# their full class and namespace scopes in the documentation. If set to YES, the
# scope will be hidden.
# The default value is: NO.

HIDE_SCOPE_NAMES       = NO

# If the HIDE_COMPOUND_REFERENCE tag is set to NO (default) then doxygen will
# append additional text to a page's title, such as Class Reference. If set to
# YES the compound reference will be hidden.
# The default value is: NO.

HIDE_COMPOUND_REFERENCE= NO

# If the SHOW_INCLUDE_FILES tag is set to YES then doxygen will put a list of
# the files that are included by a file in the documentation of that file.
# The default value is: YES.

SHOW_INCLUDE_FILES     = YES

# If the SHOW_GROUPED_MEMB_INC tag is set to YES then Doxygen will add for each
# grouped member an include statement to the documentation, telling the reader
# which file to include in order to use the member.
# The default value is: NO.

SHOW_GROUPED_MEMB_INC  = NO

# If the FORCE_LOCAL_INCLUDES tag is set to YES then doxygen will list include
# files with double quotes in the documentation rather than with sharp brackets.
# The default value is: NO.

FORCE_LOCAL_INCLUDES   = NO

# If the INLINE_INFO tag is set to YES then a tag [inline] is inserted in the
# documentation for inline members.
# The default value is: YES.

INLINE_INFO            = YES

# If the SORT_MEMBER_DOCS tag is set to YES then doxygen will sort the
# (detailed) documentation of file and class members alphabetically by member
# name. If set to NO, the members will appear in declaration order.
# The default value is: YES.

SORT_MEMBER_DOCS       = YES

# If the SORT_BRIEF_DOCS tag is set to YES then doxygen will sort the brief
# descriptions of file, namespace and class members alphabetically by member
# name. If set to NO, the members will appear in declaration order. Note that
# this will also influence the order of the classes in the class list.
# The default value is: NO.

SORT_BRIEF_DOCS        = NO

# If the SORT_MEMBERS_CTORS_1ST tag is set to YES then doxygen will sort the
# (brief and detailed) documentation of class members so that constructors and
# destructors are listed first. If set to NO the constructors will appear in the
# respective orders defined by SORT_BRIEF_DOCS and SORT_MEMBER_DOCS.
# Note: If SORT_BRIEF_DOCS is set to NO this option is ignored for sorting brief
# member documentation.
# Note: If SORT_MEMBER_DOCS is set to NO this option is ignored for sorting
# detailed member documentation.
# The default value is: NO.

SORT_MEMBERS_CTORS_1ST = NO

# If the SORT_GROUP_NAMES tag is set to YES then doxygen will sort the hierarchy
# of group names into alphabetical order. If set to NO the group names will
# appear in their defined order.
# The default value is: NO.

SORT_GROUP_NAMES       = NO

# If the SORT_BY_SCOPE_NAME tag is set to YES, the class list will be sorted by
# fully-qualified names, including namespaces. If set to NO, the class list will
# be sorted only by class name, not including the namespace part.
# Note: This option is not very useful if HIDE_SCOPE_NAMES is set to YES.
# Note: This option applies only to the class list, not to the alphabetical
# list.
# The default value is: NO.

SORT_BY_SCOPE_NAME     = NO

# If the STRICT_PROTO_MATCHING option is enabled and doxygen fails to do proper
# type resolution of all parameters of a function it will reject a match between
# the prototype and the implementation of a member function even if there is
# only one candidate or it is obvious which candidate to choose by doing a
# simple string match. By disabling STRICT_PROTO_MATCHING doxygen will still
# accept a match between prototype and implementation in such cases.
# The default value is: NO.

STRICT_PROTO_MATCHING  = NO

# The GENERATE_TODOLIST tag can be used to enable (YES) or disable (NO) the todo
# list. This list is created by putting \todo commands in the documentation.
# The default value is: YES.

GENERATE_TODOLIST      = YES

# The GENERATE_TESTLIST tag can be used to enable (YES) or disable (NO) the test
# list. This list is created by putting \test commands in the documentation.
# The default value is: YES.

GENERATE_TESTLIST      = YES

# The GENERATE_BUGLIST tag can be used to enable (YES) or disable (NO) the bug
# list. This list is created by putting \bug commands in the documentation.
# The default value is: YES.

GENERATE_BUGLIST       = YES

# The GENERATE_DEPRECATEDLIST tag can be used to enable (YES) or disable (NO)
# the deprecated list. This list is created by putting \deprecated commands in
# the documentation.
# The default value is: YES.

GENERATE_DEPRECATEDLIST= YES

# The ENABLED_SECTIONS tag can be used to enable conditional documentation
# sections, marked by \if <section_label> ... \endif and \cond <section_label>
# ... \endcond blocks.

ENABLED_SECTIONS       =

# The MAX_INITIALIZER_LINES tag determines the maximum number of lines that the
# initial value of a variable or macro / define can have for it to appear in the
# documentation. If the initializer consists of more lines than specified here
# it will be hidden. Use a value of 0 to hide initializers completely. The
# appearance of the value of individual variables and macros / defines can be
# controlled using \showinitializer or \hideinitializer command in the
# documentation regardless of this setting.
# Minimum value: 0, maximum value: 10000, default value: 30.

MAX_INITIALIZER_LINES  = 30

# Set the SHOW_USED_FILES tag to NO to disable the list of files generated at
# the bottom of the documentation of classes and structs. If set to YES, the
# list will mention the files that were used to generate the documentation.
# The default value is: YES.

SHOW_USED_FILES        = YES

# Set the SHOW_FILES tag to NO to disable the generation of the Files page. This
# will remove the Files entry from the Quick Index and from the Folder Tree View
# (if specified).
# The default value is: YES.

SHOW_FILES             = YES

# Set the SHOW_NAMESPACES tag to NO to disable the generation of the Namespaces
# page. This will remove the Namespaces entry from the Quick Index and from the
# Folder Tree View (if specified).
# The default value is: YES.

SHOW_NAMESPACES        = YES

# The FILE_VERSION_FILTER tag can be used to specify a program or script that
# doxygen should invoke to get the current version for each file (typically from
# the version control system). Doxygen will invoke the program by executing (via
# popen()) the command command input-file, where command is the value of the
# FILE_VERSION_FILTER tag, and input-file is the name of an input file provided
# by doxygen. Whatever the program writes to standard output is used as the file
# version. For an example see the documentation.

FILE_VERSION_FILTER    =

# The LAYOUT_FILE tag can be used to specify a layout file which will be parsed
# by doxygen. The layout file controls the global structure of the generated
# output files in an output format independent way. To create the layout file
# that represents doxygen's defaults, run doxygen with the -l option. You can
# optionally specify a file name after the option, if omitted DoxygenLayout.xml
# will be used as the name of the layout file.
#
# Note that if you run doxygen from a directory containing a file called
# DoxygenLayout.xml, doxygen will parse it automatically even if the LAYOUT_FILE
# tag is left empty.

LAYOUT_FILE            =

# The CITE_BIB_FILES tag can be used to specify one or more bib files containing
# the reference definitions. This must be a list of .bib files. The .bib
# extension is automatically appended if omitted. This requires the bibtex tool
# to be installed. See also https://en.wikipedia.org/wiki/BibTeX for more info.
# For LaTeX the style of the bibliography can be controlled using
# LATEX_BIB_STYLE. To use this feature you need bibtex and perl available in the
# search path. See also \cite for info how to create references.

CITE_BIB_FILES         =

#---------------------------------------------------------------------------
# Configuration options related to warning and progress messages
#---------------------------------------------------------------------------

# The QUIET tag can be used to turn on/off the messages that are generated to
# standard output by doxygen. If QUIET is set to YES this implies that the
# messages are off.
# The default value is: NO.

QUIET                  = NO

# The WARNINGS tag can be used to turn on/off the warning messages that are
# generated to standard error (stderr) by doxygen. If WARNINGS is set to YES
# this implies that the warnings are on.
#
# Tip: Turn warnings on while writing the documentation.
# The default value is: YES.

WARNINGS               = YES

# If the WARN_IF_UNDOCUMENTED tag is set to YES then doxygen will generate
# warnings for undocumented members. If EXTRACT_ALL is set to YES then this flag
# will automatically be disabled.
# The default value is: YES.

WARN_IF_UNDOCUMENTED   = YES

# If the WARN_IF_DOC_ERROR tag is set to YES, doxygen will generate warnings for
# potential errors in the documentation, such as not documenting some parameters
# in a documented function, or documenting parameters that don't exist or using
# markup commands wrongly.
# The default value is: YES.

WARN_IF_DOC_ERROR      = YES

# This WARN_NO_PARAMDOC option can be enabled to get warnings for functions that
# are documented, but have no documentation for their parameters or return
# value. If set to NO, doxygen will only warn about wrong or incomplete
# parameter documentation, but not about the absence of documentation. If
# EXTRACT_ALL is set to YES then this flag will automatically be disabled.
# The default value is: NO.

WARN_NO_PARAMDOC       = NO

# If the WARN_AS_ERROR tag is set to YES then doxygen will immediately stop when
# a warning is encountered.
# The default value is: NO.

WARN_AS_ERROR          = NO

# The WARN_FORMAT tag determines the format of the warning messages that doxygen
# can produce. The string should contain the $file, $line, and $text tags, which
# will be replaced by the file and line number from which the warning originated
# and the warning text. Optionally the format may contain $version, which will
# be replaced by the version of the file (if it could be obtained via
# FILE_VERSION_FILTER)
# The default value is: $file:$line: $text.

WARN_FORMAT            = "$file:$line: $text"

# The WARN_LOGFILE tag can be used to specify a file to which warning and error
# messages should be written. If left blank the output is written to standard
# error (stderr).

WARN_LOGFILE           =

#---------------------------------------------------------------------------
# Configuration options related to the input files
#---------------------------------------------------------------------------

# The INPUT tag is used to specify the files and/or directories that contain
# documented source files. You may enter file names like myfile.cpp or
# directories like /usr/src/myproject. Separate the files or directories with
# spaces. See also FILE_PATTERNS and EXTENSION_MAPPING
# Note: If this tag is empty the current directory is searched.

INPUT                  = SRC/include SRC/CplusplusFactor SRC/prec-independent SRC/single SRC/double SRC/complex16 SRC/cuda SRC/hip EXAMPLE/ FORTRAN/ TEST/

# This tag can be used to specify the character encoding of the source files
# that doxygen parses. Internally doxygen uses the UTF-8 encoding. Doxygen uses
# libiconv (or the iconv built into libc) for the transcoding. See the libiconv
# documentation (see: https://www.gnu.org/software/libiconv/) for the list of
# possible encodings.
# The default value is: UTF-8.

INPUT_ENCODING         = UTF-8

# If the value of the INPUT tag contains directories, you can use the
# FILE_PATTERNS tag to specify one or more wildcard patterns (like *.cpp and
# *.h) to filter out the source-files in the directories.
#
# Note that for custom extensions or not directly supported extensions you also
# need to set EXTENSION_MAPPING for the extension otherwise the files are not
# read by doxygen.
#
# If left blank the following patterns are tested:*.c, *.cc, *.cxx, *.cpp,
# *.c++, *.java, *.ii, *.ixx, *.ipp, *.i++, *.inl, *.idl, *.ddl, *.odl, *.h,
# *.hh, *.hxx, *.hpp, *.h++, *.cs, *.d, *.php, *.php4, *.php5, *.phtml, *.inc,
# *.m, *.markdown, *.md, *.mm, *.dox, *.py, *.pyw, *.f90, *.f95, *.f03, *.f08,
# *.f, *.for, *.tcl, *.vhd, *.vhdl, *.ucf, *.qsf and *.ice.

FILE_PATTERNS          = *.c \
                         *.cc \
                         *.cxx \
                         *.cpp \
                         *.c++ \
                         *.java \
                         *.ii \
                         *.ixx \
                         *.ipp \
                         *.i++ \
                         *.inl \
                         *.idl \
                         *.ddl \
                         *.odl \
                         *.h \
                         *.hh \
                         *.hxx \
                         *.hpp \
                         *.h++ \
                         *.cs \
                         *.d \
                         *.php \
                         *.php4 \
                         *.php5 \
                         *.phtml \
                         *.inc \
                         *.m \
                         *.markdown \
                         *.md \
                         *.mm \
                         *.dox \
                         *.py \
                         *.pyw \
                         *.f90 \
                         *.f95 \
                         *.f03 \
                         *.f08 \
                         *.f \
                         *.for \
                         *.tcl \
                         *.vhd \
                         *.vhdl \
                         *.ucf \
                         *.qsf \
                         *.ice \
			 *.cu \
			 *.cuh 

# The RECURSIVE tag can be used to specify whether or not subdirectories should
# be searched for input files as well.
# The default value is: NO.

RECURSIVE              = YES

# The EXCLUDE tag can be used to specify files and/or directories that should be
# excluded from the INPUT source files. This way you can easily exclude a
# subdirectory from a directory tree whose root is specified with the INPUT tag.
#
# Note that relative paths are relative to the directory from which doxygen is
# run.

EXCLUDE                =

# The EXCLUDE_SYMLINKS tag can be used to select whether or not files or
# directories that are symbolic links (a Unix file system feature) are excluded
# from the input.
# The default value is: NO.

EXCLUDE_SYMLINKS       = NO

# If the value of the INPUT tag contains directories, you can use the
# EXCLUDE_PATTERNS tag to specify one or more wildcard patterns to exclude
# certain files from those directories.
#
# Note that the wildcards are matched against the file with absolute path, so to
# exclude all test directories for example use the pattern */test/*

EXCLUDE_PATTERNS       =

# The EXCLUDE_SYMBOLS tag can be used to specify one or more symbol names
# (namespaces, classes, functions, etc.) that should be excluded from the
# output. The symbol name can be a fully qualified name, a word, or if the
# wildcard * is used, a substring. Examples: ANamespace, AClass,
# AClass::ANamespace, ANamespace::*Test
#
# Note that the wildcards are matched against the file with absolute path, so to
# exclude all test directories use the pattern */test/*

EXCLUDE_SYMBOLS        =

# The EXAMPLE_PATH tag can be used to specify one or more files or directories
# that contain example code fragments that are included (see the \include
# command).

EXAMPLE_PATH           =

# If the value of the EXAMPLE_PATH tag contains directories, you can use the
# EXAMPLE_PATTERNS tag to specify one or more wildcard pattern (like *.cpp and
# *.h) to filter out the source-files in the directories. If left blank all
# files are included.

EXAMPLE_PATTERNS       = *

# If the EXAMPLE_RECURSIVE tag is set to YES then subdirectories will be
# searched for input files to be used with the \include or \dontinclude commands
# irrespective of the value of the RECURSIVE tag.
# The default value is: NO.

EXAMPLE_RECURSIVE      = NO

# The IMAGE_PATH tag can be used to specify one or more files or directories
# that contain images that are to be included in the documentation (see the
# \image command).

IMAGE_PATH             = doc/images

# The INPUT_FILTER tag can be used to specify a program that doxygen should
# invoke to filter for each input file. Doxygen will invoke the filter program
# by executing (via popen()) the command:
#
# <filter> <input-file>
#
# where <filter> is the value of the INPUT_FILTER tag, and <input-file> is the
# name of an input file. Doxygen will then use the output that the filter
# program writes to standard output. If FILTER_PATTERNS is specified, this tag
# will be ignored.
#
# Note that the filter must not add or remove lines; it is applied before the
# code is scanned, but not when the output code is generated. If lines are added
# or removed, the anchors will not be placed correctly.
#
# Note that for custom extensions or not directly supported extensions you also
# need to set EXTENSION_MAPPING for the extension otherwise the files are not
# properly processed by doxygen.

INPUT_FILTER           =

# The FILTER_PATTERNS tag can be used to specify filters on a per file pattern
# basis. Doxygen will compare the file name with each pattern and apply the
# filter if there is a match. The filters are a list of the form: pattern=filter
# (like *.cpp=my_cpp_filter). See INPUT_FILTER for further information on how
# filters are used. If the FILTER_PATTERNS tag is empty or if none of the
# patterns match the file name, INPUT_FILTER is applied.
#
# Note that for custom extensions or not directly supported extensions you also
# need to set EXTENSION_MAPPING for the extension otherwise the files are not
# properly processed by doxygen.

FILTER_PATTERNS        =

# If the FILTER_SOURCE_FILES tag is set to YES, the input filter (if set using
# INPUT_FILTER) will also be used to filter the input files that are used for
# producing the source files to browse (i.e. when SOURCE_BROWSER is set to YES).
# The default value is: NO.

FILTER_SOURCE_FILES    = NO

# The FILTER_SOURCE_PATTERNS tag can be used to specify source filters per file
# pattern. A pattern will override the setting for FILTER_PATTERN (if any) and
# it is also possible to disable source filtering for a specific pattern using
# *.ext= (so without naming a filter).
# This tag requires that the tag FILTER_SOURCE_FILES is set to YES.

FILTER_SOURCE_PATTERNS =

# If the USE_MDFILE_AS_MAINPAGE tag refers to the name of a markdown file that
# is part of the input, its contents will be placed on the main page
# (index.html). This can be useful if you have a project on for instance GitHub
# and want to reuse the introduction page also for the doxygen output.

USE_MDFILE_AS_MAINPAGE =

#---------------------------------------------------------------------------
# Configuration options related to source browsing
#---------------------------------------------------------------------------

# If the SOURCE_BROWSER tag is set to YES then a list of source files will be
# generated. Documented entities will be cross-referenced with these sources.
#
# Note: To get rid of all source code in the generated output, make sure that
# also VERBATIM_HEADERS is set to NO.
# The default value is: NO.

SOURCE_BROWSER         = NO

# Setting the INLINE_SOURCES tag to YES will include the body of functions,
# classes and enums directly into the documentation.
# The default value is: NO.

INLINE_SOURCES         = NO

# Setting the STRIP_CODE_COMMENTS tag to YES will instruct doxygen to hide any
# special comment blocks from generated source code fragments. Normal C, C++ and
# Fortran comments will always remain visible.
# The default value is: YES.

STRIP_CODE_COMMENTS    = YES

# If the REFERENCED_BY_RELATION tag is set to YES then for each documented
# entity all documented functions referencing it will be listed.
# The default value is: NO.

REFERENCED_BY_RELATION = NO

# If the REFERENCES_RELATION tag is set to YES then for each documented function
# all documented entities called/used by that function will be listed.
# The default value is: NO.

REFERENCES_RELATION    = NO

# If the REFERENCES_LINK_SOURCE tag is set to YES and SOURCE_BROWSER tag is set
# to YES then the hyperlinks from functions in REFERENCES_RELATION and
# REFERENCED_BY_RELATION lists will link to the source code. Otherwise they will
# link to the documentation.
# The default value is: YES.

REFERENCES_LINK_SOURCE = YES

# If SOURCE_TOOLTIPS is enabled (the default) then hovering a hyperlink in the
# source code will show a tooltip with additional information such as prototype,
# brief description and links to the definition and documentation. Since this
# will make the HTML file larger and loading of large files a bit slower, you
# can opt to disable this feature.
# The default value is: YES.
# This tag requires that the tag SOURCE_BROWSER is set to YES.

SOURCE_TOOLTIPS        = YES

# If the USE_HTAGS tag is set to YES then the references to source code will
# point to the HTML generated by the htags(1) tool instead of doxygen built-in
# source browser. The htags tool is part of GNU's global source tagging system
# (see https://www.gnu.org/software/global/global.html). You will need version
# 4.8.6 or higher.
#
# To use it do the following:
# - Install the latest version of global
# - Enable SOURCE_BROWSER and USE_HTAGS in the configuration file
# - Make sure the INPUT points to the root of the source tree
# - Run doxygen as normal
#
# Doxygen will invoke htags (and that will in turn invoke gtags), so these
# tools must be available from the command line (i.e. in the search path).
#
# The result: instead of the source browser generated by doxygen, the links to
# source code will now point to the output of htags.
# The default value is: NO.
# This tag requires that the tag SOURCE_BROWSER is set to YES.

USE_HTAGS              = NO

# If the VERBATIM_HEADERS tag is set the YES then doxygen will generate a
# verbatim copy of the header file for each class for which an include is
# specified. Set to NO to disable this.
# See also: Section \class.
# The default value is: YES.

VERBATIM_HEADERS       = YES

#---------------------------------------------------------------------------
# Configuration options related to the alphabetical class index
#---------------------------------------------------------------------------

# If the ALPHABETICAL_INDEX tag is set to YES, an alphabetical index of all
# compounds will be generated. Enable this if the project contains a lot of
# classes, structs, unions or interfaces.
# The default value is: YES.

ALPHABETICAL_INDEX     = YES

# The COLS_IN_ALPHA_INDEX tag can be used to specify the number of columns in
# which the alphabetical index list will be split.
# Minimum value: 1, maximum value: 20, default value: 5.
# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.

COLS_IN_ALPHA_INDEX    = 5

# In case all classes in a project start with a common prefix, all classes will
# be put under the same header in the alphabetical index. The IGNORE_PREFIX tag
# can be used to specify a prefix (or a list of prefixes) that should be ignored
# while generating the index headers.
# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.

IGNORE_PREFIX          =

#---------------------------------------------------------------------------
# Configuration options related to the HTML output
#---------------------------------------------------------------------------

# If the GENERATE_HTML tag is set to YES, doxygen will generate HTML output
# The default value is: YES.

GENERATE_HTML          = YES

# The HTML_OUTPUT tag is used to specify where the HTML docs will be put. If a
# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of
# it.
# The default directory is: html.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_OUTPUT            = html

# The HTML_FILE_EXTENSION tag can be used to specify the file extension for each
# generated HTML page (for example: .htm, .php, .asp).
# The default value is: .html.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_FILE_EXTENSION    = .html

# The HTML_HEADER tag can be used to specify a user-defined HTML header file for
# each generated HTML page. If the tag is left blank doxygen will generate a
# standard header.
#
# To get valid HTML the header file that includes any scripts and style sheets
# that doxygen needs, which is dependent on the configuration options used (e.g.
# the setting GENERATE_TREEVIEW). It is highly recommended to start with a
# default header using
# doxygen -w html new_header.html new_footer.html new_stylesheet.css
# YourConfigFile
# and then modify the file new_header.html. See also section "Doxygen usage"
# for information on how to generate the default header that doxygen normally
# uses.
# Note: The header is subject to change so you typically have to regenerate the
# default header when upgrading to a newer version of doxygen. For a description
# of the possible markers and block names see the documentation.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_HEADER            =

# The HTML_FOOTER tag can be used to specify a user-defined HTML footer for each
# generated HTML page. If the tag is left blank doxygen will generate a standard
# footer. See HTML_HEADER for more information on how to generate a default
# footer and what special commands can be used inside the footer. See also
# section "Doxygen usage" for information on how to generate the default footer
# that doxygen normally uses.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_FOOTER            =

# The HTML_STYLESHEET tag can be used to specify a user-defined cascading style
# sheet that is used by each HTML page. It can be used to fine-tune the look of
# the HTML output. If left blank doxygen will generate a default style sheet.
# See also section "Doxygen usage" for information on how to generate the style
# sheet that doxygen normally uses.
# Note: It is recommended to use HTML_EXTRA_STYLESHEET instead of this tag, as
# it is more robust and this tag (HTML_STYLESHEET) will in the future become
# obsolete.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_STYLESHEET        =

# The HTML_EXTRA_STYLESHEET tag can be used to specify additional user-defined
# cascading style sheets that are included after the standard style sheets
# created by doxygen. Using this option one can overrule certain style aspects.
# This is preferred over using HTML_STYLESHEET since it does not replace the
# standard style sheet and is therefore more robust against future updates.
# Doxygen will copy the style sheet files to the output directory.
# Note: The order of the extra style sheet files is of importance (e.g. the last
# style sheet in the list overrules the setting of the previous ones in the
# list). For an example see the documentation.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_EXTRA_STYLESHEET  =

# The HTML_EXTRA_FILES tag can be used to specify one or more extra images or
# other source files which should be copied to the HTML output directory. Note
# that these files will be copied to the base HTML output directory. Use the
# $relpath^ marker in the HTML_HEADER and/or HTML_FOOTER files to load these
# files. In the HTML_STYLESHEET file, use the file name only. Also note that the
# files will be copied as-is; there are no commands or markers available.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_EXTRA_FILES       =

# The HTML_COLORSTYLE_HUE tag controls the color of the HTML output. Doxygen
# will adjust the colors in the style sheet and background images according to
# this color. Hue is specified as an angle on a colorwheel, see
# https://en.wikipedia.org/wiki/Hue for more information. For instance the value
# 0 represents red, 60 is yellow, 120 is green, 180 is cyan, 240 is blue, 300
# purple, and 360 is red again.
# Minimum value: 0, maximum value: 359, default value: 220.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_COLORSTYLE_HUE    = 220

# The HTML_COLORSTYLE_SAT tag controls the purity (or saturation) of the colors
# in the HTML output. For a value of 0 the output will use grayscales only. A
# value of 255 will produce the most vivid colors.
# Minimum value: 0, maximum value: 255, default value: 100.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_COLORSTYLE_SAT    = 100

# The HTML_COLORSTYLE_GAMMA tag controls the gamma correction applied to the
# luminance component of the colors in the HTML output. Values below 100
# gradually make the output lighter, whereas values above 100 make the output
# darker. The value divided by 100 is the actual gamma applied, so 80 represents
# a gamma of 0.8, The value 220 represents a gamma of 2.2, and 100 does not
# change the gamma.
# Minimum value: 40, maximum value: 240, default value: 80.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_COLORSTYLE_GAMMA  = 80

# If the HTML_TIMESTAMP tag is set to YES then the footer of each generated HTML
# page will contain the date and time when the page was generated. Setting this
# to YES can help to show when doxygen was last run and thus if the
# documentation is up to date.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_TIMESTAMP         = NO

# If the HTML_DYNAMIC_MENUS tag is set to YES then the generated HTML
# documentation will contain a main index with vertical navigation menus that
# are dynamically created via Javascript. If disabled, the navigation index will
# consists of multiple levels of tabs that are statically embedded in every HTML
# page. Disable this option to support browsers that do not have Javascript,
# like the Qt help browser.
# The default value is: YES.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_DYNAMIC_MENUS     = YES

# If the HTML_DYNAMIC_SECTIONS tag is set to YES then the generated HTML
# documentation will contain sections that can be hidden and shown after the
# page has loaded.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_DYNAMIC_SECTIONS  = NO

# With HTML_INDEX_NUM_ENTRIES one can control the preferred number of entries
# shown in the various tree structured indices initially; the user can expand
# and collapse entries dynamically later on. Doxygen will expand the tree to
# such a level that at most the specified number of entries are visible (unless
# a fully collapsed tree already exceeds this amount). So setting the number of
# entries 1 will produce a full collapsed tree by default. 0 is a special value
# representing an infinite number of entries and will result in a full expanded
# tree by default.
# Minimum value: 0, maximum value: 9999, default value: 100.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_INDEX_NUM_ENTRIES = 100

# If the GENERATE_DOCSET tag is set to YES, additional index files will be
# generated that can be used as input for Apple's Xcode 3 integrated development
# environment (see: https://developer.apple.com/xcode/), introduced with OSX
# 10.5 (Leopard). To create a documentation set, doxygen will generate a
# Makefile in the HTML output directory. Running make will produce the docset in
# that directory and running make install will install the docset in
# ~/Library/Developer/Shared/Documentation/DocSets so that Xcode will find it at
# startup. See https://developer.apple.com/library/archive/featuredarticles/Doxy
# genXcode/_index.html for more information.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

GENERATE_DOCSET        = NO

# This tag determines the name of the docset feed. A documentation feed provides
# an umbrella under which multiple documentation sets from a single provider
# (such as a company or product suite) can be grouped.
# The default value is: Doxygen generated docs.
# This tag requires that the tag GENERATE_DOCSET is set to YES.

DOCSET_FEEDNAME        = "Doxygen generated docs"

# This tag specifies a string that should uniquely identify the documentation
# set bundle. This should be a reverse domain-name style string, e.g.
# com.mycompany.MyDocSet. Doxygen will append .docset to the name.
# The default value is: org.doxygen.Project.
# This tag requires that the tag GENERATE_DOCSET is set to YES.

DOCSET_BUNDLE_ID       = org.doxygen.Project

# The DOCSET_PUBLISHER_ID tag specifies a string that should uniquely identify
# the documentation publisher. This should be a reverse domain-name style
# string, e.g. com.mycompany.MyDocSet.documentation.
# The default value is: org.doxygen.Publisher.
# This tag requires that the tag GENERATE_DOCSET is set to YES.

DOCSET_PUBLISHER_ID    = org.doxygen.Publisher

# The DOCSET_PUBLISHER_NAME tag identifies the documentation publisher.
# The default value is: Publisher.
# This tag requires that the tag GENERATE_DOCSET is set to YES.

DOCSET_PUBLISHER_NAME  = Publisher

# If the GENERATE_HTMLHELP tag is set to YES then doxygen generates three
# additional HTML index files: index.hhp, index.hhc, and index.hhk. The
# index.hhp is a project file that can be read by Microsoft's HTML Help Workshop
# (see: https://www.microsoft.com/en-us/download/details.aspx?id=21138) on
# Windows.
#
# The HTML Help Workshop contains a compiler that can convert all HTML output
# generated by doxygen into a single compiled HTML file (.chm). Compiled HTML
# files are now used as the Windows 98 help format, and will replace the old
# Windows help format (.hlp) on all Windows platforms in the future. Compressed
# HTML files also contain an index, a table of contents, and you can search for
# words in the documentation. The HTML workshop also contains a viewer for
# compressed HTML files.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

GENERATE_HTMLHELP      = NO

# The CHM_FILE tag can be used to specify the file name of the resulting .chm
# file. You can add a path in front of the file if the result should not be
# written to the html output directory.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

CHM_FILE               =

# The HHC_LOCATION tag can be used to specify the location (absolute path
# including file name) of the HTML help compiler (hhc.exe). If non-empty,
# doxygen will try to run the HTML help compiler on the generated index.hhp.
# The file has to be specified with full path.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

HHC_LOCATION           =

# The GENERATE_CHI flag controls if a separate .chi index file is generated
# (YES) or that it should be included in the master .chm file (NO).
# The default value is: NO.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

GENERATE_CHI           = NO

# The CHM_INDEX_ENCODING is used to encode HtmlHelp index (hhk), content (hhc)
# and project file content.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

CHM_INDEX_ENCODING     =

# The BINARY_TOC flag controls whether a binary table of contents is generated
# (YES) or a normal table of contents (NO) in the .chm file. Furthermore it
# enables the Previous and Next buttons.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

BINARY_TOC             = NO

# The TOC_EXPAND flag can be set to YES to add extra items for group members to
# the table of contents of the HTML help documentation and to the tree view.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

TOC_EXPAND             = NO

# If the GENERATE_QHP tag is set to YES and both QHP_NAMESPACE and
# QHP_VIRTUAL_FOLDER are set, an additional index file will be generated that
# can be used as input for Qt's qhelpgenerator to generate a Qt Compressed Help
# (.qch) of the generated HTML documentation.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

GENERATE_QHP           = NO

# If the QHG_LOCATION tag is specified, the QCH_FILE tag can be used to specify
# the file name of the resulting .qch file. The path specified is relative to
# the HTML output folder.
# This tag requires that the tag GENERATE_QHP is set to YES.

QCH_FILE               =

# The QHP_NAMESPACE tag specifies the namespace to use when generating Qt Help
# Project output. For more information please see Qt Help Project / Namespace
# (see: http://doc.qt.io/archives/qt-4.8/qthelpproject.html#namespace).
# The default value is: org.doxygen.Project.
# This tag requires that the tag GENERATE_QHP is set to YES.

QHP_NAMESPACE          = org.doxygen.Project

# The QHP_VIRTUAL_FOLDER tag specifies the namespace to use when generating Qt
# Help Project output. For more information please see Qt Help Project / Virtual
# Folders (see: http://doc.qt.io/archives/qt-4.8/qthelpproject.html#virtual-
# folders).
# The default value is: doc.
# This tag requires that the tag GENERATE_QHP is set to YES.

QHP_VIRTUAL_FOLDER     = doc

# If the QHP_CUST_FILTER_NAME tag is set, it specifies the name of a custom
# filter to add. For more information please see Qt Help Project / Custom
# Filters (see: http://doc.qt.io/archives/qt-4.8/qthelpproject.html#custom-
# filters).
# This tag requires that the tag GENERATE_QHP is set to YES.

QHP_CUST_FILTER_NAME   =

# The QHP_CUST_FILTER_ATTRS tag specifies the list of the attributes of the
# custom filter to add. For more information please see Qt Help Project / Custom
# Filters (see: http://doc.qt.io/archives/qt-4.8/qthelpproject.html#custom-
# filters).
# This tag requires that the tag GENERATE_QHP is set to YES.

QHP_CUST_FILTER_ATTRS  =

# The QHP_SECT_FILTER_ATTRS tag specifies the list of the attributes this
# project's filter section matches. Qt Help Project / Filter Attributes (see:
# http://doc.qt.io/archives/qt-4.8/qthelpproject.html#filter-attributes).
# This tag requires that the tag GENERATE_QHP is set to YES.

QHP_SECT_FILTER_ATTRS  =

# The QHG_LOCATION tag can be used to specify the location of Qt's
# qhelpgenerator. If non-empty doxygen will try to run qhelpgenerator on the
# generated .qhp file.
# This tag requires that the tag GENERATE_QHP is set to YES.

QHG_LOCATION           =

# If the GENERATE_ECLIPSEHELP tag is set to YES, additional index files will be
# generated, together with the HTML files, they form an Eclipse help plugin. To
# install this plugin and make it available under the help contents menu in
# Eclipse, the contents of the directory containing the HTML and XML files needs
# to be copied into the plugins directory of eclipse. The name of the directory
# within the plugins directory should be the same as the ECLIPSE_DOC_ID value.
# After copying Eclipse needs to be restarted before the help appears.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

GENERATE_ECLIPSEHELP   = NO

# A unique identifier for the Eclipse help plugin. When installing the plugin
# the directory name containing the HTML and XML files should also have this
# name. Each documentation set should have its own identifier.
# The default value is: org.doxygen.Project.
# This tag requires that the tag GENERATE_ECLIPSEHELP is set to YES.

ECLIPSE_DOC_ID         = org.doxygen.Project

# If you want full control over the layout of the generated HTML pages it might
# be necessary to disable the index and replace it with your own. The
# DISABLE_INDEX tag can be used to turn on/off the condensed index (tabs) at top
# of each HTML page. A value of NO enables the index and the value YES disables
# it. Since the tabs in the index contain the same information as the navigation
# tree, you can set this option to YES if you also set GENERATE_TREEVIEW to YES.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

DISABLE_INDEX          = NO

# The GENERATE_TREEVIEW tag is used to specify whether a tree-like index
# structure should be generated to display hierarchical information. If the tag
# value is set to YES, a side panel will be generated containing a tree-like
# index structure (just like the one that is generated for HTML Help). For this
# to work a browser that supports JavaScript, DHTML, CSS and frames is required
# (i.e. any modern browser). Windows users are probably better off using the
# HTML help feature. Via custom style sheets (see HTML_EXTRA_STYLESHEET) one can
# further fine-tune the look of the index. As an example, the default style
# sheet generated by doxygen has an example that shows how to put an image at
# the root of the tree instead of the PROJECT_NAME. Since the tree basically has
# the same information as the tab index, you could consider setting
# DISABLE_INDEX to YES when enabling this option.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

GENERATE_TREEVIEW      = YES

# The ENUM_VALUES_PER_LINE tag can be used to set the number of enum values that
# doxygen will group on one line in the generated HTML documentation.
#
# Note that a value of 0 will completely suppress the enum values from appearing
# in the overview section.
# Minimum value: 0, maximum value: 20, default value: 4.
# This tag requires that the tag GENERATE_HTML is set to YES.

ENUM_VALUES_PER_LINE   = 4

# If the treeview is enabled (see GENERATE_TREEVIEW) then this tag can be used
# to set the initial width (in pixels) of the frame in which the tree is shown.
# Minimum value: 0, maximum value: 1500, default value: 250.
# This tag requires that the tag GENERATE_HTML is set to YES.

TREEVIEW_WIDTH         = 250

# If the EXT_LINKS_IN_WINDOW option is set to YES, doxygen will open links to
# external symbols imported via tag files in a separate window.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

EXT_LINKS_IN_WINDOW    = NO

# Use this tag to change the font size of LaTeX formulas included as images in
# the HTML documentation. When you change the font size after a successful
# doxygen run you need to manually remove any form_*.png images from the HTML
# output directory to force them to be regenerated.
# Minimum value: 8, maximum value: 50, default value: 10.
# This tag requires that the tag GENERATE_HTML is set to YES.

FORMULA_FONTSIZE       = 10

# Use the FORMULA_TRANSPARENT tag to determine whether or not the images
# generated for formulas are transparent PNGs. Transparent PNGs are not
# supported properly for IE 6.0, but are supported on all modern browsers.
#
# Note that when changing this option you need to delete any form_*.png files in
# the HTML output directory before the changes have effect.
# The default value is: YES.
# This tag requires that the tag GENERATE_HTML is set to YES.

FORMULA_TRANSPARENT    = YES

# Enable the USE_MATHJAX option to render LaTeX formulas using MathJax (see
# https://www.mathjax.org) which uses client side Javascript for the rendering
# instead of using pre-rendered bitmaps. Use this if you do not have LaTeX
# installed or if you want to formulas look prettier in the HTML output. When
# enabled you may also need to install MathJax separately and configure the path
# to it using the MATHJAX_RELPATH option.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

USE_MATHJAX            = NO

# When MathJax is enabled you can set the default output format to be used for
# the MathJax output. See the MathJax site (see:
# http://docs.mathjax.org/en/latest/output.html) for more details.
# Possible values are: HTML-CSS (which is slower, but has the best
# compatibility), NativeMML (i.e. MathML) and SVG.
# The default value is: HTML-CSS.
# This tag requires that the tag USE_MATHJAX is set to YES.

MATHJAX_FORMAT         = HTML-CSS

# When MathJax is enabled you need to specify the location relative to the HTML
# output directory using the MATHJAX_RELPATH option. The destination directory
# should contain the MathJax.js script. For instance, if the mathjax directory
# is located at the same level as the HTML output directory, then
# MATHJAX_RELPATH should be ../mathjax. The default value points to the MathJax
# Content Delivery Network so you can quickly see the result without installing
# MathJax. However, it is strongly recommended to install a local copy of
# MathJax from https://www.mathjax.org before deployment.
# The default value is: https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/.
# This tag requires that the tag USE_MATHJAX is set to YES.

MATHJAX_RELPATH        = https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/

# The MATHJAX_EXTENSIONS tag can be used to specify one or more MathJax
# extension names that should be enabled during MathJax rendering. For example
# MATHJAX_EXTENSIONS = TeX/AMSmath TeX/AMSsymbols
# This tag requires that the tag USE_MATHJAX is set to YES.

MATHJAX_EXTENSIONS     =

# The MATHJAX_CODEFILE tag can be used to specify a file with javascript pieces
# of code that will be used on startup of the MathJax code. See the MathJax site
# (see: http://docs.mathjax.org/en/latest/output.html) for more details. For an
# example see the documentation.
# This tag requires that the tag USE_MATHJAX is set to YES.

MATHJAX_CODEFILE       =

# When the SEARCHENGINE tag is enabled doxygen will generate a search box for
# the HTML output. The underlying search engine uses javascript and DHTML and
# should work on any modern browser. Note that when using HTML help
# (GENERATE_HTMLHELP), Qt help (GENERATE_QHP), or docsets (GENERATE_DOCSET)
# there is already a search function so this one should typically be disabled.
# For large projects the javascript based search engine can be slow, then
# enabling SERVER_BASED_SEARCH may provide a better solution. It is possible to
# search using the keyboard; to jump to the search box use <access key> + S
# (what the <access key> is depends on the OS and browser, but it is typically
# <CTRL>, <ALT>/<option>, or both). Inside the search box use the <cursor down
# key> to jump into the search results window, the results can be navigated
# using the <cursor keys>. Press <Enter> to select an item or <escape> to cancel
# the search. The filter options can be selected when the cursor is inside the
# search box by pressing <Shift>+<cursor down>. Also here use the <cursor keys>
# to select a filter and <Enter> or <escape> to activate or cancel the filter
# option.
# The default value is: YES.
# This tag requires that the tag GENERATE_HTML is set to YES.

SEARCHENGINE           = YES

# When the SERVER_BASED_SEARCH tag is enabled the search engine will be
# implemented using a web server instead of a web client using Javascript. There
# are two flavors of web server based searching depending on the EXTERNAL_SEARCH
# setting. When disabled, doxygen will generate a PHP script for searching and
# an index file used by the script. When EXTERNAL_SEARCH is enabled the indexing
# and searching needs to be provided by external tools. See the section
# "External Indexing and Searching" for details.
# The default value is: NO.
# This tag requires that the tag SEARCHENGINE is set to YES.

SERVER_BASED_SEARCH    = NO

# When EXTERNAL_SEARCH tag is enabled doxygen will no longer generate the PHP
# script for searching. Instead the search results are written to an XML file
# which needs to be processed by an external indexer. Doxygen will invoke an
# external search engine pointed to by the SEARCHENGINE_URL option to obtain the
# search results.
#
# Doxygen ships with an example indexer (doxyindexer) and search engine
# (doxysearch.cgi) which are based on the open source search engine library
# Xapian (see: https://xapian.org/).
#
# See the section "External Indexing and Searching" for details.
# The default value is: NO.
# This tag requires that the tag SEARCHENGINE is set to YES.

EXTERNAL_SEARCH        = NO

# The SEARCHENGINE_URL should point to a search engine hosted by a web server
# which will return the search results when EXTERNAL_SEARCH is enabled.
#
# Doxygen ships with an example indexer (doxyindexer) and search engine
# (doxysearch.cgi) which are based on the open source search engine library
# Xapian (see: https://xapian.org/). See the section "External Indexing and
# Searching" for details.
# This tag requires that the tag SEARCHENGINE is set to YES.

SEARCHENGINE_URL       =

# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the unindexed
# search data is written to a file for indexing by an external tool. With the
# SEARCHDATA_FILE tag the name of this file can be specified.
# The default file is: searchdata.xml.
# This tag requires that the tag SEARCHENGINE is set to YES.

SEARCHDATA_FILE        = searchdata.xml

# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the
# EXTERNAL_SEARCH_ID tag can be used as an identifier for the project. This is
# useful in combination with EXTRA_SEARCH_MAPPINGS to search through multiple
# projects and redirect the results back to the right project.
# This tag requires that the tag SEARCHENGINE is set to YES.

EXTERNAL_SEARCH_ID     =

# The EXTRA_SEARCH_MAPPINGS tag can be used to enable searching through doxygen
# projects other than the one defined by this configuration file, but that are
# all added to the same external search index. Each project needs to have a
# unique id set via EXTERNAL_SEARCH_ID. The search mapping then maps the id of
# to a relative location where the documentation can be found. The format is:
# EXTRA_SEARCH_MAPPINGS = tagname1=loc1 tagname2=loc2 ...
# This tag requires that the tag SEARCHENGINE is set to YES.

EXTRA_SEARCH_MAPPINGS  =

#---------------------------------------------------------------------------
# Configuration options related to the LaTeX output
#---------------------------------------------------------------------------

# If the GENERATE_LATEX tag is set to YES, doxygen will generate LaTeX output.
# The default value is: YES.

GENERATE_LATEX         = NO

# The LATEX_OUTPUT tag is used to specify where the LaTeX docs will be put. If a
# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of
# it.
# The default directory is: latex.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_OUTPUT           = latex

# The LATEX_CMD_NAME tag can be used to specify the LaTeX command name to be
# invoked.
#
# Note that when not enabling USE_PDFLATEX the default is latex when enabling
# USE_PDFLATEX the default is pdflatex and when in the later case latex is
# chosen this is overwritten by pdflatex. For specific output languages the
# default can have been set differently, this depends on the implementation of
# the output language.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_CMD_NAME         =

# The MAKEINDEX_CMD_NAME tag can be used to specify the command name to generate
# index for LaTeX.
# Note: This tag is used in the Makefile / make.bat.
# See also: LATEX_MAKEINDEX_CMD for the part in the generated output file
# (.tex).
# The default file is: makeindex.
# This tag requires that the tag GENERATE_LATEX is set to YES.

MAKEINDEX_CMD_NAME     = makeindex

# The LATEX_MAKEINDEX_CMD tag can be used to specify the command name to
# generate index for LaTeX. In case there is no backslash (\) as first character
# it will be automatically added in the LaTeX code.
# Note: This tag is used in the generated output file (.tex).
# See also: MAKEINDEX_CMD_NAME for the part in the Makefile / make.bat.
# The default value is: makeindex.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_MAKEINDEX_CMD    = makeindex

# If the COMPACT_LATEX tag is set to YES, doxygen generates more compact LaTeX
# documents. This may be useful for small projects and may help to save some
# trees in general.
# The default value is: NO.
# This tag requires that the tag GENERATE_LATEX is set to YES.

COMPACT_LATEX          = NO

# The PAPER_TYPE tag can be used to set the paper type that is used by the
# printer.
# Possible values are: a4 (210 x 297 mm), letter (8.5 x 11 inches), legal (8.5 x
# 14 inches) and executive (7.25 x 10.5 inches).
# The default value is: a4.
# This tag requires that the tag GENERATE_LATEX is set to YES.

PAPER_TYPE             = a4

# The EXTRA_PACKAGES tag can be used to specify one or more LaTeX package names
# that should be included in the LaTeX output. The package can be specified just
# by its name or with the correct syntax as to be used with the LaTeX
# \usepackage command. To get the times font for instance you can specify :
# EXTRA_PACKAGES=times or EXTRA_PACKAGES={times}
# To use the option intlimits with the amsmath package you can specify:
# EXTRA_PACKAGES=[intlimits]{amsmath}
# If left blank no extra packages will be included.
# This tag requires that the tag GENERATE_LATEX is set to YES.

EXTRA_PACKAGES         =

# The LATEX_HEADER tag can be used to specify a personal LaTeX header for the
# generated LaTeX document. The header should contain everything until the first
# chapter. If it is left blank doxygen will generate a standard header. See
# section "Doxygen usage" for information on how to let doxygen write the
# default header to a separate file.
#
# Note: Only use a user-defined header if you know what you are doing! The
# following commands have a special meaning inside the header: $title,
# $datetime, $date, $doxygenversion, $projectname, $projectnumber,
# $projectbrief, $projectlogo. Doxygen will replace $title with the empty
# string, for the replacement values of the other commands the user is referred
# to HTML_HEADER.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_HEADER           =

# The LATEX_FOOTER tag can be used to specify a personal LaTeX footer for the
# generated LaTeX document. The footer should contain everything after the last
# chapter. If it is left blank doxygen will generate a standard footer. See
# LATEX_HEADER for more information on how to generate a default footer and what
# special commands can be used inside the footer.
#
# Note: Only use a user-defined footer if you know what you are doing!
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_FOOTER           =

# The LATEX_EXTRA_STYLESHEET tag can be used to specify additional user-defined
# LaTeX style sheets that are included after the standard style sheets created
# by doxygen. Using this option one can overrule certain style aspects. Doxygen
# will copy the style sheet files to the output directory.
# Note: The order of the extra style sheet files is of importance (e.g. the last
# style sheet in the list overrules the setting of the previous ones in the
# list).
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_EXTRA_STYLESHEET =

# The LATEX_EXTRA_FILES tag can be used to specify one or more extra images or
# other source files which should be copied to the LATEX_OUTPUT output
# directory. Note that the files will be copied as-is; there are no commands or
# markers available.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_EXTRA_FILES      =

# If the PDF_HYPERLINKS tag is set to YES, the LaTeX that is generated is
# prepared for conversion to PDF (using ps2pdf or pdflatex). The PDF file will
# contain links (just like the HTML output) instead of page references. This
# makes the output suitable for online browsing using a PDF viewer.
# The default value is: YES.
# This tag requires that the tag GENERATE_LATEX is set to YES.

PDF_HYPERLINKS         = YES

# If the USE_PDFLATEX tag is set to YES, doxygen will use pdflatex to generate
# the PDF file directly from the LaTeX files. Set this option to YES, to get a
# higher quality PDF documentation.
# The default value is: YES.
# This tag requires that the tag GENERATE_LATEX is set to YES.

USE_PDFLATEX           = YES

# If the LATEX_BATCHMODE tag is set to YES, doxygen will add the \batchmode
# command to the generated LaTeX files. This will instruct LaTeX to keep running
# if errors occur, instead of asking the user for help. This option is also used
# when generating formulas in HTML.
# The default value is: NO.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_BATCHMODE        = NO

# If the LATEX_HIDE_INDICES tag is set to YES then doxygen will not include the
# index chapters (such as File Index, Compound Index, etc.) in the output.
# The default value is: NO.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_HIDE_INDICES     = NO

# If the LATEX_SOURCE_CODE tag is set to YES then doxygen will include source
# code with syntax highlighting in the LaTeX output.
#
# Note that which sources are shown also depends on other settings such as
# SOURCE_BROWSER.
# The default value is: NO.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_SOURCE_CODE      = NO

# The LATEX_BIB_STYLE tag can be used to specify the style to use for the
# bibliography, e.g. plainnat, or ieeetr. See
# https://en.wikipedia.org/wiki/BibTeX and \cite for more info.
# The default value is: plain.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_BIB_STYLE        = plain

# If the LATEX_TIMESTAMP tag is set to YES then the footer of each generated
# page will contain the date and time when the page was generated. Setting this
# to NO can help when comparing the output of multiple runs.
# The default value is: NO.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_TIMESTAMP        = NO

# The LATEX_EMOJI_DIRECTORY tag is used to specify the (relative or absolute)
# path from which the emoji images will be read. If a relative path is entered,
# it will be relative to the LATEX_OUTPUT directory. If left blank the
# LATEX_OUTPUT directory will be used.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_EMOJI_DIRECTORY  =

#---------------------------------------------------------------------------
# Configuration options related to the RTF output
#---------------------------------------------------------------------------

# If the GENERATE_RTF tag is set to YES, doxygen will generate RTF output. The
# RTF output is optimized for Word 97 and may not look too pretty with other RTF
# readers/editors.
# The default value is: NO.

GENERATE_RTF           = NO

# The RTF_OUTPUT tag is used to specify where the RTF docs will be put. If a
# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of
# it.
# The default directory is: rtf.
# This tag requires that the tag GENERATE_RTF is set to YES.

RTF_OUTPUT             = rtf

# If the COMPACT_RTF tag is set to YES, doxygen generates more compact RTF
# documents. This may be useful for small projects and may help to save some
# trees in general.
# The default value is: NO.
# This tag requires that the tag GENERATE_RTF is set to YES.

COMPACT_RTF            = NO

# If the RTF_HYPERLINKS tag is set to YES, the RTF that is generated will
# contain hyperlink fields. The RTF file will contain links (just like the HTML
# output) instead of page references. This makes the output suitable for online
# browsing using Word or some other Word compatible readers that support those
# fields.
#
# Note: WordPad (write) and others do not support links.
# The default value is: NO.
# This tag requires that the tag GENERATE_RTF is set to YES.

RTF_HYPERLINKS         = NO

# Load stylesheet definitions from file. Syntax is similar to doxygen's
# configuration file, i.e. a series of assignments. You only have to provide
# replacements, missing definitions are set to their default value.
#
# See also section "Doxygen usage" for information on how to generate the
# default style sheet that doxygen normally uses.
# This tag requires that the tag GENERATE_RTF is set to YES.

RTF_STYLESHEET_FILE    =

# Set optional variables used in the generation of an RTF document. Syntax is
# similar to doxygen's configuration file. A template extensions file can be
# generated using doxygen -e rtf extensionFile.
# This tag requires that the tag GENERATE_RTF is set to YES.

RTF_EXTENSIONS_FILE    =

# If the RTF_SOURCE_CODE tag is set to YES then doxygen will include source code
# with syntax highlighting in the RTF output.
#
# Note that which sources are shown also depends on other settings such as
# SOURCE_BROWSER.
# The default value is: NO.
# This tag requires that the tag GENERATE_RTF is set to YES.

RTF_SOURCE_CODE        = NO

#---------------------------------------------------------------------------
# Configuration options related to the man page output
#---------------------------------------------------------------------------

# If the GENERATE_MAN tag is set to YES, doxygen will generate man pages for
# classes and files.
# The default value is: NO.

GENERATE_MAN           = NO

# The MAN_OUTPUT tag is used to specify where the man pages will be put. If a
# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of
# it. A directory man3 will be created inside the directory specified by
# MAN_OUTPUT.
# The default directory is: man.
# This tag requires that the tag GENERATE_MAN is set to YES.

MAN_OUTPUT             = man

# The MAN_EXTENSION tag determines the extension that is added to the generated
# man pages. In case the manual section does not start with a number, the number
# 3 is prepended. The dot (.) at the beginning of the MAN_EXTENSION tag is
# optional.
# The default value is: .3.
# This tag requires that the tag GENERATE_MAN is set to YES.

MAN_EXTENSION          = .3

# The MAN_SUBDIR tag determines the name of the directory created within
# MAN_OUTPUT in which the man pages are placed. If defaults to man followed by
# MAN_EXTENSION with the initial . removed.
# This tag requires that the tag GENERATE_MAN is set to YES.

MAN_SUBDIR             =

# If the MAN_LINKS tag is set to YES and doxygen generates man output, then it
# will generate one additional man file for each entity documented in the real
# man page(s). These additional files only source the real man page, but without
# them the man command would be unable to find the correct page.
# The default value is: NO.
# This tag requires that the tag GENERATE_MAN is set to YES.

MAN_LINKS              = NO

#---------------------------------------------------------------------------
# Configuration options related to the XML output
#---------------------------------------------------------------------------

# If the GENERATE_XML tag is set to YES, doxygen will generate an XML file that
# captures the structure of the code including all documentation.
# The default value is: NO.

GENERATE_XML           = NO

# The XML_OUTPUT tag is used to specify where the XML pages will be put. If a
# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of
# it.
# The default directory is: xml.
# This tag requires that the tag GENERATE_XML is set to YES.

XML_OUTPUT             = xml

# If the XML_PROGRAMLISTING tag is set to YES, doxygen will dump the program
# listings (including syntax highlighting and cross-referencing information) to
# the XML output. Note that enabling this will significantly increase the size
# of the XML output.
# The default value is: YES.
# This tag requires that the tag GENERATE_XML is set to YES.

XML_PROGRAMLISTING     = YES

# If the XML_NS_MEMB_FILE_SCOPE tag is set to YES, doxygen will include
# namespace members in file scope as well, matching the HTML output.
# The default value is: NO.
# This tag requires that the tag GENERATE_XML is set to YES.

XML_NS_MEMB_FILE_SCOPE = NO

#---------------------------------------------------------------------------
# Configuration options related to the DOCBOOK output
#---------------------------------------------------------------------------

# If the GENERATE_DOCBOOK tag is set to YES, doxygen will generate Docbook files
# that can be used to generate PDF.
# The default value is: NO.

GENERATE_DOCBOOK       = NO

# The DOCBOOK_OUTPUT tag is used to specify where the Docbook pages will be put.
# If a relative path is entered the value of OUTPUT_DIRECTORY will be put in
# front of it.
# The default directory is: docbook.
# This tag requires that the tag GENERATE_DOCBOOK is set to YES.

DOCBOOK_OUTPUT         = docbook

# If the DOCBOOK_PROGRAMLISTING tag is set to YES, doxygen will include the
# program listings (including syntax highlighting and cross-referencing
# information) to the DOCBOOK output. Note that enabling this will significantly
# increase the size of the DOCBOOK output.
# The default value is: NO.
# This tag requires that the tag GENERATE_DOCBOOK is set to YES.

DOCBOOK_PROGRAMLISTING = NO

#---------------------------------------------------------------------------
# Configuration options for the AutoGen Definitions output
#---------------------------------------------------------------------------

# If the GENERATE_AUTOGEN_DEF tag is set to YES, doxygen will generate an
# AutoGen Definitions (see http://autogen.sourceforge.net/) file that captures
# the structure of the code including all documentation. Note that this feature
# is still experimental and incomplete at the moment.
# The default value is: NO.

GENERATE_AUTOGEN_DEF   = NO

#---------------------------------------------------------------------------
# Configuration options related to the Perl module output
#---------------------------------------------------------------------------

# If the GENERATE_PERLMOD tag is set to YES, doxygen will generate a Perl module
# file that captures the structure of the code including all documentation.
#
# Note that this feature is still experimental and incomplete at the moment.
# The default value is: NO.

GENERATE_PERLMOD       = NO

# If the PERLMOD_LATEX tag is set to YES, doxygen will generate the necessary
# Makefile rules, Perl scripts and LaTeX code to be able to generate PDF and DVI
# output from the Perl module output.
# The default value is: NO.
# This tag requires that the tag GENERATE_PERLMOD is set to YES.

PERLMOD_LATEX          = NO

# If the PERLMOD_PRETTY tag is set to YES, the Perl module output will be nicely
# formatted so it can be parsed by a human reader. This is useful if you want to
# understand what is going on. On the other hand, if this tag is set to NO, the
# size of the Perl module output will be much smaller and Perl will parse it
# just the same.
# The default value is: YES.
# This tag requires that the tag GENERATE_PERLMOD is set to YES.

PERLMOD_PRETTY         = YES

# The names of the make variables in the generated doxyrules.make file are
# prefixed with the string contained in PERLMOD_MAKEVAR_PREFIX. This is useful
# so different doxyrules.make files included by the same Makefile don't
# overwrite each other's variables.
# This tag requires that the tag GENERATE_PERLMOD is set to YES.

PERLMOD_MAKEVAR_PREFIX =

#---------------------------------------------------------------------------
# Configuration options related to the preprocessor
#---------------------------------------------------------------------------

# If the ENABLE_PREPROCESSING tag is set to YES, doxygen will evaluate all
# C-preprocessor directives found in the sources and include files.
# The default value is: YES.

ENABLE_PREPROCESSING   = YES

# If the MACRO_EXPANSION tag is set to YES, doxygen will expand all macro names
# in the source code. If set to NO, only conditional compilation will be
# performed. Macro expansion can be done in a controlled way by setting
# EXPAND_ONLY_PREDEF to YES.
# The default value is: NO.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

MACRO_EXPANSION        = NO

# If the EXPAND_ONLY_PREDEF and MACRO_EXPANSION tags are both set to YES then
# the macro expansion is limited to the macros specified with the PREDEFINED and
# EXPAND_AS_DEFINED tags.
# The default value is: NO.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

EXPAND_ONLY_PREDEF     = NO

# If the SEARCH_INCLUDES tag is set to YES, the include files in the
# INCLUDE_PATH will be searched if a #include is found.
# The default value is: YES.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

SEARCH_INCLUDES        = YES

# The INCLUDE_PATH tag can be used to specify one or more directories that
# contain include files that are not input files but should be processed by the
# preprocessor.
# This tag requires that the tag SEARCH_INCLUDES is set to YES.

INCLUDE_PATH           =

# You can use the INCLUDE_FILE_PATTERNS tag to specify one or more wildcard
# patterns (like *.h and *.hpp) to filter out the header-files in the
# directories. If left blank, the patterns specified with FILE_PATTERNS will be
# used.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

INCLUDE_FILE_PATTERNS  =

# The PREDEFINED tag can be used to specify one or more macro names that are
# defined before the preprocessor is started (similar to the -D option of e.g.
# gcc). The argument of the tag is a list of macros of the form: name or
# name=definition (no spaces). If the definition and the "=" are omitted, "=1"
# is assumed. To prevent a macro definition from being undefined via #undef or
# recursively expanded use the := operator instead of the = operator.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

PREDEFINED             =

# If the MACRO_EXPANSION and EXPAND_ONLY_PREDEF tags are set to YES then this
# tag can be used to specify a list of macro names that should be expanded. The
# macro definition that is found in the sources will be used. Use the PREDEFINED
# tag if you want to use a different macro definition that overrules the
# definition found in the source code.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

EXPAND_AS_DEFINED      =

# If the SKIP_FUNCTION_MACROS tag is set to YES then doxygen's preprocessor will
# remove all references to function-like macros that are alone on a line, have
# an all uppercase name, and do not end with a semicolon. Such function macros
# are typically used for boiler-plate code, and will confuse the parser if not
# removed.
# The default value is: YES.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

SKIP_FUNCTION_MACROS   = YES

#---------------------------------------------------------------------------
# Configuration options related to external references
#---------------------------------------------------------------------------

# The TAGFILES tag can be used to specify one or more tag files. For each tag
# file the location of the external documentation should be added. The format of
# a tag file without this location is as follows:
# TAGFILES = file1 file2 ...
# Adding location for the tag files is done as follows:
# TAGFILES = file1=loc1 "file2 = loc2" ...
# where loc1 and loc2 can be relative or absolute paths or URLs. See the
# section "Linking to external documentation" for more information about the use
# of tag files.
# Note: Each tag file must have a unique name (where the name does NOT include
# the path). If a tag file is not located in the directory in which doxygen is
# run, you must also specify the path to the tagfile here.

TAGFILES               =

# When a file name is specified after GENERATE_TAGFILE, doxygen will create a
# tag file that is based on the input files it reads. See section "Linking to
# external documentation" for more information about the usage of tag files.

GENERATE_TAGFILE       =

# If the ALLEXTERNALS tag is set to YES, all external class will be listed in
# the class index. If set to NO, only the inherited external classes will be
# listed.
# The default value is: NO.

ALLEXTERNALS           = NO

# If the EXTERNAL_GROUPS tag is set to YES, all external groups will be listed
# in the modules index. If set to NO, only the current project's groups will be
# listed.
# The default value is: YES.

EXTERNAL_GROUPS        = YES

# If the EXTERNAL_PAGES tag is set to YES, all external pages will be listed in
# the related pages index. If set to NO, only the current project's pages will
# be listed.
# The default value is: YES.

EXTERNAL_PAGES         = YES

# The PERL_PATH should be the absolute path and name of the perl script
# interpreter (i.e. the result of 'which perl').
# The default file (with absolute path) is: /usr/bin/perl.

PERL_PATH              = /usr/bin/perl

#---------------------------------------------------------------------------
# Configuration options related to the dot tool
#---------------------------------------------------------------------------

# If the CLASS_DIAGRAMS tag is set to YES, doxygen will generate a class diagram
# (in HTML and LaTeX) for classes with base or super classes. Setting the tag to
# NO turns the diagrams off. Note that this option also works with HAVE_DOT
# disabled, but it is recommended to install and use dot, since it yields more
# powerful graphs.
# The default value is: YES.

CLASS_DIAGRAMS         = YES

# You can define message sequence charts within doxygen comments using the \msc
# command. Doxygen will then run the mscgen tool (see:
# http://www.mcternan.me.uk/mscgen/)) to produce the chart and insert it in the
# documentation. The MSCGEN_PATH tag allows you to specify the directory where
# the mscgen tool resides. If left empty the tool is assumed to be found in the
# default search path.

MSCGEN_PATH            =

# You can include diagrams made with dia in doxygen documentation. Doxygen will
# then run dia to produce the diagram and insert it in the documentation. The
# DIA_PATH tag allows you to specify the directory where the dia binary resides.
# If left empty dia is assumed to be found in the default search path.

DIA_PATH               =

# If set to YES the inheritance and collaboration graphs will hide inheritance
# and usage relations if the target is undocumented or is not a class.
# The default value is: YES.

HIDE_UNDOC_RELATIONS   = YES

# If you set the HAVE_DOT tag to YES then doxygen will assume the dot tool is
# available from the path. This tool is part of Graphviz (see:
# http://www.graphviz.org/), a graph visualization toolkit from AT&T and Lucent
# Bell Labs. The other options in this section have no effect if this option is
# set to NO
# The default value is: NO.

HAVE_DOT               = YES

# The DOT_NUM_THREADS specifies the number of dot invocations doxygen is allowed
# to run in parallel. When set to 0 doxygen will base this on the number of
# processors available in the system. You can set it explicitly to a value
# larger than 0 to get control over the balance between CPU load and processing
# speed.
# Minimum value: 0, maximum value: 32, default value: 0.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_NUM_THREADS        = 0

# When you want a differently looking font in the dot files that doxygen
# generates you can specify the font name using DOT_FONTNAME. You need to make
# sure dot is able to find the font, which can be done by putting it in a
# standard location or by setting the DOTFONTPATH environment variable or by
# setting DOT_FONTPATH to the directory containing the font.
# The default value is: Helvetica.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_FONTNAME           = Helvetica

# The DOT_FONTSIZE tag can be used to set the size (in points) of the font of
# dot graphs.
# Minimum value: 4, maximum value: 24, default value: 10.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_FONTSIZE           = 10

# By default doxygen will tell dot to use the default font as specified with
# DOT_FONTNAME. If you specify a different font using DOT_FONTNAME you can set
# the path where dot can find it using this tag.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_FONTPATH           =

# If the CLASS_GRAPH tag is set to YES then doxygen will generate a graph for
# each documented class showing the direct and indirect inheritance relations.
# Setting this tag to YES will force the CLASS_DIAGRAMS tag to NO.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

CLASS_GRAPH            = YES

# If the COLLABORATION_GRAPH tag is set to YES then doxygen will generate a
# graph for each documented class showing the direct and indirect implementation
# dependencies (inheritance, containment, and class references variables) of the
# class with other documented classes.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

COLLABORATION_GRAPH    = YES

# If the GROUP_GRAPHS tag is set to YES then doxygen will generate a graph for
# groups, showing the direct groups dependencies.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

GROUP_GRAPHS           = YES

# If the UML_LOOK tag is set to YES, doxygen will generate inheritance and
# collaboration diagrams in a style similar to the OMG's Unified Modeling
# Language.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

UML_LOOK               = NO

# If the UML_LOOK tag is enabled, the fields and methods are shown inside the
# class node. If there are many fields or methods and many nodes the graph may
# become too big to be useful. The UML_LIMIT_NUM_FIELDS threshold limits the
# number of items for each type to make the size more manageable. Set this to 0
# for no limit. Note that the threshold may be exceeded by 50% before the limit
# is enforced. So when you set the threshold to 10, up to 15 fields may appear,
# but if the number exceeds 15, the total amount of fields shown is limited to
# 10.
# Minimum value: 0, maximum value: 100, default value: 10.
# This tag requires that the tag HAVE_DOT is set to YES.

UML_LIMIT_NUM_FIELDS   = 10

# If the TEMPLATE_RELATIONS tag is set to YES then the inheritance and
# collaboration graphs will show the relations between templates and their
# instances.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

TEMPLATE_RELATIONS     = NO

# If the INCLUDE_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are set to
# YES then doxygen will generate a graph for each documented file showing the
# direct and indirect include dependencies of the file with other documented
# files.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

INCLUDE_GRAPH          = YES

# If the INCLUDED_BY_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are
# set to YES then doxygen will generate a graph for each documented file showing
# the direct and indirect include dependencies of the file with other documented
# files.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

INCLUDED_BY_GRAPH      = YES

# If the CALL_GRAPH tag is set to YES then doxygen will generate a call
# dependency graph for every global function or class method.
#
# Note that enabling this option will significantly increase the time of a run.
# So in most cases it will be better to enable call graphs for selected
# functions only using the \callgraph command. Disabling a call graph can be
# accomplished by means of the command \hidecallgraph.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

CALL_GRAPH             = YES

# If the CALLER_GRAPH tag is set to YES then doxygen will generate a caller
# dependency graph for every global function or class method.
#
# Note that enabling this option will significantly increase the time of a run.
# So in most cases it will be better to enable caller graphs for selected
# functions only using the \callergraph command. Disabling a caller graph can be
# accomplished by means of the command \hidecallergraph.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

CALLER_GRAPH           = YES

# If the GRAPHICAL_HIERARCHY tag is set to YES then doxygen will graphical
# hierarchy of all classes instead of a textual one.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

GRAPHICAL_HIERARCHY    = YES

# If the DIRECTORY_GRAPH tag is set to YES then doxygen will show the
# dependencies a directory has on other directories in a graphical way. The
# dependency relations are determined by the #include relations between the
# files in the directories.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

DIRECTORY_GRAPH        = YES

# The DOT_IMAGE_FORMAT tag can be used to set the image format of the images
# generated by dot. For an explanation of the image formats see the section
# output formats in the documentation of the dot tool (Graphviz (see:
# http://www.graphviz.org/)).
# Note: If you choose svg you need to set HTML_FILE_EXTENSION to xhtml in order
# to make the SVG files visible in IE 9+ (other browsers do not have this
# requirement).
# Possible values are: png, jpg, gif, svg, png:gd, png:gd:gd, png:cairo,
# png:cairo:gd, png:cairo:cairo, png:cairo:gdiplus, png:gdiplus and
# png:gdiplus:gdiplus.
# The default value is: png.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_IMAGE_FORMAT       = png

# If DOT_IMAGE_FORMAT is set to svg, then this option can be set to YES to
# enable generation of interactive SVG images that allow zooming and panning.
#
# Note that this requires a modern browser other than Internet Explorer. Tested
# and working are Firefox, Chrome, Safari, and Opera.
# Note: For IE 9+ you need to set HTML_FILE_EXTENSION to xhtml in order to make
# the SVG files visible. Older versions of IE do not have SVG support.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

INTERACTIVE_SVG        = NO

# The DOT_PATH tag can be used to specify the path where the dot tool can be
# found. If left blank, it is assumed the dot tool can be found in the path.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_PATH               =

# The DOTFILE_DIRS tag can be used to specify one or more directories that
# contain dot files that are included in the documentation (see the \dotfile
# command).
# This tag requires that the tag HAVE_DOT is set to YES.

DOTFILE_DIRS           =

# The MSCFILE_DIRS tag can be used to specify one or more directories that
# contain msc files that are included in the documentation (see the \mscfile
# command).

MSCFILE_DIRS           =

# The DIAFILE_DIRS tag can be used to specify one or more directories that
# contain dia files that are included in the documentation (see the \diafile
# command).

DIAFILE_DIRS           =

# When using plantuml, the PLANTUML_JAR_PATH tag should be used to specify the
# path where java can find the plantuml.jar file. If left blank, it is assumed
# PlantUML is not used or called during a preprocessing step. Doxygen will
# generate a warning when it encounters a \startuml command in this case and
# will not generate output for the diagram.

PLANTUML_JAR_PATH      =

# When using plantuml, the PLANTUML_CFG_FILE tag can be used to specify a
# configuration file for plantuml.

PLANTUML_CFG_FILE      =

# When using plantuml, the specified paths are searched for files specified by
# the !include statement in a plantuml block.

PLANTUML_INCLUDE_PATH  =

# The DOT_GRAPH_MAX_NODES tag can be used to set the maximum number of nodes
# that will be shown in the graph. If the number of nodes in a graph becomes
# larger than this value, doxygen will truncate the graph, which is visualized
# by representing a node as a red box. Note that doxygen if the number of direct
# children of the root node in a graph is already larger than
# DOT_GRAPH_MAX_NODES then the graph will not be shown at all. Also note that
# the size of a graph can be further restricted by MAX_DOT_GRAPH_DEPTH.
# Minimum value: 0, maximum value: 10000, default value: 50.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_GRAPH_MAX_NODES    = 50

# The MAX_DOT_GRAPH_DEPTH tag can be used to set the maximum depth of the graphs
# generated by dot. A depth value of 3 means that only nodes reachable from the
# root by following a path via at most 3 edges will be shown. Nodes that lay
# further from the root node will be omitted. Note that setting this option to 1
# or 2 may greatly reduce the computation time needed for large code bases. Also
# note that the size of a graph can be further restricted by
# DOT_GRAPH_MAX_NODES. Using a depth of 0 means no depth restriction.
# Minimum value: 0, maximum value: 1000, default value: 0.
# This tag requires that the tag HAVE_DOT is set to YES.

MAX_DOT_GRAPH_DEPTH    = 0

# Set the DOT_TRANSPARENT tag to YES to generate images with a transparent
# background. This is disabled by default, because dot on Windows does not seem
# to support this out of the box.
#
# Warning: Depending on the platform used, enabling this option may lead to
# badly anti-aliased labels on the edges of a graph (i.e. they become hard to
# read).
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_TRANSPARENT        = NO

# Set the DOT_MULTI_TARGETS tag to YES to allow dot to generate multiple output
# files in one run (i.e. multiple -o and -T options on the command line). This
# makes dot run faster, but since only newer versions of dot (>1.8.10) support
# this, this feature is disabled by default.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_MULTI_TARGETS      = NO

# If the GENERATE_LEGEND tag is set to YES doxygen will generate a legend page
# explaining the meaning of the various boxes and arrows in the dot generated
# graphs.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

GENERATE_LEGEND        = YES

# If the DOT_CLEANUP tag is set to YES, doxygen will remove the intermediate dot
# files that are used to generate the various graphs.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_CLEANUP            = YES


Copyright (c) 2003, The Regents of the University of California, through
Lawrence Berkeley National Laboratory (subject to receipt of any required 
approvals from U.S. Dept. of Energy) 

All rights reserved. 

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met: 

(1) Redistributions of source code must retain the above copyright notice,
this list of conditions and the following disclaimer. 
(2) Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation
and/or other materials provided with the distribution. 
(3) Neither the name of Lawrence Berkeley National Laboratory, U.S. Dept. of
Energy nor the names of its contributors may be used to endorse or promote
products derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 


cmake_minimum_required(VERSION 2.8)
project(ParMETIS)

set(GKLIB_PATH METIS/GKlib CACHE PATH "path to GKlib")
set(METIS_PATH METIS CACHE PATH "path to METIS")

# Symlink ./metis to wherever metis is. This allows files to be
# included from metis/libmetis/.
execute_process(COMMAND ${CMAKE_COMMAND} -E create_symlink ${METIS_PATH} metis)

# Search for MPI.
# GK commented this out as it seems to be creating problems
# include(FindMPI)
# if(NOT MPI_FOUND)
#   message(FATAL_ERROR "mpi is not found")
# endif()
# set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${MPI_COMPILE_FLAGS}")

# Prepare libraries.
if(SHARED)
  set(ParMETIS_LIBRARY_TYPE SHARED)
  set(METIS_LIBRARY_TYPE SHARED)
else()
  set(ParMETIS_LIBRARY_TYPE STATIC)
  set(METIS_LIBRARY_TYPE STATIC)
endif()

include(${GKLIB_PATH}/GKlibSystem.cmake)

# List of paths that the compiler will search for header files.
# i.e., the -I equivalent
include_directories(include)
include_directories(${MPI_INCLUDE_PATH})
include_directories(${GKLIB_PATH})
include_directories(${METIS_PATH}/include)

# List of directories that cmake will look for CMakeLists.txt
add_subdirectory(${METIS_PATH}/libmetis ${CMAKE_BINARY_DIR}/libmetis)
add_subdirectory(include)
add_subdirectory(libparmetis)
add_subdirectory(programs)

# This is for testing during development and is not being distributed
#add_subdirectory(test)


SuperLU Users’ Guide∗
Xiaoye S. Li†
James W. Demmel‡
John R. Gilbert§
Laura Grigori¶
Piyush Sao∥
Meiyue Shao∗∗
Ichitaro Yamazaki††
September 1999
Last update: June 2018
∗The work was supported by Director, Oﬃce of Science, Oﬃce of Advanced Scientic Computing Research
of the U.S. Department of Energy under Contract No.
DE-AC02-05CH11231.
Additional support was
provided by the U.S. Dept. of Energy (DOE), Oﬃce of Science, Advanced Scientiﬁc Computing Research
(and Basic Energy Sciences/Biological and Environmental Research/High Energy Physics/Fusion Energy
Sciences/Nuclear Physics), through the Scientiﬁc Discovery through Advanced Computing (SciDAC) pro-
gram.
†Lawrence Berkeley National Laboratory,
MS 50F-1650,
1 Cyclotron Rd,
Berkeley,
CA 94720.
(xsli@lbl.gov).
‡Computer Science Division, University of California, Berkeley, CA 94720. (demmel@cs.berkeley.edu).
The research of Demmel and Li was supported in part by NSF grant ASC–9313958, DOE grant DE–FG03–
94ER25219, UT Subcontract No. ORA4466 from ARPA Contract No. DAAL03–91–C0047, DOE grant DE–
FG03–94ER25206, and NSF Infrastructure grants CDA–8722788 and CDA–9401156.
§Department
of
Computer
Science,
University
of
California,
Santa
Barbara,
CA
93106.
(gilbert@cs.ucsb.edu). The research of this author was supported in part by the Institute for Mathematics
and Its Applications at the University of Minnesota and in part by DARPA Contract No. DABT63-95-C0087.
Copyright c
⃝1994-1997 by Xerox Corporation. All rights reserved.
¶INRIA Saclay-Ile de France, Laboratoire de Recherche en Informatique, Universite Paris-Sud 11.
(laura.grigori@inria.fr)
∥Computational
Science
and
Engineering
Division,
Georgia
Institute
of
Technology,
Atlanta.
(piyush3@gatech.edu). The research of this author was supported in part by the National Science Founda-
tion CAREER award number 0953100 and DOE X-Stack 1.0 under DE-FC02-10ER26006/DE-SC0004915
(PI: Richard Vuduc).
∗∗Computational Research Division, Lawrence Berkeley National Laboratory, MS 50F-1650, 1 Cyclotron
Rd, Berkeley, CA 94720. (myshao@lbl.gov).
††Innovative Computing Laboratory, Department of Electrical Engineering and Computer Science, The
University of Tennessee. (ic.yamazaki@gmail.com). The research of this author was supported in part by
the Director, Oﬃce of Advanced Scientiﬁc Computing Research of the U.S. Department of Energy under
Contract No. D-AC02-05CH11231.


Contents
1
Introduction
4
1.1
Purpose of SuperLU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
Overall Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
What the three libraries have in common
. . . . . . . . . . . . . . . . . . . . . . . .
6
1.3.1
Input and Output Data Formats . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.3.2
Tuning Parameters for BLAS . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.3
Performance Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.4
Error Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.5
Ordering the Columns of A for Sparse Factors
. . . . . . . . . . . . . . . . .
8
1.3.6
Iterative Reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.7
Error Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3.8
Solving a Sequence of Related Linear Systems . . . . . . . . . . . . . . . . . .
10
1.3.9
Interfacing to other languages . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.4
How the three libraries diﬀer
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.4.1
Input and Output Data Formats . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.4.2
Parallelism
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.4.3
Pivoting Strategies for Stability . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.4.4
Memory Management
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.4.5
Interfacing to other languages . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.5
Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.6
Software Status and Availability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.7
Acknowledgement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2
Sequential SuperLU (Version 4.2)
16
2.1
About SuperLU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.2
How to call a SuperLU routine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.3
Matrix data structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.4
Options argument . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.5
Permutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.5.1
Ordering for sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.5.2
Partial pivoting with threshold . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.6
Symmetric Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.7
Incomplete LU factorization (ILU) preconditioner . . . . . . . . . . . . . . . . . . . .
28
2.8
Memory management for L and U
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.9
User-callable routines
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
1


2.9.1
Driver routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.9.2
Computational routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.9.3
Utility routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.10 Matlab interface
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
2.11 Installation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.11.1 File structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.11.2 Testing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.11.3 Performance-tuning parameters . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.12 Example programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.13 Calling from Fortran . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3
Multithreaded SuperLU (Version 2.0)
43
3.1
About SuperLU MT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.2
Storage types for L and U . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.3
Options argument . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.4
User-callable routines
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.4.1
Driver routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.4.2
Computational routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.5
Installation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.5.1
File structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.5.2
Performance issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.6
Example programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.7
Porting to other platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.7.1
Creating multiple threads . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.7.2
Use of mutexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
4
Distributed-memory SuperLU on manycore nodes (Version 4.0)
52
4.1
About SuperLU DIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
4.2
Formats of the input matrices A and B
. . . . . . . . . . . . . . . . . . . . . . . . .
52
4.2.1
Global input
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
4.2.2
Distributed input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
4.3
Distributed data structures for L and U . . . . . . . . . . . . . . . . . . . . . . . . .
53
4.4
Process grid and MPI communicator . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
4.4.1
2D process grid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
4.4.2
Arbitrary grouping of processes . . . . . . . . . . . . . . . . . . . . . . . . . .
55
4.5
Algorithmic background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.5.1
Multicore and GPU enhancements . . . . . . . . . . . . . . . . . . . . . . . .
57
4.6
Options argument . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
4.7
Basic steps to solve a linear system . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
4.8
User-callable routines
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
4.8.1
Driver routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
4.8.2
Computational routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.8.3
Utility routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.9
Installation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
4.9.1
File structure and complilation . . . . . . . . . . . . . . . . . . . . . . . . . .
66
4.9.2
Performance-tuning parameters . . . . . . . . . . . . . . . . . . . . . . . . . .
68
2


4.10 Example programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.11 Fortran 90 Interface
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.11.1 Callable functions in the Fortran 90 module ﬁle spuerlu mod.f90 . . . . . . . .
74
4.11.2 C wrapper functions callable by Fortran in ﬁle spuerlu c2f wrap.c . . . . . . .
75
3


Chapter 1
Introduction
1.1
Purpose of SuperLU
This document describes a collection of three related ANSI C subroutine libraries for solving sparse
linear systems of equations AX = B.
Here A is a square, nonsingular, n × n sparse matrix,
and X and B are dense n × nrhs matrices, where nrhs is the number of right-hand sides and
solution vectors. The LU factorization routines can handle non-square matrices. Matrix A need
not be symmetric or deﬁnite; indeed, SuperLU is particularly appropriate for matrices with very
unsymmetric structure.
All three libraries use variations of Gaussian elimination optimized to
take advantage of both sparsity and the computer architecture, in particular memory hierarchies
(caches) and parallelism.
In this introduction we refer to all three libraries collectively as SuperLU. The three libraries
within SuperLU are as follows. Detailed references are also given (see also [23]).
• Sequential SuperLU is designed for sequential processors with one or more layers of memory
hierarchy (caches) [6].
• Multithreaded SuperLU (SuperLU MT) is designed for shared memory multiprocessors
(SMPs), and can eﬀectively use up to 16 or 32 parallel processors on suﬃciently large matrices
in order to speed up the computation [7].
• Distributed SuperLU (SuperLU DIST) is designed for distributed memory parallel pro-
cessors, using MPI [29] for interprocess communication. It can eﬀectively use hundreds of
parallel processors on suﬃciently large matrices [25, 26].
Table 1.1 summarizes the current status of the software. All the routines are implemented in
C, with parallel extensions using Pthreads or OpenMP for shared-memory programming, or MPI
for distributed-memory programming. We provide Fortran interface for all three libraries.
The rest of the Introduction is organized as follows. Section 1.2 describes the high-level algo-
rithm used by all three libraries, pointing out some common features and diﬀerences. Section 1.3
describes the detailed algorithms, data structures, and interface issues common to all three routines.
Section 1.4 describes how the three routines diﬀer, emphasizing the diﬀerences that most aﬀect the
user. Section 1.6 describes the software status, including planned developments, bug reporting, and
licensing.
4


Sequential SuperLU
SuperLU MT
SuperLU DIST
Platform
serial
shared-memory
distributed-memory
Language
C
C + Pthreads
C + MPI
(with Fortran interface)
(or OpenMP)
Data type
real/complex
real/complex
real/complex
single/double
single/double
double
Table 1.1: SuperLU software status.
1.2
Overall Algorithm
A simple description of the algorithm for solving linear equations by sparse Gaussian elimination
is as follows:
1. Compute a triangular factorization PrDrADcPc = LU. Here Dr and Dc are diagonal matrices
to equilibrate the system, Pr and Pc are permutation matrices.
Premultiplying A by Pr
reorders the rows of A, and postmultiplying A by Pc reorders the columns of A. Pr and
Pc are chosen to enhance sparsity, numerical stability, and parallelism. L is a unit lower
triangular matrix (Lii = 1) and U is an upper triangular matrix. The factorization can also
be applied to non-square matrices.
2. Solve AX = B by evaluating X = A−1B = (D−1
r P −1
r
LUP −1
c
D−1
c )−1B = Dc(Pc(U −1(L−1(Pr(DrB))))).
This is done eﬃciently by multiplying from right to left in the last expression: Scale the rows
of B by Dr. Multiplying PrB means permuting the rows of DrB. Multiplying L−1(PrDrB)
means solving nrhs triangular systems of equations with matrix L by substitution. Similarly,
multiplying U −1(L−1(PrDrB)) means solving triangular systems with U.
In addition to complete factorization, we also have limited support for incomplete factorization
(ILU) preconditioner.
The simplest implementation, used by the “simple driver” routines in SuperLU and SuperLU MT,
is as follows:
Simple Driver Algorithm
1. Choose Pc to order the columns of A to increase the sparsity of the computed L and U factors,
and hopefully increase parallelism (for SuperLU MT).
2. Compute the LU factorization of APc.
SuperLU and SuperLU MT can perform dynamic
pivoting with row interchanges for numerical stability, computing Pr, L and U at the same
time.
3. Solve the system using Pr, Pc, L and U as described above. (Dr = Dc = I)
The simple driver subroutines for double precision real data are called dgssv and pdgssv for
SuperLU and SuperLU MT, respectively.
The letter d in the subroutine names means double
precision real; other options are s for single precision real, c for single precision complex, and z for
double precision complex. The subroutine naming scheme is analogous to the one used in LAPACK
[1]. SuperLU DIST does not include this simple driver.
5


There is also an “expert driver” routine that can provide more accurate solutions, compute
error bounds, and solve a sequence of related linear systems more economically. It is available in
all three libraries.
Expert Driver Algorithm
1. Equilibrate the matrix A, i.e. compute diagonal matrices Dr and Dc so that ˆ
A = DrADc is
“better conditioned” than A, i.e.
ˆ
A−1 is less sensitive to perturbations in ˆ
A than A−1 is to
perturbations in A.
2. Preorder the rows of ˆ
A (SuperLU DIST only), i.e. replace ˆ
A by Pr ˆ
A where Pr is a permutation
matrix. We call this step “static pivoting”, and it is only done in the distributed-mmemory
algorithm.
3. Order the columns of ˆ
A to increase the sparsity of the computed L and U factors, and
hopefully increase parallelism (for SuperLU MT and SuperLU DIST). In other words, replace
ˆ
A by ˆ
AP T
c in SuperLU and SuperLU MT, or replace ˆ
A by Pc ˆ
AP T
c in SuperLU DIST, where
Pc is a permutation matrix.
4. Compute the LU factorization of ˆ
A. SuperLU and SuperLU MT can perform dynamic pivot-
ing with row interchanges for numerical stability. In contrast, SuperLU DIST uses the order
computed by the preordering step but replaces tiny pivots by larger values for stability.
5. Solve the system using the computed triangular factors.
6. Iteratively reﬁne the solution, again using the computed triangular factors. This is equivalent
to Newton’s method.
7. Compute error bounds. Both forward and backward error bounds are computed, as described
below.
The expert driver subroutines for double precision real data are called dgssvx, pdgssvx and
pdgssvx for SuperLU, SuperLU MT and SuperLU DIST, respectively.
The driver routines are
composed of several lower level computational routines for computing permutations, computing LU
factorization, solving triangular systems, and so on. For large matrices, the LU factorization steps
takes most of the time, although choosing Pc to order the columns can also be time-consuming.
1.3
What the three libraries have in common
1.3.1
Input and Output Data Formats
Sequential SuperLU and SuperLU MT accept A and B as single precision real, double precision
real, and both single and double precision complex. SuperLU DIST accepts double precision real or
complex.
A is stored in a sparse data structure according to the struct SuperMatrix, which is described
in section 3.2. In particular, A may be supplied in either column-compressed format (“Harwell-
Boeing format”), or row-compressed format (i.e. AT stored in column-compressed format). B,
which is overwritten by the solution X, is stored as a dense matrix in column-major order. In
SuperLU DIST, A and B can be either replicated or distributed across all processes.
(The storage of L and U diﬀers among the three libraries, as discussed in section 1.4.)
6


1.3.2
Tuning Parameters for BLAS
All three libraries depend on having high performance BLAS (Basic Linear Algebra Subroutine)
libraries [22, 9, 8] in order to get high performance. In particular, they depend on matrix-vector
multiplication or matrix-matrix multiplication of relatively small dense matrices. The sizes of these
small dense matrices can be tuned to match the “sweet spot” of the BLAS by setting certain
tuning parameters described in section 2.11.3 for SuperLU, in section 3.5.2 for SuperLU MT, and
in section 4.9.2 for SuperLU DIST.
(In addition, SuperLU MT and SuperLU DIST let one control the number of parallel processes
to be used, as described in section 1.4.)
1.3.3
Performance Statistics
Most of the computational routines use a struct to record certain kinds of performance data, namely
the time and number of ﬂoating point operations in each phase of the computation, and data about
the sizes of the matrices L and U. These statistics are collected during the computation. A statistic
variable is declared with the following type:
typedef struct {
int
*panel_histo; /* histogram of panel size distribution */
double
*utime;
/* time spent in various phases */
float
*ops;
/* floating-point operations at various phases */
int
TinyPivots;
/* number of tiny pivots */
int
RefineSteps;
/* number of iterative refinement steps */
} SuperLUStat_t;
For both SuperLU and SuperLU MT, there is only one copy of these statistics variable. But
for SuperLU DIST, each process keeps a local copy of this variable, and records its local statistics.
We need to use MPI reduction routines to ﬁnd any global information, such as the sum of the
ﬂoating-point operation count on all processes.
Before the computation, routine StatInit() should be called to malloc storage and perform
initialization for the ﬁelds panel histo, utime, and ops. The algorithmic phases are deﬁned by the
enumeration type PhaseType in SRC/util.h. In the end, routine StatFree() should be called to
free storage of the above statistics ﬁelds. After deallocation, the statistics are no longer accessible.
Therefore, users should extract the information they need before calling StatFree(), which can be
accomplished by calling (P)StatPrint().
An inquiry function dQuerySpace() is provided to compute memory usage statistics.
This
routine should be called after the LU factorization. It calculates the storage requirement based on
the size of the L and U data structures and working arrays.
1.3.4
Error Handling
Invalid arguments and (P)XERBLA
Similar to LAPACK, for all the SuperLU routines, we check the validity of the input arguments
to each routine. If an illegal value is supplied to one of the input arguments, the error handler
XERBLA is called, and a message is written to the standard output, indicating which argument
7


has an illegal value. The program returns immediately from the routine, with a negative value of
INFO.
Computational failures with INFO > 0
A positive value of INFO on return from a routine indicates a failure in the course of the computa-
tion, such as a matrix being singular, or the amount of memory (in bytes) already allocated when
malloc fails.
ABORT on unrecoverable errors
A macro ABORT is deﬁned in SRC/util.h to handle unrecoverable errors that occur in the middle
of the computation, such as malloc failure. The default action of ABORT is to call
superlu abort and exit(char *msg)
which prints an error message, the line number and the ﬁle name at which the error occurs, and
calls the exit function to terminate the program.
If this type of termination is not appropriate in some environment, users can alter the behavior
of the abort function. When compiling the SuperLU library, users may choose the C preprocessor
deﬁnition
-DUSER ABORT = my abort
At the same time, users would supply the following my abort function
my abort(char *msg)
which overrides the behavior of superlu abort and exit.
1.3.5
Ordering the Columns of A for Sparse Factors
There is a choice of orderings for the columns of A both in the simple or expert driver, in section 1.2:
• Natural ordering,
• Multiple Minimum Degree (MMD) [28] applied to the structure of AT A,
• Multiple Minimum Degree (MMD) [28] applied to the structure of AT + A,
• Column Approximate Minimum Degree (COLAMD) [5], and
• Use a Pc supplied by the user as input.
COLAMD is designed particularly for unsymmetric matrices when partial pivoting is needed,
and does not require explicit formation of AT A. It usually gives comparable orderings as MMD on
AT A, and is faster.
The orderings based on graph partitioning heuristics are also popular, as exempliﬁed in the
MeTiS package [20]. The user can simply input this ordering in the permutation vector for Pc. Note
that many graph partitioning algorithms are designed for symmetric matrices. The user may still
apply them to the structures of AT A or AT + A. Our routines getata() and at plus a() in the
ﬁle get perm c.c can be used to form AT A or AT + A.
8


1.3.6
Iterative Reﬁnement
Step 6 of the expert driver algorithm, iterative reﬁnement, serves to increase accuracy of the
computed solution. Given the initial approximate solution x from step 5, the algorithm for step 6
is as follows (where x and b are single columns of X and B, respectively):
Compute residual r = Ax −b
While residual too large
Solve Ad = r for correction d
Update solution x = x −d
Update residual r = Ax −b
end while
If r and then d were computed exactly, the updated solution x −d would be the exact solution.
Roundoﬀprevents immediate convergence.
The criterion “residual too large” in the iterative reﬁnement algorithm above is essentially that
BERR ≡max
i
|ri|/si
(1.1)
exceeds the machine roundoﬀlevel, or is continuing to decrease quickly enough. Here si is the scale
factor
si = (|A| · |x| + |b|)i =
X
j
|Aij| · |xj| + |bi|
In this expression |A| is the n-by-n matrix with entries |A|ij = |Aij|, |b| and |x| are similarly
column vectors of absolute entries of b and x, respectively, and |A| · |x| is conventional matrix-
vector multiplication.
The purpose of this stopping criterion is explained in the next section.
1.3.7
Error Bounds
Step 7 of the expert driver algorithm computes error bounds.
It is shown in [2, 30] that BERR deﬁned in Equation (1.1) measures the componentwise relative
backward error of the computed solution.
This means that the computed x satisﬁes a slightly
perturbed linear system of equations (A + E)x = b + f, where |Eij| ≤BERR · |Aij| and |fi| ≤
BERR · |bi| for all i and j. It is shown in [2, 35] that one step of iterative reﬁnement usually
reduces BERR to near machine epsilon. For example, if BERR is 4 times machine epsilon, then
the computed solution x is identical to the solution one would get by changing each nonzero entry
of A and b by at most 4 units in their last places, and then solving this perturbed system exactly.
If the nonzero entries of A and b are uncertain in their bottom 2 bits, then one should generally not
expect a more accurate solution. Thus BERR is a measure of backward error speciﬁcally suited
to solving sparse linear systems of equations. Despite roundoﬀ, BERR itself is always computed
to within about ±n times machine epsilon (and usually much more accurately) and so BERR is
quite accurate.
In addition to backward error, the expert driver computes a forward error bound
FERR ≥∥xtrue −x∥∞/∥x∥∞
9


Here ∥x∥∞≡maxi |xi|. Thus, if FERR = 10−6 then each component of x has an error bounded
by about 10−6 times the largest component of x. The algorithm used to compute FERR is an
approximation; see [2, 19] for a discussion. Generally FERR is accurate to within a factor of 10
or better, which is adequate to say how many digits of the large entries of x are correct.
(SuperLU DIST’s algorithm for FERR is slightly less reliable [26].)
1.3.8
Solving a Sequence of Related Linear Systems
It is very common to solve a sequence of related linear systems A(1)X(1) = B(1), A(2)X(2) = B(2), ...
rather than just one. When A(1) and A(2) are similar enough in sparsity pattern and/or numerical
entries, it is possible to save some of the work done when solving with A(1) to solve with A(2).
This can result in signiﬁcant savings. Here are the options, in increasing order of “reuse of prior
information”:
1. Factor from scratch. No previous information is used. If one were solving just one linear
system, or a sequence of unrelated linear systems, this is the option to use.
2. Reuse Pc, the column permutation. The user may save the column permutation and reuse
it. This is most useful when A(2) has the same sparsity structure as A(1), but not necessarily
the same (or similar) numerical entries.
Reusing Pc saves the sometimes quite expensive
operation of computing it.
3. Reuse Pc, Pr and data structures allocated for L and U. If Pr and Pc do not change, then
the work of building the data structures associated with L and U (including the elimination
tree [16]) can be avoided. This is most useful when A(2) has the same sparsity structure and
similar numerical entries as A(1). When the numerical entries are not similar, one can still use
this option, but at a higher risk of numerical instability (BERR will always report whether or
not the solution was computed stably, so one cannot get an unstable answer without warning).
4. Reuse Pc, Pr, L and U.
In other words, we reuse essentially everything.
This is most
commonly used when A(2) = A(1), but B(2) ̸= B(1), i.e. when only the right-hand sides diﬀer.
It could also be used when A(2) and A(1) diﬀered just slightly in numerical values, in the
hopes that iterative reﬁnement converges (using A(2) to compute residuals but the triangular
factorization of A(1) to solve).
Because of the diﬀerent ways L and U are computed and stored in the three libraries, these 4
options are speciﬁed slightly diﬀerently; see Chapters 2 through 4 for details.
1.3.9
Interfacing to other languages
It is possible to call all the drivers and the computational routines from Fortran. However, currently
the Fortran wrapper functions are not complete. The users are expected to look at the Fortran
example programs in the FORTRAN/ directory, together with the C “bridge” routine, and learn
how to call SuperLU from a Fortran program. The users can modify the C bridge routine to ﬁt
their needs.
10


1.4
How the three libraries diﬀer
1.4.1
Input and Output Data Formats
All Sequential SuperLU and SuperLU MT routines are available in single and double precision (real
or complex), but SuperLU DIST routines are available only in double precision (real or complex).
L and U are stored in diﬀerent formats in the three libraries:
• L and U in Sequential SuperLU. L is a “column-supernodal” matrix, in storage type SCformat.
This means it is stored sparsely, with supernodes (consecutive columns with identical struc-
tures) stored as dense blocks.
U is stored in column-compressed format NCformat.
See
section 2.3 for details.
• L and U in SuperLU MT. Because of parallelism, the columns of L and U may not be
computed in consecutive order, so they may be allocated and stored out of order. This means
we use the “column-supernodal-permuted” format SCPformat for L and “column-permuted”
format NCPformat for U. See section 3.2 for details.
• L and U in SuperLU DIST. Now L and U are distributed across multiple processors. As
described in detail in Sections 4.3 and 4.4, we use a 2D block-cyclic format, which has been
used for dense matrices in libraries like ScaLAPACK [4]. But for sparse matrices, the blocks
are no longer identical in size, and vary depending on the sparsity structure of L and U. The
detailed storage format is discussed in section 4.3 and illustrated in Figure 4.1.
1.4.2
Parallelism
Sequential SuperLU has no explicit parallelism. Some parallelism may still be exploited on an
SMP by using a multithreaded BLAS library if available. But it is likely to be more eﬀective to
use SuperLU MT on an SMP, described next.
SuperLU MT lets the user choose the number of parallel threads to use. The mechanism varies
from platform to platform and is described in section 3.7.
SuperLU DIST not only lets the user specify the number of processors, but how they are
arranged into a 2D grid. Furthermore, MPI permits any subset of the processors allocated to the
user may be used for SuperLU DIST, not just consecutively numbered processors (say 0 through
P-1). See section 4.4 for details.
1.4.3
Pivoting Strategies for Stability
Sequential SuperLU and SuperLU MT use the same pivoting strategy, called threshold pivoting, to
determine the row permutation Pr. Suppose we have factored the ﬁrst i −1 columns of A, and are
seeking the pivot for column i. Let ami be a largest entry in magnitude on or below the diagonal
of the partially factored A: |ami| = maxj≥i |aji|. Depending on a threshold 0 < u ≤1 input by the
user, the code will use the diagonal entry aii as the pivot in column i as long as |aii| ≥u·|ami|, and
otherwise use ami. So if the user sets u = 1, ami (or an equally large entry) will be selected as the
pivot; this corresponds to the classical partial pivoting strategy. If the user has ordered the matrix
so that choosing diagonal pivots is particularly good for sparsity or parallelism, then smaller values
of u will tend to choose those diagonal pivots, at the risk of less numerical stability. Using u = 0
11


guarantees that the pivots on the diagonal will be chosen, unless they are zero. The error bound
BERR measure how much stability is actually lost.
Threshold pivoting turns out to be hard to parallelize on distributed memory machines, because
of the ﬁne-grain communication and dynamic data structures required. So SuperLU DIST uses a
new scheme called static pivoting instead. In static pivoting the pivot order (Pr) is chosen before
numerical factorization, using a weighted perfect matching algorithm [10], and kept ﬁxed during
factorization. Since both row and column orders (Pr and Pc) are ﬁxed before numerical factoriza-
tion, we can extensively optimize the data layout, load balance, and communication schedule. The
price is a higher risk of numeric instability, which is mitigated by diagonal scaling, setting very
tiny pivots to larger values, and iterative reﬁnement [26]. Again, error bound BERR measure how
much stability is actually lost.
1.4.4
Memory Management
Because of ﬁll-in of entries during Gaussian elimination, L and U typically have many more nonzero
entries than A. If Pr and Pc are not already known, we cannot determine the number and locations
of these nonzeros before performing the numerical factorization. This means that some kind of
dynamic memory allocation is needed.
Sequential SuperLU lets the user either supply a preallocated space work[] of length lwork, or
depend on malloc/free. The variable FILL can be used to help the code predict the amount of ﬁll,
which can reduce both fragmentation and the number of calls to malloc/free. If the initial estimate
of the size of L and U from FILL is too small, the routine allocates more space and copies the
current L and U factors to the new space and frees the old space. If the routine cannot allocate
enough space, it calls a user-speciﬁable routine ABORT. See sections 1.3.4 for details.
SuperLU MT is similar, except that the current alpha version cannot reallocate more space for
L and U if the initial size estimate from FILL is too small. Instead, the program calls ABORT and
the user must start over with a larger value of FILL. See section 3.5.2.
SuperLU DIST actually has a simpler memory management chore, because once Pr and Pc are
determined, the structures of L and U can be determined eﬃciently and just the right amount of
memory allocated using malloc and later free. So it will call ABORT only if there is really not
enough memory available to solve the problem.
1.4.5
Interfacing to other languages
Sequential SuperLU has a Matlab interface to the driver via a MEX ﬁle. See section 2.10 for details.
1.5
Performance
SuperLU library incorporates a number of novel algorithmic ideas developed recently. These al-
gorithms also exploit the features of modern computer architectures, in particular, the multi-level
cache organization and parallelism. We have conducted extensive experiments on various plat-
forms, with a large collection of test matrices. The Sequential SuperLU achieved up to 40% of the
theoretical ﬂoating-point rate on a number of processors, see [6, 23]. The megaﬂop rate usually
increases with increasing ratio of ﬂoating-point operations count over the number of nonzeros in the
L and U factors. The parallel LU factorization in SuperLU MT demonstrated 5–10 fold speedups
on a range of commercially popular SMPs, and up to 2.5 Gigaﬂops factorization rate, see [7, 23].
12


The parallel LU factorization in SuperLU DIST achieved up to 100 fold speedup on a 512-processor
Cray T3E, and 10.2 Gigaﬂops factorization rate, see [25].
1.6
Software Status and Availability
All three libraries are freely available for all uses, commercial or noncommercial, subject to the
following caveats. No warranty is expressed or implied by the authors, although we will gladly
answer questions and try to ﬁx all reported bugs.
We ask that proper credit be given to the
authors and that a notice be included if any modiﬁcations are made.
The following Copyright applies to the whole SuperLU software.
Copyright (c) 2003, The Regents of the University of California, through Lawrence
Berkeley National Laboratory (subject to receipt of any required approvals from U.S.
Dept. of Energy)
All rights reserved.
Redistribution and use in source and binary forms, with or without modiﬁcation, are
permitted provided that the following conditions are met:
(1) Redistributions of source code must retain the above copyright notice, this list
of conditions and the following disclaimer. (2) Redistributions in binary form must
reproduce the above copyright notice, this list of conditions and the following disclaimer
in the documentation and/or other materials provided with the distribution. (3) Neither
the name of Lawrence Berkeley National Laboratory, U.S. Dept. of Energy nor the
names of its contributors may be used to endorse or promote products derived from
this software without speciﬁc prior written permission.
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CON-
TRIBUTORS ”AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, IN-
CLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MER-
CHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LI-
ABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,
OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PRO-
CUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSI-
BILITY OF SUCH DAMAGE.
Some routines carry the additional notices as follows.
1. Some subroutines carry the following notice:
Copyright (c) 1994 by Xerox Corporation. All rights reserved.
THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY
EXPRESSED OR IMPLIED. ANY USE IS AT YOUR OWN RISK.
13


Permission is hereby granted to use or copy this program for any purpose, provided
the above notices are retained on all copies. Permission to modify the code and to
distribute modiﬁed code is granted, provided the above notices are retained, and a
notice that the code was modiﬁed is included with the above copyright notice.
2. The MC64 routine (only used in SuperLU DIST) carries the following notice:
COPYRIGHT (c) 1999 Council for the Central Laboratory of the Research Coun-
cils. All rights reserved. PACKAGE MC64A/AD AUTHORS Iain Duﬀ(i.duﬀ@rl.ac.uk)
and Jacko Koster (jak@ii.uib.no) LAST UPDATE 20/09/99
*** Conditions on external use ***
The user shall acknowledge the contribution of this package in any publication of
material dependent upon the use of the package. The user shall use reasonable
endeavours to notify the authors of the package of this publication.
The user can modify this code but, at no time shall the right or title to all or any
part of this package pass to the user. The user shall make available free of charge
to the authors for any purpose all information relating to any alteration or addition
made to this package for the purposes of extending the capabilities or enhancing
the performance of this package.
The user shall not pass this code directly to a third party without the express prior
consent of the authors. Users wanting to licence their own copy of these routines
should send email to hsl@aeat.co.uk
None of the comments from the Copyright notice up to and including this one shall
be removed or altered in any way.
All three libraries can be obtained from the following URLs:
http://crd.lbl.gov/~xiaoye/SuperLU/
http://www.netlib.org/scalapack/prototype/
In the future, we will add more functionality in the software, such as sequential and parallel in-
complete LU factorizations, as well as parallel symbolic and ordering algorithms for SuperLU DIST;
these latter routines would replace MC64 and have no restrictions on external use.
All bugs reports and queries can be e-mailed to xsli@lbl.gov and demmel@cs.berkeley.edu.
1.7
Acknowledgement
With great gratitude, we acknowledge Stan Eisenstat and Joesph Liu for their signiﬁcant contri-
butions to the development of Sequential SuperLU. Meiyue Shao helped the development of the
incomplete factorization ILU routines in sequential SuperLU.
We would like to thank Jinqchong Teo for helping generate the code in Sequential SuperLU to
work with four ﬂoating-point data types, and Daniel Schreiber for doing this with SuperLU MT.
Yu Wang and William F. Mitchell developed the Fortran 90 interface for SuperLU DIST. Laura
Grigori developed the parallel symbolic factorization code for SuperLU DIST.
We thank Tim Davis for his contribution of some subroutines related to column ordering and
suggestions on improving the routines’ interfaces. We thank Ed Rothberg of Silicon Graphics for
14


discussions and providing us access to the SGI Power Challenge during the SuperLU MT develop-
ment.
We acknowledge the following organizations that provided the computer resources during our
code development: NERSC at Lawrence Berkeley National Laboratory, Livermore Computing at
Lawrence Livermore National Laboratory, NCSA at University of Illinois at Urbana-Champaign,
Silicon Graphics, and Xerox Palo Alto Research Center. We thank UC Berkeley and NSF Infras-
tructure grant CDA-9401156 for providing Berkeley NOW.
15


Chapter 2
Sequential SuperLU (Version 4.2)
2.1
About SuperLU
In this chapter, SuperLU will always mean Sequential SuperLU. SuperLU package contains a set of
subroutines to solve sparse linear systems AX = B. Here A is a square, nonsingular, n × n sparse
matrix, and X and B are dense n × nrhs matrices, where nrhs is the number of right-hand sides
and solution vectors. Matrix A need not be symmetric or deﬁnite; indeed, SuperLU is particularly
appropriate for matrices with very unsymmetric structure.
The package uses LU decomposition with partial (or threshold) pivoting, and forward/back
substitutions. The columns of A may be preordered before factorization (either by the user or by
SuperLU); this preordering for sparsity is completely separate from the factorization. To improve
backward stability, we provide working precision iterative reﬁnement subroutines [2].
Routines
are also available to equilibrate the system, estimate the condition number, calculate the relative
backward error, and estimate error bounds for the reﬁned solutions. We also include a Matlab
MEX-ﬁle interface, so that our factor and solve routines can be called as alternatives to those built
into Matlab. The LU factorization routines can handle non-square matrices, but the triangular
solves are performed only for square matrices.
Starting from Version 4.0, we provide the incomplete factorization (ILU) routines which can be
used as preconditioners for iterative solvers [27].
The factorization algorithm uses a graph reduction technique to reduce graph traversal time in
the symbolic analysis. We exploit dense submatrices in the numerical kernel, and organize compu-
tational loops in a way that reduces data movement between levels of the memory hierarchy. The
resulting algorithm is highly eﬃcient on modern architectures. The performance gains are particu-
larly evident for large problems. There are “tuning parameters” to optimize the peak performance
as a function of cache size. For a detailed description of the algorithm, see reference [6].
SuperLU is implemented in ANSI C, and must be compiled with a standard ANSI C compiler.
It includes versions for both real and complex matrices, in both single and double precision. The
ﬁle names for the single-precision real version start with letter “s” (such as sgstrf.c); the ﬁle
names for the double-precision real version start with letter “d” (such as dgstrf.c); the ﬁle names
for the single-precision complex version start with letter “c” (such as cgstrf.c); the ﬁle names for
the double-precision complex version start with letter “z” (such as zgstrf.c).
16









s
u u
l u
l p
e u
l
l
r














19.00
21.00
21.00
0.63 21.00 −13.26 −13.26
0.57
23.58
7.58
5.00 21.00
0.63
0.57
−0.24
−0.77 34.20







Original matrix A
Factors F = L + U −I
s = 19, u = 21, p = 16, e = 5, r = 18, l = 12
Figure 2.1: A 5 × 5 matrix and its L and U factors.
2.2
How to call a SuperLU routine
As a simple example, let us consider how to solve a 5×5 sparse linear system AX = B, by calling a
driver routine dgssv(). Figure 2.1 shows matrix A, and its L and U factors. This sample program
is located in SuperLU/EXAMPLE/superlu.c.
The program ﬁrst initializes the three arrays, a[], asub[] and xa[], which store the nonzero
coeﬃcients of matrix A, their row indices, and the indices indicating the beginning of each column in
the coeﬃcient and row index arrays. This storage format is called compressed column format, also
known as Harwell-Boeing format [12]. Next, the two utility routines dCreate CompCol Matrix()
and dCreate Dense Matrix() are called to set up the matrix structures for A and B, respectively.
The routine set default options() sets the default values to the input options argument. This
controls how the matrix will be factorized and how the system will be solved. After calling the
SuperLU routine dgssv(), the B matrix is overwritten by the solution matrix X. In the end, all
the dynamically allocated data structures are de-allocated by calling various utility routines.
SuperLU can perform more general tasks, which will be explained later.
#include "slu_ddefs.h"
main(int argc, char *argv[])
{
/*
* Purpose
* =======
*
* This is the small 5x5 example used in the Sections 2 and 3 of the
* Users’ Guide to illustrate how to call a SuperLU routine, and the
* matrix data structures used by SuperLU.
*
*/
SuperMatrix A, L, U, B;
double
*a, *rhs;
double
s, u, p, e, r, l;
int
*asub, *xa;
int
*perm_r; /* row permutations from partial pivoting */
17


int
*perm_c; /* column permutation vector */
int
nrhs, info, i, m, n, nnz, permc_spec;
superlu_options_t options;
SuperLUStat_t stat;
/* Initialize matrix A. */
m = n = 5;
nnz = 12;
if ( !(a = doubleMalloc(nnz)) ) ABORT("Malloc fails for a[].");
if ( !(asub = intMalloc(nnz)) ) ABORT("Malloc fails for asub[].");
if ( !(xa = intMalloc(n+1)) ) ABORT("Malloc fails for xa[].");
s = 19.0; u = 21.0; p = 16.0; e = 5.0; r = 18.0; l = 12.0;
a[0] = s; a[1] = l; a[2] = l; a[3] = u; a[4] = l; a[5] = l;
a[6] = u; a[7] = p; a[8] = u; a[9] = e; a[10]= u; a[11]= r;
asub[0] = 0; asub[1] = 1; asub[2] = 4; asub[3] = 1;
asub[4] = 2; asub[5] = 4; asub[6] = 0; asub[7] = 2;
asub[8] = 0; asub[9] = 3; asub[10]= 3; asub[11]= 4;
xa[0] = 0; xa[1] = 3; xa[2] = 6; xa[3] = 8; xa[4] = 10; xa[5] = 12;
/* Create matrix A in the format expected by SuperLU. */
dCreate_CompCol_Matrix(&A, m, n, nnz, a, asub, xa, SLU_NC, SLU_D, SLU_GE);
/* Create right-hand side matrix B. */
nrhs = 1;
if ( !(rhs = doubleMalloc(m * nrhs)) ) ABORT("Malloc fails for rhs[].");
for (i = 0; i < m; ++i) rhs[i] = 1.0;
dCreate_Dense_Matrix(&B, m, nrhs, rhs, m, SLU_DN, SLU_D, SLU_GE);
if ( !(perm_r = intMalloc(m)) ) ABORT("Malloc fails for perm_r[].");
if ( !(perm_c = intMalloc(n)) ) ABORT("Malloc fails for perm_c[].");
/* Set the default input options. */
set_default_options(&options);
options.ColPerm = NATURAL;
/* Initialize the statistics variables. */
StatInit(&stat);
/* Solve the linear system. */
dgssv(&options, &A, perm_c, perm_r, &L, &U, &B, &stat, &info);
dPrint_CompCol_Matrix("A", &A);
dPrint_CompCol_Matrix("U", &U);
dPrint_SuperNode_Matrix("L", &L);
print_int_vec("\nperm_r", m, perm_r);
18


/* De-allocate storage */
SUPERLU_FREE (rhs);
SUPERLU_FREE (perm_r);
SUPERLU_FREE (perm_c);
Destroy_CompCol_Matrix(&A);
Destroy_SuperMatrix_Store(&B);
Destroy_SuperNode_Matrix(&L);
Destroy_CompCol_Matrix(&U);
StatFree(&stat);
}
2.3
Matrix data structures
SuperLU uses a principal data structure SuperMatrix (deﬁned in SRC/supermatrix.h) to represent
a general matrix, sparse or dense. Figure 2.2 gives the speciﬁcation of the SuperMatrix structure.
The SuperMatrix structure contains two levels of ﬁelds. The ﬁrst level deﬁnes all the properties
of a matrix which are independent of how it is stored in memory. In particular, it speciﬁes the
following three orthogonal properties: storage type (Stype) indicates the type of the storage scheme
in *Store; data type (Dtype) encodes the four precisions; mathematical type (Mtype) speciﬁes some
mathematical properties. The second level (*Store) points to the actual storage used to store the
matrix. We associate with each Stype XX a storage format called XXformat, such as NCformat,
SCformat, etc.
The SuperMatrix type so deﬁned can accommodate various types of matrix structures and
appropriate operations to be applied on them, although currently SuperLU implements only a
subset of this collection. Speciﬁcally, matrices A, L, U, B, and X can have the following types:
A
L
U
B
X
Stype
SLU NC or SLU NR
SLU SC
SLU NC
SLU DN
SLU DN
Dtype1
any
any
any
any
any
Mtype
SLU GE
SLU TRLU
SLU TRU
SLU GE
SLU GE
In what follows, we illustrate the storage schemes deﬁned by Stype. Following C’s convention,
all array indices and locations below are zero-based.
• A may have storage type SLU NC or SLU NR. The SLU NC format is the same as the Harwell-
Boeing sparse matrix format [12], that is, the compressed column storage.
typedef struct {
int
nnz;
/* number of nonzeros in the matrix */
void *nzval;
/* array of nonzero values packed by column */
int
*rowind; /* array of row indices of the nonzeros */
int
*colptr; /* colptr[j] stores the location in nzval[] and rowind[]
which starts column j. It has ncol+1 entries,
1Dtype can be one of SLU S, SLU D, SLU C or SLU Z.
19


typedef struct {
Stype_t Stype; /* Storage type: indicates the storage format of *Store. */
Dtype_t Dtype; /* Data type. */
Mtype_t Mtype; /* Mathematical type */
int
nrow;
/* number of rows */
int
ncol;
/* number of columns */
void *Store;
/* pointer to the actual storage of the matrix */
} SuperMatrix;
typedef enum {
SLU_NC,
/* column-wise, not supernodal */
SLU_NR,
/* row-wise, not supernodal */
SLU_SC,
/* column-wise, supernodal */
SLU_SR,
/* row-wise, supernodal */
SLU_NCP,
/* column-wise, not supernodal, permuted by columns
(After column permutation, the consecutive columns of
nonzeros may not be stored contiguously. */
SLU_DN,
/* Fortran style column-wise storage for dense matrix */
SLU_NR_loc
/* distributed compressed row format */
} Stype_t;
typedef enum {
SLU_S,
/* single */
SLU_D,
/* double */
SLU_C,
/* single-complex */
SLU_Z
/* double-complex */
} Dtype_t;
typedef enum {
SLU_GE,
/* general */
SLU_TRLU,
/* lower triangular, unit diagonal */
SLU_TRUU,
/* upper triangular, unit diagonal */
SLU_TRL,
/* lower triangular */
SLU_TRU,
/* upper triangular */
SLU_SYL,
/* symmetric, store lower half */
SLU_SYU,
/* symmetric, store upper half */
SLU_HEL,
/* Hermitian, store lower half */
SLU_HEU
/* Hermitian, store upper half */
} Mtype_t;
Figure 2.2: SuperMatrix data structure.
20


and colptr[ncol] = nnz. */
} NCformat;
The SLU NR format is the compressed row storage deﬁned below.
typedef struct {
int
nnz;
/* number of nonzeros in the matrix */
void *nzval;
/* array of nonzero values packed by row */
int
*colind; /* array of column indices of the nonzeros */
int
*rowptr; /* rowptr[j] stores the location in nzval[] and colind[]
which starts row j. It has nrow+1 entries,
and rowptr[nrow] = nnz. */
} NRformat;
The factorization and solve routines in SuperLU are designed to handle column-wise storage
only. If the input matrix A is in row-oriented storage, i.e., in SLU NR format, then the driver
routines (dgssv() and dgssvx()) actually perform the LU decomposition on AT , which is
column-wise, and solve the system using the LT and U T factors. The data structures holding
L and U on output are diﬀerent (swapped) from the data structures you get from column-wise
input. For more detailed descriptions about this process, please refer to the leading comments
of the routines dgssv() and dgssvx().
Alternatively, the users may call a utility routine dCompRow to CompCol() to convert the
input matrix in SLU NR format to another matrix in SLU NC format, before calling SuperLU.
The deﬁnition of this routine is
void dCompRow_to_CompCol(int m, int n, int nnz,
double *a, int *colind, int *rowptr,
double **at, int **rowind, int **colptr);
This conversion takes time proportional to the number of nonzeros in A. However, it requires
storage for a separate copy of matrix A.
• L is a supernodal matrix with the storage type SLU SC. Due to the supernodal structure, L
is in fact stored as a sparse block lower triangular matrix [6].
typedef struct {
int
nnz;
/* number of nonzeros in the matrix */
int
nsuper;
/* index of the last supernode */
void *nzval;
/* array of nonzero values packed by column */
int
*nzval_colptr; /* nzval_colptr[j] stores the location in
nzval[] which starts column j */
int
*rowind;
/* array of compressed row indices of
rectangular supernodes */
int
*rowind_colptr;/* rowind_colptr[j] stores the location in
rowind[] which starts column j */
int
*col_to_sup;
/* col_to_sup[j] is the supernode number to
21


which column j belongs */
int
*sup_to_col;
/* sup_to_col[s] points to the starting column
of the s-th supernode */
} SCformat;
• Both B and X are stored as conventional two-dimensional arrays in column-major order, with
the storage type SLU DN.
typedef struct {
int lda;
/* leading dimension */
void *nzval; /* array of size lda-by-ncol to represent
a dense matrix */
} DNformat;
Figure 2.3 shows the data structures for the example matrices in Figure 2.1.
For a description of NCPformat, see section 2.5.1.
2.4
Options argument
The options argument is the input argument to control the behaviour of the library. The user
can tell the solver how the linear systems should be solved based on some known characteristics of
the system. For example, for diagonally dominant matrices, choosing the diagonal pivots ensures
stability; there is no need for numerical pivoting (i.e., Pr can be an Identity matrix). In another
situation where a sequence of matrices with the same sparsity pattern need be factorized, the
column permutation Pc (and also the row permutation Pr, if the numerical values are similar) need
be computed only once, and reused thereafter. In these cases, the solvers’ performance can be much
improved over using the default settings. Options is implemented as a C structure containing the
following ﬁelds:
• Fact
Speciﬁes whether or not the factored form of the matrix A is supplied on entry, and if not,
how the matrix A will be factorized base on the previous history, such as factor from scratch,
reuse Pc and/or Pr, or reuse the data structures of L and U. fact can be one of:
– DOFACT: the matrix A will be factorized from scratch.
– SamePattern: the matrix A will be factorized assuming that a factorization of a ma-
trix with the same sparsity pattern was performed prior to this one. Therefore, this
factorization will reuse column permutation vector perm c.
– SampPattern SameRowPerm: the matrix A will be factorized assuming that a factoriza-
tion of a matrix with the same sparsity pattern and similar numerical values was per-
formed prior to this one. Therefore, this factorization will reuse both row and column
permutation vectors perm r and perm c, both row and column scaling factors Dr and
Dc, and the distributed data structure set up from the previous symbolic factorization.
– FACTORED: the factored form of A is input.
22


• A = { Stype = SLU_NC; Dtype = SLU_D; Mtype = SLU_GE; nrow = 5; ncol = 5;
*Store = { nnz = 12;
nzval = [ 19.00, 12.00, 12.00, 21.00, 12.00, 12.00, 21.00,
16.00, 21.00, 5.00, 21.00, 18.00 ];
rowind = [ 0, 1, 4, 1, 2, 4, 0, 2, 0, 3, 3, 4 ];
colptr = [ 0, 3, 6, 8, 10, 12 ];
}
}
• U = { Stype = SLU_NC; Dtype = SLU_D; Mtype = SLU_TRU; nrow = 5; ncol = 5;
*Store = { nnz = 11;
nzval = [ 21.00, -13.26, 7.58, 21.00 ];
rowind = [ 0, 1, 2, 0 ];
colptr = [ 0, 0, 0, 1, 4, 4 ];
}
}
• L = { Stype = SLU_SC; Dtype = SLU_D; Mtype = SLU_TRLU; nrow = 5; ncol = 5;
*Store = { nnz = 11;
nsuper = 2;
nzval = [ 19.00, 0.63, 0.63, 21.00, 0.57, 0.57, -13.26,
23.58, -0.24, 5.00, -0.77, 21.00, 34.20 ];
nzval_colptr = [ 0 3, 6, 9, 11, 13 ];
rowind = [ 0, 1, 4, 1, 2, 4, 3, 4 ];
rowind_colptr = [ 0, 3, 6, 6, 8, 8 ];
col_to_sup = [ 0, 1, 1, 2, 2 ];
sup_to_col = [ 0, 1, 3, 5 ];
}
}
Figure 2.3: The data structures for a 5 × 5 matrix and its LU factors, as represented in the
SuperMatrix data structure. Zero-based indexing is used.
23


• Equil { YES | NO }
Speciﬁes whether to equilibrate the system (scale A’s rows and columns to have unit norm).
• ColPerm
Speciﬁes how to permute the columns of the matrix for sparsity preservation.
– NATURAL: natural ordering.
– MMD ATA: minimum degree ordering on the structure of AT A.
– MMD AT PLUS A: minimum degree ordering on the structure of AT + A.
– COLAMD: approximate minimum degree column ordering
– MY PERMC: use the ordering given in perm c input by the user.
• Trans { NOTRANS | TRANS | CONJ }
Speciﬁes whether to solve the transposed system.
• IterRefine
Speciﬁes whether to perform iterative reﬁnement, and in what precision to compute the
residual.
– NO: no iterative reﬁnement
– SINGLE: perform iterative reﬁnement in single precision
– DOUBLE: perform iterative reﬁnement in double precision
– EXTRA: perform iterative reﬁnement in extra precision
• DiagPivotThresh [0.0, 1.0]
Speciﬁes the threshold used for a diagonal entry to be an acceptable pivot.
• SymmetricMode { YES | NO }
Speciﬁes whether to use the symmetric mode. Symmetric mode gives preference to diagonal
pivots, and uses an (AT + A)-based column permutation algorithm.
• PivotGrowth { YES | NO }
Speciﬁes whether to compute the reciprocal pivot growth.
• ConditionNumber { YES | NO }
Speciﬁes whether to compute the reciprocal condition number.
• RowPerm (only for ILU or SuperLU DIST)
Speciﬁes whether to permute the rows of the original matrix.
– NO: not to permute the rows
– LargeDiag MC64: use a serial, weighted bipartite matching algorithm implemented in
MC64 to permute the rows to make the diagonal large relative to the oﬀ-diagonal [11].
– LargeDiag AWPM: use a parallel, approximate weighted bipartite matching algorithm
implemented in CombBLAS to permute the rows to make the diagonal large relative to
the oﬀ-diagonal [3].
– MY PERMR: use the permutation given by the user
24


• ILU DropRule
Speciﬁes the dropping rule for ILU: ( Default: DROP BASIC | DROP AREA )
– DROP BASIC: Basic dropping rule, supernodal based ILUTP(τ).
– DROP PROWS: Supernodal based ILUTP(p, τ), p = γ · nnz(A)/n.
– DROP COLUMN: Variant of ILUTP(p, τ), for j-th column, p = γ · nnz(A(:, j)).
– DROP AREA: Variation of ILUTP, for j-th column, use nnz(F(:, 1 : j))/nnz(A(:, 1 : j)) to
control memory.
– DROP DYNAMIC: Dynamically adjust the threshold τ during factorizaion:
If nnz(L(:, 1 : j))/nnz(A(:, 1 : j)) > γ, τL(j) := min(τ0, τL(j −1)·2); Otherwise τL(j) :=
max(τ0, τL(j −1)/2). τU(j) uses the similar rule.
– DROP INTERP: Compute the second dropping threshold by interpolation instead of quick
select (default). In this case, the actual ﬁll ratio is not guaranteed to be smaller than
gamma.
• ILU DropTol [0.0, 1.0]
Speciﬁes the numerical dropping threshold for ILU.
• ILU FillFactor (≥1.0)
Speciﬁes the expected ﬁll ratio upper bound, γ, for ILU.
• ILU MILU { SILU | SMILU 1 | SMILU 2 | SMILU 3 }
Speciﬁes which version of modiﬁed ILU to use.
• PrintStat { YES | NO }
Speciﬁes whether to print the solver’s statistics.
The routine set default options() sets the following default values:
Fact
= DOFACT
/* factor from scratch */
Equil
= YES
ColPerm
= COLAMD
Trans
= NOTRANS
IterRefine
= NOREFINE
DiagPivotThresh
= 1.0
/* partial pivoting */
SymmetricMode
= NO
PivotGrowth
= NO;
ConditionNumber
= NO;
PrintStat
= YES
To use the ILU routines, such as dgsitrf(), the user should call ilu set default options() to
set the default values (set default options() is ﬁrst called in this routine prior to the following):
DiagPivotThresh
= 0.1
/* partial pivoting */
RowPerm
= LargeDiag
ILU_DropRule
= DROP_BASIC | DROP_AREA;
ILU_DropTol
= 1e-4;
25


ILU_FillFactor
= 10.0;
ILU_Norm
= INF_NORM;
ILU_MILU
= SILU;
/* not to use MILU */
ILU_FillTol
= 1e-2;
The other possible values for each ﬁeld are documented in the source code SRC/slu util.h.
The users can reset each default value according to their needs.
2.5
Permutations
Two permutation matrices are involved in the solution process. In fact, the actual factorization we
perform is PrAP T
c = LU, where Pr is determined from partial pivoting (with a threshold pivoting
option), and Pc is a column permutation chosen either by the user or SuperLU, usually to make the
L and U factors as sparse as possible. Pr and Pc are represented by two integer vectors perm r[]
and perm c[], which are the permutations of the integers (0 : m −1) and (0 : n −1), respectively.
2.5.1
Ordering for sparsity
Column reordering for sparsity is completely separate from the LU factorization.
The column
permutation Pc should be applied before calling the factorization routine dgstrf(). In principle,
any ordering heuristic used for symmetric matrices can be applied to AT A (or A+AT if the matrix
is nearly structurally symmetric) to obtain Pc. Currently, we provide the following ordering options
through options argument. The options.ColPerm ﬁeld can take the following values:
• NATURAL: use natural ordefring (i.e., Pc = I).
• MMD AT PLUS A: use minimum degree ordering on the structure of AT + A.
• MMD ATA: use minimum degree ordering on the structure of AT A.
• COLAMD: use approximate minimum degree column ordering.
• MY PERMC: use the ordering given in the permutation vector perm c[], which is input by the
user.
If options.ColPerm is set to the last value, the library will use the permutation vector perm c[]
as an input, which may be obtained from any other ordering algorithm. For example, the nested-
dissection type of ordering codes include Metis [20], Chaco [18] and Scotch [31].
Alternatively, the users can provide their own column permutation vector.
For example, it
may be an ordering suitable for the underlying physical problem. Both driver routines dgssv and
dgssvx take perm c[] as an input argument.
After permutation Pc is applied to A, we use SLU NCP format to represent the permuted matrix
AP T
c , in which the consecutive columns of nonzeros may not be stored contiguously in memory.
Therefore, we need two separate arrays of pointers, colbeg[] and colend[], to indicate the be-
ginning and end of each column in nzval[] and rowind[].
typedef struct {
int
nnz;
/* number of nonzeros in the matrix */
26


void *nzval;
/* array of nonzero values, packed by column */
int
*rowind; /* array of row indices of the nonzeros */
int
*colbeg; /* colbeg[j] points to the location in nzval[] and rowind[]
which starts column j */
int
*colend; /* colend[j] points to one past the location in nzval[]
and rowind[] which ends column j */
} NCPformat;
2.5.2
Partial pivoting with threshold
We have included a threshold pivoting parameter u ∈[0, 1] to control numerical stability. The
user can choose to use a row permutation obtained from a previous factorization.
(The argu-
ment options.Fact = SamePattern SameRowPerm should be passed to the factorization routine
dgstrf().) The pivoting subroutine dpivotL() checks whether this choice of pivot satisﬁes the
threshold; if not, it will try the diagonal element. If neither of the above satisﬁes the threshold,
the maximum magnitude element in the column will be used as the pivot. The pseudo-code of the
pivoting policy for column j is given below.
(1)
compute thresh = u |amj|, where |amj| = maxi≥j |aij|;
(2)
if user speciﬁes pivot row k and |akj| ≥thresh and akj ̸= 0 then
pivot row = k;
else if |ajj| ≥thresh and ajj ̸= 0 then
pivot row = j;
else
pivot row = m;
endif;
Two special values of u result in the following two strategies:
• u = 0.0: either use user-speciﬁed pivot order if available, or else use diagonal pivot;
• u = 1.0: classical partial pivoting.
2.6
Symmetric Mode
In many applications, matrix A may be diagonally dominant or nearly so. In this case, pivoting on
the diagonal is suﬃcient for stability and is preferable for sparsity to oﬀ-diagonal pivoting. To do
this, the user can set a small (less-than-one) diagonal pivot threshold (e.g., 0.0, 0.01) and choose
an (AT + A)–based column permutation algorithm. We call this setting symmetric mode. In this
case, the options.SymmetricMode = YES must be set.
Note that, when a diagonal entry is smaller than the threshold, the code will still choose an
oﬀ-diagonal pivot. That is, the row permutation Pr may not be Identity. Please refer to [24] for
more discussion on the symmetric mode.
27


2.7
Incomplete LU factorization (ILU) preconditioner
Starting from SuperLU version 4.0, we provide the ILU routines to be used as preconditioners
for iterative solvers. Our ILU method can be considered to be a variant of the ILUTP method
originally proposed by Saad [33], which combines a dual dropping strategy with numerical pivoting
(“T” stands for threshold, and “P” stands for pivoting). We adapted the classic dropping strategies
of ILUTP in order to incorporate supernode structures and to accommodate dynamic supernodes
due to partial pivoting. For the secondary dropping strategy, we proposed an area-based ﬁll control
method, which is more ﬂexible and numerically robust than the traditional column-based scheme.
Furthermore, we incorporated several heuristics for adaptively modifying various threshold param-
eters as the factorization proceeds, which improves the robustness of the algorithm. The details
can be found in [27].
2.8
Memory management for L and U
In the sparse LU algorithm, the amount of space needed to hold the data structures of L and U
cannot be accurately predicted prior to the factorization. The dynamically growing arrays include
those for the nonzero values (nzval[]) and the compressed row indices (rowind[]) of L, and for
the nonzero values (nzval[]) and the row indices (rowind[]) of U.
Two alternative memory models are presented to the user:
• system-level – based on C’s dynamic allocation capability (malloc/free);
• user-level – based on a user-supplied work[] array of size lwork (in bytes). This is similar
to Fortran-style handling of work space. Work[] is organized as a two-ended stack, one end
holding the L and U data structures, the other end holding the auxiliary arrays of known
size.
Except for the diﬀerent ways to allocate/deallocate space, the logical view of the memory
organization is the same for both schemes. Now we describe the policies in the memory module.
At the outset of the factorization, we guess there will be FILL*nnz(A) ﬁlls in the factors and
allocate corresponding storage for the above four arrays, where nnz(A) is the number of nonzeros in
original matrix A, and FILL is an integer, say 20. (The value of FILL can be set in an inquiry function
sp ienv(), see section 2.11.3.) If this initial request exceeds the physical memory constraint, the
FILL factor is repeatedly reduced, and attempts are made to allocate smaller arrays, until the initial
allocation succeeds.
During the factorization, if any array size exceeds the allocated bound, we expand it as follows.
We ﬁrst allocate a chunk of new memory of size EXPAND times the old size, then copy the existing
data into the new memory, and then free the old storage. The extra copying is necessary, because the
factorization algorithm requires that each of the aforementioned four data structures be contiguous
in memory.
The values of FILL and EXPAND are normally set to 20 and 1.5, respectively.
See
xmemory.c for details.
After factorization, we do not garbage-collect the extra space that may have been allocated.
Thus, there will be external fragmentation in the L and U data structures. The settings of FILL and
EXPAND should take into account the trade-oﬀbetween the number of expansions and the amount
of fragmentation.
28


Arrays of known size, such as various column pointers and working arrays, are allocated just
once. All dynamically-allocated working arrays are freed after factorization.
2.9
User-callable routines
The naming conventions, calling sequences and functionality of these routines mimic the corre-
sponding LAPACK software [1]. In the routine names, such as dgstrf, we use the two letters GS to
denote general sparse matrices. The leading letterx stands for S, D, C, or Z, specifying the data
type.
2.9.1
Driver routines
We provide two types of driver routines for solving systems of linear equations. The driver routines
can handle both column- and row-oriented storage schemes.
• A simple driver dgssv(), which solves the system AX = B by factorizing A and overwriting
B with the solution X.
• An expert driver dgssvx(), which, in addition to the above, also performs the following
functions (some of them optionally):
– solve AT X = B;
– equilibrate the system (scale A’s rows and columns to have unit norm) if A is poorly
scaled;
– estimate the condition number of A, check for near-singularity, and check for pivot
growth;
– reﬁne the solution and compute forward and backward error bounds.
• An expert driver dgsisx(), which gives the approximate solutions of linear equations AX = B
or AT X = B, using the ILU factorization from dgsitrf(). An estimation of the condition
number is provide, and the pivot growth is computed.
These driver routines cover all the functionality of the computational routines. We expect that
most users can simply use these driver routines to fulﬁll their tasks with no need to bother with
the computational routines.
2.9.2
Computational routines
The users can invoke the following computational routines, instead of the driver routines, to directly
control the behavior of SuperLU. The computational routines can only handle column-oriented
storage.
• dgstrf(): Factorize.
This implements the ﬁrst-time factorization, or later re-factorization with the same nonzero
pattern. In re-factorizations, the code has the ability to use the same column permutation Pc
and row permutation Pr obtained from a previous factorization. The input argument options
contains several scalar arguments to control how the LU decomposition and the numerical
pivoting should be performed. dgstrf() can handle non-square matrices.
29


• dgsitrf(): ILU.
This implements the incomplete LU factorization The input argument options contains sev-
eral scalar arguments to control how the incomplete facotirzation and the numerical pivoting
should be performed.
• dgstrs(): Triangular solve.
This takes the L and U triangular factors, the row and column permutation vectors, and the
right-hand side to compute a solution matrix X of AX = B or AT X = B.
• dgscon(): Estimate condition number.
Given the matrix A and its factors L and U, this estimates the condition number in the
one-norm or inﬁnity-norm. The algorithm is due to Hager and Higham [19], and is the same
as CONDEST in sparse Matlab.
• dgsequ()/dlaqgs(): Equilibrate.
dgsequ ﬁrst computes the row and column scalings Dr and Dc which would make each row
and each column of the scaled matrix DrADc have equal norm. dlaqgs then applies them to
the original matrix A if it is indeed badly scaled. The equilibrated A overwrites the original
A.
• dgsrfs(): Reﬁne solution.
Given A, its factors L and U, and an initial solution X, this does iterative reﬁnement, using
the same precision as the input data. It also computes forward and backward error bounds
for the reﬁned solution.
2.9.3
Utility routines
The utility routines can help users create and destroy the SuperLU matrices easily. These routines
reside in two places: SRC/util.c contains the routines that are precision-independent;
SRC/{s,d,c,z}util.c contains the routines dependent on precision. Here, we list the prototypes
of these routines.
/* Create a supermatrix in compressed column format. A is the output. */
dCreate_CompCol_Matrix(SuperMatrix *A, int m, int n, int nnz,
double *nzval, int *rowind, int *colptr,
Stype_t stype, Dtype_t dtype, Mtype_t mtype);
/* Create a supermatrix in compressed row format. A is the output. */
dCreate_CompRow_Matrix(SuperMatrix *A, int m, int n, int nnz,
double *nzval, int *colind, int *rowptr,
Stype_t stype, Dtype_t dtype, Mtype_t mtype);
/* Copy matrix A into matrix B, both in compressed column format. */
dCopy_CompCol_Matrix(SuperMatrix *A, SuperMatrix *B);
/* Create a supermatrix in dense format. X is the output.*/
30


dCreate_Dense_Matrix(SuperMatrix *X, int m, int n, double *x, int ldx,
Stype_t stype, Dtype_t dtype, Mtype_t mtype);
/* Create a supermatrix in supernodal format. L is the output. */
dCreate_SuperNode_Matrix(SuperMatrix *L, int m, int n, int nnz,
double *nzval, int *nzval_colptr, int *rowind,
int *rowind_colptr, int *col_to_sup, int *sup_to_col,
Stype_t stype, Dtype_t dtype, Mtype_t mtype);
/* Convert the compressed row fromat to the compressed column format. */
dCompRow_to_CompCol(int m, int n, int nnz,
double *a, int *colind, int *rowptr,
double **at, int **rowind, int **colptr);
/* Print a supermatrix in compressed column format. */
dPrint_CompCol_Matrix(char *what, SuperMatrix *A);
/* Print a supermatrix in supernodal format. */
dPrint_SuperNode_Matrix(char *what, SuperMatrix *A);
/* Print a supermatrix in dense format. */
dPrint_Dense_Matrix(char *what, SuperMatrix *A);
/* Deallocate the storage structure *Store. */
Destroy_SuperMatrix_Store(SuperMatrix *A);
/* Deallocate the supermatrix structure in compressed column format. */
Destroy_CompCol_Matrix(SuperMatrix *A)
/* Deallocate the supermatrix structure in supernodal format. */
Destroy_SuperNode_Matrix(SuperMatrix *A)
/* Deallocate the supermatrix structure in permuted compressed column format. */
Destroy_CompCol_Permuted(SuperMatrix *A)
/* Deallocate the supermatrix structure in dense format. */
Destroy_Dense_Matrix(SuperMatrix *A)
2.10
Matlab interface
In the SuperLU/MATLAB subdirectory, we have developed a set of MEX-ﬁles interface to Matlab.
Typing make in this directory produces executables to be invoked in Matlab. The current Makefile
is set up so that the MEX-ﬁles are compatible with Matlab Version 5. The user should edit Makefile
for Matlab Version 4 compatibility. Right now, only the factor routine dgstrf() and the simple
driver routine dgssv() are callable by invoking superlu and lusolve in Matlab, respectively.
31


Superlu and lusolve correspond to the two Matlab built-in functions lu and \ . In Matlab, when
you type
help superlu
you will ﬁnd the following description about superlu’s functionality and how to use it.
SUPERLU : Supernodal LU factorization
Executive summary:
[L,U,p] = superlu(A)
is like [L,U,P] = lu(A), but faster.
[L,U,prow,pcol] = superlu(A)
preorders the columns of A by min degree,
yielding A(prow,pcol) = L*U.
Details and options:
With one input and two or three outputs, SUPERLU has the same effect as LU,
except that the pivoting permutation is returned as a vector, not a matrix:
[L,U,p] = superlu(A) returns unit lower triangular L, upper triangular U,
and permutation vector p with A(p,:) = L*U.
[L,U] = superlu(A) returns permuted triangular L and upper triangular U
with A = L*U.
With a second input, the columns of A are permuted before factoring:
[L,U,prow] = superlu(A,psparse) returns triangular L and U and permutation
prow with A(prow,psparse) = L*U.
[L,U] = superlu(A,psparse) returns permuted triangular L and triangular U
with A(:,psparse) = L*U.
Here psparse will normally be a user-supplied permutation matrix or vector
to be applied to the columns of A for sparsity.
COLMMD is one way to get
such a permutation; see below to make SUPERLU compute it automatically.
(If psparse is a permutation matrix, the matrix factored is A*psparse’.)
With a fourth output, a column permutation is computed and applied:
[L,U,prow,pcol] = superlu(A,psparse)
returns triangular L and U and
permutations prow and pcol with A(prow,pcol) = L*U.
Here psparse is a user-supplied column permutation for sparsity,
and the matrix factored is A(:,psparse) (or A*psparse’ if the
input is a permutation matrix).
Output pcol is a permutation
that first performs psparse, then postorders the etree of the
column intersection graph of A.
The postorder does not affect
sparsity, but makes supernodes in L consecutive.
[L,U,prow,pcol] = superlu(A,0) is the same as ... = superlu(A,I); it does
not permute for sparsity but it does postorder the etree.
32


[L,U,prow,pcol] = superlu(A) is the same as ... = superlu(A,colmmd(A));
it uses column minimum degree to permute columns for sparsity,
then postorders the etree and factors.
For a description about lusolve’s functionality and how to use it, you can type
help lusolve
LUSOLVE : Solve linear systems by supernodal LU factorization.
x = lusolve(A, b) returns the solution to the linear system A*x = b,
using a supernodal LU factorization that is faster than Matlab’s
builtin LU.
This m-file just calls a mex routine to do the work.
By default, A is preordered by column minimum degree before factorization.
Optionally, the user can supply a desired column ordering:
x = lusolve(A, b, pcol) uses pcol as a column permutation.
It still returns x = A\b, but it factors A(:,pcol) (if pcol is a
permutation vector) or A*Pcol (if Pcol is a permutation matrix).
x = lusolve(A, b, 0) suppresses the default minimum degree ordering;
that is, it forces the identity permutation on columns.
Two M-ﬁles trysuperlu.m and trylusolve.m are written to test the correctness of superlu
and lusolve. In addition to testing the residual norms, they also test the function invocations
with various number of input/output arguments.
2.11
Installation
2.11.1
File structure
The top level SuperLU/ directory is structured as follows:
SuperLU/README
instructions on installation
SuperLU/CBLAS/
needed BLAS routines in C, not necessarily fast
SuperLU/EXAMPLE/
example programs
SuperLU/INSTALL/
test machine dependent parameters; this Users’ Guide
SuperLU/MAKE_INC/ sample machine-specific make.inc files
SuperLU/MATLAB/
Matlab mex-file interface
SuperLU/SRC/
C source code, to be compiled into the superlu.a library
SuperLU/TESTING/
driver routines to test correctness
SuperLU/Makefile
top level Makefile that does installation and testing
SuperLU/make.inc
compiler, compile flags, library definitions and C
preprocessor definitions, included in all Makefiles
Before installing the package, you may need to edit SuperLU/make.inc for your system. This
make include ﬁle is referenced inside each of the Makefiles in the various subdirectories. As a
33


result, there is no need to edit the Makefiles in the subdirectories. All information that is machine
speciﬁc has been deﬁned in make.inc.
Sample machine-speciﬁc make.inc are provided in the MAKE INC/ subdirectory for several sys-
tems, including IBM RS/6000, DEC Alpha, SunOS 4.x, SunOS 5.x (Solaris), HP-PA and SGI Iris
4.x. When you have selected the machine on which you wish to install SuperLU, you may copy the
appropriate sample include ﬁle (if one is present) into make.inc. For example, if you wish to run
SuperLU on an IBM RS/6000, you can do:
cp MAKE INC/make.rs6k make.inc
For systems other than those listed above, slight modiﬁcations to the make.inc ﬁle will need
to be made. In particular, the following three items should be examined:
1. The BLAS library.
If there is a BLAS library available on your machine, you may deﬁne the following in make.inc:
BLASDEF = -DUSE VENDOR BLAS
BLASLIB = <BLAS library you wish to link with>
The CBLAS/ subdirectory contains the part of the C BLAS needed by the SuperLU package.
However, these codes are intended for use only if there is no faster implementation of the
BLAS already available on your machine. In this case, you should do the following:
1) In make.inc, undeﬁne (comment out) BLASDEF, deﬁne:
BLASLIB = ../blas$(PLAT).a
2) In the SuperLU/ directory, type:
make blaslib
to make the BLAS library from the routines in the CBLAS/ subdirectory.
2. C preprocessor deﬁnition CDEFS.
In the header ﬁle SRC/Cnames.h, we use macros to determine how C routines should be named
so that they are callable by Fortran.2 The possible options for CDEFS are:
• -DAdd : Fortran expects a C routine to have an underscore postﬁxed to the name;
• -DNoChange: Fortran expects a C routine name to be identical to that compiled by C;
• -DUpCase: Fortran expects a C routine name to be all uppercase.
3. The Matlab MEX-ﬁle interface.
The MATLAB/ subdirectory includes Matlab C MEX-ﬁles, so that our factor and solve routines
can be called as alternatives to those built into Matlab. In the ﬁle SuperLU/make.inc, deﬁne
MATLAB to be the directory in which Matlab is installed on your system, for example:
MATLAB = /usr/local/matlab
At the SuperLU/ directory, type:
make matlabmex
to build the MEX-ﬁle interface. After you have built the interface, you may go to the MATLAB/
subdirectory to test the correctness by typing (in Matlab):
2Some vendor-supplied BLAS libraries do not have C interfaces. So the re-naming is needed in order for the
SuperLU BLAS calls (in C) to interface with the Fortran-style BLAS.
34


Matrix type
Description
0
sparse matrix g10
1
diagonal
2
upper triangular
3
lower triangular
4
random, κ = 2
5
ﬁrst column zero
6
last column zero
7
last n/2 columns zero
8
random, κ =
p
0.1/ε
9
random, κ = 0.1/ε
10
scaled near underﬂow
11
scaled near overﬂow
Table 2.1: Properties of the test matrices. ε is
the machine epsilon and κ is the condition num-
ber of matrix A. Matrix types with one or more
columns set to zero are used to test the error
return codes.
Test Type
Test ratio
Routines
0
||LU −A||/(n||A||ε)
dgstrf
1
||b −Ax||/(||A|| ||x||ε)
dgssv, dgssvx
2
||x −x∗||/(||x∗||κε)
dgssvx
3
||x −x∗||/(||x∗|| FERR)
dgssvx
4
BERR/ε
dgssvx
Table 2.2: Types of tests. x∗is the true solution,
FERR is the error bound, and BERR is the
backward error.
trysuperlu
trylusolve
A Makefile is provided in each subdirectory. The installation can be done completely auto-
matically by simply typing make at the top level.
2.11.2
Testing
The test programs in SuperLU/INSTALL subdirectory test two routines:
• slamch()/dlamch() determines properties of the ﬂoating-point arithmetic at run-time (both
single and double precision), such as the machine epsilon, underﬂow threshold, overﬂow
threshold, and related parameters;
• SuperLU timer () returns the time in seconds used by the process. This function may need
to be modiﬁed to run on your machine.
The test programs in the SuperLU/TESTING subdirectory are designed to test all the functions of
the driver routines, especially the expert drivers. The Unix shell script ﬁles xtest.csh are used to
invoke tests with varying parameter settings. The input matrices include an actual sparse matrix
SuperLU/EXAMPLE/g10 of dimension 100 × 100,3 and numerous matrices with special properties
from the LAPACK test suite. Table 2.1 describes the properties of the test matrices.
For each command line option speciﬁed in dtest.csh, the test program ddrive reads in or
generates an appropriate matrix, calls the driver routines, and computes a number of test ratios
3Matrix g10 is ﬁrst generated with the structure of the 10-by-10 ﬁve-point grid, and random numerical values.
The columns are then permuted by COLMMD ordering from Matlab.
35


to verify that each operation has performed correctly. If the test ratio is smaller than a preset
threshold, the operation is considered to be correct. Each test matrix is subject to the tests listed
in Table 2.2.
Let r be the residual r = b −Ax, and let mi be the number of nonzeros in row i of A. Then
the componentwise backward error BERR and forward error FERR [1] are calculated by:
BERR = max
i
|r|i
(|A| |x| + |b|)i
.
FERR = || |A−1| f ||∞
||x||∞
.
Here, f is a nonnegative vector whose components are computed as fi = |r|i + mi ε (|A| |x| + |b|)i,
and the norm in the numerator is estimated using the same subroutine used for estimating the
condition number. BERR measures the smallest relative perturbation one can make to each entry
of A and of b so that the computed solution is an exact solution of the perturbed problem. FERR
is an estimated bound on the error ∥x∗−x∥∞/∥x∥∞, where x∗is the true solution. For further
details on error analysis and error bounds estimation, see [1, Chapter 4] and [2].
2.11.3
Performance-tuning parameters
SuperLU chooses such machine-dependent parameters as block size by calling an inquiry function
sp ienv(), which may be set to return diﬀerent values on diﬀerent machines. The declaration of
this function is
int sp ienv(int ispec);
Ispec speciﬁes the parameter to be returned, (See reference [6] for their deﬁnitions.)
ispec = 1: the panel size (w)
= 2: the relaxation parameter to control supernode amalgamation (relax)
= 3: the maximum allowable size for a supernode (maxsup)
= 4: the minimum row dimension for 2D blocking to be used (rowblk)
= 5: the minimum column dimension for 2D blocking to be used (colblk)
= 6: the estimated ﬁlls factor for L and U, compared with A
Users are encouraged to modify this subroutine to set the tuning parameters for their own local
environment. The optimal values depend mainly on the cache size and the BLAS speed. If your
system has a very small cache, or if you want to eﬃciently utilize the closest cache in a multilevel
cache organization, you should pay special attention to these parameter settings. In our technical
paper [6], we described a detailed methodology for setting these parameters for high performance.
The relax parameter is usually set between 4 and 8. The other parameter values which give
good performance on several machines are listed in Table 2.3. In a supernode-panel update, if the
updating supernode is too large to ﬁt in cache, then a 2D block partitioning of the supernode is
used, in which rowblk and colblk determine that a block of size rowblk × colblk is used to update
current panel.
If colblk is set greater than maxsup, then the program will never use 2D blocking. For example,
for the Cray J90 (which does not have cache), w = 1 and 1D blocking give good performance; more
levels of blocking only increase overhead.
36


On-chip
External
Machine
Cache
Cache
w
maxsup
rowblk
colblk
RS/6000-590
256 KB
–
8
100
200
40
MIPS R8000
16 KB
4 MB
20
100
800
100
Alpha 21064
8 KB
512 KB
8
100
400
40
Alpha 21164
8 KB-L1
4 MB
16
50
100
40
96 KB-L2
Sparc 20
16 KB
1 MB
8
100
400
50
UltraSparc-I
16 KB
512 KB
8
100
400
40
Cray J90
–
–
1
100
1000
100
Table 2.3: Typical blocking parameter values for several machines.
2.12
Example programs
In the SuperLU/EXAMPLE/ subdirectory, we present a few sample programs to illustrate how to
use various functions provded in SuperLU. The users can modify these examples to suit their
applications. Here are the brief descriptions of the double precision version of the examples:
• dlinsol: use simple driver dgssv() to solve a linear system one time.
• dlinsol1: use simple driver dgssv() in the symmetric mode.
• dlinsolx: use dgssvx() with the full (default) set of options to solve a linear system.
• dlinsolx1: use dgssvx() to factorize A ﬁrst, then solve the system later.
• dlinsolx2: use dgssvx() to solve systems repeatedly with the same sparsity pattern of
matrix A.
• superlu: the small 5x5 sample program in Section 2.2.
In this directory, a Makefile is provided to generate the executables, and a README ﬁle describes
how to run these examples.
2.13
Calling from Fortran
The SuperLU/FORTRAN/ subdirectory contains an example of using SuperLU from a Fortran pro-
gram. The General rules for mixing Fortran and C programs are as follows.
• Arguments in C are passed by value, while in Fortran are passed by reference. So we always
pass the address (as a pointer) in the C calling routine. (You cannot make a call with numbers
directly in the parameters.)
• Fortran uses 1-based array addressing, while C uses 0-based.
Therefore, the row indices
(rowind[]) and the integer pointers to arrays (colptr[]) should be adjusted before they are
passed into a C routine.
37


Because of the above language diﬀerences, in order to embed SuperLU in a Fortran environment,
users are required to use “wrapper” routines (in C) for all the SuperLU routines that will be called
from Fortran programs. The example c fortran dgssv.c in the FORTRAN/ directory shows how a
wrapper program should be written. This program is listed below.
#include "dsp_defs.h"
#define HANDLE_SIZE
8
typedef struct {
SuperMatrix *L;
SuperMatrix *U;
int *perm_c;
int *perm_r;
} factors_t;
int
c_fortran_dgssv_(int *iopt, int *n, int *nnz, int *nrhs, double *values,
int *rowind, int *colptr, double *b, int *ldb,
int factors[HANDLE_SIZE], /* a handle containing the pointer
to the factored matrices */
int *info)
{
/*
* This routine can be called from Fortran.
*
* iopt (input) int
*
Specifies the operation:
*
= 1, performs LU decomposition for the first time
*
= 2, performs triangular solve
*
= 3, free all the storage in the end
*
* factors (input/output) integer array of size 8
*
If iopt == 1, it is an output and contains the pointer pointing to
*
the structure of the factored matrices.
*
Otherwise, it it an input.
*
*/
SuperMatrix A, AC, B;
SuperMatrix *L, *U;
int *perm_r; /* row permutations from partial pivoting */
int *perm_c; /* column permutation vector */
int *etree;
/* column elimination tree */
SCformat *Lstore;
NCformat *Ustore;
38


int
i, panel_size, permc_spec, relax;
trans_t
trans;
double
drop_tol = 0.0;
mem_usage_t
mem_usage;
superlu_options_t options;
SuperLUStat_t stat;
factors_t *LUfactors;
trans = NOTRANS;
if ( *iopt == 1 ) { /* LU decomposition */
/* Set the default input options. */
set_default_options(&options);
/* Initialize the statistics variables. */
StatInit(&stat);
/* Adjust to 0-based indexing */
for (i = 0; i < *nnz; ++i) --rowind[i];
for (i = 0; i <= *n; ++i) --colptr[i];
dCreate_CompCol_Matrix(&A, *n, *n, *nnz, values, rowind, colptr,
SLU_NC, SLU_D, SLU_GE);
L = (SuperMatrix *) SUPERLU_MALLOC( sizeof(SuperMatrix) );
U = (SuperMatrix *) SUPERLU_MALLOC( sizeof(SuperMatrix) );
if ( !(perm_r = intMalloc(*n)) ) ABORT("Malloc fails for perm_r[].");
if ( !(perm_c = intMalloc(*n)) ) ABORT("Malloc fails for perm_c[].");
if ( !(etree = intMalloc(*n)) ) ABORT("Malloc fails for etree[].");
/*
* Get column permutation vector perm_c[], according to permc_spec:
*
permc_spec = 0: natural ordering
*
permc_spec = 1: minimum degree on structure of A’*A
*
permc_spec = 2: minimum degree on structure of A’+A
*
permc_spec = 3: approximate minimum degree for unsymmetric matrices
*/
permc_spec = 3;
get_perm_c(permc_spec, &A, perm_c);
sp_preorder(&options, &A, perm_c, etree, &AC);
panel_size = sp_ienv(1);
relax = sp_ienv(2);
39


dgstrf(&options, &AC, drop_tol, relax, panel_size,
etree, NULL, 0, perm_c, perm_r, L, U, &stat, info);
if ( *info == 0 ) {
Lstore = (SCformat *) L->Store;
Ustore = (NCformat *) U->Store;
printf("No of nonzeros in factor L = %d\n", Lstore->nnz);
printf("No of nonzeros in factor U = %d\n", Ustore->nnz);
printf("No of nonzeros in L+U = %d\n", Lstore->nnz + Ustore->nnz);
dQuerySpace(L, U, &mem_usage);
printf("L\\U MB %.3f\ttotal MB needed %.3f\texpansions %d\n",
mem_usage.for_lu/1e6, mem_usage.total_needed/1e6,
mem_usage.expansions);
} else {
printf("dgstrf() error returns INFO= %d\n", *info);
if ( *info <= *n ) { /* factorization completes */
dQuerySpace(L, U, &mem_usage);
printf("L\\U MB %.3f\ttotal MB needed %.3f\texpansions %d\n",
mem_usage.for_lu/1e6, mem_usage.total_needed/1e6,
mem_usage.expansions);
}
}
/* Restore to 1-based indexing */
for (i = 0; i < *nnz; ++i) ++rowind[i];
for (i = 0; i <= *n; ++i) ++colptr[i];
/* Save the LU factors in the factors handle */
LUfactors = (factors_t*) SUPERLU_MALLOC(sizeof(factors_t));
LUfactors->L = L;
LUfactors->U = U;
LUfactors->perm_c = perm_c;
LUfactors->perm_r = perm_r;
factors[0] = (int) LUfactors;
/* Free un-wanted storage */
SUPERLU_FREE(etree);
Destroy_SuperMatrix_Store(&A);
Destroy_CompCol_Permuted(&AC);
StatFree(&stat);
} else if ( *iopt == 2 ) { /* Triangular solve */
/* Initialize the statistics variables. */
StatInit(&stat);
40


/* Extract the LU factors in the factors handle */
LUfactors = (factors_t*) factors[0];
L = LUfactors->L;
U = LUfactors->U;
perm_c = LUfactors->perm_c;
perm_r = LUfactors->perm_r;
dCreate_Dense_Matrix(&B, *n, *nrhs, b, *ldb, SLU_DN, SLU_D, SLU_GE);
/* Solve the system A*X=B, overwriting B with X. */
dgstrs (trans, L, U, perm_c, perm_r, &B, &stat, info);
Destroy_SuperMatrix_Store(&B);
StatFree(&stat);
} else if ( *iopt == 3 ) { /* Free storage */
/* Free the LU factors in the factors handle */
LUfactors = (factors_t*) factors[0];
SUPERLU_FREE (LUfactors->perm_r);
SUPERLU_FREE (LUfactors->perm_c);
Destroy_SuperNode_Matrix(LUfactors->L);
Destroy_CompCol_Matrix(LUfactors->U);
SUPERLU_FREE (LUfactors->L);
SUPERLU_FREE (LUfactors->U);
SUPERLU_FREE (LUfactors);
} else {
fprintf(stderr, "Invalid iopt=%d passed to c_fortran_dgssv()\n");
exit(-1);
}
}
Since the matrix structures in C cannot be directly returned to Fortran, we use a handle named
factors to access those structures. The handle is essentially an integer pointer pointing to the
factored matrices obtained from SuperLU. So the factored matrices are opaque objects to the Fortran
program, but can only be manipulated from the C wrapper program.
The Fortran program FORTRAN/f77 main.f shows how a Fortran program may call
c fortran dgssv(), and is listed below. A README ﬁle in this directory describes how to compile
and run this program.
program f77_main
integer maxn, maxnz
parameter ( maxn = 10000, maxnz = 100000 )
integer rowind(maxnz), colptr(maxn)
real*8
values(maxnz), b(maxn)
integer n, nnz, nrhs, ldb, info
integer factors(8), iopt
41


*
*
Read the matrix file in Harwell-Boeing format
call hbcode1(n, n, nnz, values, rowind, colptr)
*
nrhs = 1
ldb = n
do i = 1, n
b(i) = 1
enddo
*
* First, factorize the matrix. The factors are stored in factor() handle.
iopt = 1
call c_fortran_dgssv( iopt, n, nnz, nrhs, values, rowind, colptr,
$
b, ldb, factors, info )
*
if (info .eq. 0) then
write (*,*) ’Factorization succeeded’
else
write(*,*) ’INFO from factorization = ’, info
endif
*
* Second, solve the system using the existing factors.
iopt = 2
call c_fortran_dgssv( iopt, n, nnz, nrhs, values, rowind, colptr,
$
b, ldb, factors, info )
*
if (info .eq. 0) then
write (*,*) ’Solve succeeded’
write (*,*) (b(i), i=1, 10)
else
write(*,*) ’INFO from triangular solve = ’, info
endif
* Last, free the storage allocated inside SuperLU
iopt = 3
call c_fortran_dgssv( iopt, n, nnz, nrhs, values, rowind, colptr,
$
b, ldb, factors, info )
*
stop
end
42


Chapter 3
Multithreaded SuperLU (Version 2.0)
3.1
About SuperLU MT
Among the various steps of the solution process in the sequential SuperLU, the LU factorization
dominates the computation; it usually takes more than 95% of the sequential runtime for large
sparse linear systems. We have designed and implemented an algorithm to perform the factorization
in parallel on machines with a shared address space and multithreading. The parallel algorithm
is based on the eﬃcient sequential algorithm implemented in SuperLU. Although we attempted
to minimize the amount of changes to the sequential code, there are still a number of non-trivial
modiﬁcations to the serial SuperLU, mostly related to the matrix data structures and memory
organization. All these changes are summarized in Table 3.1 and their impacts on performance are
studied thoroughly in [7, 23]. In this part of the Users’ Guide, we describe only the changes that
the user should be aware of. Other than these diﬀerences, most of the material in chapter 2 is still
applicable.
Construct
Parallel algorithm
panel
restricted so it does not contain branchings in the elimination tree
supernode
restricted to be a fundamental supernode in the elimination tree
supernode storage
use either static or dynamic upper bound (section 3.5.2)
pruning & DFS
use both G(LT ) and pruned G(LT ) to avoid locking
Table 3.1: The diﬀerences between the parallel and the sequential algorithms.
3.2
Storage types for L and U
As in the sequential code, the type for the factored matrices L and U is SuperMatrix (Figure 2.2),
however, their storage formats (stored in *Store) are changed.
In the parallel algorithm, the
adjacent panels of the columns may be assigned to diﬀerent processes, and they may be ﬁnished
and put in global memory out of order. That is, the consecutive columns or supernodes may not be
stored contiguously in memory. Thus, in addition to the pointers to the beginning of each column
or supernode, we need pointers to the end of the column or supernode. In particular, the storage
type for L is SCP (Supernode, Column-wise and Permuted), deﬁned as:
43


typedef struct {
int
nnz;
/* number of nonzeros in the matrix */
int
nsuper;
/* number of supernodes */
void *nzval;
/* pointer to array of nonzero values,
packed by column */
int *nzval_colbeg; /* nzval_colbeg[j] points to beginning of column j
in nzval[] */
int *nzval_colend; /* nzval_colend[j] points to one past the last
element of column j in nzval[] */
int *rowind;
/* pointer to array of compressed row indices of
the supernodes */
int *rowind_colbeg;/* rowind_colbeg[j] points to beginning of column j
in rowind[] */
int *rowind_colend;/* rowind_colend[j] points to one past the last
element of column j in rowind[] */
int *col_to_sup;
/* col_to_sup[j] is the supernode number to which
column j belongs */
int *sup_to_colbeg;/* sup_to_colbeg[s] points to the first column
of the s-th supernode /
int *sup_to_colend;/* sup_to_colend[s] points to one past the last
column of the s-th supernode */
} SCPformat;
The storage type for U is NCP, deﬁned as:
typedef struct {
int
nnz;
/* number of nonzeros in the matrix */
void *nzval;
/* pointer to array of nonzero values, packed by column */
int
*rowind; /* pointer to array of row indices of the nonzeros */
int
*colbeg; /* colbeg[j] points to the location in nzval[] and rowind[]
which starts column j */
int
*colend; /* colend[j] points to one past the location in nzval[]
and rowind[] which ends column j */
} NCPformat;
The table below summarizes the data and storage types of all the matrices involved in the
parallel routines:
A
L
U
B
X
Stype
SLU NC or SLU NR
SLU SCP
SLU NCP
SLU DN
SLU DN
Dtype
any
any
any
any
any
Mtype
SLU GE
SLU TRLU
SLU TRU
SLU GE
SLU GE
3.3
Options argument
The options argument is the input argument to control the behaviour of the libraries. Options is
implemented as a C structure containing the following ﬁelds:
44


• nprocs
Speciﬁes the number of threads to be spawned.
• Fact
Speciﬁes whether or not the factored form of the matrix A is supplied on entry, and if not,
how the matrix A will be factorized base on the previous history, such as factor from scratch,
reuse Pc and/or Pr, or reuse the data structures of L and U. fact can be one of:
– DOFACT: the matrix A will be factorized from scratch.
– EQUILIBRATE: the matrix A will be equilibrated, then factored into L and U.
– FACTORED: the factored form of A is input.
• Trans { NOTRANS | TRANS | CONJ }
Speciﬁes whether to solve the transposed system.
• panel size
Speciﬁes the number of consecutive columns to be treated as a unit of task.
• relax
Speciﬁes the number of columns to be grouped as a relaxed supernode.
• refact { YES | NO }
Speciﬁes whether this is ﬁrst time or subsequent factorization.
• diag pivot thresh [0.0, 1.0]
Speciﬁes the threshold used for a diagonal entry to be an acceptable pivot.
• SymmetricMode { YES | NO }
Speciﬁes whether to use the symmetric mode.
• PrintStat { YES | NO }
Speciﬁes whether to print the solver’s statistics.
3.4
User-callable routines
As in the sequential SuperLU, we provide both computational routines and driver routines. To
name those routines that involve parallelization in the call-graph, we prepend a letter p to the
names of their sequential counterparts, for example pdgstrf. For the purely sequential routines,
we use the same names as before.
Here, we only list the routines that are diﬀerent from the
sequential ones.
3.4.1
Driver routines
We provide two types of driver routines for solving systems of linear equations. The driver routines
can handle both column- and row-oriented storage schemes.
• A simple driver pdgssv, which solves the system AX = B by factorizing A and overwriting
B with the solution X.
45


• An expert driver pdgssvx, which, in addition to the above, also performs the following func-
tions (some of them optionally):
– solve AT X = B;
– equilibrate the system (scale A’s rows and columns to have unit norm) if A is poorly
scaled;
– estimate the condition number of A, check for near-singularity, and check for pivot
growth;
– reﬁne the solution and compute forward and backward error bounds.
3.4.2
Computational routines
The user can invoke the following computational routines to directly control the behavior of Su-
perLU. The computational routines can only handle column-oriented storage. Except for the par-
allel factorization routine pdgstrf, all the other routines are identical to those appeared in the
sequential superlu.
• pdgstrf: Factorize (in parallel).
This implements the ﬁrst-time factorization, or later re-factorization with the same nonzero
pattern. In re-factorizations, the code has the ability to use the same column permutation
Pc and row permutation Pr obtained from a previous factorization. Several scalar arguments
control how the LU decomposition and the numerical pivoting should be performed. pdgstrf
can handle non-square matrices.
• dgstrs: Triangular solve.
This takes the L and U triangular factors, the row and column permutation vectors, and the
right-hand side to compute a solution matrix X of AX = B or AT X = B.
• dgscon: Estimate condition number.
Given the matrix A and its factors L and U, this estimates the condition number in the
one-norm or inﬁnity-norm. The algorithm is due to Hager and Higham [19], and is the same
as condest in sparse Matlab.
• dgsequ/dlaqgs: Equilibrate.
dgsequ ﬁrst computes the row and column scalings Dr and Dc which would make each row
and each column of the scaled matrix DrADc have equal norm. dlaqgs then applies them to
the original matrix A if it is indeed badly scaled. The equilibrated A overwrites the original
A.
• dgsrfs: Reﬁne solution.
Given A, its factors L and U, and an initial solution X, this does iterative reﬁnement, using
the same precision as the input data. It also computes forward and backward error bounds
for the reﬁned solution.
46


3.5
Installation
3.5.1
File structure
The top level SuperLU MT/ directory is structured as follows:
SuperLU_MT_2.0/README
instructions on installation
SuperLU_MT_2.0/CBLAS/
BLAS routines in C, functional but not fast
SuperLU_MT_2.0/DOC/
Users’ Guide
SuperLU_MT_2.0/EXAMPLE/
example programs
SuperLU_MT_2.0/INSTALL/
test machine dependent parameters
SuperLU_MT_2.0/SRC/
C source code, to be compiled into libsuperlu_mt.a
SuperLU_MT_2.0/TESTING/
driver routines to test correctness
SuperLU_MT_2.0/lib/
SuperLU_MT library archive libsuperlu_mt.a
SuperLU_MT_2.0/Makefile
top level Makefile that does installation and testing
SuperLU_MT_2.0/MAKE_INC
sample machine-specific make.inc files
SuperLU_MT_2.0/make.inc
compiler, compiler flags, library definitions and C
preprocessor definitions, included in all Makefiles.
(You may need to edit it to suit for your system
before compiling the whole package.)
We have ported the parallel programs to a number of platforms, which are reﬂected in the
make include ﬁles provided in the top level directory, for example, make.pthreads, make.openmp,
make.ibm, make.sun, make.sgi, make.cray. If you are using one of these machines, such as an
IBM, you can simply copy make.sun into make.inc before compiling. If you are not using any of the
machines to which we have ported, you will need to read section 3.7 about the porting instructions.
The rest of the installation and testing procedure is similar to that described in section 2.11 for
the serial SuperLU. Then, you can type make at the top level directory to ﬁnish installation. In
the SuperLU MT/TESTING subdirectory, you can type pdtest.csh to perform testings.
3.5.2
Performance issues
Memory management for L and U
In the sequential SuperLU, four data arrays associated with the L and U factors can be expanded
dynamically, as described in section 2.8. In the parallel code, the expansion is hard and costly to
implement, because when a process detects that an array bound is exceeded, it has to send a signal
to and suspend the execution of the other processes. Then the detecting process can proceed with
the array expansion. After the expansion, this process must wake up all the suspended processes.
In this release of the parallel code, we have not yet implemented the above expansion mechanism.
For now, the user must pre-determine an estimated size for each of the four arrays through the
inquiry function sp ienv(). There are two interpretations for each integer value FILL returned
by calling this function with ispec = 6, 7, or 8. A negative number is interpreted as the ﬁlls
growth factor, that is, the program will allocate (-FILL)*nnz(A) elements for the corresponding
array. A positive number is interpreted as the true amount the user wants to allocate, that is, the
program will allocate FILL elements for the corresponding array. In both cases, if the initial request
47


exceeds the physical memory constraint, the sizes of the arrays are repeatedly reduced until the
initial allocation succeeds.
int sp ienv(int ispec);
Ispec speciﬁes the parameter to be returned:
ispec = . . .
= 6: size of the array to store the values of the L supernodes (nzval)
= 7: size of the array to store the columns in U (nzval/rowind)
= 8: size of the array to store the subscripts of the L supernodes (rowind);
If the actual ﬁll exceeds any array size, the program will abort with a message showing the
current column when failure occurs, and indicating how many elements are needed up to the
current column. The user may reset a larger ﬁll parameter for this array and then restart the
program.
To make the storage allocation more eﬃcient for the supernodes in L, we devised a special
storage scheme. The need for this special treatment and how we implement it are fully explained
and studied in [7, 23]. Here, we only sketch the main idea. Recall that the parallel algorithm assigns
one panel of columns to one process. Two consecutive panels may be assigned to two diﬀerent
processes, even though they may belong to the same supernode discovered later. Moreover, a third
panel may be ﬁnished by a third process and put in memory between these two panels, resulting
in the columns of a supernode being noncontiguous in memory. This is undesirable, because then
we cannot directly call BLAS routines using this supernode unless we pay the cost of copying the
columns into contiguous memory ﬁrst. To overcome this problem, we exploited the observation that
the nonzero structure for L is contained in that of the Householder matrix H from the Householder
sparse QR transformation [13, 14]. Furthermore, it can be shown that a fundamental supernode of
L is always contained in a fundamental supernode of H. This containment property is true for any
row permutation Pr in PrA = LU. Therefore, we can pre-allocate storage for the L supernodes
based on the size of H supernodes. Fortunately, there exists a fast algorithm (almost linear in the
number of nonzeros of A) to compute the size of H and the supernodes partition in H [15].
In practice, the above static prediction is fairly tight for most problems. However, for some
others, the number of nonzeros in H greatly exceeds the number of nonzeros in L. To handle
this situation, we implemented an algorithm that still uses the supernodes partition in H, but
dynamically searches the supernodal graph of L to obtain a much tighter bound for the storage.
Table 6 in [7] demonstrates the storage eﬃciency achieved by both static and dynamic approach.
In summary, our program tries to use the static prediction ﬁrst for the L supernodes. In this
case, we ignore the integer value given in the function sp ienv(6), and simply use the nonzero
count of H. If the user ﬁnds that the size of H is too large, he can invoke the dynamic algorithm
at runtime by setting the following Linux shell environment variable:
setenv SuperLU DYNAMIC SNODE STORE 1
The dynamic algorithm incurs runtime overhead. For example, this overhead is usually between
2% and 15% on a single processor RS/6000-590 for a range of test matrices.
48


Symmetric structure pruning
In both serial and parallel algorithms, we have implemented Eisenstat and Liu’s symmetric pruning
idea of representing the graph G(LT ) by a reduced graph G′, and thereby reducing the DFS traversal
time. A subtle diﬃculty arises in the parallel implementation.
When the owning process of a panel starts DFS (depth-ﬁrst search) on G′ built so far, it
only sees the partial graph, because the part of G′ corresponding to the busy panels down the
elimination tree is not yet complete.
So the structural prediction at this stage can miss some
nonzeros. After performing the updates from the ﬁnished supernodes, the process will wait for
all the busy descendant panels to ﬁnish and perform more updates from them. Now, we make
a conservative assumption that all these busy panels will update the current panel so that their
nonzero structures are included in the current panel.
This approximate scheme works ﬁne for most problems. However, we found that this conser-
vatism may sometimes cause a large number of structural zeros (they are related to the supernode
amalgamation performed at the bottom of the elimination tree) to be included and they in turn
are propagated through the rest of the factorization.
We have implemented an exact structural prediction scheme to overcome this problem. In this
scheme, when each numerical nonzero is scattered into the sparse accumulator array, we set the
occupied ﬂag as well. Later when we accumulate the updates from the busy descendant panels, we
check the occupied ﬂags to determine the exact nonzero structure. This scheme avoids unnecessary
zero propagation at the expense of runtime overhead, because setting the occupied ﬂags must be
done in the inner loop of the numeric updates.
We recommend that the user use the approximate scheme (by default) ﬁrst. If the user ﬁnds
that the amount of ﬁll from the parallel factorization is substantially greater than that from the
sequential factorization, he can then use the accurate scheme. To invoke the second scheme, the
user should recompile the code by deﬁning the macro:
-D SCATTER FOUND
for the C preprocessor.
The inquiry function sp ienv()
For some user controllable constants, such as the blocking parameters and the size of the global
storage for L and U, SuperLU MT calls the inquiry function sp ienv() to retrieve their values.
The declaration of this function is
int sp ienv(int ispec).
The full meanings of the returned values are as follows:
ispec = 1: the panel size w
= 2: the relaxation parameter to control supernode amalgamation (relax)
= 3: the maximum allowable size for a supernode (maxsup)
= 4: the minimum row dimension for 2D blocking to be used (rowblk)
= 5: the minimum column dimension for 2D blocking to be used (colblk)
= 6: size of the array to store the values of the L supernodes (nzval)
= 7: size of the array to store the columns in U (nzval/rowind)
= 8: size of the array to store the subscripts of the L supernodes (rowind)
49


Programming
Environment
make.inc
Platforms
Model
Variable
make.pthreads
Machines with POSIX threads
pthreads
make.openmp
Machines with OpenMP
OpenMP
OMP NUM THREADS
make.alpha
DEC Alpha Servers
DECthreads
make.cray
Cray C90/J90
microtasking
NCPUS
make.ibm
IBM Power series
pthreads
make.origin
SGI/Cray Origin2000
parallel C
MP SET NUMTHREADS
make.sgi
SGI Power Challenge
parallel C
MPC NUM THREADS
make.sun
Sun Ultra Enterprise
Solaris threads
Table 3.2: Platforms on which SuperLU MT was tested.
We should take into account the trade-oﬀbetween cache reuse and amount of parallelism in order
to set the appropriate w and maxsup. Since the parallel algorithm assigns one panel factorization to
one process, large values may constrain concurrency, even though they may be good for uniprocessor
performance. We recommend that w and maxsup be set a bit smaller than the best values used in
the sequential code.
The settings for parameters 2, 4 and 5 are the same as those described in section 2.11.3. The
settings for parameters 6, 7 and 8 are discussed in section 3.5.2.
In the ﬁle SRC/sp ienv.c, we provide sample settings of these parameters for several machines.
3.6
Example programs
In the SuperLU MT/EXAMPLE/ subdirectory, we present a few sample programs to illustrate the
complete calling sequences to use the simple and expert drivers to solve systems of equations. Ex-
amples are also given to illustrate how to perform a sequence of factorizations for the matrices with
the same sparsity pattern, and how SuperLU MT can be integrated into the other multithreaded
application such that threads are created only once. A Makefile is provided to generate the exe-
cutables. A README ﬁle in this directory shows how to run these examples. The leading comment
in each routine describes the functionality of the example.
3.7
Porting to other platforms
We have provided the parallel interfaces for a number of shared-memory machines. Table 3.2 lists
the platforms on which we have tested the library, and the respective make.inc ﬁles. The most
portable interface for shared memory programming is POSIX threads [32], since nowadays many
commercial UNIX operating systems have support for it. We call our POSIX threads interface the
Pthreads interface. To use this interface, you can copy make.pthreads into make.inc and then
compile the library. In the last column of Table 3.2, we list the runtime environment variable to
be set in order to use multiple CPUs. For example, to use 4 CPUs on the Origin2000, you need to
set the following before running the program:
setenv MP SET NUMTHREADS 4
50


Mutex
Critical region
ULOCK
allocate storage for a column of matrix U
LLOCK
allocate storage for row subscripts of matrix L
LULOCK
allocate storage for the values of the supernodes
NSUPER LOCK
increment supernode number nsuper
SCHED LOCK
invoke Scheduler() which may update global task queue
Table 3.3: Five mutex variables.
In the source code, all the platform speciﬁc constructs are enclosed in the C #ifdef preprocessor
statement. If your platform is diﬀerent from any one listed in Table 3.2, you need to go to these
places and create the parallel constructs suitable for your machine. The two constructs, concurrency
and synchronization, are explained in the following two subsections, respectively.
3.7.1
Creating multiple threads
Right now, only the factorization routine pdgstrf is parallelized, since this is the most time-
consuming part in the whole solution process. There is one single thread of control on entering and
exiting pdgstrf. Inside this routine, more than one thread may be created. All the newly created
threads begin by calling the thread function pdgstrf thread and they are concurrently executed
on multiple processors. The thread function pdgstrf thread expects a single argument of type
void*, which is a pointer to the structure containing all the shared data objects.
3.7.2
Use of mutexes
Although the threads pdgstrf thread execute independently of each other, they share the same
address space and can communicate eﬃciently through shared variables. Problems may arise if
two threads try to access (at least one is to modify) the shared data at the same time. Therefore,
we must ensure that all memory accesses to the same data are mutually exclusive. There are ﬁve
critical regions in the program that must be protected by mutual exclusion. Since we want to allow
diﬀerent processors to enter diﬀerent critical regions simultaneously, we use ﬁve mutex variables as
listed in Table 3.3. The user should properly initialize them in routine ParallelInit, and destroy
them in routine ParallelFinalize. Both these routines are in ﬁle pxgstrf synch.c.
51


Chapter 4
Distributed-memory SuperLU on
manycore nodes (Version 4.0)
4.1
About SuperLU DIST
In this part, we describe the SuperLU DIST library designed for distributed-memory pearallel com-
puters using SPMD parallel programming model, together with multithreading for manycore node
architectures. The library is implemented in ANSI C, using MPI [29] for communication, OpenMP
for multithreading and CUDA for GPU. We have tested the code on a number of platforms, in-
cluding IBM, Cray XE6, Cray XT7, and numerous Linux clusters. The library includes routines
to handle both real and complex matrices in double precision. The parallel routine names for the
double-precision real version start with letters “pd” (such as pdgstrf); the parallel routine names
for double-precision complex version start with letters “pz” (such as pzgstrf).
4.2
Formats of the input matrices A and B
We provide two input interfaces for matrices A and B—one is global, and the other is entirely
distributed.
4.2.1
Global input
The input matrices A and B are globally available (replicated) on all the processes. The storage
type for A is SLU NC (compressed column), as in sequential case (see Section 2.3). The user-callable
routines with this interface all have the names “xxxxxxx ABglobal”. If there is suﬃcient memory,
this interface is faster than the distributed input interface described in the next section, because
the latter requires more data re-distribution at diﬀerent stages of the algorithm.
4.2.2
Distributed input
Both input matrices A and B are distributed among all the processes. They use the same distri-
bution based on block rows. That is, each process owns a block of consecutive rows of A and B.
Each local part of sparse matrix A is stored in a compressed row format, called SLU NR loc storage
type, which is deﬁned below.
52


typedef struct {
int nnz_loc;
/* number of nonzeros in the local submatrix */
int m_loc;
/* number of rows local to this process */
int fst_row;
/* row number of the first row in the local submatrix */
void *nzval;
/* pointer to array of nonzero values, packed by row */
int *rowptr;
/* pointer to array of beginning of rows in nzval[]
and colind[]
*/
int *colind;
/* pointer to array of column indices of the nonzeros */
} NRformat_loc;
Let mi be the number of rows owned by the ith process. Then the global row dimension for A
is nrow = PP−1
i=0 mi. The global column dimension is ncol. Both nrow and ncol are recorded in
the higher level SuperMatrix data structure, see Figure 2.2. The utility routine
dCreate CompRowLoc Matrix dist can help the user to create the structure for A. The deﬁnition
of this routine is
void dCreate_CompRowLoc_Matrix_dist(SuperMatrix *A, int m, int n,
int nnz_loc, int m_loc, int fst_row,
double *nzval, int *colind, int *rowptr,
Stype_t stype, Dtype_t dtype, Mtype_t mtype);
where, the ﬁrst argument is output and the rest are inputs.
The local full matrix B is stored in the standard Fortran-style column major format, with
dimension m loc × nrhs, and ldb refers to the local leading dimension in the local storage.
4.3
Distributed data structures for L and U
We distribute both L and U matrices in a two-dimensional block-cyclic fashion. We ﬁrst identify
the supernode boundary based on the nonzero structure of L. This supernode partition is then
used as the block partition in both row and column dimensions for both L and U. The size of each
block is matrix dependent. It should be clear that all the diagonal blocks are square and full (we
store zeros from U in the upper triangle of the diagonal block), whereas the oﬀ-diagonal blocks
may be rectangular and may not be full. The matrix in Figure 4.1 illustrates such a partition.
By block-cyclic mapping we mean block (I, J) (0 ≤I, J ≤N −1) is mapped into the process at
coordinate {I mod nprow, J mod npcol} of the nprow×npcol 2D process grid. Using this mapping,
a block L(I, J) in the factorization is only needed by the row of processes that own blocks in row
I. Similarly, a block U(I, J) is only needed by the column of processes that own blocks in column
J.
In this 2D mapping, each block column of L resides on more than one process, namely, a
column of processes.
For example in Figure 4.1, the second block column of L resides on the
column processes {1, 4}. Process 4 only owns two nonzero blocks, which are not contiguous in
the global matrix. The schema on the right of Figure 4.1 depicts the data structure to store the
nonzero blocks on a process. Besides the numerical values stored in a Fortran-style array nzval[]
in column major order, we need the information to interpret the location and row subscript of each
nonzero. This is stored in an integer array index[], which includes the information for the whole
block column and for each individual block in it. Note that many oﬀ-diagonal blocks are zero and
53


Figure 4.1: The 2 block-cyclic layout and the data structure to store a local block column of L.
hence not stored. Neither do we store the zeros in a nonzero block. Both lower and upper triangles
of the diagonal block are stored in the L data structure. A process owns ⌈N/npcol⌉block columns
of L, so it needs ⌈N/nprow⌉pairs of index/nzval arrays.
For U, we use a row oriented storage for the block rows owned by a process, although for the
numerical values within each block we still use column major order. Similar to L, we also use a pair
of index/nzval arrays to store a block row of U. Due to asymmetry, each nonzero block in U has
the skyline structure as shown in Figure 4.1 (see [6] for details on the skyline structure). Therefore,
the organization of the index[] array is diﬀerent from that for L, which we omit showing in the
ﬁgure.
4.4
Process grid and MPI communicator
All MPI applications begin with a default communication domain that includes all processes, say
Np, of this parallel job. The default communicator MPI COMM WORLD represents this communication
domain. The Np processes are identiﬁed as a linear array of process IDs in the range 0 . . . Np −1.
4.4.1
2D process grid
For SuperLU DIST library, we create a new process group derived from an existing group using Ng
processes. There is a good reason to use a new group rather than MPI COMM WORLD, that is, the
message passing calls of the SuperLU library will be isolated from those in other libraries or in the
user’s code. For better scalability of the LU factorization, we map the 1D array of Ng processes
into a logical 2D process grid. This grid will have nprow process rows and npcol process columns,
such that nprow ∗npcol = Ng. A process can be referenced either by its rank in the new group or
by its coordinates within the grid. The routine superlu gridinit maps already-existing processes
to a 2D process grid.
54


superlu_gridinit(MPI_Comm Bcomm, int nprow, int npcol, gridinfo_t *grid);
This process grid will use the ﬁrst nprow ∗npcol processes from the base MPI communicator
Bcomm, and assign them to the grid in a row-major ordering. The input argument Bcomm is an MPI
communicator representing the existing base group upon which the new group will be formed. For
example, it can be MPI COMM WORLD. The output argument grid represents the derived group to be
used in SuperLU DIST. Grid is a structure containing the following ﬁelds:
struct {
MPI_Comm comm;
/* MPI communicator for this group */
int iam;
/* my process rank in this group
*/
int nprow;
/* number of process rows
*/
int npcol;
/* number of process columns
*/
superlu_scope_t rscp; /* process row scope
*/
superlu_scope_t cscp; /* process column scope
*/
} grid;
In the LU factorization, some communications occur only among the processes in a row (col-
umn), not among all processes. For this purpose, we introduce two process subgroups, namely rscp
(row scope) and cscp (column scope). For rscp (cscp) subgroup, all processes in a row (column)
participate in the communication.
The macros MYROW(iam, grid) and MYCOL(iam, grid) give the row and column coordinates
in the 2D grid of the process who has rank iam.
NOTE: All processes in the base group, including those not in the new group, must call this grid
creation routine. This is required by the MPI routine MPI Comm create to create a new communi-
cator.
4.4.2
Arbitrary grouping of processes
It is sometimes desirable to divide up the processes into several subgroups, each of which per-
forms independent work of a single application. In this situation, we cannot simply use the ﬁrst
nprow ∗npcol processes to deﬁne the grid. A more sophisticated process-to-grid mapping routine
superlu gridmap is designed to create a grid with processes of arbitrary ranks.
superlu_gridmap(MPI_Comm Bcomm, int nprow, int npcol,
int usermap[], int ldumap, gridinfo_t *grid);
The array usermap[] contains the processes to be used in the newly created grid. usermap[] is
indexed like a Fortran-style 2D array with ldumap as the leading dimension. So usermap[i+j∗ldumap]
(i.e., usermap(i,j) in Fortran notation) holds the process rank to be placed in {i, j} position
of the 2D process grid. After grid creation, this subset of processes is logically numbered in a
consistent manner with the initial set of processes; that is, they have the ranks in the range
0 . . . nprow ∗npcol −1 in the new grid. For example, if we want to map 6 processes with ranks
11 . . . 16 into a 2 × 3 grid, we deﬁne usermap = {11, 14, 12, 15, 13, 16} and ldumap = 2. Such a
mapping is shown below
0
1
2
0
11
12
13
1
14
15
16
55


NOTE: All processes in the base group, including those not in the new group, must call this
routine.
Superlu gridinit simply calls superlu gridmap with usermap[] holding the ﬁrst nprow ∗
npcol process ranks.
4.5
Algorithmic background
Although partial pivoting is used in both sequential and shared-memory parallel factorization al-
gorithms, it is not used in the distributed-memory parallel algorithm, because it requires dynamic
adaptation of data structures and load balancing, and so is hard to make it scalable.
We use
alternative techniques to stabilize the algorithm, which include statically pivot large elements to
the diagonal, single-precision diagonal adjustment to avoid small pivots, and iterative reﬁnement.
Figure 4.2 sketches our GESP algorithm (Gaussian elimination with Static Pivoting). Numerical
experiments show that for a wide range of problems, GESP is as stable as GEPP [26].
(1) Perform row/column equilibration and row permutation: A ←Pr · Dr · A · Dc,
where Dr and Dc are diagonal matrices and Pr is a row permutation chosen
to make the diagonal large compared to the oﬀ-diagonal.
(2) Find a column permutation Pc to preserve sparsity: A ←Pc · A · P T
c
(3) Perform symbolic analysis to determine the nonzero structures of L and U.
(4) Factorize A = L · U with control of diagonal magnitude:
if ( |aii| < √ε · ∥A∥1 ) then
set aii to √ε · ∥A∥1
endif
(5) Perform triangular solutions using L and U.
(6) If needed, use an iterative solver like GMRES or iterative reﬁnement (shown below)
iterate:
r = b −A · x
. . . sparse matrix-vector multiply
Solve A · dx = r
. . . triangular solution
berr = maxi
|r|i
(|A|·|x|+|b|)i
. . . componentwise backward error
if ( berr > ε and berr ≤1
2 · lastberr ) then
x = x + dx
lastberr = berr
goto iterate
endif
(7) If desired, estimate the condition number of A
Figure 4.2: The outline of the GESP algorithm.
Step (1) is accomplished by a weighted bipartite matching algorithm due to Duﬀand Koster [10].
Currently, process 0 computes Pr and then broadcasts it to all the other processes. If the distributed
input interface is used (Section 4.2.2), we ﬁrst gather the distributed matrix A onto processor 0.
Work is underway to remove this sequential bottleneck.
In Step (2), we provide several ordering options, such as multiple minimum degree ordering [28]
on the graphs of A + AT , or the MeTiS [20] ordering on the graphs of A + AT . The user can use
56


any other ordering in place of the ones provided in the library. (Note, since we will pivot on the
diagonal in step (4), an ordering based on the structure of A + AT almost always yields sparser
factors than that based on the structure of AT A. This is diﬀerent from SuperLU and SuperLU MT,
where we allow to pivot oﬀ-diagonal.) In this step, when a sequential ordering algorithm is used,
every process runs the same algorithm independently.
Step (3) can be done either sequentially or in parallel depending on how the options argument
is set (see Section 4.8.1 for details.) The parallel symbolic factorization was a newly added feature
since the v2.1 release.
It is designed tightly around the separator tree returned from a graph
partitioning type of ordering (presently we use ParMeTiS [21]), and works only on power-of-two
processors. We ﬁrst re-distribute the graph of A onto the largest 2q number of processors which
is smaller than the total Np processors, then perform parallel symbolic factorization, and ﬁnally
re-populate the {L\U} structure to all Np processors. The algorithm and performance was studied
in [17]. To invoke parallel symbolic factorization, the user needs to set the two ﬁelds of the options
argument as follows:
options.ParSymbFact
= YES
options.ColPerm
= PARMETIS;
Note that, even if the user sets options.ColPerm to use an ordering algorithm other than ParMeTiS,
the driver routine overrides it with ParMeTiS when it sees options.ParSymbFact = YES.
Steps (4) to (7) are the most time-consuming steps and were parallelized a while ago, see the
papers [26, 24].
4.5.1
Multicore and GPU enhancements
Given that the node architecture trend is to have many simpler cores on a NUMA node, possibly
with GPU-like accelerators, and the amount of memory per-core becomes smaller, the pure MPI
model does not match the light-weight processor architecture. We need to resort to other forms of
parallelism at node level. In sparse factorization (step (4) in Figure 4.2), the Schur complement
update after each panel factorization step exhibits ample ﬁne-grid parallelism. We have designed
the OpenMP + CUDA code to exploit the on-node parallelism. For each MPI task of the Schur
complement update, we aggegrate the small L and U matrix blocks into a larger block, divide the
GEMM work between CPU and GPU using some heuristic performance model, oﬄoad the larger
block of GEMM to GPU. The key to success is to overlap the CPU work (multithreaded Scat-
ter/Gather, some portion of GEMM) with the GPU work (GEMM with multiple CUDA streams).
This way, we are able to hide the latency time due to PCI bus transfer between CPU and GPU.
The detailed algorithm description and performance data were given in the paper by Sao et al. [34].
To use Nvidia GPU, you must set the following Linux shell environment variable before compi-
lation:
setenv ACC GPU
Hybrid MPI+OpenMP setting may outperform MPI-only conﬁgurations in some cases and in
most cases hybrid MPI+OpenMP would require less memory. The environment variable OMP NUM THREADS
needs to be set appropriately. Hybrid conﬁguration obtains threaded parallelism from both, explicit
OpenMP pragmas and multithreaded BLAS. Thus for good performance it is better if OpenMP
threading library and BLAS threading are synergistic. For example, when using Intel MKL libray,
57


just setting OMP NUM THREADS would set the number of threads for both MKL and OpenMP. How-
ever, it is possible to have diﬀerent number of threads for MKL, in which case MKL NUM THREADS
controls the number of threads used by MKL. In our case, just setting OMP NUM THREADS is suﬃcient.
Triangular solve phase does not use multithreading yet. The MPI-only conﬁguration may be
more suitable in case of many right hand sides or in other cases, where solve phase seems to be a
performance bottleneck.
4.6
Options argument
One important input argument is options, which controls how the linear system will be solved.
Although the algorithm presented in Figure 4.2 consists of seven steps, for some matrices not all
steps are needed to get accurate solution. For example, for diagonally dominant matrices, choosing
the diagonal pivots ensures the stability; there is no need for row pivoting in step (1). In another
situation where a sequence of matrices with the same sparsity pattern need be factorized, the
column permutation Pc (and also the row permutation Pr, if the numerical values are similar) need
be computed only once, and reused thereafter. (Pr and Pc are implemented as permutation vectors
perm r and perm c.) For the above examples, performing all seven steps does more work than
necessary. Options is used to accommodate the various requirements of applications; it contains
the following ﬁelds:
• Fact
Speciﬁes whether or not the factored form of the matrix A is supplied on entry, and if not,
how the matrix A will be factored base on some assumptions of the previous history. fact
can be one of:
– DOFACT: the matrix A will be factorized from scratch.
– SamePattern: the matrix A will be factorized assuming that a factorization of a ma-
trix with the same sparsity pattern was performed prior to this one. Therefore, this
factorization will reuse column permutation vector perm c.
– SampPattern SameRowPerm: the matrix A will be factorized assuming that a factoriza-
tion of a matrix with the same sparsity pattern and similar numerical values was per-
formed prior to this one. Therefore, this factorization will reuse both row and column
permutation vectors perm r and perm c, both row and column scaling factors Dr and
Dc, and the distributed data structure set up from the previous symbolic factorization.
– FACTORED: the factored form of A is input.
• Equil { YES | NO }
Speciﬁes whether to equilibrate the system.
• ParSymbFact { YES | NO }
Speciﬁes whether to perform parallel symbolic factorization. If it is set to YES, the ColPerm
ﬁeld should be set to PARMETIS. Otherwise, the driver routine pdgssvx will use ParMeTiS
anyway, ignoring the other setting in ColPerm.
• ColPerm
Speciﬁes the column ordering method for ﬁll reduction.
58


– NATURAL: natural ordering.
– MMD AT PLUS A: minimum degree ordering on the structure of AT + A.
– MMD ATA: minimum degree ordering on the structure of AT A.
– METIS AT PLUS A: MeTiS ordering on the structure of AT + A.
– PARMETIS: ParMeTiS ordering on the structure of AT + A.
– MY PERMC: use the ordering given in perm c input by the user.
• RowPerm
Speciﬁes how to permute rows of the original matrix.
– NATURAL: use the natural ordering.
– LargeDiag MC64: use a serial, weighted bipartite matching algorithm implemented in
MC64 to permute the rows to make the diagonal large relative to the oﬀ-diagonal [11].
– LargeDiag AWPM: use a parallel, approximate weighted bipartite matching algorithm
implemented in CombBLAS to permute the rows to make the diagonal large relative to
the oﬀ-diagonal [3].
– MY PERMR: use the ordering given in perm r input by the user.
• ReplaceTinyPivot { YES | NO }
Speciﬁes whether to replace the tiny diagonals by √ε · ||A|| during LU factorization.
• IterRefine
Speciﬁes how to perform iterative reﬁnement.
– NO: no iterative reﬁnement.
– DOUBLE: accumulate residual in double precision.
– EXTRA: accumulate residual in extra precision. (not yet implemented.)
• Trans { NOTRANS | TRANS | CONJ }
Speciﬁes whether to solve the transposed system.
• SolveInitialized { YES | NO }
Speciﬁes whether the initialization has been performed to the triangular solve.
(used only by the distributed input interface)
• RefineInitialized { YES | NO }
Speciﬁes whether the initialization has been performed to the sparse matrix-vector multipli-
cation routine needed in the iterative reﬁnement.
(used only by the distributed input interface)
• num lookaheads { integer }
Speciﬁes the number of levels in the look-ahead factorization
• lookahead etree { YES | NO }
Speciﬁes whether to use the elimination tree computed from the serial symbolic factorization
to perform static scheduling.
59


• SymPattern { YES | NO }
Gives the scheduling algorithm a hint whether the matrix has the symmetric pattern.
• PrintStat { YES | NO }
Speciﬁes whether to print the solver’s statistics.
There is a routine named set default options dist() that sets the default values of these
options, which are:
fact
= DOFACT
/* factor from scratch */
equil
= YES
ParSymbFact
= NO
colperm
= MMD_AT_PLUS_A
rowperm
= LargeDiag
/* use MC64 */
ReplaceTinyPivot
= YES
IterRefine
= DOUBLE
Trans
= NOTRANS
SolveInitialized
= NO
RefineInitialized = NO
num_lookaheads
= 10;
lookahead_etree
= NO;
SymPattern
= NO;
PrintStat
= YES
4.7
Basic steps to solve a linear system
In this section, we use a complete sample program to illustrate the basic steps required to use
SuperLU DIST. This program is listed below, and is also available as EXAMPLE/pddrive.c in the
source code distribution.
All the routines must include the header ﬁle superlu ddefs.h (or
superlu zdefs.h, the complex counterpart) which contains the deﬁnitions of the data types, the
macros and the function prototypes.
#include <math.h>
#include "superlu_ddefs.h"
main(int argc, char *argv[])
/*
* Purpose
* =======
*
* The driver program PDDRIVE.
*
* This example illustrates how to use PDGSSVX with the full
* (default) options to solve a linear system.
*
* Five basic steps are required:
*
1. Initialize the MPI environment and the SuperLU process grid
60


*
2. Set up the input matrix and the right-hand side
*
3. Set the options argument
*
4. Call pdgssvx
*
5. Release the process grid and terminate the MPI environment
*
* On the Cray T3E, the program may be run by typing
*
mpprun -n <procs> pddrive -r <proc rows> -c <proc columns> <input_file>
*
*/
{
superlu_options_t options;
SuperLUStat_t stat;
SuperMatrix A;
ScalePermstruct_t ScalePermstruct;
LUstruct_t LUstruct;
SOLVEstruct_t SOLVEstruct;
gridinfo_t grid;
double
*berr;
double
*b, *xtrue;
int_t
m, n, nnz;
int_t
nprow, npcol;
int
iam, info, ldb, ldx, nrhs;
char
trans[1];
char
**cpp, c;
FILE *fp, *fopen();
nprow = 1;
/* Default process rows.
*/
npcol = 1;
/* Default process columns.
*/
nrhs = 1;
/* Number of right-hand side. */
/* ------------------------------------------------------------
INITIALIZE MPI ENVIRONMENT.
------------------------------------------------------------*/
MPI_Init( &argc, &argv );
/* Parse command line argv[]. */
for (cpp = argv+1; *cpp; ++cpp) {
if ( **cpp == ’-’ ) {
c = *(*cpp+1);
++cpp;
switch (c) {
case ’h’:
printf("Options:\n");
printf("\t-r <int>: process rows
(default %d)\n", nprow);
printf("\t-c <int>: process columns (default %d)\n", npcol);
61


exit(0);
break;
case ’r’: nprow = atoi(*cpp);
break;
case ’c’: npcol = atoi(*cpp);
break;
}
} else { /* Last arg is considered a filename */
if ( !(fp = fopen(*cpp, "r")) ) {
ABORT("File does not exist");
}
break;
}
}
/* ------------------------------------------------------------
INITIALIZE THE SUPERLU PROCESS GRID.
------------------------------------------------------------*/
superlu_gridinit(MPI_COMM_WORLD, nprow, npcol, &grid);
/* Bail out if I do not belong in the grid. */
iam = grid.iam;
if ( iam >= nprow * npcol ) goto out;
/* ------------------------------------------------------------
GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE.
------------------------------------------------------------*/
dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
if ( !(berr = doubleMalloc_dist(nrhs)) )
ABORT("Malloc fails for berr[].");
/* ------------------------------------------------------------
NOW WE SOLVE THE LINEAR SYSTEM.
------------------------------------------------------------*/
/* Set the default input options. */
set_default_options_dist(&options);
m = A.nrow;
n = A.ncol;
/* Initialize ScalePermstruct and LUstruct. */
ScalePermstructInit(m, n, &ScalePermstruct);
LUstructInit(n, &LUstruct);
62


/* Initialize the statistics variables. */
PStatInit(&stat);
/* Call the linear equation solver. */
pdgssvx(&options, &A, &ScalePermstruct, b, ldb, nrhs, &grid,
&LUstruct, &SOLVEstruct, berr, &stat, &info);
/* Check the accuracy of the solution. */
pdinf_norm_error(iam, ((NRformat_loc *)A.Store)->m_loc,
nrhs, b, ldb, xtrue, ldx, &grid);
PStatPrint(&options, &stat, &grid);
/* Print the statistics. */
/* ------------------------------------------------------------
DEALLOCATE STORAGE.
------------------------------------------------------------*/
PStatFree(&stat);
Destroy_CompRowLoc_Matrix_dist(&A);
ScalePermstructFree(&ScalePermstruct);
Destroy_LU(n, &grid, &LUstruct);
LUstructFree(&LUstruct);
if ( options.SolveInitialized ) {
dSolveFinalize(&options, &SOLVEstruct);
}
SUPERLU_FREE(b);
SUPERLU_FREE(xtrue);
SUPERLU_FREE(berr);
/* ------------------------------------------------------------
RELEASE THE SUPERLU PROCESS GRID.
------------------------------------------------------------*/
out:
superlu_gridexit(&grid);
/* ------------------------------------------------------------
TERMINATES THE MPI EXECUTION ENVIRONMENT.
------------------------------------------------------------*/
MPI_Finalize();
}
Five basic steps are required to call a SuperLU routine:
1. Initialize the MPI environment and the SuperLU process grid.
63


This is achieved by the calls to the MPI routine MPI Init() and the SuperLU routine
superlu gridinit(). In this example, the communication domain for SuperLU is built upon
the MPI default communicator MPI COMM WORLD. In general, it can be built upon any MPI
communicator. Section 4.4 contains the details about this step.
2. Set up the input matrix and the right-hand side.
This example uses the interface with the distributed input matrices, see Section 4.2.2. In
most practical applications, the matrices can be generated on each process without the need
to have a centralized place to hold them. But for this example, we let process 0 read the input
matrix stored on disk in Harwell-Boeing format [12] (a.k.a. compressed column storage), and
distribute it to all the other processes, so that each process only owns a block of rows of
matrix. The right-hand side matrix is generated so that the exact solution matrix consists of
all ones. The subroutine dcreate matrix() accomplishes this task.
3. Initialize the input arguments: options, ScalePermstruct, LUstruct, stat.
The input argument options controls how the linear system would be solved—use equilibra-
tion or not, how to order the rows and the columns of the matrix, use iterative reﬁnement or
not. The subroutine set default options dist() sets the options argument so that the
solver performs all the functionality. You can also set it up according to your own needs,
see section 4.6 for the ﬁelds of this structure. ScalePermstruct is the data structure that
stores the several vectors describing the transformations done to A. LUstruct is the data
structure in which the distributed L and U factors are stored. Stat is a structure collecting
the statistics about runtime and ﬂop count, etc.
4. Call the SuperLU routine pdgssvx().
5. Release the process grid and terminate the MPI environment.
After the computation on a process grid has been completed, the process grid should be
released by a call to the SuperLU routine superlu gridexit(). When all computations have
been completed, the MPI routine MPI Finalize() should be called.
4.8
User-callable routines
4.8.1
Driver routines
There are two driver routines to solve systems of linear equations, which are named pdgssvx ABglobal
for the global input interface, and pdgssvx for the distributed interface. We recommend that the
general users, especially the beginners, use a driver routine rather than the computational rou-
tines, because correctly using the driver routine does not require thorough understanding of the
underlying data structures. Although the interface of these routines are simple, we expect their
rich functionality can meet the requirements of most applications. Pdgssvx ABglobal/pdgssvx
perform the following functions:
• Equilibrate the system (scale A’s rows and columns to have unit norm) if A is poorly scaled;
• Find a row permutation that makes diagonal of A large relative to the oﬀ-diagonal;
• Find a column permutation that preserves the sparsity of the L and U factors;
64


• Solve the system AX = B for X by factoring A followed by forward and back substitutions;
• Reﬁne the solution X.
4.8.2
Computational routines
The experienced users can invoke the following computational routines to directly control the
behavior of SuperLU DIST in order to meet their requirements.
• pdgstrf(): Factorize in parallel.
This routine factorizes the input matrix A (or the scaled and permuted A). It assumes that
the distributed data structures for L and U factors are already set up, and the initial values
of A are loaded into the data structures. If not, the routine symbfact() should be called to
determine the nonzero patterns of the factors, and the routine pddistribute() should be
called to distribute the matrix. Pdgstrf() can factor non-square matrices.
• pdgstrs()/pdgstrs Bglobal(): Triangular solve in parallel.
This routine solves the system by forward and back substitutions using the the L and U
factors computed by pdgstrf(). Pdgstrs() takes distributed B. For pdgstrs Bglobal(),
B must be globally available on all processes.
• pdgsrfs()/pdgsrfs ABXglobal(): Reﬁne solution in parallel.
Given A, its factors L and U, and an initial solution X, this routine performs iterative
reﬁnement. Pdgsrfs() takes distributed A, B and X. For pdgsrfs ABXglobal(), A, B and
X must be globally available on all processes.
4.8.3
Utility routines
The following utility routines can help users create and destroy the SuperLU DIST matrices. These
routines reside in three places: SRC/util.c, SRC/{d,z}util.c, and SRC/p{d,z}util.c. Most of
the utility routines in sequential SuperLU can also be used in SuperLU DIST for the local data, see
Section 2.9.3. Here, we only list those new routines speciﬁc to SuperLU DIST. Note that in order to
avoid name clash between SuperLU and SuperLU DIST, we append “ dist” to each routine name
in SuperLU DIST.
/* Create a supermatrix in distributed compressed row format. A is output. */
dCreate_CompRowLoc_Matrix_dist(SuperMatrix *A, int_t m, int_t n,
int_t nnz_loc, int_t m_loc, int_t fst_row,
double *nzval, int_t *colind, int_t *rowptr,
Stype_t stype, Dtype_t dtype, Mtype_t mtype);
/* Deallocate the supermatrix in distributed compressed row format. */
Destroy_CompRowLoc_Matrix_dist(SuperMatrix *A);
/* Allocate storage in ScalePermstruct. */
ScalePermstructInit(const int_t m, const int_t n,
ScalePermstruct_t *ScalePermstruct);
65


/* Deallocate ScalePermstruct */
ScalePermstructFree(ScalePermstruct_t *ScalePermstruct);
/* Allocate storage in LUstruct. */
LUstructInit(const int_t n, LUstruct_t *LUstruct);
/* Deallocate the distributed L & U factors in LUstruct. */
Destroy_LU(int_t n, gridinfo_t *grid, LUstruct_t *LUstruct);
/* Deallocate LUstruct. */
LUstructFree(LUstruct_t *LUstruct);
/* Initialize the statistics variable. */
PStatInit(SuperLUStat_t *stat);
/* Print the statistics. */
PStatPrint(superlu_options_t *options, SuperLUStat_t *stat,
gridinfo_t *grid);
/* Deallocate the statistics variable. */
PStatFree(SuperLUStat_t *stat);
4.9
Installation
4.9.1
File structure and complilation
The top level SuperLU DIST/ directory is structured as follows:
SuperLU_DIST/README.md instructions on installation
SuperLU_DIST/CBLAS/
BLAS routines in C, functional but not fast
SuperLU_DIST/DOC/
Users’ Guide
SuperLU_DIST/EXAMPLE/
example programs
SuperLU_DIST/INSTALL/
test machine dependent parameters
SuperLU_DIST/SRC/
C source code, to be compiled into libsuperlu_dist.a
SuperLU_DIST/lib/
contains library archive libsuperlu_dist.a
SuperLU_DIST/Makefile
top level Makefile that does installation and testing
SuperLU_DIST/make.inc
compiler, compiler flags, library definitions and C
preprocessor definitions, included in all Makefiles.
(You may need to edit it to suit for your system
before compiling the whole package.)
SuperLU_DIST/MAKE_INC/ sample machine-specific make.inc files
You can use CMake automic build system to install the package. Please see README.md for
instruction. The following describes how to install manually by editing a Makeﬁle.
Before installing the package, you may need to edit SuperLU DIST/make.inc for your system.
This make include ﬁle is referenced inside all the Makeﬁles in the various subdirectories. As a
66


result, there is no need to edit the Makeﬁles in the subdirectories. All information that is machine
speciﬁc has been deﬁned in this include ﬁle.
Sample machine-speciﬁc make.inc are provided in the MAKE INC/ directory for several platforms,
such as Cray XE6 and IBM SP. When you have selected the machine to which you wish to install
SuperLU DIST, you may copy the appropriate sample include ﬁle (if one is present) into make.inc.
For example, if you wish to run on a Cray XE6, you can do:
cp MAKE INC/make.xe6 make.inc
For the systems other than those listed above, slight modiﬁcations to the make.inc ﬁle will
need to be made. In particular, the following items should be examined:
1. The BLAS library.
If there is a BLAS library available on your machine, you may deﬁne the following in make.inc:
BLASDEF = -DUSE VENDOR BLAS
BLASLIB = <BLAS library you wish to link with>
The CBLAS/ subdirectory contains the part of the BLAS (in C) needed by SuperLU DIST
package. However, these routines are intended for use only if there is no faster implementation
of the BLAS already available on your machine. In this case, you should do the following:
1) In make.inc, undeﬁne (comment out) BLASDEF, deﬁne:
BLASLIB = ../lib/libblas$(PLAT).a
2) At the top level SuperLU DIST directory, type:
make blaslib
to create the BLAS library from the routines in CBLAS/ subdirectory.
2. External libraries: MeTiS and ParMeTiS.
If you will use MeTiS or ParMeTiS ordering, or parallel symbolic factorization (which depends
on ParMeTiS), you will need to install them yourself. Since ParMeTiS package already contains
the source code for the MeTiS library, you can just download ParMeTiS at:
http://glaros.dtc.umn.edu/gkhome/metis/parmetis/download
After you have installed it, you should deﬁne the following in make.inc:
PARMETIS DIR = <top-level ParMeTiS directory>
METISLIB = -L$(PARMETIS DIR)/build/Linux-x86 64/libmetis -lmetis
PARMETISLIB = -L$(PARMETIS DIR)/build/Linux-x86 64/libparmetis -lparmetis
I PARMETIS = -I$(PARMETIS DIR)/include -I$(PARMETIS DIR)/metis/include
3. C preprocessor deﬁnition CDEFS.
In the header ﬁle SRC/Cnames.h, we use macros to determine how C routines should be named
so that they are callable by Fortran.1 The possible options for CDEFS are:
• -DAdd : Fortran expects a C routine to have an underscore postﬁxed to the name;
• -DNoChange: Fortran expects a C routine name to be identical to that compiled by C;
• -DUpCase: Fortran expects a C routine name to be all uppercase.
1Some vendor-supplied BLAS libraries do not have C interfaces. So the re-naming is needed in order for the
SuperLU BLAS calls (in C) to interface with the Fortran-style BLAS.
67


4. (optional) Enable Nvidia GPU access.
(a) Set the following Linux environment variable: setenv ACC GPU
(b) Add the CUDA library location in make.inc:
ifeq "${ACC}" "GPU"
CFLAGS += -DGPU_ACC
INCS += -I<CUDA directory>/include
LIBS += -L<CUDA directory>/lib64 -lcublas -lcudart
endif
A Makefile is provided in each subdirectory. The installation can be done automatically by
simply typing “make” at the top level.
Hybrid MPI+OpenMP setting is implemented in the factorization routines. It may outperform
MPI-only conﬁgurations in some cases and requires less memory. To use OpenMP parallelism, you
need to compile the code with the following CPP deﬁnition:
-D_OPENMP
and set the number of threads to be used in the environment variable:
setenv OMP_NUM_THREADS <##>
needs to be set to enable this feature.
4.9.2
Performance-tuning parameters
Similar to sequential SuperLU, several performance related parameters are set in the inquiry func-
tion sp ienv(). The declaration of this function is
int sp ienv(int ispec);
Ispec speciﬁes the parameter to be returned2:
ispec = 2: the relaxation parameter to control supernode amalgamation
= 3: the maximum allowable size for a block (supernode)
= 6: the estimated ﬁlls factor for the adjacency structures of L and U
The values to be returned may be set diﬀerently on diﬀerent machines. The setting of maximum
block size (parameter 3) should take into account the local Level 3 BLAS speed, the load balance and
the degree of parallelism. Small block size may result in better load balance and more parallelism,
but poor individual node performance, and vice versa for large block size.
These parameters can also be set as Linux environment variables, so that the routine sp ienv()
does not need to be recompiled every time when you change the settings.
setenv NREL <##>
/* parameter #2: maximum size of the relaxed supernode */
setenv NSUP <##>
/* parameter #3: maximum supernode size */
2The numbering of 2, 3 and 6 is consistent with that used in SuperLU and SuperLU MT.
68


The following parameters are related to GPU usage:
setenv CUBLAS_NB <##>
/* middle dimension of CUDA GEMM (default 64) */
setenv MAX_BUFFER_SIZE <##>
/* maximum buffer size on GPU (default 5M words) */
These parameters are described in detail in various algorithm papers, see [24, 34].
4.10
Example programs
In the SuperLU DIST/EXAMPLE/ directory, we present a few sample programs to illustrate the com-
plete calling sequences to use the expert driver to solve systems of equations. These include how to
set up the process grid and the input matrix, how to obtain a ﬁll-reducing ordering. A Makefile
is provided to generate the executables. A README ﬁle in this directory shows how to run these
examples. The leading comment in each routine describes the functionality of the example. The
two basic examples are pddrive ABglobal() and pddrive(). The ﬁrst shows how to use the global
input interface, and the second shows how to use the distributed input interface.
4.11
Fortran 90 Interface
We developed a complete Fortran 90 interface for SuperLU DIST. All the interface ﬁles and an
example driver program are located in the SuperLU DIST/FORTRAN/ directory. Table 4.1 lists all
the ﬁles.
f pddrive.f90
An example Fortran driver routine.
superlu mod.f90
Fortran 90 module that deﬁnes the interface functions to access SuperLU DIST’s
data structures.
superlupara.f90
It contains parameters that correspond to SuperLU DIST’s enums.
hbcode1.f90
Fortran function for reading a sparse Harwell-Boeing matrix from
the ﬁle.
superlu c2f wrap.c
C wrapper functions, callable from Fortran. The functions fall
into three classes: 1) Those that allocate a structure and return
a handle, or deallocate the memory of a structure. 2) Those that
get or set the value of a component of a struct. 3) Those that
are wrappers for SuperLU DIST functions.
dcreate dist matrix.c
C function for distributing the matrix in a distributed
compressed row format.
Table 4.1: The Fortran 90 interface ﬁles and an example driver routine.
Note that in this interface, all objects (such as grid, options, etc.) in SuperLU DIST are opaque,
meaning their size and structure are not visible to the Fortran user. These opaque objects are
allocated, deallocated and operated in the C side and not directly accessible from Fortran side.
They can only be accessed via handles that exist in Fortran’s user space. In Fortran, all handles
69


have type INTEGER. Speciﬁcally, in our interface, the size of Fortran handle is deﬁned by superlu ptr
in superlupara.f90. For diﬀerent systems, the size might need to be changed. Then using these
handles, Fortran user can call C wrapper routines to manipulate the opaque objects. For example,
you can call f create gridinfo(grid handle) to allocate memory for structure grid, and return a handle
grid handle.
The sample program illustrates the basic steps required to use SuperLU DIST in Fortran to solve
systems of equations. These include how to set up the processor grid and the input matrix, how to
call the linear equation solver. This program is listed below, and is also available as f pddrive.f90 in
the subdirectory. Note that the routine must include the moudle superlu mod which contains the
deﬁnitions of all parameters and the Fortran wrapper functions. A Makeﬁle is provided to generate
the executable. A README ﬁle in this directory shows how to run the example.
program f_pddrive
!
! Purpose
! =======
!
! The driver program F_PDDRIVE.
!
! This example illustrates how to use F_PDGSSVX with the full
! (default) options to solve a linear system.
!
! Seven basic steps are required:
!
1. Create C structures used in SuperLU
!
2. Initialize the MPI environment and the SuperLU process grid
!
3. Set up the input matrix and the right-hand side
!
4. Set the options argument
!
5. Call f_pdgssvx
!
6. Release the process grid and terminate the MPI environment
!
7. Release all structures
!
use superlu_mod
include ’mpif.h’
implicit none
integer maxn, maxnz, maxnrhs
parameter ( maxn = 10000, maxnz = 100000, maxnrhs = 10 )
integer rowind(maxnz), colptr(maxn)
real*8
values(maxnz), b(maxn), berr(maxnrhs)
integer n, m, nnz, nrhs, ldb, i, ierr, info, iam
integer nprow, npcol
integer init
integer(superlu_ptr) :: grid
integer(superlu_ptr) :: options
integer(superlu_ptr) :: ScalePermstruct
integer(superlu_ptr) :: LUstruct
70


integer(superlu_ptr) :: SOLVEstruct
integer(superlu_ptr) :: A
integer(superlu_ptr) :: stat
! Create Fortran handles for the C structures used in SuperLU_DIST
call f_create_gridinfo(grid)
call f_create_options(options)
call f_create_ScalePermstruct(ScalePermstruct)
call f_create_LUstruct(LUstruct)
call f_create_SOLVEstruct(SOLVEstruct)
call f_create_SuperMatrix(A)
call f_create_SuperLUStat(stat)
! Initialize MPI environment
call mpi_init(ierr)
! Initialize the SuperLU_DIST process grid
nprow = 2
npcol = 2
call f_superlu_gridinit(MPI_COMM_WORLD, nprow, npcol, grid)
! Bail out if I do not belong in the grid.
call get_GridInfo(grid, iam=iam)
if ( iam >= nprow * npcol ) then
go to 100
endif
if ( iam == 0 ) then
write(*,*) ’ Process grid ’, nprow, ’ X ’, npcol
endif
! Read Harwell-Boeing matrix, and adjust the pointers and indices
! to 0-based indexing, as required by C routines.
if ( iam == 0 ) then
open(file = "g20.rua", status = "old", unit = 5)
call hbcode1(m, n, nnz, values, rowind, colptr)
close(unit = 5)
!
do i = 1, n+1
colptr(i) = colptr(i) - 1
enddo
do i = 1, nnz
rowind(i) = rowind(i) - 1
enddo
endif
71


! Distribute the matrix to the gird
call
f_dcreate_matrix_dist(A, m, n, nnz, values, rowind, colptr, grid)
! Setup the right hand side
nrhs = 1
call
get_CompRowLoc_Matrix(A, nrow_loc=ldb)
do i = 1, ldb
b(i) = 1.0
enddo
! Set the default input options
call f_set_default_options(options)
! Change one or more options
!
call set_superlu_options(options,Fact=FACTORED)
! Initialize ScalePermstruct and LUstruct
call get_SuperMatrix(A,nrow=m,ncol=n)
call f_ScalePermstructInit(m, n, ScalePermstruct)
call f_LUstructInit(m, n, LUstruct)
! Initialize the statistics variables
call f_PStatInit(stat)
! Call the linear equation solver
call f_pdgssvx(options, A, ScalePermstruct, b, ldb, nrhs, &
grid, LUstruct, SOLVEstruct, berr, stat, info)
if (info == 0) then
write (*,*) ’Backward error: ’, (berr(i), i = 1, nrhs)
else
write(*,*) ’INFO from f_pdgssvx = ’, info
endif
! Deallocate SuperLU allocated storage
call f_PStatFree(stat)
call f_Destroy_CompRowLoc_Matrix_dist(A)
call f_ScalePermstructFree(ScalePermstruct)
call f_Destroy_LU(n, grid, LUstruct)
call f_LUstructFree(LUstruct)
call get_superlu_options(options, SolveInitialized=init)
if (init == YES) then
call f_dSolveFinalize(options, SOLVEstruct)
endif
72


! Release the SuperLU process grid
100
call f_superlu_gridexit(grid)
! Terminate the MPI execution environment
call mpi_finalize(ierr)
! Destroy all C structures
call f_destroy_gridinfo(grid)
call f_destroy_options(options)
call f_destroy_ScalePermstruct(ScalePermstruct)
call f_destroy_LUstruct(LUstruct)
call f_destroy_SOLVEstruct(SOLVEstruct)
call f_destroy_SuperMatrix(A)
call f_destroy_SuperLUStat(stat)
stop
end
Similar to the driver routine pddrive.c in C, seven basic steps are required to call a SuperLU DIST
routine in Fortran:
1. Create C structures used in SuperLU: grid, options, ScalePermstruct, LUstruct, SOLVEstruct,
A and stat. This is achieved by the calls to the C wrapper “create” routines f create XXX(),
where XXX is the name of the corresponding structure.
2. Initialize the MPI environment and the SuperLU process grid. This is achieved by the calls to
mpi init() and the C wrapper routine f superlu gridinit(). Note that f superlu gridinit() requires
the numbers of row and column of the process grid. In this example, we set them to be 2,
respectively.
3. Set up the input matrix and the right-hand side. This example uses the distributed input
interface, so we need to convert the input matrix to the distributed compressed row format.
Process 0 ﬁrst reads the input matrix stored on disk in Harwell-Boeing format by calling
Fortran routine hbcode1(). The ﬁle name in this example is g20.rua. Then all processes
call a C wrapper routine f dcreate dist matrix() to distribute the matrix to all the processes
distributed by block rows. The right-hand side matrix in this example is a column vector of
all ones. Note that, before setting the right-hand side, we use get CompRowLoc Matrix() to
get the number of local rows in the distributed matrix A.
One important note is that all the C routines use 0-based indexing scheme. Therefore, after
process 0 reads the matrix in compressed column format, we decrement its column pointers
(colptr) and row indices (rowind) by 1 so they become 0-based indexing.
4. Set the input arguments: options, ScalePermstruct, LUstruct, and stat. The input argument
options controls how the linear system would be sloved. The routine f set default options dist()
sets the options argument so that the slover performs all the functionalities. You can also set it
according to your own needs, using a call to the Fortran routine set superlu options(). LUstruct
73


is the data struture in which the distributed L and U factors are stored. ScalePermstruct is
the data struture in which several vectors describing the transformations done to matrix A
are stored. stat is a structure collecting the statistcs about runtime and ﬂop count. These
three structures can be set by calling the C wrapper “init” routines f XXXInit.
5. Call the C wrapper routine f pdgssvx() to solve the equation.
6. Release the process grid and terminate the MPI environment. After the computation on a pro-
cess grid has been completed, the process grid should be released by a call to f spuerlu gridexit().
When all computations have been completed, the C wrapper routine mpi ﬁnalize() should be
called.
7. Deallocate all the structures. First we need to deallocate the storage allocated by SuperLU DIST
by a set of “free” calls. Note that this should be called before f spuerlu gridexit(), since some of
the “free” calls use the grid. Then we call the C wrapper “destroy” routines f destroy XXX()
to destroy all the Fortran handles.
Note that f destroy gridinfo() should be called after
f spuerlu gridexit().
4.11.1
Callable functions in the Fortran 90 module ﬁle spuerlu mod.f90
The Fortran 90 module superlu mod contains the interface routines that can manipulate a SuperLU DIST
object from Fortran. The object is pointed to by the corresponding handle input to these routines.
The routines are divided into two sets. One set is to get the properties of an object, with the
routine names “get XXX()”. Another set is to set some properties for an object, with the routine
names “set XXX()”. These functions have optional arguments, so the users do not have to provide
the full set of parameters. Superlu mod module uses superluparam mod module that deﬁnes all
the integer constants corresponding to the enumeration constants in SuperLU DIST. Below are the
calling sequences of all the routines.
subroutine get_GridInfo(grid, iam, nprow, npcol)
integer(superlu_ptr) :: grid
integer, optional :: iam, nprow, npcol
subroutine get_SuperMatrix(A, nrow, ncol)
integer(superlu_ptr) :: A
integer, optional :: nrow, ncol
subroutine set_SuperMatrix(A, nrow, ncol)
integer(superlu_ptr) :: A
integer, optional :: nrow, ncol
subroutine get_CompRowLoc_Matrix(A, nrow, ncol, nnz_loc, nrow_loc, fst_row)
integer(superlu_ptr) :: A
integer, optional :: nrow, ncol, nnz_loc, nrow_loc, fst_row
subroutine set_CompRowLoc_Matrix(A, nrow, ncol, nnz_loc, nrow_loc, fst_row)
integer(superlu_ptr) :: A
74


integer, optional :: nrow, ncol, nnz_loc, nrow_loc, fst_row
subroutine get_superlu_options(opt, Fact, Trans, Equil, RowPerm, &
ColPerm, ReplaceTinyPivot, IterRefine, &
SolveInitialized, RefineInitialized)
integer(superlu_ptr) :: opt
integer, optional :: Fact, Trans, Equil, RowPerm, ColPerm, &
ReplaceTinyPivot, IterRefine, SolveInitialized, &
RefineInitialized
subroutine set_superlu_options(opt, Fact, Trans, Equil, RowPerm, &
ColPerm, ReplaceTinyPivot, IterRefine, &
SolveInitialized, RefineInitialized)
integer(superlu_ptr) :: opt
integer, optional :: Fact, Trans, Equil, RowPerm, ColPerm, &
ReplaceTinyPivot, IterRefine, SolveInitialized, &
RefineInitialized
4.11.2
C wrapper functions callable by Fortran in ﬁle spuerlu c2f wrap.c
This ﬁle contains the Fortran-callable C functions which wraps around the user-callable C routines
in SuperLU DIST. The functions are divided into three classes: 1) allocate a C structure and return
a handle to Fortran, or deallocate the memory of of a C structure given its Fortran handle; 2) get
or set the value of certain ﬁelds of a C structure given its Fortran handle; 3) wrapper functions for
the SuperLU DIST C functions. Below are the calling sequences of these routines.
/* functions that allocate memory for a structure and return a handle */
void f_create_gridinfo(fptr *handle)
void f_create_options(fptr *handle)
void f_create_ScalePermstruct(fptr *handle)
void f_create_LUstruct(fptr *handle)
void f_create_SOLVEstruct(fptr *handle)
void f_create_SuperMatrix(fptr *handle)
void f_create_SuperLUStat(fptr *handle)
/* functions that free the memory allocated by the above functions */
void f_destroy_gridinfo(fptr *handle)
void f_destroy_options(fptr *handle)
void f_destroy_ScalePermstruct(fptr *handle)
void f_destroy_LUstruct(fptr *handle)
void f_destroy_SOLVEstruct(fptr *handle)
void f_destroy_SuperMatrix(fptr *handle)
void f_destroy_SuperLUStat(fptr *handle)
/* functions that get or set certain fields in a C structure. */
void f_get_gridinfo(fptr *grid, int *iam, int *nprow, int *npcol)
75


void f_get_SuperMatrix(fptr *A, int *nrow, int *ncol)
void f_set_SuperMatrix(fptr *A, int *nrow, int *ncol)
void f_get_CompRowLoc_Matrix(fptr *A, int *m, int *n, int *nnz_loc,
int *m_loc, int *fst_row)
void f_set_CompRowLoc_Matrix(fptr *A, int *m, int *n, int *nnz_loc,
int *m_loc, int *fst_row)
void f_get_superlu_options(fptr *opt, int *Fact, int *Trans, int *Equil,
int *RowPerm, int *ColPerm, int *ReplaceTinyPivot,
int *IterRefine, int *SolveInitialized,
int *RefineInitialized)
void f_set_superlu_options(fptr *opt, int *Fact, int *Trans, int *Equil,
int *RowPerm, int *ColPerm, int *ReplaceTinyPivot,
int *IterRefine, int *SolveInitialized,
int *RefineInitialized)
/* wrappers for SuperLU_DIST routines */
void f_dCreate_CompRowLoc_Matrix_dist(fptr *A, int *m, int *n, int *nnz_loc,
int *m_loc, int *fst_row, double *nzval,
int *colind, int *rowptr, int *stype,
int *dtype, int *mtype)
void f_set_default_options(fptr *options)
void f_superlu_gridinit(int *Bcomm, int *nprow, int *npcol, fptr *grid)
void f_superlu_gridexit(fptr *grid)
void f_ScalePermstructInit(int *m, int *n, fptr *ScalePermstruct)
void f_ScalePermstructFree(fptr *ScalePermstruct)
void f_PStatInit(fptr *stat)
void f_PStatFree(fptr *stat)
void f_LUstructInit(int *m, int *n, fptr *LUstruct)
void f_LUstructFree(fptr *LUstruct)
void f_Destroy_LU(int *n, fptr *grid, fptr *LUstruct)
void f_Destroy_CompRowLoc_Matrix_dist(fptr *A)
void f_dSolveFinalize(fptr *options, fptr *SOLVEstruct)
void f_pdgssvx(fptr *options, fptr *A, fptr *ScalePermstruct, double *B,
int *ldb, int *nrhs, fptr *grid, fptr *LUstruct,
fptr *SOLVEstruct, double *berr, fptr *stat, int *info)
void f_check_malloc(int *iam)
76


Bibliography
[1] E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Ham-
marling, A. McKenney, S. Ostrouchov, and D. Sorensen. LAPACK Users’ Guide, Release 2.0.
SIAM, Philadelphia, 1995. 324 pages.
[2] M. Arioli, J. W. Demmel, and I. S. Duﬀ. Solving sparse linear systems with sparse backward
error. SIAM J. Matrix Anal. Appl., 10(2):165–190, April 1989.
[3] A. Azad, A. Buluc, X.S. Li, X. Wang, and J. Langguth. A Distributed-Memory Approximation
Algorithm for Maximum Weight Perfect Bipartite Matching. SIAM J. Scientiﬁc Computing,
2018. arXiv:1801.09809v1, 30 Jan 2018. Submitted.
[4] L. S. Blackford, J. Choi, E. D’Azevedo, J. Demmel, I. Dhillon, J. Dongarra, S. Hammarling,
G. Henry, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. ScaLAPACK Users’ Guide.
SIAM, Philadelphia, 1997. http://www.netlib.org/scalapack.
[5] T. A. Davis, J. R. Gilbert, S. Larimore, and E. Ng. A column approximate minimum degree
ordering algorithm. ACM Trans. Mathematical Software, 30(3):353–376, 2004.
[6] J. W. Demmel, S. C. Eisenstat, J. R. Gilbert, X. S. Li, and J. W. H. Liu. A supernodal approach
to sparse partial pivoting. SIAM J. Matrix Analysis and Applications, 20(3):720–755, 1999.
[7] J. W. Demmel, J. R. Gilbert, and X. S. Li. An asynchronous parallel supernodal algorithm for
sparse gaussian elimination. SIAM J. Matrix Analysis and Applications, 20(4):915–952, 1999.
[8] J. Dongarra, J. Du Croz, I. S. Duﬀ, and S. Hammarling. A Set of Level 3 Basic Linear Algebra
Subprograms. ACM Trans. Math. Soft., 16:1–17, 1990.
[9] J. Dongarra, J. Du Croz, S. Hammarling, and Richard J. Hanson. An Extended Set of FOR-
TRAN Basic Linear Algebra Subprograms. ACM Trans. Math. Soft., 14(1):1–17, March 1988.
[10] I. S. Duﬀand J. Koster. The design and use of algorithms for permuting large entries to the
diagonal of sparse matrices. SIAM J. Matrix Analysis and Applications, 20(4):889–901, 1999.
[11] I. S. Duﬀand J. Koster. On algorithms for permuting large entries to the diagonal of a sparse
matrix. SIAM J. Matrix Analysis and Applications, 22(4):973–996, 2001.
[12] I.S. Duﬀ, R.G. Grimes, and J.G. Lewis. Users’ guide for the Harwell-Boeing sparse matrix col-
lection (release 1). Technical Report RAL-92-086, Rutherford Appleton Laboratory, December
1992.
77


[13] A. George, J. Liu, and E. Ng. A data structure for sparse QR and LU factorizations. SIAM
J. Sci. Stat. Comput., 9:100–121, 1988.
[14] A. George and E. Ng. Symbolic factorization for sparse Gaussian elimination with partial
pivoting. SIAM J. Sci. Stat. Comput., 8(6):877–898, 1987.
[15] J. R. Gilbert, X. S. Li, E. G. Ng, and B. W. Peyton. Computing row and column counts for
sparse QR and LU factorization. BIT, 41(4):693–710, 2001.
[16] John R. Gilbert and Esmond G. Ng.
Predicting structure in nonsymmetric sparse matrix
factorizations. In A. George, J. R. Gilbert, and J. W. H. Liu, editors, Graph theory and sparse
matrix computation, pages 107–139. Springer-Verlag, New York, 1993.
[17] L. Grigori, J. W. Demmel, and X. S. Li. Parallel symbolic factorization for sparse LU with
static pivoting. SIAM J. Scientiﬁc Computing, 29(3):1289–1314, 2007.
[18] B. Hendrickson and R. Leland.
The CHACO’s User’s Guide.
Technical Report SAND95-
2344•UC-405, Sandia National Laboratories, Albuquerque, 1995. http://www.cs.sandia.
gov/~bahendr/chaco.html.
[19] N. J. Higham. Accuracy and Stability of Numerical Algorithms. SIAM, Philadelphia, PA, 1996.
[20] G. Karypis and V. Kumar. Me
T
iS – serial graph partitioning and computing ﬁll-reducing ma-
trix ordering. University of Minnesota. http://glaros.dtc.umn.edu/gkhome/views/metis.
[21] G. Karypis, K. Schloegel, and V. Kumar. ParMe
T
iS: Parallel graph partitioning and sparse
matrix ordering library – version 3.1. University of Minnesota, 2003. http://www-users.cs.
umn.edu/~karypis/metis/parmetis/.
[22] C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. Basic Linear Algebra Subprograms for
Fortran usage. ACM Trans. Math. Soft., 5:308–323, 1979.
[23] X. S. Li.
Sparse Gaussian elimination on high performance computers.
Technical Report
UCB//CSD-96-919, Computer Science Division, U.C. Berkeley, September 1996. Ph.D disser-
tation.
[24] X. S. Li. An overview of SuperLU: Algorithms, implementation, and user interface. ACM
Trans. Mathematical Software, 31(3):302–325, September 2005.
[25] X. S. Li and J. W. Demmel. Making sparse Gaussian elimination scalable by static pivoting.
In Proceedings of SC98: High Performance Networking and Computing Conference, Orlando,
Florida, November 7–13 1998.
[26] X. S. Li and J. W. Demmel. SuperLU DIST: A scalable distributed-memory sparse direct
solver for unsymmetric linear systems. ACM Trans. Mathematical Software, 29(2):110–140,
June 2003.
[27] X. S. Li and M. Shao. A supernodal approach to imcomplete LU factorization with partial
pivoting. ACM Trans. Mathematical Software, 37(4), 2010.
78


[28] J. W. H. Liu. Modiﬁcation of the minimum degree algorithm by multiple elimination. ACM
Trans. Mathematical Software, 11:141–153, 1985.
[29] Message Passing Interface (MPI) forum. http://www.mpi-forum.org/.
[30] W. Oettli and W. Prager. Compatibility of approximate solution of linear equations with given
error bounds for coeﬃcients and right hand sides. Num. Math., 6:405–409, 1964.
[31] F. Pellegrini. Scotch and libScotch 5.1 User’s Guide (version 5.1.11). INRIA Bordeaux
Sud-Ouest, Universit´
e Bordeaux I. November, 2010. http://www.labri.fr/perso/pelegrin/
scotch/.
[32] POSIX System Application Arogram Interface:
Threads extension [C Language], POSIX
1003.1c draft 4. IEEE Standards Department.
[33] Y. Saad. ILUT: A dual threshold incomplete LU factorization. Numerical Linear Algebra with
Applications, 1(4):387–402, 1994.
[34] P. Sao, R. Vuduc, and X. Li.
A distributed CPU-GPU sparse direct solver.
In Proc. of
Euro-Par 2014, LNCS Vol. 8632, pp. 487-498, Porto, Portugal, August 25-29 2014.
[35] R.D. Skeel. Iterative reﬁnement implies numerical stability for Gaussian elimination. Mathe-
matics of Computation, 35(151):817–832, 1980.
79


# module load itac/2018.beta
module swap intel/18.0.0.128 intel/17.0.3.191 

module load itac/2017.up1
module rm cray-mpich/7.6.0
module load impi
export LD_PRELOAD=$VT_ROOT/slib/libVT.so
export VT_LOGFILE_FORMAT=STFSINGLE
export I_MPI_PMI_LIBRARY=/usr/lib64/slurmpmi/libpmi.so
export VT_PCTRACE=5

export OMP_NUM_THREADS=1
export KMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export NSUP=128
export NREL=20

srun -trace -n 32 ./EXAMPLE/pddrive -r 4 -c 8 ./EXAMPLE/torso3.mtx 

traceanalyzer pddrive.single.stf



# export I_MPI_FABRICS=ofi
# export I_MPI_OFI_PROVIDER=gni
# export I_MPI_OFI_LIBRARY=/usr/common/software/libfabric/1.5.0/gnu/lib/libfabric.so




module load itac/2017.up1
module swap intel/18.0.0.128 intel/17.0.3.191
export LD_PRELOAD=$VT_ROOT/slib/libVT.so
export VT_LOGFILE_FORMAT=STFSINGLE
export I_MPI_PMI_LIBRARY=/usr/lib64/slurmpmi/libpmi.so


export OMP_NUM_THREADS=1
export KMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export NSUP=128
export NREL=20

srun -trace -n 32 ./EXAMPLE/pddrive -r 4 -c 8 ./EXAMPLE/torso3.mtx 

traceanalyzer pddrive.single.stf



# export I_MPI_FABRICS=ofi
# export I_MPI_OFI_PROVIDER=gni
# export I_MPI_OFI_LIBRARY=/usr/common/software/libfabric/1.5.0/gnu/lib/libfabric.so




include_directories(${SuperLU_DIST_SOURCE_DIR}/SRC)

# Libs linked to all of the tests
set(all_link_libs superlu_dist ${BLAS_LIB})
if (NOT MSVC)
  list(APPEND all_link_libs m)
endif ()

set(MATRICES ../EXAMPLE/g20.rua)  # sample sparse matrix from a file
set(NPROWS 1 2 5)	  # process rows
set(NPCOLS 1 2 3) 	  # process columns
set(NVAL 9 19)	  	  # generated matrix dimensions
set(NRHS 1 3)		  # number of RHS
# set(FILLRATIO 2 10)	  # estimated fill ratio
set(FILLRATIO 2)	  # estimated fill ratio
# following are blocking parameters, see sp_ienv.c
set(RELAX 8)	   	  # relaxed supernode size: 4 8
set(SUPERSIZE 20)   	  # maximum supernode size: 10 20
set(MINGEMM 10000)	  # minimum GEMM size for GPU offload

function(cat IN_FILE OUT_FILE)
  file(READ ${IN_FILE} CONTENTS)
  file(APPEND ${OUT_FILE} "${CONTENTS}")
endfunction()

# Function to perform test
# call API:  add_superlu_dist_tests(pddrive big.rua)
function(add_superlu_dist_tests target input)
   set(TEST_INPUT "${SuperLU_DIST_SOURCE_DIR}/EXAMPLE/${input}")
   set(TEST_OUTPUT "${SuperLU_DIST_BINARY_DIR}/TEST/${target}.out")

  # Prepare a temporary file to "cat" to:
  # file(WRITE ${TEST_OUTPUT} "")

##  get_target_property(TEST_LOC ${target} LOCATION)
   set(TEST_LOC ${CMAKE_CURRENT_BINARY_DIR})

   foreach (r ${NPROWS})
      foreach (c ${NPCOLS})
        MATH( EXPR np "${r}*${c}" )
        foreach (s ${NRHS})
	  foreach (b ${FILLRATIO})
	    foreach (x ${RELAX})
	      foreach (m ${SUPERSIZE})
                set(testName "${target}_${r}x${c}_${s}_${b}_${x}_${m}")
	  	set(SINGLE_OUTPUT ${SuperLU_DIST_BINARY_DIR}/TEST/${testName}.out)
          add_test( NAME ${testName}_SP
	    	    COMMAND ${MPIEXEC_EXECUTABLE} ${MPIEXEC_NUMPROC_FLAG} ${np}
            	    ${MPIEXEC_PREFLAGS} ${TEST_LOC}/${target} ${MPIEXEC_POSTFLAGS}
		    -r ${r} -c ${c} -s ${s} -b ${b} -x ${x} -m ${m} -f ${TEST_INPUT}
		  )
#          add_test( ${testName}_SP "${CMAKE_COMMAND}"
#	    -DTEST=${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} ${np}
#            ${MPIEXEC_PREFLAGS} ${target} ${MPIEXEC_POSTFLAGS} -r ${r} -c ${c} -s ${s} -b ${b} -x ${x} -m ${m} -f ${TEST_INPUT}
#	    -DOUTPUT=${SINGLE_OUTPUT}
#	    -DALL_OUTPUT=${TEST_OUTPUT}
#	    -DHEADING=Sparse\ matrix\ ${TEST_INPUT}\ --\ r=${r},\ c=${c},\ s=${s},\ x=${x},\ m=${m}
#	    -P "${SuperLU_DIST_SOURCE_DIR}/TEST/runtest.cmake"
#		  )
	      endforeach()
	    endforeach()
	  endforeach()
	endforeach()
      endforeach()
   endforeach()

# MPI variables:
# ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} PROCS
#  	${MPIEXEC_PREFLAGS} EXECUTABLE ${MPIEXEC_POSTFLAGS} ARGS)

endfunction(add_superlu_dist_tests)

if(enable_double)
  set(DTEST pdtest.c dcreate_matrix.c pdcompute_resid.c)
  add_executable(pdtest ${DTEST})
  target_link_libraries(pdtest ${all_link_libs})
  target_compile_features(pdtest PUBLIC c_std_99)
  add_superlu_dist_tests(pdtest g20.rua)
endif()

#if(enable_complex16)
#  set(ZTEST pztest.c zcreate_matrix.c pzcompute_resid.c)
#endif()


# include the paths for header files
include_directories(${SuperLU_DIST_SOURCE_DIR}/SRC)
include_directories(${SuperLU_DIST_BINARY_DIR}/FORTRAN)

set(sources
    superlu_c2f_wrap.c  # initialize precision-independent file
    superlupara.f90
    superlu_mod.f90
    )

if(enable_double)
  list(APPEND sources c2f_dcreate_matrix_x_b.c superlu_c2f_dwrap.c)
endif()
if(enable_complex16)
  list(APPEND sources c2f_zcreate_matrix_x_b.c superlu_c2f_zwrap.c)
endif()  

add_library(superlu_dist_fortran ${sources})
add_library(superlu_dist_fortran-static STATIC ${sources})
# set(targets superlu_dist_fortran)
get_target_property(superlu_dist_version superlu_dist VERSION)
get_target_property(superlu_dist_soversion superlu_dist SOVERSION)
set_target_properties(superlu_dist_fortran PROPERTIES VERSION ${superlu_dist_version})
set_target_properties(superlu_dist_fortran PROPERTIES SOVERSION ${superlu_dist_soversion})
target_link_libraries(superlu_dist_fortran superlu_dist)

set_target_properties(superlu_dist_fortran-static PROPERTIES OUTPUT_NAME superlu_dist_fortran)
set_target_properties(superlu_dist_fortran-static PROPERTIES VERSION ${superlu_dist_version})
set_target_properties(superlu_dist_fortran-static PROPERTIES SOVERSION ${superlu_dist_soversion})
target_link_libraries(superlu_dist_fortran-static superlu_dist)

# depends on FPP defs
add_dependencies(superlu_dist_fortran config_f)
add_dependencies(superlu_dist_fortran-static config_f)
add_dependencies(superlu_dist_fortran-static superlu_dist_fortran)

install(TARGETS superlu_dist_fortran
# DESTINATION ${CMAKE_INSTALL_LIBDIR}
    RUNTIME DESTINATION "${INSTALL_BIN_DIR}"
    LIBRARY DESTINATION "${INSTALL_LIB_DIR}"
    ARCHIVE DESTINATION "${INSTALL_LIB_DIR}"
)
install(TARGETS superlu_dist_fortran-static
# DESTINATION ${CMAKE_INSTALL_LIBDIR}
    RUNTIME DESTINATION "${INSTALL_BIN_DIR}"
    LIBRARY DESTINATION "${INSTALL_LIB_DIR}"
    ARCHIVE DESTINATION "${INSTALL_LIB_DIR}"
)

install(DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}
  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}
  FILES_MATCHING PATTERN *.mod
  )
install(FILES superlu_dist_config.fh
  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}
  )

# Fortran MPI stuff
add_definitions(${MPI_Fortran_COMPILE_FLAGS})
include_directories(${MPI_Fortran_INCLUDE_PATH})
link_directories(${MPI_Fortran_LIBRARIES})

# Libs to be linked with the Fortran codes
set(fortran_link_libs superlu_dist_fortran ${MPI_Fortran_LIBRARIES} ${BLAS_LIB} ${CMAKE_Fortran_IMPLICIT_LINK_LIBRARIES})
set(fortran_link_libs superlu_dist_fortran-static ${MPI_Fortran_LIBRARIES} ${BLAS_LIB} ${CMAKE_Fortran_IMPLICIT_LINK_LIBRARIES})
set(all_link_libs ${fortran_link_libs} superlu_dist)

#message("!!! in Fortran: MPI_Fortran_LINK_FLAGS='${MPI_Fortran_LINK_FLAGS}'")
#message("!!! in Fortran: all_link_libs='${all_link_libs}'")
#message("!!! in Fortran: cxx_implicit='${CMAKE_CXX_IMPLICIT_LINK_LIBRARIES}'")
if (NOT MSVC)
  list(APPEND all_link_libs m)
endif ()

if(enable_examples)

if(enable_double)
## if(FALSE)
  set(F_DEXM f_pddrive.F90)
  add_executable(f_pddrive ${F_DEXM})
  target_link_libraries(f_pddrive ${all_link_libs})
# set_target_properties(f_pddrive PROPERTIES LINKER_LANGUAGE Fortran CUDA_RESOLVE_DEVICE_SYMBOLS ON)
  set_target_properties(f_pddrive PROPERTIES LINKER_LANGUAGE Fortran)
## CXX linker does not work with Intel compiler
# set_target_properties(f_pddrive PROPERTIES LINKER_LANGUAGE CXX LINK_FLAGS "${MPI_Fortran_LINK_FLAGS}")
  
  set(F_DEXM3D f_pddrive3d.F90)
  add_executable(f_pddrive3d ${F_DEXM3D})
  target_link_libraries(f_pddrive3d ${all_link_libs})
  set_target_properties(f_pddrive3d PROPERTIES LINKER_LANGUAGE Fortran)

  set(F_5x5 f_5x5.F90 sp_ienv.c)
  add_executable(f_5x5 ${F_5x5})
  target_link_libraries(f_5x5 ${all_link_libs})
  set_target_properties(f_5x5 PROPERTIES LINKER_LANGUAGE Fortran)

endif() ## enable_double

##if(FALSE)
if(enable_complex16)
  set(F_ZEXM f_pzdrive.F90)
  add_executable(f_pzdrive ${F_ZEXM})
  target_link_libraries(f_pzdrive ${all_link_libs})
  set_target_properties(f_pzdrive PROPERTIES LINKER_LANGUAGE Fortran)
#  set_target_properties(f_pzdrive PROPERTIES LINKER_LANGUAGE CXX LINK_FLAGS "${MPI_Fortran_LINK_FLAGS}")

  set(F_ZEXM3D f_pzdrive3d.F90)
  add_executable(f_pzdrive3d ${F_ZEXM3D})
  target_link_libraries(f_pzdrive3d ${all_link_libs})
  set_target_properties(f_pzdrive3d PROPERTIES LINKER_LANGUAGE Fortran)
#  set_target_properties(f_pzdrive3d PROPERTIES LINKER_LANGUAGE CXX LINK_FLAGS "${MPI_Fortran_LINK_FLAGS}")
endif() ## enable_complex16

endif ()  ## enable_example

# Format superlu_dist_config.fh from superlu_dist_config.h in C
add_custom_command(
  OUTPUT superlu_dist_config.fh
  COMMAND sed;'/^\\//;d';<;superlu_dist_config.h;>;superlu_dist_config.fh
  COMMAND cp;superlu_dist_config.fh;${SuperLU_DIST_SOURCE_DIR}/FORTRAN/.
  WORKING_DIRECTORY ${CMAKE_BINARY_DIR}/FORTRAN
)
add_custom_target(config_f DEPENDS superlu_dist_config.fh)


include_directories(${SuperLU_DIST_SOURCE_DIR}/SRC)

# Libs linked to all of the examples
#set(all_link_libs superlu_dist ${BLAS_LIB} ${NVSHMEM_LIB})
set(all_link_libs superlu_dist ${BLAS_LIB})
if (NOT MSVC)
  list(APPEND all_link_libs m)
endif ()

function(add_superlu_dist_example target input nprow npcol)
    set(EXAMPLE_INPUT "${SuperLU_DIST_SOURCE_DIR}/EXAMPLE/${input}")
    set(EXAMPLE_OUTPUT "${SuperLU_DIST_BINARY_DIR}/EXAMPLE/${target}.out")

##  get_target_property(TEST_LOC ${target} LOCATION)
    set(EXAMPLE_LOC ${CMAKE_CURRENT_BINARY_DIR})

    MATH( EXPR procs "${nprow}*${npcol}" )
#    message("MPIEXEC_FLAG is ${MPIEXEC_NUMPROC_FLAG}")

# corresponding to mpiexec -n 4 pddrive -r <nprow> -c <npcol> g20.rua
    add_test(${target} ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} ${procs}
             ${MPIEXEC_PREFLAGS} ${EXAMPLE_LOC}/${target} ${MPIEXEC_POSTFLAGS} -r "${nprow}" -c "${npcol}" "${EXAMPLE_INPUT}")

#     add_test(NAME ${target} COMMAND "${CMAKE_COMMAND}"
#              -DTEST=${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} ${procs}
#             ${MPIEXEC_PREFLAGS} ${target} ${MPIEXEC_POSTFLAGS} -r "${nprow}" -c "${npcol}" "${TEST_INPUT}"
#	     -DOUTPUT=${target}.out
#	    -P "${SuperLU_DIST_SOURCE_DIR}/EXAMPLE/runexample.cmake" )

# MPI variables:
# ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} PROCS
#  	${MPIEXEC_PREFLAGS} EXECUTABLE ${MPIEXEC_POSTFLAGS} ARGS)

endfunction(add_superlu_dist_example)


if(enable_double)
  set(DEXM pddrive.c dcreate_matrix.c)
  add_executable(pddrive ${DEXM})
  target_link_libraries(pddrive ${all_link_libs})
  install(TARGETS pddrive RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXM1 pddrive1.c dcreate_matrix.c)
  add_executable(pddrive1 ${DEXM1})
  target_link_libraries(pddrive1 ${all_link_libs})
  add_superlu_dist_example(pddrive1 big.rua 2 2)
  install(TARGETS pddrive1 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXM2 pddrive2.c dcreate_matrix.c dcreate_matrix_perturbed.c)
  add_executable(pddrive2 ${DEXM2})
  target_link_libraries(pddrive2 ${all_link_libs})
  add_superlu_dist_example(pddrive2 big.rua 2 2)
  install(TARGETS pddrive2 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXM3 pddrive3.c dcreate_matrix.c)
  add_executable(pddrive3 ${DEXM3})
  target_link_libraries(pddrive3 ${all_link_libs})
  add_superlu_dist_example(pddrive3 big.rua 2 2)
  install(TARGETS pddrive3 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXM4 pddrive4.c dcreate_matrix.c)
  add_executable(pddrive4 ${DEXM4})
  target_link_libraries(pddrive4 ${all_link_libs})
  install(TARGETS pddrive4 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  

  set(DEXM3D pddrive3d.c dcreate_matrix.c dcreate_matrix3d.c)
  add_executable(pddrive3d ${DEXM3D})
  target_link_libraries(pddrive3d ${all_link_libs})
  install(TARGETS pddrive3d RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  

  set(DEXM3D pddrive3d_block_diag.c dcreate_matrix.c dcreate_matrix3d.c)
  add_executable(pddrive3d_block_diag ${DEXM3D})
  target_link_libraries(pddrive3d_block_diag ${all_link_libs})
  install(TARGETS pddrive3d_block_diag RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")
  
  set(DEXM3D1 pddrive3d1.c dcreate_matrix.c dcreate_matrix3d.c)
  add_executable(pddrive3d1 ${DEXM3D1})
  target_link_libraries(pddrive3d1 ${all_link_libs})
  install(TARGETS pddrive3d1 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXM3D2 pddrive3d2.c dcreate_matrix.c dcreate_matrix3d.c)
  add_executable(pddrive3d2 ${DEXM3D2})
  target_link_libraries(pddrive3d2 ${all_link_libs})
  install(TARGETS pddrive3d2 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXM3D3 pddrive3d3.c dcreate_matrix.c dcreate_matrix3d.c)
  add_executable(pddrive3d3 ${DEXM3D3})
  target_link_libraries(pddrive3d3 ${all_link_libs})
  install(TARGETS pddrive3d3 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXMG pddrive_ABglobal.c)
  add_executable(pddrive_ABglobal ${DEXMG})
  target_link_libraries(pddrive_ABglobal ${all_link_libs})
  install(TARGETS pddrive_ABglobal RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXMG1 pddrive1_ABglobal.c)
  add_executable(pddrive1_ABglobal ${DEXMG1})
  target_link_libraries(pddrive1_ABglobal ${all_link_libs})
  install(TARGETS pddrive1_ABglobal RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXMG2 pddrive2_ABglobal.c)
  add_executable(pddrive2_ABglobal ${DEXMG2})
  target_link_libraries(pddrive2_ABglobal ${all_link_libs})
  install(TARGETS pddrive2_ABglobal RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXMG3 pddrive3_ABglobal.c)
  add_executable(pddrive3_ABglobal ${DEXMG3})
  target_link_libraries(pddrive3_ABglobal ${all_link_libs})
  install(TARGETS pddrive3_ABglobal RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXMG4 pddrive4_ABglobal.c)
  add_executable(pddrive4_ABglobal ${DEXMG4})
  target_link_libraries(pddrive4_ABglobal ${all_link_libs})
  install(TARGETS pddrive4_ABglobal RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(DEXMS pddrive_spawn.c dcreate_matrix.c)
  add_executable(pddrive_spawn ${DEXMS})
  target_link_libraries(pddrive_spawn ${all_link_libs})
  install(TARGETS pddrive_spawn RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  

endif() #### end enable_double

if(enable_single)
  set(SEXM psdrive.c screate_matrix.c screate_A_x_b.c)
  add_executable(psdrive ${SEXM})
  target_link_libraries(psdrive ${all_link_libs})
  install(TARGETS psdrive RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(SEXM1 psdrive1.c screate_matrix.c)
  add_executable(psdrive1 ${SEXM1})
  target_link_libraries(psdrive1 ${all_link_libs})
  add_superlu_dist_example(psdrive1 big.rua 2 2)
  install(TARGETS psdrive1 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(SEXM2 psdrive2.c screate_matrix.c screate_matrix_perturbed.c)
  add_executable(psdrive2 ${SEXM2})
  target_link_libraries(psdrive2 ${all_link_libs})
  add_superlu_dist_example(psdrive2 big.rua 2 2)
  install(TARGETS psdrive2 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(SEXM3 psdrive3.c screate_matrix.c)
  add_executable(psdrive3 ${SEXM3})
  target_link_libraries(psdrive3 ${all_link_libs})
  add_superlu_dist_example(psdrive3 big.rua 2 2)
  install(TARGETS psdrive3 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(SEXM4 psdrive4.c screate_matrix.c)
  add_executable(psdrive4 ${SEXM4})
  target_link_libraries(psdrive4 ${all_link_libs})
  install(TARGETS psdrive4 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(SEXM3D psdrive3d.c screate_matrix.c screate_matrix3d.c)
  add_executable(psdrive3d ${SEXM3D})
  target_link_libraries(psdrive3d ${all_link_libs})
  install(TARGETS psdrive3d RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(SEXM3D1 psdrive3d1.c screate_matrix.c screate_matrix3d.c)
  add_executable(psdrive3d1 ${SEXM3D1})
  target_link_libraries(psdrive3d1 ${all_link_libs})
  install(TARGETS psdrive3d1 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(SEXM3D2 psdrive3d2.c screate_matrix.c screate_matrix3d.c)
  add_executable(psdrive3d2 ${SEXM3D2})
  target_link_libraries(psdrive3d2 ${all_link_libs})
  install(TARGETS psdrive3d2 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(SEXM3D3 psdrive3d3.c screate_matrix.c screate_matrix3d.c)
  add_executable(psdrive3d3 ${SEXM3D3})
  target_link_libraries(psdrive3d3 ${all_link_libs})
  install(TARGETS psdrive3d3 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  

endif() #### end enable_single


if(enable_complex16)

  set(ZEXM pzdrive.c zcreate_matrix.c)
  add_executable(pzdrive ${ZEXM})
  target_link_libraries(pzdrive ${all_link_libs})
  install(TARGETS pzdrive RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXM1 pzdrive1.c zcreate_matrix.c)
  add_executable(pzdrive1 ${ZEXM1})
  target_link_libraries(pzdrive1 ${all_link_libs})
  add_superlu_dist_example(pzdrive1 cg20.cua 2 2)
  install(TARGETS pzdrive1 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXM2 pzdrive2.c zcreate_matrix.c zcreate_matrix_perturbed.c)
  add_executable(pzdrive2 ${ZEXM2})
  target_link_libraries(pzdrive2 ${all_link_libs})
  add_superlu_dist_example(pzdrive2 cg20.cua 2 2)
  install(TARGETS pzdrive2 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXM3 pzdrive3.c zcreate_matrix.c)
  add_executable(pzdrive3 ${ZEXM3})
  target_link_libraries(pzdrive3 ${all_link_libs})
  add_superlu_dist_example(pzdrive3 cg20.cua 2 2)
  install(TARGETS pzdrive3 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXM4 pzdrive4.c zcreate_matrix.c)
  add_executable(pzdrive4 ${ZEXM4})
  target_link_libraries(pzdrive4 ${all_link_libs})
  install(TARGETS pzdrive4 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXM3D pzdrive3d.c zcreate_matrix.c zcreate_matrix3d.c)
  add_executable(pzdrive3d ${ZEXM3D})
  target_link_libraries(pzdrive3d ${all_link_libs})
  install(TARGETS pzdrive3d RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXM3D1 pzdrive3d1.c zcreate_matrix.c zcreate_matrix3d.c)
  add_executable(pzdrive3d1 ${ZEXM3D1})
  target_link_libraries(pzdrive3d1 ${all_link_libs})
  install(TARGETS pzdrive3d1 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXM3D2 pzdrive3d2.c zcreate_matrix.c zcreate_matrix3d.c)
  add_executable(pzdrive3d2 ${ZEXM3D2})
  target_link_libraries(pzdrive3d2 ${all_link_libs})
  install(TARGETS pzdrive3d2 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXM3D3 pzdrive3d3.c zcreate_matrix.c zcreate_matrix3d.c)
  add_executable(pzdrive3d3 ${ZEXM3D3})
  target_link_libraries(pzdrive3d3 ${all_link_libs})
  install(TARGETS pzdrive3d3 RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXMG pzdrive_ABglobal.c)
  add_executable(pzdrive_ABglobal ${ZEXMG})
  target_link_libraries(pzdrive_ABglobal ${all_link_libs})
  install(TARGETS pzdrive_ABglobal RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXMG1 pzdrive1_ABglobal.c)
  add_executable(pzdrive1_ABglobal ${ZEXMG1})
  target_link_libraries(pzdrive1_ABglobal ${all_link_libs})
  install(TARGETS pzdrive1_ABglobal RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")  
  
  set(ZEXMG2 pzdrive2_ABglobal.c)
  add_executable(pzdrive2_ABglobal ${ZEXMG2})
  target_link_libraries(pzdrive2_ABglobal ${all_link_libs})
  install(TARGETS pzdrive2_ABglobal RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")   
  
  set(ZEXMG3 pzdrive3_ABglobal.c)
  add_executable(pzdrive3_ABglobal ${ZEXMG3})
  target_link_libraries(pzdrive3_ABglobal ${all_link_libs})
  install(TARGETS pzdrive3_ABglobal RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")   
  
  set(ZEXMG4 pzdrive4_ABglobal.c)
  add_executable(pzdrive4_ABglobal ${ZEXMG4})
  target_link_libraries(pzdrive4_ABglobal ${all_link_libs})
  install(TARGETS pzdrive4_ABglobal RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")   
  
  set(ZEXMS pzdrive_spawn.c zcreate_matrix.c)
  add_executable(pzdrive_spawn ${ZEXMS})
  target_link_libraries(pzdrive_spawn ${all_link_libs})  
  install(TARGETS pzdrive_spawn RUNTIME DESTINATION "${INSTALL_LIB_DIR}/EXAMPLE")   

endif()


set(headers 
    f2c.h
)
set(sources input_error_dist.c)

if (enable_single)
    list(APPEND sources
      isamax.c
      sasum.c
      saxpy.c
      scopy.c
      sdot.c
      snrm2.c
      srot.c
      sscal.c
      sgemv.c
      ssymv.c
      strsv.c
      sger.c
      ssyr2.c
      sgemm.c
      strsm.c
    )
endif()

if (enable_double)
    list(APPEND sources
      idamax.c
      dasum.c
      daxpy.c
      dcopy.c
      ddot.c
      dnrm2.c
      drot.c
      dscal.c
      dgemv.c
      dsymv.c
      dtrsv.c
      dger.c
      dsyr2.c
      dgemm.c
      dtrsm.c
    )
endif()

if (enable_complex16)
    list(APPEND sources
      izamax.c
      dzasum.c
      zaxpy.c
      zcopy.c
      dznrm2.c
      zscal.c
      dcabs1.c
      z_internal.c
      zgemv.c
      zhemv.c
      ztrsv.c
      zgerc.c
      zgeru.c
      zher2.c
      zgemm.c
      ztrsm.c
    )
endif()

add_library(blas ${sources} ${HEADERS})

install(TARGETS blas DESTINATION "${INSTALL_LIB_DIR}")
install(FILES ${headers} DESTINATION ${CMAKE_INSTALL_INCLUDEDIR})


set(headers
    include/superlu_FCnames.h
    include/dcomplex.h
    include/machines.h
    include/psymbfact.h
    include/superlu_defs.h
    include/superlu_enum_consts.h
    include/supermatrix.h
    include/util_dist.h
    include/gpu_api_utils.h
    include/gpu_wrapper.h
    include/superlu_upacked.h
    include/superlu_dist_config.h
    include/superlu_FortranCInterface.h
    include/oneside.h
    # CplusplusFactor/lupanels.hpp
    CplusplusFactor/commWrapper.hpp
    CplusplusFactor/lupanels_GPU.cuh
    CplusplusFactor/batch_block_copy.h
    CplusplusFactor/anc25d-GPU_impl.hpp
    CplusplusFactor/anc25d.hpp
    CplusplusFactor/anc25d_impl.hpp
    CplusplusFactor/lupanelsComm3dGPU_impl.hpp
    CplusplusFactor/lupanels_GPU_impl.hpp
    CplusplusFactor/lupanels_comm3d_impl.hpp
    CplusplusFactor/cublas_cusolver_wrappers.hpp
    CplusplusFactor/lupanels_impl.hpp
    CplusplusFactor/dAncestorFactor_impl.hpp
#    CplusplusFactor/pdgstrf3d_upacked_impl.hpp
    CplusplusFactor/dsparseTreeFactorGPU_impl.hpp
    CplusplusFactor/sparseTreeFactor_impl.hpp
#    CplusplusFactor/dsparseTreeFactor_upacked_impl.hpp
    CplusplusFactor/superlu_blas.hpp
    CplusplusFactor/l_panels_impl.hpp
    CplusplusFactor/u_panels_impl.hpp
    CplusplusFactor/luAuxStructTemplated.hpp
    # CplusplusFactor/xlupanels.hpp
    CplusplusFactor/lu_common.hpp
    CplusplusFactor/xlupanels_GPU.cuh
    CplusplusFactor/schurCompUpdate_impl.cuh
    CplusplusFactor/batch_factorize.h 
    CplusplusFactor/batch_factorize_marshall.h 
)
#    ${CMAKE_CURRENT_BINARY_DIR}/superlu_dist_config.h
#    ${PROJECT_SOURCE_DIR}/SRC/superlu_FortranCInterface.h
#    colamd.h
if (MSVC)
  list(APPEND headers wingetopt.h)
endif ()

# first: precision-independent files
set(sources
  prec-independent/sp_ienv.c
  prec-independent/etree.c 
  prec-independent/sp_colorder.c
  prec-independent/get_perm_c.c
  prec-independent/mmd.c
  prec-independent/comm.c
  prec-independent/memory.c
  prec-independent/util.c
  prec-independent/gpu_api_utils.c
  prec-independent/superlu_grid.c
  prec-independent/pxerr_dist.c
  prec-independent/superlu_timer.c
  prec-independent/symbfact.c
  prec-independent/ilu_level_symbfact.c
  prec-independent/psymbfact.c
  prec-independent/psymbfact_util.c
  prec-independent/get_perm_c_parmetis.c
  prec-independent/mc64ad_dist.c
  prec-independent/xerr_dist.c
  prec-independent/smach_dist.c
  prec-independent/dmach_dist.c
  prec-independent/superlu_dist_version.c
  prec-independent/comm_tree.c
  prec-independent/superlu_grid3d.c    ## 3D code
  prec-independent/supernodal_etree.c
  prec-independent/supernodalForest.c
  prec-independent/trfAux.c 
  prec-independent/communication_aux.c
  prec-independent/treeFactorization.c
  prec-independent/sec_structs.c  
  prec-independent/get_perm_c_batch.c
)

# Add all the BLAS routines: CplusplusFactor/supelu_blas.hpp needs all
list(APPEND sources
  double/dsuperlu_blas.c
  single/ssuperlu_blas.c
  complex16/zsuperlu_blas.c
)

# if (HAVE_CUDA)
#   list(APPEND sources cuda/superlu_gpu_utils.cu)
# endif()

# if (HAVE_HIP)
#   list(APPEND sources hip/superlu_gpu_utils.hip.cpp)
# endif()

if (MSVC)
  list(APPEND sources pred-independent/wingetopt.c)
endif ()

set_source_files_properties(superlu_timer.c PROPERTIES COMPILE_FLAGS -O0)

if(enable_double)
  list(APPEND headers
  include/superlu_ddefs.h include/dlustruct_gpu.h)

  list(APPEND sources
    double/dlangs_dist.c
    double/dgsequ_dist.c
    double/dlaqgs_dist.c
    double/dutil_dist.c
    double/dmemory_dist.c
    double/dmyblas2_dist.c
    double/dsp_blas2_dist.c
    double/dsp_blas3_dist.c
    double/pdgssvx.c
    double/pdgssvx_ABglobal.c
    double/dreadhb.c
    double/dreadrb.c
    double/dreadtriple.c
    double/dreadtriple_noheader.c
    double/dbinary_io.c	
    double/dreadMM.c
    double/pdgsequ.c
    double/pdlaqgs.c
    double/dldperm_dist.c
    double/pdlangs.c
    double/pdutil.c
    double/pdsymbfact_distdata.c
    double/ddistribute.c
    double/pddistribute.c
    double/pddistribute3d.c
    double/d3DPartition.c
    double/distCheckArray.c
    double/pddistribute-aux3d.c
    double/pdgstrf.c
    double/dstatic_schedule.c
    double/pdgstrf2.c
    double/pdgstrs.c
    double/pdgstrs3d.c
    double/pdgstrs1.c
    double/pdgstrs_lsum.c
    double/pdgstrs_Bglobal.c
    double/pdgsrfs.c
    double/pdgsmv.c
    double/pdgsrfs_ABXglobal.c
    double/pdgsmv_AXglobal.c
    double/pdGetDiagU.c
    double/pdgssvx3d.c     ## 3D code
    double/dssvx3dAux.c    
    double/dnrformat_loc3d.c 
    double/pdgstrf3d.c 
    double/dtreeFactorization.c
    double/dtreeFactorizationGPU.c
    double/dgather.c
    double/dscatter3d.c
    double/pd3dcomm.c
    double/dtrfAux.c	
    double/dcommunication_aux.c 
    double/dtrfCommWrapper.c
    double/dsuperlu_blas.c
    double/pdgssvx3d_csc_batch.c # batch in CSC format
    double/dequil_batch.c # batch in CSC format
    double/dpivot_batch.c
  )
  
if (TPL_ENABLE_CUDALIB)
  list(APPEND sources
       cuda/pdgstrs_lsum_cuda.cu cuda/superlu_gpu_utils.cu cuda/dsuperlu_gpu.cu
##       CplusplusFactor/schurCompUpdate.cu 
#       CplusplusFactor/pdgstrf3d_upacked.cpp
#       CplusplusFactor/dsparseTreeFactor_upacked.cpp
##      CplusplusFactor/dsparseTreeFactorGPU.cpp 
##       CplusplusFactor/lupanels.cpp
      #  CplusplusFactor/lupanels_comm3d.cpp
      #  CplusplusFactor/lupanelsComm3dGPU.cpp
       CplusplusFactor/commWrapper.cpp 
##       CplusplusFactor/l_panels.cpp
##       CplusplusFactor/u_panels.cpp
      #  CplusplusFactor/lupanels_GPU.cpp
##       CplusplusFactor/anc25d.cpp 
##       CplusplusFactor/anc25d-GPU.cpp 
##       CplusplusFactor/LUgpuCHandle_interface.cpp
       CplusplusFactor/LUgpuCHandle_interface_impl.cu  
       CplusplusFactor/batch_factorize.cu 
       CplusplusFactor/batch_block_copy.cu
  )
endif()

if (HAVE_COMBBLAS)
  list(APPEND sources double/d_c2cpp_GetHWPM.cpp double/dHWPM_CombBLAS.hpp)
endif()

endif() ########## enable double

if(enable_single)
  list(APPEND headers
       include/superlu_sdefs.h include/slustruct_gpu.h)

  list(APPEND sources
    single/slangs_dist.c
    single/sgsequ_dist.c
    single/slaqgs_dist.c
    single/sutil_dist.c
    single/smemory_dist.c
    single/smyblas2_dist.c
    single/ssp_blas2_dist.c
    single/ssp_blas3_dist.c
    single/psgssvx.c
    single/psgssvx_d2.c
    single/psgssvx_ABglobal.c
    single/sreadhb.c
    single/sreadrb.c
    single/sreadtriple.c
    single/sreadtriple_noheader.c
    single/sbinary_io.c	
    single/sreadMM.c
    single/psgsequ.c
    single/pslaqgs.c
    single/sldperm_dist.c
    single/pslangs.c
    single/psutil.c
    single/pssymbfact_distdata.c
    single/sdistribute.c
    single/psdistribute.c
    single/psdistribute3d.c
    single/s3DPartition.c
    single/psdistribute-aux3d.c
    single/psgstrf.c
    single/sstatic_schedule.c
    single/psgstrf2.c
    single/psgstrs.c
    single/psgstrs3d.c
    single/psgstrs1.c
    single/psgstrs_lsum.c
    single/psgstrs_Bglobal.c
    single/psgsrfs.c
    single/psgsrfs_d2.c
    single/psgsmv.c
    single/psgsrfs_ABXglobal.c
    single/psgsmv_AXglobal.c
    single/psGetDiagU.c
    single/psgssvx3d.c     ## 3D code
    single/sssvx3dAux.c  
    single/snrformat_loc3d.c 
    single/psgstrf3d.c 
    single/streeFactorization.c
    single/streeFactorizationGPU.c
    single/sgather.c
    single/sscatter3d.c
    single/ps3dcomm.c
    single/strfAux.c	
    single/scommunication_aux.c 
    single/strfCommWrapper.c
    single/ssuperlu_blas.c
    single/psgssvx_d2.c     # with double-precision IR
    single/psgsrfs_d2.c
    single/psgsmv_d2.c
    single/psgsequb.c
    single/psgssvx3d_csc_batch.c # batch in CSC format
    single/sequil_batch.c # batch in CSC format
    single/spivot_batch.c
  )
if (TPL_ENABLE_CUDALIB)
    list(APPEND sources cuda/psgstrs_lsum_cuda.cu cuda/ssuperlu_gpu.cu)
endif()
if (HAVE_COMBBLAS)
   if (enable_double)
       list(APPEND sources double/d_c2cpp_GetHWPM.cpp)
   else()
       list(APPEND sources single/s_c2cpp_GetHWPM.cpp double/dHWPM_CombBLAS.hpp)
   endif()
endif()

endif() ########## enable single


if(enable_complex16)
  list(APPEND headers
       include/superlu_zdefs.h include/zlustruct_gpu.h)

      list(APPEND sources
      complex16/dcomplex_dist.c
      complex16/zlangs_dist.c
      complex16/zgsequ_dist.c
      complex16/zlaqgs_dist.c
      complex16/zutil_dist.c
      complex16/zmemory_dist.c
      complex16/zmyblas2_dist.c
      complex16/zsp_blas2_dist.c
      complex16/zsp_blas3_dist.c
      complex16/pzgssvx.c
      complex16/pzgssvx_ABglobal.c
      complex16/zreadhb.c
      complex16/zreadrb.c
      complex16/zreadtriple.c
      complex16/zreadtriple_noheader.c
      complex16/zbinary_io.c	
      complex16/zreadMM.c
      complex16/pzgsequ.c
      complex16/pzlaqgs.c
      complex16/zldperm_dist.c
      complex16/pzlangs.c
      complex16/pzutil.c
      complex16/pzsymbfact_distdata.c
      complex16/zdistribute.c
      complex16/pzdistribute.c
      complex16/pzdistribute3d.c
      complex16/z3DPartition.c
      complex16/pzdistribute-aux3d.c
      complex16/pzgstrf.c
      complex16/zstatic_schedule.c
      complex16/pzgstrf2.c
      complex16/pzgstrs.c
      complex16/pzgstrs3d.c
      complex16/pzgstrs1.c
      complex16/pzgstrs_lsum.c
      complex16/pzgstrs_Bglobal.c
      complex16/pzgsrfs.c
      complex16/pzgsmv.c
      complex16/pzgsrfs_ABXglobal.c
      complex16/pzgsmv_AXglobal.c
      complex16/pzGetDiagU.c
      complex16/pzgssvx3d.c     ## 3D code
      complex16/zssvx3dAux.c    
      complex16/znrformat_loc3d.c 
      complex16/pzgstrf3d.c 
      complex16/ztreeFactorization.c
      complex16/ztreeFactorizationGPU.c
      complex16/zgather.c
      complex16/zscatter3d.c
      complex16/pz3dcomm.c
      complex16/ztrfAux.c	
      complex16/zcommunication_aux.c 
      complex16/ztrfCommWrapper.c
      complex16/zsuperlu_blas.c
      complex16/pzgssvx3d_csc_batch.c # batch in CSC format
      complex16/zequil_batch.c # batch in CSC format
      complex16/zpivot_batch.c
     )
if (TPL_ENABLE_CUDALIB)
    list(APPEND sources cuda/pzgstrs_lsum_cuda.cu cuda/zsuperlu_gpu.cu)
endif()
if (HAVE_COMBBLAS)
    list(APPEND sources
         complex16/z_c2cpp_GetHWPM.cpp complex16/zHWPM_CombBLAS.hpp)
endif()
endif() ######### enable compex16

if (TPL_ENABLE_HIPLIB)
  file(GLOB MyFiles hip/*.hip.cpp)
  set_source_files_properties(
    ${MyFiles}
    PROPERTIES HIP_SOURCE_PROPERTY_FORMAT 1)

  set(hipsources
    hip/pdgstrs_lsum_cuda.hip.cpp
    hip/dsuperlu_gpu.hip.cpp
    hip/superlu_gpu_utils.hip.cpp
  )
  if(enable_single)
  list(APPEND hipsources hip/psgstrs_lsum_cuda.hip.cpp hip/ssuperlu_gpu.hip.cpp)
  endif()
  if(enable_complex16)
  list(APPEND hipsources hip/pzgstrs_lsum_cuda.hip.cpp hip/zsuperlu_gpu.hip.cpp)
  endif()
  hip_add_library(superlu_dist ${hipsources})
  if (BUILD_SHARED_LIBS AND BUILD_STATIC_LIBS)
    hip_add_library(superlu_dist-static STATIC  ${hipsources})
  endif()

else()
  add_library(superlu_dist "")
  if (BUILD_SHARED_LIBS AND BUILD_STATIC_LIBS)
    add_library(superlu_dist-static STATIC "")
  endif()
endif()


target_sources(superlu_dist PRIVATE ${sources} ${HEADERS})
set(targets superlu_dist)
target_compile_features(superlu_dist PUBLIC c_std_99)

if (BUILD_SHARED_LIBS AND BUILD_STATIC_LIBS)
  # build both shared and static libs
  target_sources(superlu_dist-static PRIVATE ${sources} ${HEADERS})
  target_compile_features(superlu_dist-static PUBLIC c_std_99)
  list(APPEND targets superlu_dist-static)
endif()

if (TPL_ENABLE_NVSHMEM)
set(superlu_dist_libs ${MPI_C_LIBRARIES} ${MPI_CXX_LIBRARIES} ${BLAS_LIB} ${LAPACK_LIB}
    ${PARMETIS_LIB} ${COLAMD_LIB} ${COMBBLAS_LIB} ${MAGMA_LIB} ${CUDA_LIB} ${HIP_LIB} ${NVSHMEM_LIB}) 
else()
set(superlu_dist_libs ${MPI_C_LIBRARIES} ${MPI_CXX_LIBRARIES} ${BLAS_LIB} ${LAPACK_LIB}
    ${PARMETIS_LIB} ${COLAMD_LIB} ${COMBBLAS_LIB} ${MAGMA_LIB} ${CUDA_LIB} ${HIP_LIB})
endif()

if (NOT MSVC)
  list(APPEND superlu_dist_libs m)
endif ()

foreach(target ${targets})
    target_link_libraries(${target} ${superlu_dist_libs})
    if (HAVE_COMBBLAS)
        set_target_properties(${target} PROPERTIES
                              CUDA_SEPARABLE_COMPILATION ON
                              CUDA_RESOLVE_DEVICE_SYMBOLS ON
							  OUTPUT_NAME superlu_dist
                              VERSION ${PROJECT_VERSION}
                              SOVERSION ${VERSION_MAJOR}
			      LINKER_LANGUAGE CXX
			      )
    else()
        set_target_properties(${target} PROPERTIES
		                      CUDA_SEPARABLE_COMPILATION ON
                                      CUDA_RESOLVE_DEVICE_SYMBOLS ON
                              OUTPUT_NAME superlu_dist
                              VERSION ${PROJECT_VERSION}
                              SOVERSION ${VERSION_MAJOR}
			      )
    endif()
endforeach(target)

# Add CUDA runtime library and CUBLAS library
if(CUDAToolkit_FOUND)  # this is found in top-level CMakeLists.txt
    target_link_libraries(superlu_dist CUDA::cudart CUDA::cublas)
endif()

# This is recommended by modern cmake:
# https://cliutils.gitlab.io/modern-cmake/chapters/packages/OpenMP.html
if(OpenMP_FOUND) # this is found in top-level CMakeLists.txt
  target_link_libraries(superlu_dist OpenMP::OpenMP_C)
endif()

if (XSDK_ENABLE_Fortran)
## target_link_libraries(superlu_dist PUBLIC MPI::MPI_CXX MPI::MPI_C MPI::MPI_Fortran)
## PUBLIC keyword causes error:
##   The plain signature for target_link_libraries has already been used ...
  target_link_libraries(superlu_dist MPI::MPI_CXX MPI::MPI_C MPI::MPI_Fortran)
else()
  target_link_libraries(superlu_dist MPI::MPI_CXX MPI::MPI_C)
endif()

target_compile_definitions(superlu_dist PRIVATE SUPERLU_DIST_EXPORTS)
if(MSVC AND BUILD_SHARED_LIBS)
  set_target_properties(superlu_dist PROPERTIES
                        WINDOWS_EXPORT_ALL_SYMBOLS ON
  )
endif()

# Define GNU standard installation directories
include(GNUInstallDirs)

install(TARGETS ${targets}
# DESTINATION ${CMAKE_INSTALL_LIBDIR}
    RUNTIME DESTINATION "${INSTALL_BIN_DIR}"
    LIBRARY DESTINATION "${INSTALL_LIB_DIR}"
    ARCHIVE DESTINATION "${INSTALL_LIB_DIR}"
)
install(FILES ${headers}
# DESTINATION ${CMAKE_INSTALL_PREFIX}/include)
  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}
)