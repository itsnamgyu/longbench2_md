from __future__ import annotations

import logging
import re
import sys
from functools import lru_cache, wraps
from os import environ
from os.path import abspath, basename, dirname, isfile, join
from pathlib import Path
from shutil import which

from . import CondaError
from .auxlib.compat import Utf8NamedTemporaryFile, shlex_split_unicode
from .common.compat import isiterable, on_win
from .common.path import win_path_to_unix
from .common.url import path_to_url
from .deprecations import deprecated

log = logging.getLogger(__name__)


def path_identity(path):
    """Used as a dummy path converter where no conversion necessary"""
    return path


def unix_path_to_win(path, root_prefix=""):
    """Convert a path or :-separated string of paths into a Windows representation

    Does not add cygdrive.  If you need that, set root_prefix to "/cygdrive"
    """
    if len(path) > 1 and (";" in path or (path[1] == ":" and path.count(":") == 1)):
        # already a windows path
        return path.replace("/", "\\")
    path_re = root_prefix + r'(/[a-zA-Z]/(?:(?![:\s]/)[^:*?"<>])*)'

    def _translation(found_path):
        group = found_path.group(0)
        return "{}:{}".format(
            group[len(root_prefix) + 1],
            group[len(root_prefix) + 2 :].replace("/", "\\"),
        )

    translation = re.sub(path_re, _translation, path)
    translation = re.sub(
        ":([a-zA-Z]):\\\\", lambda match: ";" + match.group(0)[1] + ":\\", translation
    )
    return translation


@deprecated(
    "25.3",
    "25.9",
    addendum="Use `conda.common.path.win_path_to_unix` instead.",
)
def win_path_to_cygwin(path):
    return win_path_to_unix(path, "/cygdrive")


@deprecated(
    "25.3",
    "25.9",
    addendum="Use `conda.utils.unix_path_to_win` instead.",
)
def cygwin_path_to_win(path):
    return unix_path_to_win(path, "/cygdrive")


@deprecated("25.3", "25.9", addendum="Unused.")
def translate_stream(stream, translator):
    return "\n".join(translator(line) for line in stream.split("\n"))


def human_bytes(n):
    """
    Return the number of bytes n in more human readable form.

    Examples:
        >>> human_bytes(42)
        '42 B'
        >>> human_bytes(1042)
        '1 KB'
        >>> human_bytes(10004242)
        '9.5 MB'
        >>> human_bytes(100000004242)
        '93.13 GB'
    """
    if n < 1024:
        return "%d B" % n
    k = n / 1024
    if k < 1024:
        return "%d KB" % round(k)
    m = k / 1024
    if m < 1024:
        return f"{m:.1f} MB"
    g = m / 1024
    return f"{g:.2f} GB"


# TODO: this should be done in a more extensible way
#     (like files for each shell, with some registration mechanism.)

# defaults for unix shells.  Note: missing "exe" entry, which should be set to
#    either an executable on PATH, or a full path to an executable for a shell
_UNIX_SHELL_BASE = dict(
    binpath="/bin/",  # mind the trailing slash.
    echo="echo",
    env_script_suffix=".sh",
    nul="2>/dev/null",
    path_from=path_identity,
    path_to=path_identity,
    pathsep=":",
    printdefaultenv="echo $CONDA_DEFAULT_ENV",
    printpath="echo $PATH",
    printps1="echo $CONDA_PROMPT_MODIFIER",
    promptvar="PS1",
    sep="/",
    set_var="export ",
    shell_args=["-l", "-c"],
    shell_suffix="",
    slash_convert=("\\", "/"),
    source_setup="source",
    test_echo_extra="",
    var_format="${}",
)

deprecated.constant(
    "25.3",
    "25.9",
    "unix_shell_base",
    _UNIX_SHELL_BASE,
    addendum="Use `conda.activate` instead.",
)

_MSYS2_SHELL_BASE = dict(
    _UNIX_SHELL_BASE,
    path_from=unix_path_to_win,
    path_to=win_path_to_unix,
    binpath="/bin/",  # mind the trailing slash.
    printpath="python -c \"import os; print(';'.join(os.environ['PATH'].split(';')[1:]))\" | cygpath --path -f -",  # NOQA
)

deprecated.constant(
    "25.3",
    "25.9",
    "msys2_shell_base",
    _MSYS2_SHELL_BASE,
    addendum="Use `conda.activate` instead.",
)

if on_win:
    _SHELLS = {
        # "powershell.exe": dict(
        #    echo="echo",
        #    test_echo_extra=" .",
        #    var_format="${var}",
        #    binpath="/bin/",  # mind the trailing slash.
        #    source_setup="source",
        #    nul='2>/dev/null',
        #    set_var='export ',
        #    shell_suffix=".ps",
        #    env_script_suffix=".ps",
        #    printps1='echo $PS1',
        #    printdefaultenv='echo $CONDA_DEFAULT_ENV',
        #    printpath="echo %PATH%",
        #    exe="powershell.exe",
        #    path_from=path_identity,
        #    path_to=path_identity,
        #    slash_convert = ("/", "\\"),
        # ),
        "cmd.exe": dict(
            echo="@echo",
            var_format="%{}%",
            binpath="\\Scripts\\",  # mind the trailing slash.
            source_setup="call",
            test_echo_extra="",
            nul="1>NUL 2>&1",
            set_var="set ",
            shell_suffix=".bat",
            env_script_suffix=".bat",
            printps1="@echo %PROMPT%",
            promptvar="PROMPT",
            # parens mismatched intentionally.  See http://stackoverflow.com/questions/20691060/how-do-i-echo-a-blank-empty-line-to-the-console-from-a-windows-batch-file # NOQA
            printdefaultenv='IF NOT "%CONDA_DEFAULT_ENV%" == "" (\n'
            "echo %CONDA_DEFAULT_ENV% ) ELSE (\n"
            "echo()",
            printpath="@echo %PATH%",
            exe="cmd.exe",
            shell_args=["/d", "/c"],
            path_from=path_identity,
            path_to=path_identity,
            slash_convert=("/", "\\"),
            sep="\\",
            pathsep=";",
        ),
        "cygwin": dict(
            _UNIX_SHELL_BASE,
            exe="bash.exe",
            binpath="/Scripts/",  # mind the trailing slash.
            path_from=cygwin_path_to_win,
            path_to=win_path_to_cygwin,
        ),
        # bash is whichever bash is on PATH.  If using Cygwin, you should use the cygwin
        #    entry instead.  The only major difference is that it handle's cygwin's /cygdrive
        #    filesystem root.
        "bash.exe": dict(
            _MSYS2_SHELL_BASE,
            exe="bash.exe",
        ),
        "bash": dict(
            _MSYS2_SHELL_BASE,
            exe="bash",
        ),
        "sh.exe": dict(
            _MSYS2_SHELL_BASE,
            exe="sh.exe",
        ),
        "zsh.exe": dict(
            _MSYS2_SHELL_BASE,
            exe="zsh.exe",
        ),
        "zsh": dict(
            _MSYS2_SHELL_BASE,
            exe="zsh",
        ),
    }

else:
    _SHELLS = {
        "bash": dict(
            _UNIX_SHELL_BASE,
            exe="bash",
        ),
        "dash": dict(
            _UNIX_SHELL_BASE,
            exe="dash",
            source_setup=".",
        ),
        "zsh": dict(
            _UNIX_SHELL_BASE,
            exe="zsh",
        ),
        "fish": dict(
            _UNIX_SHELL_BASE,
            exe="fish",
            pathsep=" ",
        ),
    }

deprecated.constant(
    "25.3",
    "25.9",
    "shells",
    _SHELLS,
    addendum="Use `conda.activate` instead.",
)


# ##########################################
# put back because of conda build
# ##########################################

urlpath = url_path = path_to_url


@lru_cache(maxsize=None)
def sys_prefix_unfollowed():
    """Since conda is installed into non-root environments as a symlink only
    and because sys.prefix follows symlinks, this function can be used to
    get the 'unfollowed' sys.prefix.

    This value is usually the same as the prefix of the environment into
    which conda has been symlinked. An example of when this is necessary
    is when conda looks for external sub-commands in find_commands.py
    """
    try:
        frame = next(iter(sys._current_frames().values()))
        while frame.f_back:
            frame = frame.f_back
        code = frame.f_code
        filename = code.co_filename
        unfollowed = dirname(dirname(filename))
    except Exception:
        return sys.prefix
    return unfollowed


def quote_for_shell(*arguments):
    """Properly quote arguments for command line passing.

    For POSIX uses `shlex.join`, for Windows uses a custom implementation to properly escape
    metacharacters.

    :param arguments: Arguments to quote.
    :type arguments: list of str
    :return: Quoted arguments.
    :rtype: str
    """
    # [backport] Support passing in a list of strings or args of string.
    if len(arguments) == 1 and isiterable(arguments[0]):
        arguments = arguments[0]

    return _args_join(arguments)


if on_win:
    # https://ss64.com/nt/syntax-esc.html
    # https://docs.microsoft.com/en-us/archive/blogs/twistylittlepassagesallalike/everyone-quotes-command-line-arguments-the-wrong-way

    _RE_UNSAFE = re.compile(r'["%\s^<>&|]')
    _RE_DBL = re.compile(r'(["%])')

    def _args_join(args):
        """Return a shell-escaped string from *args*."""

        def quote(s):
            # derived from shlex.quote
            if not s:
                return '""'
            # if any unsafe chars are present we must quote
            if not _RE_UNSAFE.search(s):
                return s
            # double escape (" -> "")
            s = _RE_DBL.sub(r"\1\1", s)
            # quote entire string
            return f'"{s}"'

        return " ".join(quote(arg) for arg in args)

else:
    try:
        from shlex import join as _args_join
    except ImportError:
        # [backport] Python <3.8
        def _args_join(args):
            """Return a shell-escaped string from *args*."""
            from shlex import quote

            return " ".join(quote(arg) for arg in args)


# Ensures arguments are a tuple or a list. Strings are converted
# by shlex_split_unicode() which is bad; we warn about it or else
# we assert (and fix the code).
def massage_arguments(arguments, errors="assert"):
    # For reference and in-case anything breaks ..
    # .. one of the places (run_command in conda_env/utils.py) this
    # gets called from used to do this too:
    #
    #    def escape_for_winpath(p):
    #        return p.replace('\\', '\\\\')
    #
    #    if not isinstance(arguments, list):
    #        arguments = list(map(escape_for_winpath, arguments))

    if isinstance(arguments, str):
        if errors == "assert":
            # This should be something like 'conda programming bug', it is an assert
            assert False, "Please ensure arguments are not strings"
        else:
            arguments = shlex_split_unicode(arguments)
            log.warning(
                "Please ensure arguments is not a string; "
                "used `shlex_split_unicode()` on it"
            )

    if not isiterable(arguments):
        arguments = (arguments,)

    assert not any(
        [isiterable(arg) for arg in arguments]
    ), "Individual arguments must not be iterable"  # NOQA
    arguments = list(arguments)

    return arguments


def wrap_subprocess_call(
    root_prefix,
    prefix,
    dev_mode,
    debug_wrapper_scripts,
    arguments,
    use_system_tmp_path=False,
):
    arguments = massage_arguments(arguments)
    if not use_system_tmp_path:
        tmp_prefix = abspath(join(prefix, ".tmp"))
    else:
        tmp_prefix = None
    script_caller = None
    multiline = False
    if len(arguments) == 1 and "\n" in arguments[0]:
        multiline = True
    if on_win:
        comspec = get_comspec()  # fail early with KeyError if undefined
        if dev_mode:
            from . import CONDA_PACKAGE_ROOT

            conda_bat = join(CONDA_PACKAGE_ROOT, "shell", "condabin", "conda.bat")
        else:
            conda_bat = environ.get(
                "CONDA_BAT", abspath(join(root_prefix, "condabin", "conda.bat"))
            )
        with Utf8NamedTemporaryFile(
            mode="w", prefix=tmp_prefix, suffix=".bat", delete=False
        ) as fh:
            silencer = "" if debug_wrapper_scripts else "@"
            fh.write(f"{silencer}ECHO OFF\n")
            fh.write(f"{silencer}SET PYTHONIOENCODING=utf-8\n")
            fh.write(f"{silencer}SET PYTHONUTF8=1\n")
            fh.write(
                f'{silencer}FOR /F "tokens=2 delims=:." %%A in (\'chcp\') do for %%B in (%%A) do set "_CONDA_OLD_CHCP=%%B"\n'  # noqa
            )
            fh.write(f"{silencer}chcp 65001 > NUL\n")
            if dev_mode:
                from . import CONDA_SOURCE_ROOT

                fh.write(f"{silencer}SET CONDA_DEV=1\n")
                # In dev mode, conda is really:
                # 'python -m conda'
                # *with* PYTHONPATH set.
                fh.write(f"{silencer}SET PYTHONPATH={CONDA_SOURCE_ROOT}\n")
                fh.write(f"{silencer}SET CONDA_EXE={sys.executable}\n")
                fh.write(f"{silencer}SET _CE_M=-m\n")
                fh.write(f"{silencer}SET _CE_CONDA=conda\n")
            if debug_wrapper_scripts:
                fh.write("echo *** environment before *** 1>&2\n")
                fh.write("SET 1>&2\n")
            # Not sure there is any point in backing this up, nothing will get called with it reset
            # after all!
            # fh.write("@FOR /F \"tokens=100\" %%F IN ('chcp') DO @SET CONDA_OLD_CHCP=%%F\n")
            # fh.write('@chcp 65001>NUL\n')
            fh.write(f'{silencer}CALL "{conda_bat}" activate "{prefix}"\n')
            fh.write(f"{silencer}IF %ERRORLEVEL% NEQ 0 EXIT /b %ERRORLEVEL%\n")
            if debug_wrapper_scripts:
                fh.write("echo *** environment after *** 1>&2\n")
                fh.write("SET 1>&2\n")
            if multiline:
                # No point silencing the first line. If that's what's wanted then
                # it needs doing for each line and the caller may as well do that.
                fh.write(f"{arguments[0]}\n")
            else:
                assert not any("\n" in arg for arg in arguments), (
                    "Support for scripts where arguments contain newlines not implemented.\n"
                    ".. requires writing the script to an external file and knowing how to "
                    "transform the command-line (e.g. `python -c args` => `python file`) "
                    "in a tool dependent way, or attempting something like:\n"
                    ".. https://stackoverflow.com/a/15032476 (adds unacceptable escaping"
                    "requirements)"
                )
                fh.write(f"{silencer}{quote_for_shell(*arguments)}\n")
            fh.write(f"{silencer}IF %ERRORLEVEL% NEQ 0 EXIT /b %ERRORLEVEL%\n")
            fh.write(f"{silencer}chcp %_CONDA_OLD_CHCP%>NUL\n")
            script_caller = fh.name
        command_args = [comspec, "/d", "/c", script_caller]
    else:
        shell_path = which("bash") or which("sh")
        if shell_path is None:
            raise Exception("No compatible shell found!")

        # During tests, we sometimes like to have a temp env with e.g. an old python in it
        # and have it run tests against the very latest development sources. For that to
        # work we need extra smarts here, we want it to be instead:
        if dev_mode:
            conda_exe = [abspath(join(root_prefix, "bin", "python")), "-m", "conda"]
            dev_arg = "--dev"
            dev_args = [dev_arg]
        else:
            conda_exe = [
                environ.get("CONDA_EXE", abspath(join(root_prefix, "bin", "conda")))
            ]
            dev_arg = ""
            dev_args = []
        with Utf8NamedTemporaryFile(mode="w", prefix=tmp_prefix, delete=False) as fh:
            if dev_mode:
                from . import CONDA_SOURCE_ROOT

                fh.write(">&2 export PYTHONPATH=" + CONDA_SOURCE_ROOT + "\n")
            hook_quoted = quote_for_shell(*conda_exe, "shell.posix", "hook", *dev_args)
            if debug_wrapper_scripts:
                fh.write(">&2 echo '*** environment before ***'\n>&2 env\n")
                fh.write(f'>&2 echo "$({hook_quoted})"\n')
            fh.write(f'eval "$({hook_quoted})"\n')
            fh.write(f"conda activate {dev_arg} {quote_for_shell(prefix)}\n")
            if debug_wrapper_scripts:
                fh.write(">&2 echo '*** environment after ***'\n>&2 env\n")
            if multiline:
                # The ' '.join() is pointless since mutliline is only True when there's 1 arg
                # still, if that were to change this would prevent breakage.
                fh.write("{}\n".format(" ".join(arguments)))
            else:
                fh.write(f"{quote_for_shell(*arguments)}\n")
            script_caller = fh.name
        if debug_wrapper_scripts:
            command_args = [shell_path, "-x", script_caller]
        else:
            command_args = [shell_path, script_caller]

    return script_caller, command_args


def get_comspec():
    """Returns COMSPEC from envvars.

    Ensures COMSPEC envvar is set to cmd.exe, if not attempt to find it.

    :raises KeyError: COMSPEC is undefined and cannot be found.
    :returns: COMSPEC value.
    :rtype: str
    """
    if basename(environ.get("COMSPEC", "")).lower() != "cmd.exe":
        for comspec in (
            # %SystemRoot%\System32\cmd.exe
            environ.get("SystemRoot")
            and join(environ["SystemRoot"], "System32", "cmd.exe"),
            # %windir%\System32\cmd.exe
            environ.get("windir") and join(environ["windir"], "System32", "cmd.exe"),
        ):
            if comspec and isfile(comspec):
                environ["COMSPEC"] = comspec
                break
        else:
            log.warning(
                "cmd.exe could not be found. Looked in SystemRoot and windir env vars.\n"
            )

    # fails with KeyError if still undefined
    return environ["COMSPEC"]


def ensure_dir_exists(func):
    """
    Ensures that the directory exists for functions returning
    a Path object containing a directory
    """

    @wraps(func)
    def wrapper(*args, **kwargs):
        result = func(*args, **kwargs)

        if isinstance(result, Path):
            try:
                result.mkdir(parents=True, exist_ok=True)
            except OSError as exc:
                raise CondaError(
                    "Error encountered while attempting to create cache directory."
                    f"\n  Directory: {result}"
                    f"\n  Exception: {exc}"
                )

        return result

    return wrapper


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Define the instruction set (constants) for conda operations."""

from logging import getLogger
from os.path import isfile, join

from .core.link import UnlinkLinkTransaction
from .core.package_cache_data import ProgressiveFetchExtract
from .deprecations import deprecated
from .exceptions import CondaFileIOError
from .gateways.disk.link import islink

log = getLogger(__name__)

# op codes
CHECK_FETCH = "CHECK_FETCH"
FETCH = "FETCH"
CHECK_EXTRACT = "CHECK_EXTRACT"
EXTRACT = "EXTRACT"
RM_EXTRACTED = "RM_EXTRACTED"
RM_FETCHED = "RM_FETCHED"
deprecated.constant("24.9", "25.3", "PREFIX", "PREFIX")
PRINT = "PRINT"
PROGRESS = "PROGRESS"
SYMLINK_CONDA = "SYMLINK_CONDA"
UNLINK = "UNLINK"
LINK = "LINK"
UNLINKLINKTRANSACTION = "UNLINKLINKTRANSACTION"
PROGRESSIVEFETCHEXTRACT = "PROGRESSIVEFETCHEXTRACT"


PROGRESS_COMMANDS = {EXTRACT, RM_EXTRACTED}
ACTION_CODES = (
    CHECK_FETCH,
    FETCH,
    CHECK_EXTRACT,
    EXTRACT,
    UNLINK,
    LINK,
    SYMLINK_CONDA,
    RM_EXTRACTED,
    RM_FETCHED,
)


def PRINT_CMD(state, arg):  # pragma: no cover
    if arg.startswith(("Unlinking packages", "Linking packages")):
        return
    getLogger("conda.stdout.verbose").info(arg)


def FETCH_CMD(state, package_cache_entry):
    raise NotImplementedError()


def EXTRACT_CMD(state, arg):
    raise NotImplementedError()


def PROGRESSIVEFETCHEXTRACT_CMD(state, progressive_fetch_extract):  # pragma: no cover
    assert isinstance(progressive_fetch_extract, ProgressiveFetchExtract)
    progressive_fetch_extract.execute()


def UNLINKLINKTRANSACTION_CMD(state, arg):  # pragma: no cover
    unlink_link_transaction = arg
    assert isinstance(unlink_link_transaction, UnlinkLinkTransaction)
    unlink_link_transaction.execute()


def check_files_in_package(source_dir, files):
    for f in files:
        source_file = join(source_dir, f)
        if isfile(source_file) or islink(source_file):
            return True
        else:
            raise CondaFileIOError(source_file, f"File {f} does not exist in tarball")


# Map instruction to command (a python function)
commands = {
    PRINT: PRINT_CMD,
    FETCH: FETCH_CMD,
    PROGRESS: lambda x, y: None,
    EXTRACT: EXTRACT_CMD,
    RM_EXTRACTED: lambda x, y: None,
    RM_FETCHED: lambda x, y: None,
    UNLINK: None,
    LINK: None,
    SYMLINK_CONDA: lambda x, y: None,
    UNLINKLINKTRANSACTION: UNLINKLINKTRANSACTION_CMD,
    PROGRESSIVEFETCHEXTRACT: PROGRESSIVEFETCHEXTRACT_CMD,
}


OP_ORDER = (
    RM_FETCHED,
    FETCH,
    RM_EXTRACTED,
    EXTRACT,
    UNLINK,
    LINK,
)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Handle the planning of installs and their execution.

NOTE:
    conda.install uses canonical package names in its interface functions,
    whereas conda.resolve uses package filenames, as those are used as index
    keys.  We try to keep fixes to this "impedance mismatch" local to this
    module.
"""

import sys
from collections import defaultdict
from logging import getLogger

from boltons.setutils import IndexedSet

from .base.constants import DEFAULTS_CHANNEL_NAME, UNKNOWN_CHANNEL
from .base.context import context, reset_context
from .common.constants import TRACE
from .common.io import dashlist, env_vars, time_recorder
from .common.iterators import groupby_to_dict as groupby
from .core.index import LAST_CHANNEL_URLS
from .core.link import PrefixSetup, UnlinkLinkTransaction
from .deprecations import deprecated
from .instructions import FETCH, LINK, SYMLINK_CONDA, UNLINK
from .models.channel import Channel, prioritize_channels
from .models.dist import Dist
from .models.enums import LinkType
from .models.match_spec import MatchSpec
from .models.records import PackageRecord
from .models.version import normalized_version
from .utils import human_bytes

log = getLogger(__name__)


@deprecated("24.9", "25.3", addendum="Unused.")
def print_dists(dists_extras):
    fmt = "    %-27s|%17s"
    print(fmt % ("package", "build"))
    print(fmt % ("-" * 27, "-" * 17))
    for prec, extra in dists_extras:
        line = fmt % (prec.name + "-" + prec.version, prec.build)
        if extra:
            line += extra
        print(line)


@deprecated("24.9", "25.3", addendum="Unused.")
def display_actions(
    actions, index, show_channel_urls=None, specs_to_remove=(), specs_to_add=()
):
    prefix = actions.get("PREFIX")
    builder = ["", "## Package Plan ##\n"]
    if prefix:
        builder.append(f"  environment location: {prefix}")
        builder.append("")
    if specs_to_remove:
        builder.append(
            f"  removed specs: {dashlist(sorted(str(s) for s in specs_to_remove), indent=4)}"
        )
        builder.append("")
    if specs_to_add:
        builder.append(
            f"  added / updated specs: {dashlist(sorted(str(s) for s in specs_to_add), indent=4)}"
        )
        builder.append("")
    print("\n".join(builder))

    if show_channel_urls is None:
        show_channel_urls = context.show_channel_urls

    def channel_str(rec):
        if rec.get("schannel"):
            return rec["schannel"]
        if rec.get("url"):
            return Channel(rec["url"]).canonical_name
        if rec.get("channel"):
            return Channel(rec["channel"]).canonical_name
        return UNKNOWN_CHANNEL

    def channel_filt(s):
        if show_channel_urls is False:
            return ""
        if show_channel_urls is None and s == DEFAULTS_CHANNEL_NAME:
            return ""
        return s

    if actions.get(FETCH):
        print("\nThe following packages will be downloaded:\n")

        disp_lst = []
        for prec in actions[FETCH]:
            assert isinstance(prec, PackageRecord)
            extra = "%15s" % human_bytes(prec["size"])
            schannel = channel_filt(prec.channel.canonical_name)
            if schannel:
                extra += "  " + schannel
            disp_lst.append((prec, extra))
        print_dists(disp_lst)

        if index and len(actions[FETCH]) > 1:
            num_bytes = sum(prec["size"] for prec in actions[FETCH])
            print(" " * 4 + "-" * 60)
            print(" " * 43 + "Total: %14s" % human_bytes(num_bytes))

    # package -> [oldver-oldbuild, newver-newbuild]
    packages = defaultdict(lambda: list(("", "")))
    features = defaultdict(lambda: list(("", "")))
    channels = defaultdict(lambda: list(("", "")))
    records = defaultdict(lambda: list((None, None)))
    linktypes = {}

    for prec in actions.get(LINK, []):
        assert isinstance(prec, PackageRecord)
        pkg = prec["name"]
        channels[pkg][1] = channel_str(prec)
        packages[pkg][1] = prec["version"] + "-" + prec["build"]
        records[pkg][1] = prec
        # TODO: this is a lie; may have to give this report after
        # UnlinkLinkTransaction.verify()
        linktypes[pkg] = LinkType.hardlink
        features[pkg][1] = ",".join(prec.get("features") or ())
    for prec in actions.get(UNLINK, []):
        assert isinstance(prec, PackageRecord)
        pkg = prec["name"]
        channels[pkg][0] = channel_str(prec)
        packages[pkg][0] = prec["version"] + "-" + prec["build"]
        records[pkg][0] = prec
        features[pkg][0] = ",".join(prec.get("features") or ())

    new = {p for p in packages if not packages[p][0]}
    removed = {p for p in packages if not packages[p][1]}
    # New packages are actually listed in the left-hand column,
    # so let's move them over there
    for pkg in new:
        for var in (packages, features, channels, records):
            var[pkg] = var[pkg][::-1]

    updated = set()
    downgraded = set()
    channeled = set()
    oldfmt = {}
    newfmt = {}
    empty = True
    if packages:
        empty = False
        maxpkg = max(len(p) for p in packages) + 1
        maxoldver = max(len(p[0]) for p in packages.values())
        maxnewver = max(len(p[1]) for p in packages.values())
        maxoldfeatures = max(len(p[0]) for p in features.values())
        maxnewfeatures = max(len(p[1]) for p in features.values())
        maxoldchannels = max(len(channel_filt(p[0])) for p in channels.values())
        maxnewchannels = max(len(channel_filt(p[1])) for p in channels.values())
        for pkg in packages:
            # That's right. I'm using old-style string formatting to generate a
            # string with new-style string formatting.
            oldfmt[pkg] = f"{{pkg:<{maxpkg}}} {{vers[0]:<{maxoldver}}}"
            if maxoldchannels:
                oldfmt[pkg] += f" {{channels[0]:<{maxoldchannels}}}"
            if features[pkg][0]:
                oldfmt[pkg] += f" [{{features[0]:<{maxoldfeatures}}}]"

            lt = LinkType(linktypes.get(pkg, LinkType.hardlink))
            lt = "" if lt == LinkType.hardlink else (f" ({lt})")
            if pkg in removed or pkg in new:
                oldfmt[pkg] += lt
                continue

            newfmt[pkg] = f"{{vers[1]:<{maxnewver}}}"
            if maxnewchannels:
                newfmt[pkg] += f" {{channels[1]:<{maxnewchannels}}}"
            if features[pkg][1]:
                newfmt[pkg] += f" [{{features[1]:<{maxnewfeatures}}}]"
            newfmt[pkg] += lt

            P0 = records[pkg][0]
            P1 = records[pkg][1]
            pri0 = P0.get("priority")
            pri1 = P1.get("priority")
            if pri0 is None or pri1 is None:
                pri0 = pri1 = 1
            try:
                if str(P1.version) == "custom":
                    newver = str(P0.version) != "custom"
                    oldver = not newver
                else:
                    # <= here means that unchanged packages will be put in updated
                    N0 = normalized_version(P0.version)
                    N1 = normalized_version(P1.version)
                    newver = N0 < N1
                    oldver = N0 > N1
            except TypeError:
                newver = P0.version < P1.version
                oldver = P0.version > P1.version
            oldbld = P0.build_number > P1.build_number
            newbld = P0.build_number < P1.build_number
            if (
                context.channel_priority
                and pri1 < pri0
                and (oldver or not newver and not newbld)
            ):
                channeled.add(pkg)
            elif newver:
                updated.add(pkg)
            elif pri1 < pri0 and (oldver or not newver and oldbld):
                channeled.add(pkg)
            elif oldver:
                downgraded.add(pkg)
            elif not oldbld:
                updated.add(pkg)
            else:
                downgraded.add(pkg)

    arrow = " --> "
    lead = " " * 4

    def format(s, pkg):
        chans = [channel_filt(c) for c in channels[pkg]]
        return lead + s.format(
            pkg=pkg + ":", vers=packages[pkg], channels=chans, features=features[pkg]
        )

    if new:
        print("\nThe following NEW packages will be INSTALLED:\n")
        for pkg in sorted(new):
            # New packages have been moved to the "old" column for display
            print(format(oldfmt[pkg], pkg))

    if removed:
        print("\nThe following packages will be REMOVED:\n")
        for pkg in sorted(removed):
            print(format(oldfmt[pkg], pkg))

    if updated:
        print("\nThe following packages will be UPDATED:\n")
        for pkg in sorted(updated):
            print(format(oldfmt[pkg] + arrow + newfmt[pkg], pkg))

    if channeled:
        print(
            "\nThe following packages will be SUPERSEDED by a higher-priority channel:\n"
        )
        for pkg in sorted(channeled):
            print(format(oldfmt[pkg] + arrow + newfmt[pkg], pkg))

    if downgraded:
        print("\nThe following packages will be DOWNGRADED:\n")
        for pkg in sorted(downgraded):
            print(format(oldfmt[pkg] + arrow + newfmt[pkg], pkg))

    if empty and actions.get(SYMLINK_CONDA):
        print("\nThe following empty environments will be CREATED:\n")
        print(actions["PREFIX"])

    print()


@deprecated("24.9", "25.3", addendum="Unused.")
def add_unlink(actions, dist):
    assert isinstance(dist, Dist)
    if UNLINK not in actions:
        actions[UNLINK] = []
    actions[UNLINK].append(dist)


@deprecated("24.9", "25.3", addendum="Unused.")
def add_defaults_to_specs(r, linked, specs, update=False, prefix=None):
    return


@deprecated(
    "24.9",
    "25.3",
    addendum="Use `conda.misc._get_best_prec_match` instead.",
)
def _get_best_prec_match(precs):
    from .misc import _get_best_prec_match

    return _get_best_prec_match(precs)


@deprecated(
    "24.9",
    "25.3",
    addendum="Use `conda.cli.install.revert_actions` instead.",
)
def revert_actions(prefix, revision=-1, index=None):
    from .cli.install import revert_actions

    return revert_actions(prefix, revision, index)


@deprecated("24.9", "25.3", addendum="Unused.")
@time_recorder("execute_actions")
def execute_actions(actions, index, verbose=False):  # pragma: no cover
    plan = _plan_from_actions(actions, index)
    execute_instructions(plan, index, verbose)


@deprecated("24.9", "25.3", addendum="Unused.")
def _plan_from_actions(actions, index):  # pragma: no cover
    from .instructions import ACTION_CODES, PREFIX, PRINT, PROGRESS, PROGRESS_COMMANDS

    if "op_order" in actions and actions["op_order"]:
        op_order = actions["op_order"]
    else:
        op_order = ACTION_CODES

    assert PREFIX in actions and actions[PREFIX]
    prefix = actions[PREFIX]
    plan = [("PREFIX", f"{prefix}")]

    unlink_link_transaction = actions.get("UNLINKLINKTRANSACTION")
    if unlink_link_transaction:
        raise RuntimeError()
        # progressive_fetch_extract = actions.get('PROGRESSIVEFETCHEXTRACT')
        # if progressive_fetch_extract:
        #     plan.append((PROGRESSIVEFETCHEXTRACT, progressive_fetch_extract))
        # plan.append((UNLINKLINKTRANSACTION, unlink_link_transaction))
        # return plan

    axn = actions.get("ACTION") or None
    specs = actions.get("SPECS", [])

    log.debug(f"Adding plans for operations: {op_order}")
    for op in op_order:
        if op not in actions:
            log.log(TRACE, f"action {op} not in actions")
            continue
        if not actions[op]:
            log.log(TRACE, f"action {op} has None value")
            continue
        if "_" not in op:
            plan.append((PRINT, f"{op.capitalize()}ing packages ..."))
        elif op.startswith("RM_"):
            plan.append(
                (PRINT, f"Pruning {op[3:].lower()} packages from the cache ...")
            )
        if op in PROGRESS_COMMANDS:
            plan.append((PROGRESS, "%d" % len(actions[op])))
        for arg in actions[op]:
            log.debug(f"appending value {arg} for action {op}")
            plan.append((op, arg))

    plan = _inject_UNLINKLINKTRANSACTION(plan, index, prefix, axn, specs)

    return plan


@deprecated("24.9", "25.3", addendum="Unused.")
def _inject_UNLINKLINKTRANSACTION(plan, index, prefix, axn, specs):  # pragma: no cover
    from os.path import isdir

    from .core.package_cache_data import ProgressiveFetchExtract
    from .instructions import (
        LINK,
        PROGRESSIVEFETCHEXTRACT,
        UNLINK,
        UNLINKLINKTRANSACTION,
    )
    from .models.dist import Dist

    # this is only used for conda-build at this point
    first_unlink_link_idx = next(
        (q for q, p in enumerate(plan) if p[0] in (UNLINK, LINK)), -1
    )
    if first_unlink_link_idx >= 0:
        grouped_instructions = groupby(lambda x: x[0], plan)
        unlink_dists = tuple(Dist(d[1]) for d in grouped_instructions.get(UNLINK, ()))
        link_dists = tuple(Dist(d[1]) for d in grouped_instructions.get(LINK, ()))
        unlink_dists, link_dists = _handle_menuinst(unlink_dists, link_dists)

        if isdir(prefix):
            unlink_precs = tuple(index[d] for d in unlink_dists)
        else:
            # there's nothing to unlink in an environment that doesn't exist
            # this is a hack for what appears to be a logic error in conda-build
            # caught in tests/test_subpackages.py::test_subpackage_recipes[python_test_dep]
            unlink_precs = ()
        link_precs = tuple(index[d] for d in link_dists)

        pfe = ProgressiveFetchExtract(link_precs)
        pfe.prepare()

        stp = PrefixSetup(prefix, unlink_precs, link_precs, (), specs, ())
        plan.insert(
            first_unlink_link_idx, (UNLINKLINKTRANSACTION, UnlinkLinkTransaction(stp))
        )
        plan.insert(first_unlink_link_idx, (PROGRESSIVEFETCHEXTRACT, pfe))
    elif axn in ("INSTALL", "CREATE"):
        plan.insert(0, (UNLINKLINKTRANSACTION, (prefix, (), (), (), specs)))

    return plan


@deprecated("24.9", "25.3", addendum="Unused.")
def _handle_menuinst(unlink_dists, link_dists):  # pragma: no cover
    # Always link/unlink menuinst first/last in case a subsequent
    # package tries to import it to create/remove a shortcut

    # unlink
    menuinst_idx = next(
        (q for q, d in enumerate(unlink_dists) if d.name == "menuinst"), None
    )
    if menuinst_idx is not None:
        unlink_dists = (
            *unlink_dists[:menuinst_idx],
            *unlink_dists[menuinst_idx + 1 :],
            *unlink_dists[menuinst_idx : menuinst_idx + 1],
        )

    # link
    menuinst_idx = next(
        (q for q, d in enumerate(link_dists) if d.name == "menuinst"), None
    )
    if menuinst_idx is not None:
        link_dists = (
            *link_dists[menuinst_idx : menuinst_idx + 1],
            *link_dists[:menuinst_idx],
            *link_dists[menuinst_idx + 1 :],
        )

    return unlink_dists, link_dists


@deprecated("24.9", "25.3", addendum="Unused.")
@time_recorder("install_actions")
def install_actions(
    prefix,
    index,
    specs,
    force=False,
    only_names=None,
    always_copy=False,
    pinned=True,
    update_deps=True,
    prune=False,
    channel_priority_map=None,
    is_update=False,
    minimal_hint=False,
):  # pragma: no cover
    # this is for conda-build
    with env_vars(
        {
            "CONDA_ALLOW_NON_CHANNEL_URLS": "true",
            "CONDA_SOLVER_IGNORE_TIMESTAMPS": "false",
        },
        reset_context,
    ):
        from os.path import basename

        from .models.channel import Channel
        from .models.dist import Dist

        if channel_priority_map:
            channel_names = IndexedSet(
                Channel(url).canonical_name for url in channel_priority_map
            )
            channels = IndexedSet(Channel(cn) for cn in channel_names)
            subdirs = IndexedSet(basename(url) for url in channel_priority_map)
        else:
            # a hack for when conda-build calls this function without giving channel_priority_map
            if LAST_CHANNEL_URLS:
                channel_priority_map = prioritize_channels(LAST_CHANNEL_URLS)
                channels = IndexedSet(Channel(url) for url in channel_priority_map)
                subdirs = (
                    IndexedSet(
                        subdir for subdir in (c.subdir for c in channels) if subdir
                    )
                    or context.subdirs
                )
            else:
                channels = subdirs = None

        specs = tuple(MatchSpec(spec) for spec in specs)

        from .core.prefix_data import PrefixData

        PrefixData._cache_.clear()

        solver_backend = context.plugin_manager.get_cached_solver_backend()
        solver = solver_backend(prefix, channels, subdirs, specs_to_add=specs)
        if index:
            solver._index = {prec: prec for prec in index.values()}
        txn = solver.solve_for_transaction(prune=prune, ignore_pinned=not pinned)
        prefix_setup = txn.prefix_setups[prefix]
        actions = get_blank_actions(prefix)
        actions["UNLINK"].extend(Dist(prec) for prec in prefix_setup.unlink_precs)
        actions["LINK"].extend(Dist(prec) for prec in prefix_setup.link_precs)
        return actions


@deprecated("24.9", "25.3", addendum="Unused.")
def get_blank_actions(prefix):  # pragma: no cover
    from collections import defaultdict

    from .instructions import (
        CHECK_EXTRACT,
        CHECK_FETCH,
        EXTRACT,
        FETCH,
        LINK,
        PREFIX,
        RM_EXTRACTED,
        RM_FETCHED,
        SYMLINK_CONDA,
        UNLINK,
    )

    actions = defaultdict(list)
    actions[PREFIX] = prefix
    actions["op_order"] = (
        CHECK_FETCH,
        RM_FETCHED,
        FETCH,
        CHECK_EXTRACT,
        RM_EXTRACTED,
        EXTRACT,
        UNLINK,
        LINK,
        SYMLINK_CONDA,
    )
    return actions


@deprecated("24.9", "25.3")
@time_recorder("execute_plan")
def execute_plan(old_plan, index=None, verbose=False):  # pragma: no cover
    """Deprecated: This should `conda.instructions.execute_instructions` instead."""
    plan = _update_old_plan(old_plan)
    execute_instructions(plan, index, verbose)


@deprecated("24.9", "25.3")
def execute_instructions(
    plan, index=None, verbose=False, _commands=None
):  # pragma: no cover
    """Execute the instructions in the plan
    :param plan: A list of (instruction, arg) tuples
    :param index: The meta-data index
    :param verbose: verbose output
    :param _commands: (For testing only) dict mapping an instruction to executable if None
    then the default commands will be used
    """
    from .base.context import context
    from .instructions import PROGRESS_COMMANDS, commands
    from .models.dist import Dist

    if _commands is None:
        _commands = commands

    log.debug("executing plan %s", plan)

    state = {"i": None, "prefix": context.root_prefix, "index": index}

    for instruction, arg in plan:
        log.debug(" %s(%r)", instruction, arg)

        if state["i"] is not None and instruction in PROGRESS_COMMANDS:
            state["i"] += 1
            getLogger("progress.update").info((Dist(arg).dist_name, state["i"] - 1))
        cmd = _commands[instruction]

        if callable(cmd):
            cmd(state, arg)

        if (
            state["i"] is not None
            and instruction in PROGRESS_COMMANDS
            and state["maxval"] == state["i"]
        ):
            state["i"] = None
            getLogger("progress.stop").info(None)


@deprecated("24.9", "25.3")
def _update_old_plan(old_plan):  # pragma: no cover
    """
    Update an old plan object to work with
    `conda.instructions.execute_instructions`
    """
    plan = []
    for line in old_plan:
        if line.startswith("#"):
            continue
        if " " not in line:
            from .exceptions import ArgumentError

            raise ArgumentError(f"The instruction {line!r} takes at least one argument")

        instruction, arg = line.split(" ", 1)
        plan.append((instruction, arg))
    return plan


if __name__ == "__main__":
    # for testing new revert_actions() only
    from pprint import pprint

    from .cli.install import revert_actions

    deprecated.topic("24.9", "25.3", topic="`conda.plan` as an entrypoint")

    pprint(dict(revert_actions(sys.prefix, int(sys.argv[1]))))


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Conda as a module entry point."""

import sys

from .cli import main

sys.exit(main())


# file generated by setuptools_scm
# don't change, don't track in version control
TYPE_CHECKING = False
if TYPE_CHECKING:
    from typing import Tuple, Union
    VERSION_TUPLE = Tuple[Union[int, str], ...]
else:
    VERSION_TUPLE = object

version: str
__version__: str
__version_tuple__: VERSION_TUPLE
version_tuple: VERSION_TUPLE

__version__ = version = '24.7.1'
__version_tuple__ = version_tuple = (24, 7, 1)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Low-level SAT solver wrapper/interface for the classic solver.

See conda.core.solver.Solver for the high-level API.
"""

from __future__ import annotations

import copy
import itertools
from collections import defaultdict, deque
from functools import lru_cache
from logging import DEBUG, getLogger

from tqdm import tqdm

from .auxlib.decorators import memoizemethod
from .base.constants import MAX_CHANNEL_PRIORITY, ChannelPriority, SatSolverChoice
from .base.context import context
from .common.compat import on_win
from .common.io import dashlist, time_recorder
from .common.iterators import groupby_to_dict as groupby
from .common.logic import (
    TRUE,
    Clauses,
    PycoSatSolver,
    PyCryptoSatSolver,
    PySatSolver,
    minimal_unsatisfiable_subset,
)
from .common.toposort import toposort
from .exceptions import (
    CondaDependencyError,
    InvalidSpec,
    ResolvePackageNotFound,
    UnsatisfiableError,
)
from .models.channel import Channel, MultiChannel
from .models.enums import NoarchType, PackageType
from .models.match_spec import MatchSpec
from .models.records import PackageRecord
from .models.version import VersionOrder

try:
    from frozendict import frozendict
except ImportError:
    from ._vendor.frozendict import FrozenOrderedDict as frozendict

log = getLogger(__name__)
stdoutlog = getLogger("conda.stdoutlog")

# used in conda build
Unsatisfiable = UnsatisfiableError
ResolvePackageNotFound = ResolvePackageNotFound

_sat_solvers = {
    SatSolverChoice.PYCOSAT: PycoSatSolver,
    SatSolverChoice.PYCRYPTOSAT: PyCryptoSatSolver,
    SatSolverChoice.PYSAT: PySatSolver,
}


@lru_cache(maxsize=None)
def _get_sat_solver_cls(sat_solver_choice=SatSolverChoice.PYCOSAT):
    def try_out_solver(sat_solver):
        c = Clauses(sat_solver=sat_solver)
        required = {c.new_var(), c.new_var()}
        c.Require(c.And, *required)
        solution = set(c.sat())
        if not required.issubset(solution):
            raise RuntimeError(f"Wrong SAT solution: {solution}. Required: {required}")

    sat_solver = _sat_solvers[sat_solver_choice]
    try:
        try_out_solver(sat_solver)
    except Exception as e:
        log.warning(
            "Could not run SAT solver through interface '%s'.", sat_solver_choice
        )
        log.debug("SAT interface error due to: %s", e, exc_info=True)
    else:
        log.debug("Using SAT solver interface '%s'.", sat_solver_choice)
        return sat_solver
    for sat_solver in _sat_solvers.values():
        try:
            try_out_solver(sat_solver)
        except Exception as e:
            log.debug(
                "Attempted SAT interface '%s' but unavailable due to: %s",
                sat_solver_choice,
                e,
            )
        else:
            log.debug("Falling back to SAT solver interface '%s'.", sat_solver_choice)
            return sat_solver
    raise CondaDependencyError(
        "Cannot run solver. No functioning SAT implementations available."
    )


def exactness_and_number_of_deps(resolve_obj, ms):
    """Sorting key to emphasize packages that have more strict
    requirements. More strict means the reduced index can be reduced
    more, so we want to consider these more constrained deps earlier in
    reducing the index.
    """
    if ms.strictness == 3:
        prec = resolve_obj.find_matches(ms)
        value = 3
        if prec:
            for dep in prec[0].depends:
                value += MatchSpec(dep).strictness
    else:
        value = ms.strictness
    return value


class Resolve:
    def __init__(self, index, processed=False, channels=()):
        self.index = index

        self.channels = channels
        self._channel_priorities_map = (
            self._make_channel_priorities(channels) if channels else {}
        )
        self._channel_priority = context.channel_priority
        self._solver_ignore_timestamps = context.solver_ignore_timestamps

        groups = groupby(lambda x: x.name, index.values())
        trackers = defaultdict(list)

        for name in groups:
            unmanageable_precs = [prec for prec in groups[name] if prec.is_unmanageable]
            if unmanageable_precs:
                log.debug("restricting to unmanageable packages: %s", name)
                groups[name] = unmanageable_precs
            tf_precs = (prec for prec in groups[name] if prec.track_features)
            for prec in tf_precs:
                for feature_name in prec.track_features:
                    trackers[feature_name].append(prec)

        self.groups = groups  # dict[package_name, list[PackageRecord]]
        self.trackers = trackers  # dict[track_feature, set[PackageRecord]]
        self._cached_find_matches = {}  # dict[MatchSpec, set[PackageRecord]]
        self.ms_depends_ = {}  # dict[PackageRecord, list[MatchSpec]]
        self._reduced_index_cache = {}
        self._pool_cache = {}
        self._strict_channel_cache = {}

        self._system_precs = {
            _
            for _ in index
            if (
                hasattr(_, "package_type")
                and _.package_type == PackageType.VIRTUAL_SYSTEM
            )
        }

        # sorting these in reverse order is effectively prioritizing
        # constraint behavior from newer packages. It is applying broadening
        # reduction based on the latest packages, which may reduce the space
        # more, because more modern packages utilize constraints in more sane
        # ways (for example, using run_exports in conda-build 3)
        for name, group in self.groups.items():
            self.groups[name] = sorted(group, key=self.version_key, reverse=True)

    def __hash__(self):
        return (
            super().__hash__()
            ^ hash(frozenset(self.channels))
            ^ hash(frozendict(self._channel_priorities_map))
            ^ hash(self._channel_priority)
            ^ hash(self._solver_ignore_timestamps)
            ^ hash(frozendict((k, tuple(v)) for k, v in self.groups.items()))
            ^ hash(frozendict((k, tuple(v)) for k, v in self.trackers.items()))
            ^ hash(frozendict((k, tuple(v)) for k, v in self.ms_depends_.items()))
        )

    def default_filter(self, features=None, filter=None):
        # TODO: fix this import; this is bad
        from .core.subdir_data import make_feature_record

        if filter is None:
            filter = {}
        else:
            filter.clear()

        filter.update(
            {make_feature_record(fstr): False for fstr in self.trackers.keys()}
        )
        if features:
            filter.update({make_feature_record(fstr): True for fstr in features})
        return filter

    def valid(self, spec_or_prec, filter, optional=True):
        """Tests if a package, MatchSpec, or a list of both has satisfiable
        dependencies, assuming cyclic dependencies are always valid.

        Args:
            spec_or_prec: a package record, a MatchSpec, or an iterable of these.
            filter: a dictionary of (fkey,valid) pairs, used to consider a subset
                of dependencies, and to eliminate repeated searches.
            optional: if True (default), do not enforce optional specifications
                when considering validity. If False, enforce them.

        Returns:
            True if the full set of dependencies can be satisfied; False otherwise.
            If filter is supplied and update is True, it will be updated with the
            search results.
        """

        def v_(spec):
            return v_ms_(spec) if isinstance(spec, MatchSpec) else v_fkey_(spec)

        def v_ms_(ms):
            return (
                optional
                and ms.optional
                or any(v_fkey_(fkey) for fkey in self.find_matches(ms))
            )

        def v_fkey_(prec):
            val = filter.get(prec)
            if val is None:
                filter[prec] = True
                try:
                    depends = self.ms_depends(prec)
                except InvalidSpec:
                    val = filter[prec] = False
                else:
                    val = filter[prec] = all(v_ms_(ms) for ms in depends)
            return val

        result = v_(spec_or_prec)
        return result

    def valid2(self, spec_or_prec, filter_out, optional=True):
        def is_valid(_spec_or_prec):
            if isinstance(_spec_or_prec, MatchSpec):
                return is_valid_spec(_spec_or_prec)
            else:
                return is_valid_prec(_spec_or_prec)

        @memoizemethod
        def is_valid_spec(_spec):
            return (
                optional
                and _spec.optional
                or any(is_valid_prec(_prec) for _prec in self.find_matches(_spec))
            )

        def is_valid_prec(prec):
            val = filter_out.get(prec)
            if val is None:
                filter_out[prec] = False
                try:
                    has_valid_deps = all(
                        is_valid_spec(ms) for ms in self.ms_depends(prec)
                    )
                except InvalidSpec:
                    val = filter_out[prec] = "invalid dep specs"
                else:
                    val = filter_out[prec] = (
                        False if has_valid_deps else "invalid depends specs"
                    )
            return not val

        return is_valid(spec_or_prec)

    def invalid_chains(self, spec, filter, optional=True):
        """Constructs a set of 'dependency chains' for invalid specs.

        A dependency chain is a tuple of MatchSpec objects, starting with
        the requested spec, proceeding down the dependency tree, ending at
        a specification that cannot be satisfied.

        Args:
            spec: a package key or MatchSpec
            filter: a dictionary of (prec, valid) pairs to be used when
                testing for package validity.

        Returns:
            A tuple of tuples, empty if the MatchSpec is valid.
        """

        def chains_(spec, names):
            if spec.name in names:
                return
            names.add(spec.name)
            if self.valid(spec, filter, optional):
                return
            precs = self.find_matches(spec)
            found = False

            conflict_deps = set()
            for prec in precs:
                for m2 in self.ms_depends(prec):
                    for x in chains_(m2, names):
                        found = True
                        yield (spec,) + x
                    else:
                        conflict_deps.add(m2)
            if not found:
                conflict_groups = groupby(lambda x: x.name, conflict_deps)
                for group in conflict_groups.values():
                    yield (spec,) + MatchSpec.union(group)

        return chains_(spec, set())

    def verify_specs(self, specs):
        """Perform a quick verification that specs and dependencies are reasonable.

        Args:
            specs: An iterable of strings or MatchSpec objects to be tested.

        Returns:
            Nothing, but if there is a conflict, an error is thrown.

        Note that this does not attempt to resolve circular dependencies.
        """
        non_tf_specs = []
        bad_deps = []
        feature_names = set()
        for ms in specs:
            _feature_names = ms.get_exact_value("track_features")
            if _feature_names:
                feature_names.update(_feature_names)
            else:
                non_tf_specs.append(ms)
        bad_deps.extend(
            (spec,)
            for spec in non_tf_specs
            if (not spec.optional and not self.find_matches(spec))
        )
        if bad_deps:
            raise ResolvePackageNotFound(bad_deps)
        return tuple(non_tf_specs), feature_names

    def _classify_bad_deps(
        self, bad_deps, specs_to_add, history_specs, strict_channel_priority
    ):
        classes = {
            "python": set(),
            "request_conflict_with_history": set(),
            "direct": set(),
            "virtual_package": set(),
        }
        specs_to_add = {MatchSpec(_) for _ in specs_to_add or []}
        history_specs = {MatchSpec(_) for _ in history_specs or []}
        for chain in bad_deps:
            # sometimes chains come in as strings
            if (
                len(chain) > 1
                and chain[-1].name == "python"
                and not any(_.name == "python" for _ in specs_to_add)
                and any(_[0] for _ in bad_deps if _[0].name == "python")
            ):
                python_first_specs = [_[0] for _ in bad_deps if _[0].name == "python"]
                if python_first_specs:
                    python_spec = python_first_specs[0]
                    if not (
                        set(self.find_matches(python_spec))
                        & set(self.find_matches(chain[-1]))
                    ):
                        classes["python"].add(
                            (
                                tuple([chain[0], chain[-1]]),
                                str(MatchSpec(python_spec, target=None)),
                            )
                        )
            elif chain[-1].name.startswith("__"):
                version = [_ for _ in self._system_precs if _.name == chain[-1].name]
                virtual_package_version = (
                    version[0].version if version else "not available"
                )
                classes["virtual_package"].add((tuple(chain), virtual_package_version))
            elif chain[0] in specs_to_add:
                match = False
                for spec in history_specs:
                    if spec.name == chain[-1].name:
                        classes["request_conflict_with_history"].add(
                            (tuple(chain), str(MatchSpec(spec, target=None)))
                        )
                        match = True

                if not match:
                    classes["direct"].add(
                        (tuple(chain), str(MatchSpec(chain[0], target=None)))
                    )
            else:
                if len(chain) > 1 or any(
                    len(c) >= 1 and c[0] == chain[0] for c in bad_deps
                ):
                    classes["direct"].add(
                        (tuple(chain), str(MatchSpec(chain[0], target=None)))
                    )

        if classes["python"]:
            # filter out plain single-entry python conflicts.  The python section explains these.
            classes["direct"] = [
                _
                for _ in classes["direct"]
                if _[1].startswith("python ") or len(_[0]) > 1
            ]
        return classes

    def find_matches_with_strict(self, ms, strict_channel_priority):
        matches = self.find_matches(ms)
        if not strict_channel_priority:
            return matches
        sole_source_channel_name = self._get_strict_channel(ms.name)
        return tuple(f for f in matches if f.channel.name == sole_source_channel_name)

    def find_conflicts(self, specs, specs_to_add=None, history_specs=None):
        if context.unsatisfiable_hints:
            if not context.json:
                print(
                    "\nFound conflicts! Looking for incompatible packages.\n"
                    "This can take several minutes.  Press CTRL-C to abort."
                )
            bad_deps = self.build_conflict_map(specs, specs_to_add, history_specs)
        else:
            bad_deps = {}
        strict_channel_priority = context.channel_priority == ChannelPriority.STRICT
        raise UnsatisfiableError(bad_deps, strict=strict_channel_priority)

    def breadth_first_search_for_dep_graph(
        self, root_spec, target_name, dep_graph, num_targets=1
    ):
        """Return shorted path from root_spec to target_name"""
        queue = []
        queue.append([root_spec])
        visited = []
        target_paths = []
        while queue:
            path = queue.pop(0)
            node = path[-1]
            if node in visited:
                continue
            visited.append(node)
            if node.name == target_name:
                if len(target_paths) == 0:
                    target_paths.append(path)
                if len(target_paths[-1]) == len(path):
                    last_spec = MatchSpec.union((path[-1], target_paths[-1][-1]))[0]
                    target_paths[-1][-1] = last_spec
                else:
                    target_paths.append(path)

                found_all_targets = len(target_paths) == num_targets and any(
                    len(_) != len(path) for _ in queue
                )
                if len(queue) == 0 or found_all_targets:
                    return target_paths
            sub_graph = dep_graph
            for p in path[0:-1]:
                sub_graph = sub_graph[p]
            children = [_ for _ in sub_graph.get(node, {})]
            if children is None:
                continue
            for adj in children:
                if len(target_paths) < num_targets:
                    new_path = list(path)
                    new_path.append(adj)
                    queue.append(new_path)
        return target_paths

    def build_graph_of_deps(self, spec):
        dep_graph = {spec: {}}
        all_deps = set()
        queue = [[spec]]
        while queue:
            path = queue.pop(0)
            sub_graph = dep_graph
            for p in path:
                sub_graph = sub_graph[p]
            parent_node = path[-1]
            matches = self.find_matches(parent_node)
            for mat in matches:
                if len(mat.depends) > 0:
                    for i in mat.depends:
                        new_node = MatchSpec(i)
                        sub_graph.update({new_node: {}})
                        all_deps.add(new_node)
                        new_path = list(path)
                        new_path.append(new_node)
                        if len(new_path) <= context.unsatisfiable_hints_check_depth:
                            queue.append(new_path)
        return dep_graph, all_deps

    def build_conflict_map(self, specs, specs_to_add=None, history_specs=None):
        """Perform a deeper analysis on conflicting specifications, by attempting
        to find the common dependencies that might be the cause of conflicts.

        Args:
            specs: An iterable of strings or MatchSpec objects to be tested.
            It is assumed that the specs conflict.

        Returns:
            bad_deps: A list of lists of bad deps

        Strategy:
            If we're here, we know that the specs conflict. This could be because:
            - One spec conflicts with another; e.g.
                  ['numpy 1.5*', 'numpy >=1.6']
            - One spec conflicts with a dependency of another; e.g.
                  ['numpy 1.5*', 'scipy 0.12.0b1']
            - Each spec depends on *the same package* but in a different way; e.g.,
                  ['A', 'B'] where A depends on numpy 1.5, and B on numpy 1.6.
            Technically, all three of these cases can be boiled down to the last
            one if we treat the spec itself as one of the "dependencies". There
            might be more complex reasons for a conflict, but this code only
            considers the ones above.

            The purpose of this code, then, is to identify packages (like numpy
            above) that all of the specs depend on *but in different ways*. We
            then identify the dependency chains that lead to those packages.
        """
        # if only a single package matches the spec use the packages depends
        # rather than the spec itself
        strict_channel_priority = context.channel_priority == ChannelPriority.STRICT

        specs = set(specs) | (specs_to_add or set())
        # Remove virtual packages
        specs = {spec for spec in specs if not spec.name.startswith("__")}
        if len(specs) == 1:
            matches = self.find_matches(next(iter(specs)))
            if len(matches) == 1:
                specs = set(self.ms_depends(matches[0]))
        specs.update({_.to_match_spec() for _ in self._system_precs})
        for spec in specs:
            self._get_package_pool((spec,))

        dep_graph = {}
        dep_list = {}
        with tqdm(
            total=len(specs),
            desc="Building graph of deps",
            leave=False,
            disable=context.json,
        ) as t:
            for spec in specs:
                t.set_description(f"Examining {spec}")
                t.update()
                dep_graph_for_spec, all_deps_for_spec = self.build_graph_of_deps(spec)
                dep_graph.update(dep_graph_for_spec)
                if dep_list.get(spec.name):
                    dep_list[spec.name].append(spec)
                else:
                    dep_list[spec.name] = [spec]
                for dep in all_deps_for_spec:
                    if dep_list.get(dep.name):
                        dep_list[dep.name].append(spec)
                    else:
                        dep_list[dep.name] = [spec]

        chains = []
        conflicting_pkgs_pkgs = {}
        for k, v in dep_list.items():
            set_v = frozenset(v)
            # Packages probably conflicts if many specs depend on it
            if len(set_v) > 1:
                if conflicting_pkgs_pkgs.get(set_v) is None:
                    conflicting_pkgs_pkgs[set_v] = [k]
                else:
                    conflicting_pkgs_pkgs[set_v].append(k)
            # Conflict if required virtual package is not present
            elif k.startswith("__") and any(s for s in set_v if s.name != k):
                conflicting_pkgs_pkgs[set_v] = [k]

        with tqdm(
            total=len(specs),
            desc="Determining conflicts",
            leave=False,
            disable=context.json,
        ) as t:
            for roots, nodes in conflicting_pkgs_pkgs.items():
                t.set_description(
                    "Examining conflict for {}".format(" ".join(_.name for _ in roots))
                )
                t.update()
                lroots = [_ for _ in roots]
                current_shortest_chain = []
                shortest_node = None
                requested_spec_unsat = frozenset(nodes).intersection(
                    {_.name for _ in roots}
                )
                if requested_spec_unsat:
                    chains.append([_ for _ in roots if _.name in requested_spec_unsat])
                    shortest_node = chains[-1][0]
                    for root in roots:
                        if root != chains[0][0]:
                            search_node = shortest_node.name
                            num_occurances = dep_list[search_node].count(root)
                            c = self.breadth_first_search_for_dep_graph(
                                root, search_node, dep_graph, num_occurances
                            )
                            chains.extend(c)
                else:
                    for node in nodes:
                        num_occurances = dep_list[node].count(lroots[0])
                        chain = self.breadth_first_search_for_dep_graph(
                            lroots[0], node, dep_graph, num_occurances
                        )
                        chains.extend(chain)
                        if len(current_shortest_chain) == 0 or len(chain) < len(
                            current_shortest_chain
                        ):
                            current_shortest_chain = chain
                            shortest_node = node
                    for root in lroots[1:]:
                        num_occurances = dep_list[shortest_node].count(root)
                        c = self.breadth_first_search_for_dep_graph(
                            root, shortest_node, dep_graph, num_occurances
                        )
                        chains.extend(c)

        bad_deps = self._classify_bad_deps(
            chains, specs_to_add, history_specs, strict_channel_priority
        )
        return bad_deps

    def _get_strict_channel(self, package_name):
        channel_name = None
        try:
            channel_name = self._strict_channel_cache[package_name]
        except KeyError:
            if package_name in self.groups:
                all_channel_names = {
                    prec.channel.name for prec in self.groups[package_name]
                }
                by_cp = {
                    self._channel_priorities_map.get(cn, 1): cn
                    for cn in all_channel_names
                }
                highest_priority = sorted(by_cp)[
                    0
                ]  # highest priority is the lowest number
                channel_name = self._strict_channel_cache[package_name] = by_cp[
                    highest_priority
                ]
        return channel_name

    @memoizemethod
    def _broader(self, ms, specs_by_name):
        """Prevent introduction of matchspecs that broaden our selection of choices."""
        if not specs_by_name:
            return False
        return ms.strictness < specs_by_name[0].strictness

    def _get_package_pool(self, specs):
        specs = frozenset(specs)
        if specs in self._pool_cache:
            pool = self._pool_cache[specs]
        else:
            pool = self.get_reduced_index(specs)
            grouped_pool = groupby(lambda x: x.name, pool)
            pool = {k: set(v) for k, v in grouped_pool.items()}
            self._pool_cache[specs] = pool
        return pool

    @time_recorder(module_name=__name__)
    def get_reduced_index(
        self, explicit_specs, sort_by_exactness=True, exit_on_conflict=False
    ):
        # TODO: fix this import; this is bad
        from .core.subdir_data import make_feature_record

        strict_channel_priority = context.channel_priority == ChannelPriority.STRICT

        cache_key = strict_channel_priority, tuple(explicit_specs)
        if cache_key in self._reduced_index_cache:
            return self._reduced_index_cache[cache_key]

        if log.isEnabledFor(DEBUG):
            log.debug(
                "Retrieving packages for: %s",
                dashlist(sorted(str(s) for s in explicit_specs)),
            )

        explicit_specs, features = self.verify_specs(explicit_specs)
        filter_out = {
            prec: False if val else "feature not enabled"
            for prec, val in self.default_filter(features).items()
        }
        snames = set()
        top_level_spec = None
        cp_filter_applied = set()  # values are package names
        if sort_by_exactness:
            # prioritize specs that are more exact.  Exact specs will evaluate to 3,
            #    constrained specs will evaluate to 2, and name only will be 1
            explicit_specs = sorted(
                list(explicit_specs),
                key=lambda x: (exactness_and_number_of_deps(self, x), x.dist_str()),
                reverse=True,
            )
        # tuple because it needs to be hashable
        explicit_specs = tuple(explicit_specs)

        explicit_spec_package_pool = {}
        for s in explicit_specs:
            explicit_spec_package_pool[s.name] = explicit_spec_package_pool.get(
                s.name, set()
            ) | set(self.find_matches(s))

        def filter_group(_specs):
            # all _specs should be for the same package name
            name = next(iter(_specs)).name
            group = self.groups.get(name, ())

            # implement strict channel priority
            if group and strict_channel_priority and name not in cp_filter_applied:
                sole_source_channel_name = self._get_strict_channel(name)
                for prec in group:
                    if prec.channel.name != sole_source_channel_name:
                        filter_out[prec] = "removed due to strict channel priority"
                cp_filter_applied.add(name)

            # Prune packages that don't match any of the patterns,
            # have unsatisfiable dependencies, or conflict with the explicit specs
            nold = nnew = 0
            for prec in group:
                if not filter_out.setdefault(prec, False):
                    nold += 1
                    if (not self.match_any(_specs, prec)) or (
                        explicit_spec_package_pool.get(name)
                        and prec not in explicit_spec_package_pool[name]
                    ):
                        filter_out[prec] = (
                            f"incompatible with required spec {top_level_spec}"
                        )
                        continue
                    unsatisfiable_dep_specs = set()
                    for ms in self.ms_depends(prec):
                        if not ms.optional and not any(
                            rec
                            for rec in self.find_matches(ms)
                            if not filter_out.get(rec, False)
                        ):
                            unsatisfiable_dep_specs.add(ms)
                    if unsatisfiable_dep_specs:
                        filter_out[prec] = "unsatisfiable dependencies {}".format(
                            " ".join(str(s) for s in unsatisfiable_dep_specs)
                        )
                        continue
                    filter_out[prec] = False
                    nnew += 1

            reduced = nnew < nold
            if reduced:
                log.debug("%s: pruned from %d -> %d" % (name, nold, nnew))
            if any(ms.optional for ms in _specs):
                return reduced
            elif nnew == 0:
                # Indicates that a conflict was found; we can exit early
                return None

            # Perform the same filtering steps on any dependencies shared across
            # *all* packages in the group. Even if just one of the packages does
            # not have a particular dependency, it must be ignored in this pass.
            # Otherwise, we might do more filtering than we should---and it is
            # better to have extra packages here than missing ones.
            if reduced or name not in snames:
                snames.add(name)

                _dep_specs = groupby(
                    lambda s: s.name,
                    (
                        dep_spec
                        for prec in group
                        if not filter_out.get(prec, False)
                        for dep_spec in self.ms_depends(prec)
                        if not dep_spec.optional
                    ),
                )
                _dep_specs.pop("*", None)  # discard track_features specs

                for deps_name, deps in sorted(
                    _dep_specs.items(), key=lambda x: any(_.optional for _ in x[1])
                ):
                    if len(deps) >= nnew:
                        res = filter_group(set(deps))
                        if res:
                            reduced = True
                        elif res is None:
                            # Indicates that a conflict was found; we can exit early
                            return None

            return reduced

        # Iterate on pruning until no progress is made. We've implemented
        # what amounts to "double-elimination" here; packages get one additional
        # chance after their first "False" reduction. This catches more instances
        # where one package's filter affects another. But we don't have to be
        # perfect about this, so performance matters.
        pruned_to_zero = set()
        for _ in range(2):
            snames.clear()
            slist = deque(explicit_specs)
            while slist:
                s = slist.popleft()
                if filter_group([s]):
                    slist.append(s)
                else:
                    pruned_to_zero.add(s)

        if pruned_to_zero and exit_on_conflict:
            return {}

        # Determine all valid packages in the dependency graph
        reduced_index2 = {
            prec: prec for prec in (make_feature_record(fstr) for fstr in features)
        }
        specs_by_name_seed = {}
        for s in explicit_specs:
            specs_by_name_seed[s.name] = specs_by_name_seed.get(s.name, []) + [s]
        for explicit_spec in explicit_specs:
            add_these_precs2 = tuple(
                prec
                for prec in self.find_matches(explicit_spec)
                if prec not in reduced_index2 and self.valid2(prec, filter_out)
            )

            if strict_channel_priority and add_these_precs2:
                strict_channel_name = self._get_strict_channel(add_these_precs2[0].name)

                add_these_precs2 = tuple(
                    prec
                    for prec in add_these_precs2
                    if prec.channel.name == strict_channel_name
                )
            reduced_index2.update((prec, prec) for prec in add_these_precs2)

            for pkg in add_these_precs2:
                # what we have seen is only relevant within the context of a single package
                #    that is picked up because of an explicit spec.  We don't want the
                #    broadening check to apply across packages at the explicit level; only
                #    at the level of deps below that explicit package.
                seen_specs = set()
                specs_by_name = copy.deepcopy(specs_by_name_seed)

                dep_specs = set(self.ms_depends(pkg))
                for dep in dep_specs:
                    specs = specs_by_name.get(dep.name, [])
                    if dep not in specs and (
                        not specs or dep.strictness >= specs[0].strictness
                    ):
                        specs.insert(0, dep)
                    specs_by_name[dep.name] = specs

                while dep_specs:
                    # used for debugging
                    # size_index = len(reduced_index2)
                    # specs_added = []
                    ms = dep_specs.pop()
                    seen_specs.add(ms)
                    for dep_pkg in (
                        _ for _ in self.find_matches(ms) if _ not in reduced_index2
                    ):
                        if not self.valid2(dep_pkg, filter_out):
                            continue

                        # expand the reduced index if not using strict channel priority,
                        #    or if using it and this package is in the appropriate channel
                        if not strict_channel_priority or (
                            self._get_strict_channel(dep_pkg.name)
                            == dep_pkg.channel.name
                        ):
                            reduced_index2[dep_pkg] = dep_pkg

                            # recurse to deps of this dep
                            new_specs = set(self.ms_depends(dep_pkg)) - seen_specs
                            for new_ms in new_specs:
                                # We do not pull packages into the reduced index due
                                # to a track_features dependency. Remember, a feature
                                # specifies a "soft" dependency: it must be in the
                                # environment, but it is not _pulled_ in. The SAT
                                # logic doesn't do a perfect job of capturing this
                                # behavior, but keeping these packags out of the
                                # reduced index helps. Of course, if _another_
                                # package pulls it in by dependency, that's fine.
                                if "track_features" not in new_ms and not self._broader(
                                    new_ms,
                                    tuple(specs_by_name.get(new_ms.name, ())),
                                ):
                                    dep_specs.add(new_ms)
                                    # if new_ms not in dep_specs:
                                    #     specs_added.append(new_ms)
                                else:
                                    seen_specs.add(new_ms)
                    # debugging info - see what specs are bringing in the largest blobs
                    # if size_index != len(reduced_index2):
                    #     print("MS {} added {} pkgs to index".format(ms,
                    #           len(reduced_index2) - size_index))
                    # if specs_added:
                    #     print("MS {} added {} specs to further examination".format(ms,
                    #                                                                specs_added))

        reduced_index2 = frozendict(reduced_index2)
        self._reduced_index_cache[cache_key] = reduced_index2
        return reduced_index2

    def match_any(self, mss, prec):
        return any(ms.match(prec) for ms in mss)

    def find_matches(self, spec: MatchSpec) -> tuple[PackageRecord]:
        res = self._cached_find_matches.get(spec, None)
        if res is not None:
            return res

        spec_name = spec.get_exact_value("name")
        if spec_name:
            candidate_precs = self.groups.get(spec_name, ())
        elif spec.get_exact_value("track_features"):
            feature_names = spec.get_exact_value("track_features")
            candidate_precs = itertools.chain.from_iterable(
                self.trackers.get(feature_name, ()) for feature_name in feature_names
            )
        else:
            candidate_precs = self.index.values()

        res = tuple(p for p in candidate_precs if spec.match(p))
        self._cached_find_matches[spec] = res
        return res

    def ms_depends(self, prec: PackageRecord) -> list[MatchSpec]:
        deps = self.ms_depends_.get(prec)
        if deps is None:
            deps = [MatchSpec(d) for d in prec.combined_depends]
            deps.extend(MatchSpec(track_features=feat) for feat in prec.features)
            self.ms_depends_[prec] = deps
        return deps

    def version_key(self, prec, vtype=None):
        channel = prec.channel
        channel_priority = self._channel_priorities_map.get(
            channel.name, 1
        )  # TODO: ask @mcg1969 why the default value is 1 here  # NOQA
        valid = 1 if channel_priority < MAX_CHANNEL_PRIORITY else 0
        version_comparator = VersionOrder(prec.get("version", ""))
        build_number = prec.get("build_number", 0)
        build_string = prec.get("build")
        noarch = -int(prec.subdir == "noarch")
        if self._channel_priority != ChannelPriority.DISABLED:
            vkey = [valid, -channel_priority, version_comparator, build_number, noarch]
        else:
            vkey = [valid, version_comparator, -channel_priority, build_number, noarch]
        if self._solver_ignore_timestamps:
            vkey.append(build_string)
        else:
            vkey.extend((prec.get("timestamp", 0), build_string))
        return vkey

    @staticmethod
    def _make_channel_priorities(channels):
        priorities_map = {}
        for priority_counter, chn in enumerate(
            itertools.chain.from_iterable(
                (Channel(cc) for cc in c._channels)
                if isinstance(c, MultiChannel)
                else (c,)
                for c in (Channel(c) for c in channels)
            )
        ):
            channel_name = chn.name
            if channel_name in priorities_map:
                continue
            priorities_map[channel_name] = min(
                priority_counter, MAX_CHANNEL_PRIORITY - 1
            )
        return priorities_map

    def get_pkgs(self, ms, emptyok=False):  # pragma: no cover
        # legacy method for conda-build
        ms = MatchSpec(ms)
        precs = self.find_matches(ms)
        if not precs and not emptyok:
            raise ResolvePackageNotFound([(ms,)])
        return sorted(precs, key=self.version_key)

    @staticmethod
    def to_sat_name(val):
        # val can be a PackageRecord or MatchSpec
        if isinstance(val, PackageRecord):
            return val.dist_str()
        elif isinstance(val, MatchSpec):
            return "@s@" + str(val) + ("?" if val.optional else "")
        else:
            raise NotImplementedError()

    @staticmethod
    def to_feature_metric_id(prec_dist_str, feat):
        return f"@fm@{prec_dist_str}@{feat}"

    def push_MatchSpec(self, C, spec):
        spec = MatchSpec(spec)
        sat_name = self.to_sat_name(spec)
        m = C.from_name(sat_name)
        if m is not None:
            # the spec has already been pushed onto the clauses stack
            return sat_name

        simple = spec._is_single()
        nm = spec.get_exact_value("name")
        tf = frozenset(
            _tf
            for _tf in (f.strip() for f in spec.get_exact_value("track_features") or ())
            if _tf
        )

        if nm:
            tgroup = libs = self.groups.get(nm, [])
        elif tf:
            assert len(tf) == 1
            k = next(iter(tf))
            tgroup = libs = self.trackers.get(k, [])
        else:
            tgroup = libs = self.index.keys()
            simple = False
        if not simple:
            libs = [fkey for fkey in tgroup if spec.match(fkey)]
        if len(libs) == len(tgroup):
            if spec.optional:
                m = TRUE
            elif not simple:
                ms2 = MatchSpec(track_features=tf) if tf else MatchSpec(nm)
                m = C.from_name(self.push_MatchSpec(C, ms2))
        if m is None:
            sat_names = [self.to_sat_name(prec) for prec in libs]
            if spec.optional:
                ms2 = MatchSpec(track_features=tf) if tf else MatchSpec(nm)
                sat_names.append("!" + self.to_sat_name(ms2))
            m = C.Any(sat_names)
        C.name_var(m, sat_name)
        return sat_name

    @time_recorder(module_name=__name__)
    def gen_clauses(self):
        C = Clauses(sat_solver=_get_sat_solver_cls(context.sat_solver))
        for name, group in self.groups.items():
            group = [self.to_sat_name(prec) for prec in group]
            # Create one variable for each package
            for sat_name in group:
                C.new_var(sat_name)
            # Create one variable for the group
            m = C.new_var(self.to_sat_name(MatchSpec(name)))

            # Exactly one of the package variables, OR
            # the negation of the group variable, is true
            C.Require(C.ExactlyOne, group + [C.Not(m)])

        # If a package is installed, its dependencies must be as well
        for prec in self.index.values():
            nkey = C.Not(self.to_sat_name(prec))
            for ms in self.ms_depends(prec):
                # Virtual packages can't be installed, we ignore them
                if not ms.name.startswith("__"):
                    C.Require(C.Or, nkey, self.push_MatchSpec(C, ms))

        if log.isEnabledFor(DEBUG):
            log.debug(
                "gen_clauses returning with clause count: %d", C.get_clause_count()
            )
        return C

    def generate_spec_constraints(self, C, specs):
        result = [(self.push_MatchSpec(C, ms),) for ms in specs]
        if log.isEnabledFor(DEBUG):
            log.debug(
                "generate_spec_constraints returning with clause count: %d",
                C.get_clause_count(),
            )
        return result

    def generate_feature_count(self, C):
        result = {
            self.push_MatchSpec(C, MatchSpec(track_features=name)): 1
            for name in self.trackers.keys()
        }
        if log.isEnabledFor(DEBUG):
            log.debug(
                "generate_feature_count returning with clause count: %d",
                C.get_clause_count(),
            )
        return result

    def generate_update_count(self, C, specs):
        return {
            "!" + ms.target: 1 for ms in specs if ms.target and C.from_name(ms.target)
        }

    def generate_feature_metric(self, C):
        eq = {}  # a C.minimize() objective: dict[varname, coeff]
        # Given a pair (prec, feature), assign a "1" score IF:
        # - The prec is installed
        # - The prec does NOT require the feature
        # - At least one package in the group DOES require the feature
        # - A package that tracks the feature is installed
        for name, group in self.groups.items():
            prec_feats = {self.to_sat_name(prec): set(prec.features) for prec in group}
            active_feats = set.union(*prec_feats.values()).intersection(self.trackers)
            for feat in active_feats:
                clause_id_for_feature = self.push_MatchSpec(
                    C, MatchSpec(track_features=feat)
                )
                for prec_sat_name, features in prec_feats.items():
                    if feat not in features:
                        feature_metric_id = self.to_feature_metric_id(
                            prec_sat_name, feat
                        )
                        C.name_var(
                            C.And(prec_sat_name, clause_id_for_feature),
                            feature_metric_id,
                        )
                        eq[feature_metric_id] = 1
        return eq

    def generate_removal_count(self, C, specs):
        return {"!" + self.push_MatchSpec(C, ms.name): 1 for ms in specs}

    def generate_install_count(self, C, specs):
        return {self.push_MatchSpec(C, ms.name): 1 for ms in specs if ms.optional}

    def generate_package_count(self, C, missing):
        return {self.push_MatchSpec(C, nm): 1 for nm in missing}

    def generate_version_metrics(self, C, specs, include0=False):
        # each of these are weights saying how well packages match the specs
        #    format for each: a C.minimize() objective: dict[varname, coeff]
        eqc = {}  # channel
        eqv = {}  # version
        eqb = {}  # build number
        eqa = {}  # arch/noarch
        eqt = {}  # timestamp

        sdict = {}  # dict[package_name, PackageRecord]

        for s in specs:
            s = MatchSpec(s)  # needed for testing
            sdict.setdefault(s.name, [])
            # # TODO: this block is important! can't leave it commented out
            # rec = sdict.setdefault(s.name, [])
            # if s.target:
            #     dist = Dist(s.target)
            #     if dist in self.index:
            #         if self.index[dist].get('priority', 0) < MAX_CHANNEL_PRIORITY:
            #             rec.append(dist)

        for name, targets in sdict.items():
            pkgs = [(self.version_key(p), p) for p in self.groups.get(name, [])]
            pkey = None
            # keep in mind that pkgs is already sorted according to version_key (a tuple,
            #    so composite sort key).  Later entries in the list are, by definition,
            #    greater in some way, so simply comparing with != suffices.
            for version_key, prec in pkgs:
                if targets and any(prec == t for t in targets):
                    continue
                if pkey is None:
                    ic = iv = ib = it = ia = 0
                # valid package, channel priority
                elif pkey[0] != version_key[0] or pkey[1] != version_key[1]:
                    ic += 1
                    iv = ib = it = ia = 0
                # version
                elif pkey[2] != version_key[2]:
                    iv += 1
                    ib = it = ia = 0
                # build number
                elif pkey[3] != version_key[3]:
                    ib += 1
                    it = ia = 0
                # arch/noarch
                elif pkey[4] != version_key[4]:
                    ia += 1
                    it = 0
                elif not self._solver_ignore_timestamps and pkey[5] != version_key[5]:
                    it += 1

                prec_sat_name = self.to_sat_name(prec)
                if ic or include0:
                    eqc[prec_sat_name] = ic
                if iv or include0:
                    eqv[prec_sat_name] = iv
                if ib or include0:
                    eqb[prec_sat_name] = ib
                if ia or include0:
                    eqa[prec_sat_name] = ia
                if it or include0:
                    eqt[prec_sat_name] = it
                pkey = version_key

        return eqc, eqv, eqb, eqa, eqt

    def dependency_sort(
        self,
        must_have: dict[str, PackageRecord],
    ) -> list[PackageRecord]:
        assert isinstance(must_have, dict)

        digraph = {}  # dict[str, set[dependent_package_names]]
        for package_name, prec in must_have.items():
            if prec in self.index:
                digraph[package_name] = {ms.name for ms in self.ms_depends(prec)}

        # There are currently at least three special cases to be aware of.
        # 1. The `toposort()` function, called below, contains special case code to remove
        #    any circular dependency between python and pip.
        # 2. conda/plan.py has special case code for menuinst
        #       Always link/unlink menuinst first/last on windows in case a subsequent
        #       package tries to import it to create/remove a shortcut
        # 3. On windows, python noarch packages need an implicit dependency on conda added, if
        #    conda is in the list of packages for the environment.  Python noarch packages
        #    that have entry points use conda's own conda.exe python entry point binary. If conda
        #    is going to be updated during an operation, the unlink / link order matters.
        #    See issue #6057.

        if on_win and "conda" in digraph:
            for package_name, dist in must_have.items():
                record = self.index.get(prec)
                if hasattr(record, "noarch") and record.noarch == NoarchType.python:
                    digraph[package_name].add("conda")

        sorted_keys = toposort(digraph)
        must_have = must_have.copy()
        # Take all of the items in the sorted keys
        # Don't fail if the key does not exist
        result = [must_have.pop(key) for key in sorted_keys if key in must_have]
        # Take any key that were not sorted
        result.extend(must_have.values())
        return result

    def environment_is_consistent(self, installed):
        log.debug("Checking if the current environment is consistent")
        if not installed:
            return None, []
        sat_name_map = {}  # dict[sat_name, PackageRecord]
        specs = []
        for prec in installed:
            sat_name_map[self.to_sat_name(prec)] = prec
            specs.append(MatchSpec(f"{prec.name} {prec.version} {prec.build}"))
        r2 = Resolve({prec: prec for prec in installed}, True, channels=self.channels)
        C = r2.gen_clauses()
        constraints = r2.generate_spec_constraints(C, specs)
        solution = C.sat(constraints)
        return bool(solution)

    def get_conflicting_specs(self, specs, explicit_specs):
        if not specs:
            return ()

        all_specs = set(specs) | set(explicit_specs)
        reduced_index = self.get_reduced_index(all_specs)

        # Check if satisfiable
        def mysat(specs, add_if=False):
            constraints = r2.generate_spec_constraints(C, specs)
            return C.sat(constraints, add_if)

        if reduced_index:
            r2 = Resolve(reduced_index, True, channels=self.channels)
            C = r2.gen_clauses()
            solution = mysat(all_specs, True)
        else:
            solution = None

        if solution:
            final_unsat_specs = ()
        elif context.unsatisfiable_hints:
            r2 = Resolve(self.index, True, channels=self.channels)
            C = r2.gen_clauses()
            # This first result is just a single unsatisfiable core. There may be several.
            final_unsat_specs = tuple(
                minimal_unsatisfiable_subset(
                    specs, sat=mysat, explicit_specs=explicit_specs
                )
            )
        else:
            final_unsat_specs = None
        return final_unsat_specs

    def bad_installed(self, installed, new_specs):
        log.debug("Checking if the current environment is consistent")
        if not installed:
            return None, []
        sat_name_map = {}  # dict[sat_name, PackageRecord]
        specs = []
        for prec in installed:
            sat_name_map[self.to_sat_name(prec)] = prec
            specs.append(MatchSpec(f"{prec.name} {prec.version} {prec.build}"))
        new_index = {prec: prec for prec in sat_name_map.values()}
        name_map = {p.name: p for p in new_index}
        if "python" in name_map and "pip" not in name_map:
            python_prec = new_index[name_map["python"]]
            if "pip" in python_prec.depends:
                # strip pip dependency from python if not installed in environment
                new_deps = [d for d in python_prec.depends if d != "pip"]
                python_prec.depends = new_deps
        r2 = Resolve(new_index, True, channels=self.channels)
        C = r2.gen_clauses()
        constraints = r2.generate_spec_constraints(C, specs)
        solution = C.sat(constraints)
        limit = xtra = None
        if not solution or xtra:

            def get_(name, snames):
                if name not in snames:
                    snames.add(name)
                    for fn in self.groups.get(name, []):
                        for ms in self.ms_depends(fn):
                            get_(ms.name, snames)

            # New addition: find the largest set of installed packages that
            # are consistent with each other, and include those in the
            # list of packages to maintain consistency with
            snames = set()
            eq_optional_c = r2.generate_removal_count(C, specs)
            solution, _ = C.minimize(eq_optional_c, C.sat())
            snames.update(
                sat_name_map[sat_name]["name"]
                for sat_name in (C.from_index(s) for s in solution)
                if sat_name and sat_name[0] != "!" and "@" not in sat_name
            )
            # Existing behavior: keep all specs and their dependencies
            for spec in new_specs:
                get_(MatchSpec(spec).name, snames)
            if len(snames) < len(sat_name_map):
                limit = snames
                xtra = [
                    rec
                    for sat_name, rec in sat_name_map.items()
                    if rec["name"] not in snames
                ]
                log.debug(
                    "Limiting solver to the following packages: %s", ", ".join(limit)
                )
        if xtra:
            log.debug("Packages to be preserved: %s", xtra)
        return limit, xtra

    def restore_bad(self, pkgs, preserve):
        if preserve:
            sdict = {prec.name: prec for prec in pkgs}
            pkgs.extend(p for p in preserve if p.name not in sdict)

    def install_specs(self, specs, installed, update_deps=True):
        specs = list(map(MatchSpec, specs))
        snames = {s.name for s in specs}
        log.debug("Checking satisfiability of current install")
        limit, preserve = self.bad_installed(installed, specs)
        for prec in installed:
            if prec not in self.index:
                continue
            name, version, build = prec.name, prec.version, prec.build
            schannel = prec.channel.canonical_name
            if name in snames or limit is not None and name not in limit:
                continue
            # If update_deps=True, set the target package in MatchSpec so that
            # the solver can minimize the version change. If update_deps=False,
            # fix the version and build so that no change is possible.
            if update_deps:
                # TODO: fix target here
                spec = MatchSpec(name=name, target=prec.dist_str())
            else:
                spec = MatchSpec(
                    name=name, version=version, build=build, channel=schannel
                )
            specs.insert(0, spec)
        return tuple(specs), preserve

    def install(self, specs, installed=None, update_deps=True, returnall=False):
        specs, preserve = self.install_specs(specs, installed or [], update_deps)
        pkgs = []
        if specs:
            pkgs = self.solve(specs, returnall=returnall, _remove=False)
        self.restore_bad(pkgs, preserve)
        return pkgs

    def remove_specs(self, specs, installed):
        nspecs = []
        # There's an imperfect thing happening here. "specs" nominally contains
        # a list of package names or track_feature values to be removed. But
        # because of add_defaults_to_specs it may also contain version constraints
        # like "python 2.7*", which are *not* asking for python to be removed.
        # We need to separate these two kinds of specs here.
        for s in map(MatchSpec, specs):
            # Since '@' is an illegal version number, this ensures that all of
            # these matches will never match an actual package. Combined with
            # optional=True, this has the effect of forcing their removal.
            if s._is_single():
                nspecs.append(MatchSpec(s, version="@", optional=True))
            else:
                nspecs.append(MatchSpec(s, optional=True))
        snames = {s.name for s in nspecs if s.name}
        limit, _ = self.bad_installed(installed, nspecs)
        preserve = []
        for prec in installed:
            nm, ver = prec.name, prec.version
            if nm in snames:
                continue
            elif limit is not None:
                preserve.append(prec)
            else:
                # TODO: fix target here
                nspecs.append(
                    MatchSpec(
                        name=nm,
                        version=">=" + ver if ver else None,
                        optional=True,
                        target=prec.dist_str(),
                    )
                )
        return nspecs, preserve

    def remove(self, specs, installed):
        specs, preserve = self.remove_specs(specs, installed)
        pkgs = self.solve(specs, _remove=True)
        self.restore_bad(pkgs, preserve)
        return pkgs

    @time_recorder(module_name=__name__)
    def solve(
        self,
        specs: list,
        returnall: bool = False,
        _remove=False,
        specs_to_add=None,
        history_specs=None,
        should_retry_solve=False,
    ) -> list[PackageRecord]:
        if specs and not isinstance(specs[0], MatchSpec):
            specs = tuple(MatchSpec(_) for _ in specs)

        specs = set(specs)
        if log.isEnabledFor(DEBUG):
            dlist = dashlist(
                str("%i: %s target=%s optional=%s" % (i, s, s.target, s.optional))
                for i, s in enumerate(specs)
            )
            log.debug("Solving for: %s", dlist)

        if not specs:
            return ()

        # Find the compliant packages
        log.debug("Solve: Getting reduced index of compliant packages")
        len0 = len(specs)

        reduced_index = self.get_reduced_index(
            specs, exit_on_conflict=not context.unsatisfiable_hints
        )
        if not reduced_index:
            # something is intrinsically unsatisfiable - either not found or
            # not the right version
            not_found_packages = set()
            wrong_version_packages = set()
            for s in specs:
                if not self.find_matches(s):
                    if s.name in self.groups:
                        wrong_version_packages.add(s)
                    else:
                        not_found_packages.add(s)
            if not_found_packages:
                raise ResolvePackageNotFound(not_found_packages)
            elif wrong_version_packages:
                raise UnsatisfiableError(
                    [[d] for d in wrong_version_packages], chains=False
                )
            if should_retry_solve:
                # We don't want to call find_conflicts until our last try.
                # This jumps back out to conda/cli/install.py, where the
                # retries happen
                raise UnsatisfiableError({})
            else:
                self.find_conflicts(specs, specs_to_add, history_specs)

        # Check if satisfiable
        log.debug("Solve: determining satisfiability")

        def mysat(specs, add_if=False):
            constraints = r2.generate_spec_constraints(C, specs)
            return C.sat(constraints, add_if)

        # Return a solution of packages
        def clean(sol):
            return [
                q
                for q in (C.from_index(s) for s in sol)
                if q and q[0] != "!" and "@" not in q
            ]

        def is_converged(solution):
            """Determine if the SAT problem has converged to a single solution.

            This is determined by testing for a SAT solution with the current
            clause set and a clause in which at least one of the packages in
            the current solution is excluded. If a solution exists the problem
            has not converged as multiple solutions still exist.
            """
            psolution = clean(solution)
            nclause = tuple(C.Not(C.from_name(q)) for q in psolution)
            if C.sat((nclause,), includeIf=False) is None:
                return True
            return False

        r2 = Resolve(reduced_index, True, channels=self.channels)
        C = r2.gen_clauses()
        solution = mysat(specs, True)
        if not solution:
            if should_retry_solve:
                # we don't want to call find_conflicts until our last try
                raise UnsatisfiableError({})
            else:
                self.find_conflicts(specs, specs_to_add, history_specs)

        speco = []  # optional packages
        specr = []  # requested packages
        speca = []  # all other packages
        specm = set(r2.groups)  # missing from specs
        for k, s in enumerate(specs):
            if s.name in specm:
                specm.remove(s.name)
            if not s.optional:
                (speca if s.target or k >= len0 else specr).append(s)
            elif any(r2.find_matches(s)):
                s = MatchSpec(s.name, optional=True, target=s.target)
                speco.append(s)
                speca.append(s)
        speca.extend(MatchSpec(s) for s in specm)

        if log.isEnabledFor(DEBUG):
            log.debug("Requested specs: %s", dashlist(sorted(str(s) for s in specr)))
            log.debug("Optional specs: %s", dashlist(sorted(str(s) for s in speco)))
            log.debug("All other specs: %s", dashlist(sorted(str(s) for s in speca)))
            log.debug("missing specs: %s", dashlist(sorted(str(s) for s in specm)))

        # Removed packages: minimize count
        log.debug("Solve: minimize removed packages")
        if _remove:
            eq_optional_c = r2.generate_removal_count(C, speco)
            solution, obj7 = C.minimize(eq_optional_c, solution)
            log.debug("Package removal metric: %d", obj7)

        # Requested packages: maximize versions
        log.debug("Solve: maximize versions of requested packages")
        eq_req_c, eq_req_v, eq_req_b, eq_req_a, eq_req_t = r2.generate_version_metrics(
            C, specr
        )
        solution, obj3a = C.minimize(eq_req_c, solution)
        solution, obj3 = C.minimize(eq_req_v, solution)
        log.debug("Initial package channel/version metric: %d/%d", obj3a, obj3)

        # Track features: minimize feature count
        log.debug("Solve: minimize track_feature count")
        eq_feature_count = r2.generate_feature_count(C)
        solution, obj1 = C.minimize(eq_feature_count, solution)
        log.debug("Track feature count: %d", obj1)

        # Featured packages: minimize number of featureless packages
        # installed when a featured alternative is feasible.
        # For example, package name foo exists with two built packages. One with
        # 'track_features: 'feat1', and one with 'track_features': 'feat2'.
        # The previous "Track features" minimization pass has chosen 'feat1' for the
        # environment, but not 'feat2'. In this case, the 'feat2' version of foo is
        # considered "featureless."
        eq_feature_metric = r2.generate_feature_metric(C)
        solution, obj2 = C.minimize(eq_feature_metric, solution)
        log.debug("Package misfeature count: %d", obj2)

        # Requested packages: maximize builds
        log.debug("Solve: maximize build numbers of requested packages")
        solution, obj4 = C.minimize(eq_req_b, solution)
        log.debug("Initial package build metric: %d", obj4)

        # prefer arch packages where available for requested specs
        log.debug("Solve: prefer arch over noarch for requested packages")
        solution, noarch_obj = C.minimize(eq_req_a, solution)
        log.debug("Noarch metric: %d", noarch_obj)

        # Optional installations: minimize count
        if not _remove:
            log.debug("Solve: minimize number of optional installations")
            eq_optional_install = r2.generate_install_count(C, speco)
            solution, obj49 = C.minimize(eq_optional_install, solution)
            log.debug("Optional package install metric: %d", obj49)

        # Dependencies: minimize the number of packages that need upgrading
        log.debug("Solve: minimize number of necessary upgrades")
        eq_u = r2.generate_update_count(C, speca)
        solution, obj50 = C.minimize(eq_u, solution)
        log.debug("Dependency update count: %d", obj50)

        # Remaining packages: maximize versions, then builds
        log.debug(
            "Solve: maximize versions and builds of indirect dependencies.  "
            "Prefer arch over noarch where equivalent."
        )
        eq_c, eq_v, eq_b, eq_a, eq_t = r2.generate_version_metrics(C, speca)
        solution, obj5a = C.minimize(eq_c, solution)
        solution, obj5 = C.minimize(eq_v, solution)
        solution, obj6 = C.minimize(eq_b, solution)
        solution, obj6a = C.minimize(eq_a, solution)
        log.debug(
            "Additional package channel/version/build/noarch metrics: %d/%d/%d/%d",
            obj5a,
            obj5,
            obj6,
            obj6a,
        )

        # Prune unnecessary packages
        log.debug("Solve: prune unnecessary packages")
        eq_c = r2.generate_package_count(C, specm)
        solution, obj7 = C.minimize(eq_c, solution, trymax=True)
        log.debug("Weak dependency count: %d", obj7)

        if not is_converged(solution):
            # Maximize timestamps
            eq_t.update(eq_req_t)
            solution, obj6t = C.minimize(eq_t, solution)
            log.debug("Timestamp metric: %d", obj6t)

        log.debug("Looking for alternate solutions")
        nsol = 1
        psolutions = []
        psolution = clean(solution)
        psolutions.append(psolution)
        while True:
            nclause = tuple(C.Not(C.from_name(q)) for q in psolution)
            solution = C.sat((nclause,), True)
            if solution is None:
                break
            nsol += 1
            if nsol > 10:
                log.debug("Too many solutions; terminating")
                break
            psolution = clean(solution)
            psolutions.append(psolution)

        if nsol > 1:
            psols2 = list(map(set, psolutions))
            common = set.intersection(*psols2)
            diffs = [sorted(set(sol) - common) for sol in psols2]
            if not context.json:
                stdoutlog.info(
                    "\nWarning: {} possible package resolutions "
                    "(only showing differing packages):{}{}".format(
                        ">10" if nsol > 10 else nsol,
                        dashlist(", ".join(diff) for diff in diffs),
                        "\n  ... and others" if nsol > 10 else "",
                    )
                )

        # def stripfeat(sol):
        #     return sol.split('[')[0]

        new_index = {self.to_sat_name(prec): prec for prec in self.index.values()}

        if returnall:
            if len(psolutions) > 1:
                raise RuntimeError()
            # TODO: clean up this mess
            # return [sorted(Dist(stripfeat(dname)) for dname in psol) for psol in psolutions]
            # return [sorted((new_index[sat_name] for sat_name in psol), key=lambda x: x.name)
            #         for psol in psolutions]

            # return sorted(Dist(stripfeat(dname)) for dname in psolutions[0])
        return sorted(
            (new_index[sat_name] for sat_name in psolutions[0]), key=lambda x: x.name
        )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Error handling and error reporting."""

import os
import sys
from functools import lru_cache, partial
from logging import getLogger

from .common.compat import ensure_text_type, on_win

log = getLogger(__name__)


class ExceptionHandler:
    def __call__(self, func, *args, **kwargs):
        try:
            return func(*args, **kwargs)
        except:
            _, exc_val, exc_tb = sys.exc_info()
            return self.handle_exception(exc_val, exc_tb)

    def write_out(self, *content):
        from logging import getLogger

        from .cli.main import init_loggers

        init_loggers()
        getLogger("conda.stderr").info("\n".join(content))

    @property
    def http_timeout(self):
        from .base.context import context

        return context.remote_connect_timeout_secs, context.remote_read_timeout_secs

    @property
    def user_agent(self):
        from .base.context import context

        return context.user_agent

    @property
    def error_upload_url(self):
        from .base.context import context

        return context.error_upload_url

    def handle_exception(self, exc_val, exc_tb):
        from errno import ENOSPC

        from .exceptions import (
            CondaError,
            CondaMemoryError,
            NoSpaceLeftError,
        )

        if isinstance(exc_val, CondaError):
            if exc_val.reportable:
                return self.handle_reportable_application_exception(exc_val, exc_tb)
            else:
                return self.handle_application_exception(exc_val, exc_tb)
        if isinstance(exc_val, EnvironmentError):
            if getattr(exc_val, "errno", None) == ENOSPC:
                return self.handle_application_exception(
                    NoSpaceLeftError(exc_val), exc_tb
                )
        if isinstance(exc_val, MemoryError):
            return self.handle_application_exception(CondaMemoryError(exc_val), exc_tb)
        if isinstance(exc_val, KeyboardInterrupt):
            self._print_conda_exception(CondaError("KeyboardInterrupt"), exc_tb)
            return 1
        if isinstance(exc_val, SystemExit):
            return exc_val.code
        return self.handle_unexpected_exception(exc_val, exc_tb)

    def handle_application_exception(self, exc_val, exc_tb):
        self._print_conda_exception(exc_val, exc_tb)
        return exc_val.return_code

    def _print_conda_exception(self, exc_val, exc_tb):
        from .exceptions import print_conda_exception

        print_conda_exception(exc_val, exc_tb)

    def handle_unexpected_exception(self, exc_val, exc_tb):
        error_report = self.get_error_report(exc_val, exc_tb)
        self.print_unexpected_error_report(error_report)
        self._upload(error_report)
        rc = getattr(exc_val, "return_code", None)
        return rc if rc is not None else 1

    def handle_reportable_application_exception(self, exc_val, exc_tb):
        error_report = self.get_error_report(exc_val, exc_tb)
        from .base.context import context

        if context.json:
            error_report.update(exc_val.dump_map())
        self.print_expected_error_report(error_report)
        self._upload(error_report)
        return exc_val.return_code

    def get_error_report(self, exc_val, exc_tb):
        from .exceptions import CondaError, _format_exc

        command = " ".join(ensure_text_type(s) for s in sys.argv)
        info_dict = {}
        if " info" not in command:
            # get info_dict, but if we get an exception here too, record it without trampling
            # the original exception
            try:
                from .cli.main_info import get_info_dict

                info_dict = get_info_dict()
            except Exception as info_e:
                info_traceback = _format_exc()
                info_dict = {
                    "error": repr(info_e),
                    "exception_name": info_e.__class__.__name__,
                    "exception_type": str(exc_val.__class__),
                    "traceback": info_traceback,
                }

        error_report = {
            "error": repr(exc_val),
            "exception_name": exc_val.__class__.__name__,
            "exception_type": str(exc_val.__class__),
            "command": command,
            "traceback": _format_exc(exc_val, exc_tb),
            "conda_info": info_dict,
        }

        if isinstance(exc_val, CondaError):
            error_report["conda_error_components"] = exc_val.dump_map()

        return error_report

    def print_unexpected_error_report(self, error_report):
        from .base.context import context

        if context.json:
            from .cli.common import stdout_json

            stdout_json(error_report)
        else:
            message_builder = []
            message_builder.append("")
            message_builder.append(
                "# >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<"
            )
            message_builder.append("")
            message_builder.extend(
                "    " + line for line in error_report["traceback"].splitlines()
            )
            message_builder.append("")
            message_builder.append("`$ {}`".format(error_report["command"]))
            message_builder.append("")
            if error_report["conda_info"]:
                from .cli.main_info import get_env_vars_str, get_main_info_str

                try:
                    # TODO: Sanitize env vars to remove secrets (e.g credentials for PROXY)
                    message_builder.append(get_env_vars_str(error_report["conda_info"]))
                    message_builder.append(
                        get_main_info_str(error_report["conda_info"])
                    )
                except Exception as e:
                    log.warning("%r", e, exc_info=True)
                    message_builder.append("conda info could not be constructed.")
                    message_builder.append(f"{e!r}")
            message_builder.extend(
                [
                    "",
                    "An unexpected error has occurred. Conda has prepared the above report."
                    "",
                    "If you suspect this error is being caused by a malfunctioning plugin,",
                    "consider using the --no-plugins option to turn off plugins.",
                    "",
                    "Example: conda --no-plugins install <package>",
                    "",
                    "Alternatively, you can set the CONDA_NO_PLUGINS environment variable on",
                    "the command line to run the command without plugins enabled.",
                    "",
                    "Example: CONDA_NO_PLUGINS=true conda install <package>",
                    "",
                ]
            )
            self.write_out(*message_builder)

    def print_expected_error_report(self, error_report):
        from .base.context import context

        if context.json:
            from .cli.common import stdout_json

            stdout_json(error_report)
        else:
            message_builder = []
            message_builder.append("")
            message_builder.append(
                "# >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<"
            )
            message_builder.append("")
            message_builder.append("`$ {}`".format(error_report["command"]))
            message_builder.append("")
            if error_report["conda_info"]:
                from .cli.main_info import get_env_vars_str, get_main_info_str

                try:
                    # TODO: Sanitize env vars to remove secrets (e.g credentials for PROXY)
                    message_builder.append(get_env_vars_str(error_report["conda_info"]))
                    message_builder.append(
                        get_main_info_str(error_report["conda_info"])
                    )
                except Exception as e:
                    log.warning("%r", e, exc_info=True)
                    message_builder.append("conda info could not be constructed.")
                    message_builder.append(f"{e!r}")
            message_builder.append("")
            message_builder.append(
                "V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V"
            )
            message_builder.append("")

            message_builder.extend(error_report["error"].splitlines())
            message_builder.append("")

            message_builder.append(
                "A reportable application error has occurred. Conda has prepared the above report."
            )
            message_builder.append("")
            self.write_out(*message_builder)

    # FUTURE: Python 3.8+, replace with functools.cached_property
    @property
    @lru_cache(maxsize=None)
    def _isatty(self):
        try:
            return os.isatty(0) or on_win
        except Exception as e:
            log.debug("%r", e)
            return True

    def _upload(self, error_report) -> None:
        """Determine whether or not to upload the error report."""
        from .base.context import context

        post_upload = False
        if context.report_errors is False:
            # no prompt and no submission
            do_upload = False
        elif context.report_errors is True or context.always_yes:
            # no prompt and submit
            do_upload = True
        elif context.json or context.quiet or not self._isatty:
            # never prompt under these conditions, submit iff always_yes
            do_upload = bool(not context.offline and context.always_yes)
        else:
            # prompt whether to submit
            do_upload = self._ask_upload()
            post_upload = True

        # the upload state is one of the following:
        #   - True: upload error report
        #   - False: do not upload error report
        #   - None: while prompting a timeout occurred

        if do_upload:
            # user wants report to be submitted
            self._execute_upload(error_report)

        if post_upload:
            # post submission text
            self._post_upload(do_upload)

    def _ask_upload(self):
        from .auxlib.type_coercion import boolify
        from .common.io import timeout

        try:
            do_upload = timeout(
                40,
                partial(
                    input,
                    "If submitted, this report will be used by core maintainers to improve\n"
                    "future releases of conda.\n"
                    "Would you like conda to send this report to the core maintainers? "
                    "[y/N]: ",
                ),
            )
            return do_upload and boolify(do_upload)
        except Exception as e:
            log.debug("%r", e)
            return False

    def _execute_upload(self, error_report):
        import getpass
        import json

        from .auxlib.entity import EntityEncoder

        headers = {
            "User-Agent": self.user_agent,
        }
        _timeout = self.http_timeout
        username = getpass.getuser()
        error_report["is_ascii"] = (
            True if all(ord(c) < 128 for c in username) else False
        )
        error_report["has_spaces"] = True if " " in str(username) else False
        data = json.dumps(error_report, sort_keys=True, cls=EntityEncoder) + "\n"
        data = data.replace(str(username), "USERNAME_REMOVED")
        response = None
        try:
            # requests does not follow HTTP standards for redirects of non-GET methods
            # That is, when following a 301 or 302, it turns a POST into a GET.
            # And no way to disable.  WTF
            import requests

            redirect_counter = 0
            url = self.error_upload_url
            response = requests.post(
                url, headers=headers, timeout=_timeout, data=data, allow_redirects=False
            )
            response.raise_for_status()
            while response.status_code in (301, 302) and response.headers.get(
                "Location"
            ):
                url = response.headers["Location"]
                response = requests.post(
                    url,
                    headers=headers,
                    timeout=_timeout,
                    data=data,
                    allow_redirects=False,
                )
                response.raise_for_status()
                redirect_counter += 1
                if redirect_counter > 15:
                    from . import CondaError

                    raise CondaError("Redirect limit exceeded")
            log.debug("upload response status: %s", response and response.status_code)
        except Exception as e:  # pragma: no cover
            log.info("%r", e)
        try:
            if response and response.ok:
                self.write_out("Upload successful.")
            else:
                self.write_out("Upload did not complete.")
                if response and response.status_code:
                    self.write_out(f" HTTP {response.status_code}")
        except Exception as e:
            log.debug(f"{e!r}")

    def _post_upload(self, do_upload):
        if do_upload is True:
            # report was submitted
            self.write_out(
                "",
                "Thank you for helping to improve conda.",
                "Opt-in to always sending reports (and not see this message again)",
                "by running",
                "",
                "    $ conda config --set report_errors true",
                "",
            )
        elif do_upload is None:
            # timeout was reached while prompting user
            self.write_out(
                "",
                "Timeout reached. No report sent.",
                "",
            )
        else:
            # no report submitted
            self.write_out(
                "",
                "No report sent. To permanently opt-out, use",
                "",
                "    $ conda config --set report_errors false",
                "",
            )


def conda_exception_handler(func, *args, **kwargs):
    exception_handler = ExceptionHandler()
    return_value = exception_handler(func, *args, **kwargs)
    return return_value


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Backported exports for conda-build."""

import errno
import functools
import os
from builtins import input  # noqa: F401, UP029
from io import StringIO  # noqa: F401, for conda-build

from . import CondaError, plan  # noqa: F401
from .auxlib.entity import EntityEncoder  # noqa: F401
from .base.constants import (  # noqa: F401
    DEFAULT_CHANNELS,
    DEFAULT_CHANNELS_UNIX,
    DEFAULT_CHANNELS_WIN,
    PREFIX_PLACEHOLDER,
)
from .base.context import (  # noqa: F401
    context,
    non_x86_machines,
    reset_context,
    sys_rc_path,
)
from .cli.common import spec_from_line, specs_from_args, specs_from_url  # noqa: F401
from .cli.conda_argparse import ArgumentParser  # noqa: F401
from .cli.helpers import (  # noqa: F401
    add_parser_channels,
    add_parser_prefix,
)
from .common import compat  # noqa: F401
from .common.compat import on_win  # noqa: F401
from .common.path import win_path_to_unix  # noqa: F401
from .common.toposort import _toposort  # noqa: F401
from .core.index import dist_str_in_index  # noqa: F401
from .core.index import fetch_index as _fetch_index  # noqa: F401
from .core.index import get_index as _get_index
from .core.package_cache_data import ProgressiveFetchExtract, rm_fetched  # noqa: F401
from .core.prefix_data import delete_prefix_from_linked_data
from .core.solve import Solver  # noqa: F401
from .core.subdir_data import cache_fn_url  # noqa: F401
from .deprecations import deprecated
from .exceptions import (  # noqa: F401
    CondaHTTPError,
    CondaOSError,
    LinkError,
    LockError,
    PaddingError,
    PathNotFoundError,
    UnsatisfiableError,
)
from .gateways.connection.download import TmpDownload  # noqa: F401
from .gateways.connection.download import download as _download  # noqa: F401
from .gateways.connection.session import CondaSession  # noqa: F401
from .gateways.disk.create import TemporaryDirectory  # noqa: F401
from .gateways.disk.delete import delete_trash, move_to_trash  # noqa: F401
from .gateways.disk.delete import rm_rf as _rm_rf
from .gateways.disk.link import lchmod  # noqa: F401
from .gateways.subprocess import ACTIVE_SUBPROCESSES, subprocess_call  # noqa: F401
from .misc import untracked, walk_prefix  # noqa: F401
from .models.channel import Channel, get_conda_build_local_url  # noqa: F401
from .models.dist import Dist
from .models.enums import FileMode, PathType  # noqa: F401
from .models.records import PackageRecord
from .models.version import VersionOrder, normalized_version  # noqa: F401
from .plan import display_actions as _display_actions
from .plan import (  # noqa: F401
    execute_actions,
    execute_instructions,
    execute_plan,
    install_actions,
)
from .resolve import (  # noqa: F401
    MatchSpec,
    Resolve,
    ResolvePackageNotFound,
    Unsatisfiable,
)
from .utils import human_bytes, unix_path_to_win, url_path  # noqa: F401

reset_context()  # initialize context when conda.exports is imported


NoPackagesFound = NoPackagesFoundError = ResolvePackageNotFound
non_x86_linux_machines = non_x86_machines
get_default_urls = lambda: DEFAULT_CHANNELS  # noqa: E731
_PREFIX_PLACEHOLDER = prefix_placeholder = PREFIX_PLACEHOLDER
arch_name = context.arch_name
binstar_upload = context.anaconda_upload
bits = context.bits
default_prefix = context.default_prefix
default_python = context.default_python
envs_dirs = context.envs_dirs
pkgs_dirs = context.pkgs_dirs
platform = context.platform
root_dir = context.root_prefix
root_writable = context.root_writable
subdir = context.subdir
conda_build = context.conda_build
get_rc_urls = lambda: list(context.channels)  # noqa: E731
get_local_urls = lambda: list(get_conda_build_local_url()) or []  # noqa: E731
load_condarc = lambda fn: reset_context([fn])  # noqa: E731
PaddingError = PaddingError
LinkError = LinkError
CondaOSError = CondaOSError
# PathNotFoundError is the conda 4.4.x name for it - let's plan ahead.
CondaFileNotFoundError = PathNotFoundError
deprecated.constant(
    "24.3",
    "24.9",
    "IndexRecord",
    PackageRecord,
    addendum="Use `conda.models.records.PackageRecord` instead.",
)
# Replacements for six exports for compatibility
PY3 = True  # noqa: F401
string_types = str  # noqa: F401
text_type = str  # noqa: F401


@deprecated(
    "25.3",
    "25.9",
    addendum="Use builtin `dict.items()` instead.",
)
def iteritems(d, **kw):
    return iter(d.items(**kw))


@deprecated("25.3", "25.9", addendum="Unused.")
class Completer:  # pragma: no cover
    def get_items(self):
        return self._get_items()

    def __contains__(self, item):
        return True

    def __iter__(self):
        return iter(self.get_items())


@deprecated("25.3", "25.9", addendum="Unused.")
class InstalledPackages:
    pass


def rm_rf(path, max_retries=5, trash=True):
    _rm_rf(path, max_retries, trash)
    delete_prefix_from_linked_data(path)


deprecated.constant("25.3", "25.9", "KEYS", None, addendum="Unused.")
deprecated.constant("25.3", "25.9", "KEYS_DIR", None, addendum="Unused.")


@deprecated("25.3", "25.9", addendum="Unused.")
def hash_file(_):
    return None  # pragma: no cover


@deprecated("25.3", "25.9", addendum="Unused.")
def verify(_):
    return False  # pragma: no cover


def display_actions(
    actions, index, show_channel_urls=None, specs_to_remove=(), specs_to_add=()
):
    if "FETCH" in actions:
        actions["FETCH"] = [index[d] for d in actions["FETCH"]]
    if "LINK" in actions:
        actions["LINK"] = [index[d] for d in actions["LINK"]]
    if "UNLINK" in actions:
        actions["UNLINK"] = [index[d] for d in actions["UNLINK"]]
    index = {prec: prec for prec in index.values()}
    return _display_actions(
        actions, index, show_channel_urls, specs_to_remove, specs_to_add
    )


def get_index(
    channel_urls=(),
    prepend=True,
    platform=None,
    use_local=False,
    use_cache=False,
    unknown=None,
    prefix=None,
):
    index = _get_index(
        channel_urls, prepend, platform, use_local, use_cache, unknown, prefix
    )
    return {Dist(prec): prec for prec in index.values()}


@deprecated("24.3", "24.9", addendum="Use `conda.core.index.fetch_index` instead.")
def fetch_index(channel_urls, use_cache=False, index=None):
    index = _fetch_index(channel_urls, use_cache, index)
    return {Dist(prec): prec for prec in index.values()}


def package_cache():
    from .core.package_cache_data import PackageCacheData

    class package_cache:
        def __contains__(self, dist):
            return bool(
                PackageCacheData.first_writable().get(Dist(dist).to_package_ref(), None)
            )

        def keys(self):
            return (Dist(v) for v in PackageCacheData.first_writable().values())

        def __delitem__(self, dist):
            PackageCacheData.first_writable().remove(Dist(dist).to_package_ref())

    return package_cache()


@deprecated("25.3", "25.9", addendum="Use `conda.activate` instead.")
def symlink_conda(prefix, root_dir, shell=None):  # pragma: no cover
    # do not symlink root env - this clobbers activate incorrectly.
    # prefix should always be longer than, or outside the root dir.
    if os.path.normcase(os.path.normpath(prefix)) in os.path.normcase(
        os.path.normpath(root_dir)
    ):
        return
    if on_win:
        where = "condabin"
        symlink_fn = functools.partial(win_conda_bat_redirect, shell=shell)
    else:
        where = "bin"
        symlink_fn = os.symlink
    if not os.path.isdir(os.path.join(prefix, where)):
        os.makedirs(os.path.join(prefix, where))
    _symlink_conda_hlp(prefix, root_dir, where, symlink_fn)


@deprecated("25.3", "25.9", addendum="Use `conda.activate` instead.")
def _symlink_conda_hlp(prefix, root_dir, where, symlink_fn):  # pragma: no cover
    scripts = ["conda", "activate", "deactivate"]
    prefix_where = os.path.join(prefix, where)
    if not os.path.isdir(prefix_where):
        os.makedirs(prefix_where)
    for f in scripts:
        root_file = os.path.join(root_dir, where, f)
        prefix_file = os.path.join(prefix_where, f)
        try:
            # try to kill stale links if they exist
            if os.path.lexists(prefix_file):
                rm_rf(prefix_file)
            # if they're in use, they won't be killed.  Skip making new symlink.
            if not os.path.lexists(prefix_file):
                symlink_fn(root_file, prefix_file)
        except OSError as e:
            if os.path.lexists(prefix_file) and (
                e.errno in (errno.EPERM, errno.EACCES, errno.EROFS, errno.EEXIST)
            ):
                # Cannot symlink root_file to prefix_file. Ignoring since link already exists
                pass
            else:
                raise


if on_win:  # pragma: no cover

    @deprecated("25.3", "25.9", addendum="Use `conda.activate` instead.")
    def win_conda_bat_redirect(src, dst, shell):
        """Special function for Windows XP where the `CreateSymbolicLink`
        function is not available.

        Simply creates a `.bat` file at `dst` which calls `src` together with
        all command line arguments.

        Works of course only with callable files, e.g. `.bat` or `.exe` files.
        """
        from .utils import _SHELLS

        try:
            os.makedirs(os.path.dirname(dst))
        except OSError as exc:  # Python >2.5
            if exc.errno == errno.EEXIST and os.path.isdir(os.path.dirname(dst)):
                pass
            else:
                raise

        # bat file redirect
        if not os.path.isfile(dst + ".bat"):
            with open(dst + ".bat", "w") as f:
                f.write(f'@echo off\ncall "{src}" %*\n')

        # TODO: probably need one here for powershell at some point

        # This one is for bash/cygwin/msys
        # set default shell to bash.exe when not provided, as that's most common
        if not shell:
            shell = "bash.exe"

        # technically these are "links" - but islink doesn't work on win
        if not os.path.isfile(dst):
            with open(dst, "w") as f:
                f.write("#!/usr/bin/env bash \n")
                if src.endswith("conda"):
                    f.write('{} "$@"'.format(_SHELLS[shell]["path_to"](src + ".exe")))
                else:
                    f.write('source {} "$@"'.format(_SHELLS[shell]["path_to"](src)))
            # Make the new file executable
            # http://stackoverflow.com/a/30463972/1170370
            mode = os.stat(dst).st_mode
            mode |= (mode & 292) >> 2  # copy R bits to X
            os.chmod(dst, mode)


def linked_data(prefix, ignore_channels=False):
    """Return a dictionary of the linked packages in prefix."""
    from .core.prefix_data import PrefixData
    from .models.dist import Dist

    pd = PrefixData(prefix)
    return {
        Dist(prefix_record): prefix_record
        for prefix_record in pd._prefix_records.values()
    }


def linked(prefix, ignore_channels=False):
    """Return the Dists of linked packages in prefix."""
    from .models.enums import PackageType

    conda_package_types = PackageType.conda_package_types()
    ld = linked_data(prefix, ignore_channels=ignore_channels).items()
    return {
        dist
        for dist, prefix_rec in ld
        if prefix_rec.package_type in conda_package_types
    }


# exports
def is_linked(prefix, dist):
    """
    Return the install metadata for a linked package in a prefix, or None
    if the package is not linked in the prefix.
    """
    # FIXME Functions that begin with `is_` should return True/False
    from .core.prefix_data import PrefixData

    pd = PrefixData(prefix)
    prefix_record = pd.get(dist.name, None)
    if prefix_record is None:
        return None
    elif MatchSpec(dist).match(prefix_record):
        return prefix_record
    else:
        return None


def download(
    url,
    dst_path,
    session=None,
    md5sum=None,
    urlstxt=False,
    retries=3,
    sha256=None,
    size=None,
):
    return _download(url, dst_path, md5=md5sum, sha256=sha256, size=size)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Miscellaneous utility functions."""

import os
import re
import shutil
import sys
from collections import defaultdict
from logging import getLogger
from os.path import abspath, dirname, exists, isdir, isfile, join, relpath

from .base.context import context
from .common.compat import on_mac, on_win, open
from .common.io import dashlist
from .common.path import expand
from .common.url import is_url, join_url, path_to_url
from .core.index import get_index
from .core.link import PrefixSetup, UnlinkLinkTransaction
from .core.package_cache_data import PackageCacheData, ProgressiveFetchExtract
from .core.prefix_data import PrefixData
from .exceptions import (
    CondaExitZero,
    DisallowedPackageError,
    DryRunExit,
    PackagesNotFoundError,
    ParseError,
)
from .gateways.disk.delete import rm_rf
from .gateways.disk.link import islink, readlink, symlink
from .models.match_spec import ChannelMatch, MatchSpec
from .models.prefix_graph import PrefixGraph

log = getLogger(__name__)


def conda_installed_files(prefix, exclude_self_build=False):
    """
    Return the set of files which have been installed (using conda) into
    a given prefix.
    """
    res = set()
    for meta in PrefixData(prefix).iter_records():
        if exclude_self_build and "file_hash" in meta:
            continue
        res.update(set(meta.get("files", ())))
    return res


url_pat = re.compile(
    r"(?:(?P<url_p>.+)(?:[/\\]))?"
    r"(?P<fn>[^/\\#]+(?:\.tar\.bz2|\.conda))"
    r"(:?#(?P<md5>[0-9a-f]{32}))?$"
)


def explicit(
    specs, prefix, verbose=False, force_extract=True, index_args=None, index=None
):
    actions = defaultdict(list)
    actions["PREFIX"] = prefix

    fetch_specs = []
    for spec in specs:
        if spec == "@EXPLICIT":
            continue

        if not is_url(spec):
            """
            # This does not work because url_to_path does not enforce Windows
            # backslashes. Should it? Seems like a dangerous change to make but
            # it would be cleaner.
            expanded = expand(spec)
            urled = path_to_url(expanded)
            pathed = url_to_path(urled)
            assert pathed == expanded
            """
            spec = path_to_url(expand(spec))

        # parse URL
        m = url_pat.match(spec)
        if m is None:
            raise ParseError(f"Could not parse explicit URL: {spec}")
        url_p, fn, md5sum = m.group("url_p"), m.group("fn"), m.group("md5")
        url = join_url(url_p, fn)
        # url_p is everything but the tarball_basename and the md5sum

        fetch_specs.append(MatchSpec(url, md5=md5sum) if md5sum else MatchSpec(url))

    if context.dry_run:
        raise DryRunExit()

    pfe = ProgressiveFetchExtract(fetch_specs)
    pfe.execute()

    if context.download_only:
        raise CondaExitZero(
            "Package caches prepared. "
            "UnlinkLinkTransaction cancelled with --download-only option."
        )

    # now make an UnlinkLinkTransaction with the PackageCacheRecords as inputs
    # need to add package name to fetch_specs so that history parsing keeps track of them correctly
    specs_pcrecs = tuple(
        [spec, next(PackageCacheData.query_all(spec), None)] for spec in fetch_specs
    )

    # Assert that every spec has a PackageCacheRecord
    specs_with_missing_pcrecs = [
        str(spec) for spec, pcrec in specs_pcrecs if pcrec is None
    ]
    if specs_with_missing_pcrecs:
        if len(specs_with_missing_pcrecs) == len(specs_pcrecs):
            raise AssertionError("No package cache records found")
        else:
            missing_precs_list = ", ".join(specs_with_missing_pcrecs)
            raise AssertionError(
                f"Missing package cache records for: {missing_precs_list}"
            )

    precs_to_remove = []
    prefix_data = PrefixData(prefix)
    for q, (spec, pcrec) in enumerate(specs_pcrecs):
        new_spec = MatchSpec(spec, name=pcrec.name)
        specs_pcrecs[q][0] = new_spec

        prec = prefix_data.get(pcrec.name, None)
        if prec:
            # If we've already got matching specifications, then don't bother re-linking it
            if next(prefix_data.query(new_spec), None):
                specs_pcrecs[q][0] = None
            else:
                precs_to_remove.append(prec)

    stp = PrefixSetup(
        prefix,
        precs_to_remove,
        tuple(sp[1] for sp in specs_pcrecs if sp[0]),
        (),
        tuple(sp[0] for sp in specs_pcrecs if sp[0]),
        (),
    )

    txn = UnlinkLinkTransaction(stp)
    if not context.json and not context.quiet:
        txn.print_transaction_summary()
    txn.execute()


def rel_path(prefix, path, windows_forward_slashes=True):
    res = path[len(prefix) + 1 :]
    if on_win and windows_forward_slashes:
        res = res.replace("\\", "/")
    return res


def walk_prefix(prefix, ignore_predefined_files=True, windows_forward_slashes=True):
    """Return the set of all files in a given prefix directory."""
    res = set()
    prefix = abspath(prefix)
    ignore = {
        "pkgs",
        "envs",
        "conda-bld",
        "conda-meta",
        ".conda_lock",
        "users",
        "LICENSE.txt",
        "info",
        "conda-recipes",
        ".index",
        ".unionfs",
        ".nonadmin",
    }
    binignore = {"conda", "activate", "deactivate"}
    if on_mac:
        ignore.update({"python.app", "Launcher.app"})
    for fn in (entry.name for entry in os.scandir(prefix)):
        if ignore_predefined_files and fn in ignore:
            continue
        if isfile(join(prefix, fn)):
            res.add(fn)
            continue
        for root, dirs, files in os.walk(join(prefix, fn)):
            should_ignore = ignore_predefined_files and root == join(prefix, "bin")
            for fn2 in files:
                if should_ignore and fn2 in binignore:
                    continue
                res.add(relpath(join(root, fn2), prefix))
            for dn in dirs:
                path = join(root, dn)
                if islink(path):
                    res.add(relpath(path, prefix))

    if on_win and windows_forward_slashes:
        return {path.replace("\\", "/") for path in res}
    else:
        return res


def untracked(prefix, exclude_self_build=False):
    """Return (the set) of all untracked files for a given prefix."""
    conda_files = conda_installed_files(prefix, exclude_self_build)
    return {
        path
        for path in walk_prefix(prefix) - conda_files
        if not (
            path.endswith("~")
            or on_mac
            and path.endswith(".DS_Store")
            or path.endswith(".pyc")
            and path[:-1] in conda_files
        )
    }


def touch_nonadmin(prefix):
    """Creates $PREFIX/.nonadmin if sys.prefix/.nonadmin exists (on Windows)."""
    if on_win and exists(join(context.root_prefix, ".nonadmin")):
        if not isdir(prefix):
            os.makedirs(prefix)
        with open(join(prefix, ".nonadmin"), "w") as fo:
            fo.write("")


def clone_env(prefix1, prefix2, verbose=True, quiet=False, index_args=None):
    """Clone existing prefix1 into new prefix2."""
    untracked_files = untracked(prefix1)

    # Discard conda, conda-env and any package that depends on them
    filter = {}
    found = True
    while found:
        found = False
        for prec in PrefixData(prefix1).iter_records():
            name = prec["name"]
            if name in filter:
                continue
            if name == "conda":
                filter["conda"] = prec
                found = True
                break
            if name == "conda-env":
                filter["conda-env"] = prec
                found = True
                break
            for dep in prec.combined_depends:
                if MatchSpec(dep).name in filter:
                    filter[name] = prec
                    found = True

    if filter:
        if not quiet:
            fh = sys.stderr if context.json else sys.stdout
            print(
                "The following packages cannot be cloned out of the root environment:",
                file=fh,
            )
            for prec in filter.values():
                print(" - " + prec.dist_str(), file=fh)
        drecs = {
            prec
            for prec in PrefixData(prefix1).iter_records()
            if prec["name"] not in filter
        }
    else:
        drecs = {prec for prec in PrefixData(prefix1).iter_records()}

    # Resolve URLs for packages that do not have URLs
    index = {}
    unknowns = [prec for prec in drecs if not prec.get("url")]
    notfound = []
    if unknowns:
        index_args = index_args or {}
        index = get_index(**index_args)

        for prec in unknowns:
            spec = MatchSpec(name=prec.name, version=prec.version, build=prec.build)
            precs = tuple(prec for prec in index.values() if spec.match(prec))
            if not precs:
                notfound.append(spec)
            elif len(precs) > 1:
                drecs.remove(prec)
                drecs.add(_get_best_prec_match(precs))
            else:
                drecs.remove(prec)
                drecs.add(precs[0])
    if notfound:
        raise PackagesNotFoundError(notfound)

    # Assemble the URL and channel list
    urls = {}
    for prec in drecs:
        urls[prec] = prec["url"]

    precs = tuple(PrefixGraph(urls).graph)
    urls = [urls[prec] for prec in precs]

    disallowed = tuple(MatchSpec(s) for s in context.disallowed_packages)
    for prec in precs:
        if any(d.match(prec) for d in disallowed):
            raise DisallowedPackageError(prec)

    if verbose:
        print("Packages: %d" % len(precs))
        print("Files: %d" % len(untracked_files))

    if context.dry_run:
        raise DryRunExit()

    for f in untracked_files:
        src = join(prefix1, f)
        dst = join(prefix2, f)
        dst_dir = dirname(dst)
        if islink(dst_dir) or isfile(dst_dir):
            rm_rf(dst_dir)
        if not isdir(dst_dir):
            os.makedirs(dst_dir)
        if islink(src):
            symlink(readlink(src), dst)
            continue

        try:
            with open(src, "rb") as fi:
                data = fi.read()
        except OSError:
            continue

        try:
            s = data.decode("utf-8")
            s = s.replace(prefix1, prefix2)
            data = s.encode("utf-8")
        except UnicodeDecodeError:  # data is binary
            pass

        with open(dst, "wb") as fo:
            fo.write(data)
        shutil.copystat(src, dst)

    actions = explicit(
        urls,
        prefix2,
        verbose=not quiet,
        index=index,
        force_extract=False,
        index_args=index_args,
    )
    return actions, untracked_files


def _get_best_prec_match(precs):
    assert precs
    for channel in context.channels:
        channel_matcher = ChannelMatch(channel)
        prec_matches = tuple(
            prec for prec in precs if channel_matcher.match(prec.channel.name)
        )
        if prec_matches:
            break
    else:
        prec_matches = precs
    log.warning("Multiple packages found: %s", dashlist(prec_matches))
    return prec_matches[0]


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Tools to aid in deprecating code."""

from __future__ import annotations

import sys
import warnings
from argparse import Action
from functools import wraps
from types import ModuleType
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace
    from typing import Any, Callable, ParamSpec, Self, TypeVar

    from packaging.version import Version

    T = TypeVar("T")
    P = ParamSpec("P")

    ActionType = TypeVar("ActionType", bound=type[Action])

from . import __version__


class DeprecatedError(RuntimeError):
    pass


# inspired by deprecation (https://deprecation.readthedocs.io/en/latest/) and
# CPython's warnings._deprecated
class DeprecationHandler:
    _version: str | None
    _version_tuple: tuple[int, ...] | None
    _version_object: Version | None

    def __init__(self: Self, version: str) -> None:
        """Factory to create a deprecation handle for the specified version.

        :param version: The version to compare against when checking deprecation statuses.
        """
        self._version = version
        # Try to parse the version string as a simple tuple[int, ...] to avoid
        # packaging.version import and costlier version comparisons.
        self._version_tuple = self._get_version_tuple(version)
        self._version_object = None

    @staticmethod
    def _get_version_tuple(version: str) -> tuple[int, ...] | None:
        """Return version as non-empty tuple of ints if possible, else None.

        :param version: Version string to parse.
        """
        try:
            return tuple(int(part) for part in version.strip().split(".")) or None
        except (AttributeError, ValueError):
            return None

    def _version_less_than(self: Self, version: str) -> bool:
        """Test whether own version is less than the given version.

        :param version: Version string to compare against.
        """
        if self._version_tuple and (version_tuple := self._get_version_tuple(version)):
            return self._version_tuple < version_tuple

        # If self._version or version could not be represented by a simple
        # tuple[int, ...], do a more elaborate version parsing and comparison.
        # Avoid this import otherwise to reduce import time for conda activate.
        from packaging.version import parse

        if self._version_object is None:
            try:
                self._version_object = parse(self._version)  # type: ignore[arg-type]
            except TypeError:
                # TypeError: self._version could not be parsed
                self._version_object = parse("0.0.0.dev0+placeholder")
        return self._version_object < parse(version)

    def __call__(
        self: Self,
        deprecate_in: str,
        remove_in: str,
        *,
        addendum: str | None = None,
        stack: int = 0,
    ) -> Callable[[Callable[P, T]], Callable[P, T]]:
        """Deprecation decorator for functions, methods, & classes.

        :param deprecate_in: Version in which code will be marked as deprecated.
        :param remove_in: Version in which code is expected to be removed.
        :param addendum: Optional additional messaging. Useful to indicate what to do instead.
        :param stack: Optional stacklevel increment.
        """

        def deprecated_decorator(func: Callable[P, T]) -> Callable[P, T]:
            # detect function name and generate message
            category, message = self._generate_message(
                deprecate_in=deprecate_in,
                remove_in=remove_in,
                prefix=f"{func.__module__}.{func.__qualname__}",
                addendum=addendum,
            )

            # alert developer that it's time to remove something
            if not category:
                raise DeprecatedError(message)

            # alert user that it's time to remove something
            @wraps(func)
            def inner(*args: P.args, **kwargs: P.kwargs) -> T:
                warnings.warn(message, category, stacklevel=2 + stack)

                return func(*args, **kwargs)

            return inner

        return deprecated_decorator

    def argument(
        self: Self,
        deprecate_in: str,
        remove_in: str,
        argument: str,
        *,
        rename: str | None = None,
        addendum: str | None = None,
        stack: int = 0,
    ) -> Callable[[Callable[P, T]], Callable[P, T]]:
        """Deprecation decorator for keyword arguments.

        :param deprecate_in: Version in which code will be marked as deprecated.
        :param remove_in: Version in which code is expected to be removed.
        :param argument: The argument to deprecate.
        :param rename: Optional new argument name.
        :param addendum: Optional additional messaging. Useful to indicate what to do instead.
        :param stack: Optional stacklevel increment.
        """

        def deprecated_decorator(func: Callable[P, T]) -> Callable[P, T]:
            # detect function name and generate message
            category, message = self._generate_message(
                deprecate_in=deprecate_in,
                remove_in=remove_in,
                prefix=f"{func.__module__}.{func.__qualname__}({argument})",
                # provide a default addendum if renaming and no addendum is provided
                addendum=(
                    f"Use '{rename}' instead." if rename and not addendum else addendum
                ),
            )

            # alert developer that it's time to remove something
            if not category:
                raise DeprecatedError(message)

            # alert user that it's time to remove something
            @wraps(func)
            def inner(*args: P.args, **kwargs: P.kwargs) -> T:
                # only warn about argument deprecations if the argument is used
                if argument in kwargs:
                    warnings.warn(message, category, stacklevel=2 + stack)

                    # rename argument deprecations as needed
                    value = kwargs.pop(argument, None)
                    if rename:
                        kwargs.setdefault(rename, value)

                return func(*args, **kwargs)

            return inner

        return deprecated_decorator

    def action(
        self: Self,
        deprecate_in: str,
        remove_in: str,
        action: ActionType,
        *,
        addendum: str | None = None,
        stack: int = 0,
    ) -> ActionType:
        """Wraps any argparse.Action to issue a deprecation warning."""

        class DeprecationMixin(Action):
            category: type[Warning]
            help: str  # override argparse.Action's help type annotation

            def __init__(inner_self: Self, *args: Any, **kwargs: Any) -> None:
                super().__init__(*args, **kwargs)

                category, message = self._generate_message(
                    deprecate_in=deprecate_in,
                    remove_in=remove_in,
                    prefix=(
                        # option_string are ordered shortest to longest,
                        # use the longest as it's the most descriptive
                        f"`{inner_self.option_strings[-1]}`"
                        if inner_self.option_strings
                        # if not a flag/switch, use the destination itself
                        else f"`{inner_self.dest}`"
                    ),
                    addendum=addendum,
                    deprecation_type=FutureWarning,
                )

                # alert developer that it's time to remove something
                if not category:
                    raise DeprecatedError(message)

                inner_self.category = category
                inner_self.help = message

            def __call__(
                inner_self: Self,
                parser: ArgumentParser,
                namespace: Namespace,
                values: Any,
                option_string: str | None = None,
            ) -> None:
                # alert user that it's time to remove something
                warnings.warn(
                    inner_self.help,
                    inner_self.category,
                    stacklevel=7 + stack,
                )

                super().__call__(parser, namespace, values, option_string)

        return type(action.__name__, (DeprecationMixin, action), {})  # type: ignore[return-value]

    def module(
        self: Self,
        deprecate_in: str,
        remove_in: str,
        *,
        addendum: str | None = None,
        stack: int = 0,
    ) -> None:
        """Deprecation function for modules.

        :param deprecate_in: Version in which code will be marked as deprecated.
        :param remove_in: Version in which code is expected to be removed.
        :param addendum: Optional additional messaging. Useful to indicate what to do instead.
        :param stack: Optional stacklevel increment.
        """
        self.topic(
            deprecate_in=deprecate_in,
            remove_in=remove_in,
            topic=self._get_module(stack)[1],
            addendum=addendum,
            stack=2 + stack,
        )

    def constant(
        self: Self,
        deprecate_in: str,
        remove_in: str,
        constant: str,
        value: Any,
        *,
        addendum: str | None = None,
        stack: int = 0,
    ) -> None:
        """Deprecation function for module constant/global.

        :param deprecate_in: Version in which code will be marked as deprecated.
        :param remove_in: Version in which code is expected to be removed.
        :param constant:
        :param value:
        :param addendum: Optional additional messaging. Useful to indicate what to do instead.
        :param stack: Optional stacklevel increment.
        """
        # detect calling module
        module, fullname = self._get_module(stack)
        # detect function name and generate message
        category, message = self._generate_message(
            deprecate_in=deprecate_in,
            remove_in=remove_in,
            prefix=f"{fullname}.{constant}",
            addendum=addendum,
        )

        # alert developer that it's time to remove something
        if not category:
            raise DeprecatedError(message)

        # patch module level __getattr__ to alert user that it's time to remove something
        super_getattr = getattr(module, "__getattr__", None)

        def __getattr__(name: str) -> Any:
            if name == constant:
                warnings.warn(message, category, stacklevel=2 + stack)
                return value

            if super_getattr:
                return super_getattr(name)

            raise AttributeError(f"module '{fullname}' has no attribute '{name}'")

        module.__getattr__ = __getattr__  # type: ignore[method-assign]

    def topic(
        self: Self,
        deprecate_in: str,
        remove_in: str,
        *,
        topic: str,
        addendum: str | None = None,
        stack: int = 0,
    ) -> None:
        """Deprecation function for a topic.

        :param deprecate_in: Version in which code will be marked as deprecated.
        :param remove_in: Version in which code is expected to be removed.
        :param topic: The topic being deprecated.
        :param addendum: Optional additional messaging. Useful to indicate what to do instead.
        :param stack: Optional stacklevel increment.
        """
        # detect function name and generate message
        category, message = self._generate_message(
            deprecate_in=deprecate_in,
            remove_in=remove_in,
            prefix=topic,
            addendum=addendum,
        )

        # alert developer that it's time to remove something
        if not category:
            raise DeprecatedError(message)

        # alert user that it's time to remove something
        warnings.warn(message, category, stacklevel=2 + stack)

    def _get_module(self: Self, stack: int) -> tuple[ModuleType, str]:
        """Detect the module from which we are being called.

        :param stack: The stacklevel increment.
        :return: The module and module name.
        """
        try:
            frame = sys._getframe(2 + stack)
        except IndexError:
            # IndexError: 2 + stack is out of range
            pass
        else:
            # Shortcut finding the module by manually inspecting loaded modules.
            try:
                filename = frame.f_code.co_filename
            except AttributeError:
                # AttributeError: frame.f_code.co_filename is undefined
                pass
            else:
                # use a copy of sys.modules to avoid RuntimeError during iteration
                # see https://github.com/conda/conda/issues/13754
                for loaded in tuple(sys.modules.values()):
                    if not isinstance(loaded, ModuleType):
                        continue
                    if not hasattr(loaded, "__file__"):
                        continue
                    if loaded.__file__ == filename:
                        return (loaded, loaded.__name__)

            # If above failed, do an expensive import and costly getmodule call.
            import inspect

            module = inspect.getmodule(frame)
            if module is not None:
                return (module, module.__name__)

        raise DeprecatedError("unable to determine the calling module")

    def _generate_message(
        self: Self,
        deprecate_in: str,
        remove_in: str,
        prefix: str,
        addendum: str | None,
        *,
        deprecation_type: type[Warning] = DeprecationWarning,
    ) -> tuple[type[Warning] | None, str]:
        """Generate the standardized deprecation message and determine whether the
        deprecation is pending, active, or past.

        :param deprecate_in: Version in which code will be marked as deprecated.
        :param remove_in: Version in which code is expected to be removed.
        :param prefix: The message prefix, usually the function name.
        :param addendum: Additional messaging. Useful to indicate what to do instead.
        :param deprecation_type: The warning type to use for active deprecations.
        :return: The warning category (if applicable) and the message.
        """
        category: type[Warning] | None
        if self._version_less_than(deprecate_in):
            category = PendingDeprecationWarning
            warning = f"is pending deprecation and will be removed in {remove_in}."
        elif self._version_less_than(remove_in):
            category = deprecation_type
            warning = f"is deprecated and will be removed in {remove_in}."
        else:
            category = None
            warning = f"was slated for removal in {remove_in}."

        return (
            category,
            " ".join(filter(None, [prefix, warning, addendum])),  # message
        )


deprecated = DeprecationHandler(__version__)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Conda exceptions."""

from __future__ import annotations

import json
import os
import sys
from datetime import timedelta
from logging import getLogger
from os.path import join
from textwrap import dedent
from traceback import format_exception, format_exception_only
from typing import TYPE_CHECKING

from requests.exceptions import JSONDecodeError

from . import CondaError, CondaExitZero, CondaMultiError
from .auxlib.entity import EntityEncoder
from .auxlib.ish import dals
from .auxlib.logz import stringify
from .base.constants import COMPATIBLE_SHELLS, PathConflict, SafetyChecks
from .common.compat import on_win
from .common.io import dashlist
from .common.iterators import groupby_to_dict as groupby
from .common.signals import get_signal_name
from .common.url import join_url, maybe_unquote
from .deprecations import DeprecatedError  # noqa: F401
from .exception_handler import ExceptionHandler, conda_exception_handler  # noqa: F401
from .models.channel import Channel

if TYPE_CHECKING:
    import requests

log = getLogger(__name__)


# TODO: for conda-build compatibility only
# remove in conda 4.4
class ResolvePackageNotFound(CondaError):
    def __init__(self, bad_deps):
        # bad_deps is a list of lists
        # bad_deps should really be named 'invalid_chains'
        self.bad_deps = tuple(dep for deps in bad_deps for dep in deps if dep)
        formatted_chains = tuple(
            " -> ".join(map(str, bad_chain)) for bad_chain in bad_deps
        )
        self._formatted_chains = formatted_chains
        message = "\n" + "\n".join(
            (f"  - {bad_chain}") for bad_chain in formatted_chains
        )
        super().__init__(message)


NoPackagesFound = NoPackagesFoundError = ResolvePackageNotFound  # NOQA


class LockError(CondaError):
    def __init__(self, message):
        msg = f"{message}"
        super().__init__(msg)


class ArgumentError(CondaError):
    return_code = 2

    def __init__(self, message, **kwargs):
        super().__init__(message, **kwargs)


class Help(CondaError):
    pass


class ActivateHelp(Help):
    def __init__(self):
        message = dals(
            """
        usage: conda activate [-h] [--[no-]stack] [env_name_or_prefix]

        Activate a conda environment.

        Options:

        positional arguments:
          env_name_or_prefix    The environment name or prefix to activate. If the
                                prefix is a relative path, it must start with './'
                                (or '.\\' on Windows).

        optional arguments:
          -h, --help            Show this help message and exit.
          --stack               Stack the environment being activated on top of the
                                previous active environment, rather replacing the
                                current active environment with a new one. Currently,
                                only the PATH environment variable is stacked. This
                                may be enabled implicitly by the 'auto_stack'
                                configuration variable.
          --no-stack            Do not stack the environment. Overrides 'auto_stack'
                                setting.
        """
        )
        super().__init__(message)


class DeactivateHelp(Help):
    def __init__(self):
        message = dals(
            """
        usage: conda deactivate [-h]

        Deactivate the current active conda environment.

        Options:

        optional arguments:
          -h, --help            Show this help message and exit.
        """
        )
        super().__init__(message)


class GenericHelp(Help):
    def __init__(self, command):
        message = f"help requested for {command}"
        super().__init__(message)


class CondaSignalInterrupt(CondaError):
    def __init__(self, signum):
        signal_name = get_signal_name(signum)
        super().__init__(
            "Signal interrupt %(signal_name)s", signal_name=signal_name, signum=signum
        )


class TooManyArgumentsError(ArgumentError):
    def __init__(
        self, expected, received, offending_arguments, optional_message="", *args
    ):
        self.expected = expected
        self.received = received
        self.offending_arguments = offending_arguments
        self.optional_message = optional_message

        suffix = "s" if received - expected > 1 else ""
        msg = "{} Got {} argument{} ({}) but expected {}.".format(
            optional_message,
            received,
            suffix,
            ", ".join(offending_arguments),
            expected,
        )
        super().__init__(msg, *args)


class ClobberError(CondaError):
    def __init__(self, message, path_conflict, **kwargs):
        self.path_conflict = path_conflict
        super().__init__(message, **kwargs)

    def __repr__(self):
        clz_name = (
            "ClobberWarning"
            if self.path_conflict == PathConflict.warn
            else "ClobberError"
        )
        return f"{clz_name}: {self}\n"


class BasicClobberError(ClobberError):
    def __init__(self, source_path, target_path, context):
        message = dals(
            """
        Conda was asked to clobber an existing path.
          source path: %(source_path)s
          target path: %(target_path)s
        """
        )
        if context.path_conflict == PathConflict.prevent:
            message += (
                "Conda no longer clobbers existing paths without the use of the "
                "--clobber option\n."
            )
        super().__init__(
            message,
            context.path_conflict,
            target_path=target_path,
            source_path=source_path,
        )


class KnownPackageClobberError(ClobberError):
    def __init__(
        self, target_path, colliding_dist_being_linked, colliding_linked_dist, context
    ):
        message = dals(
            """
        The package '%(colliding_dist_being_linked)s' cannot be installed due to a
        path collision for '%(target_path)s'.
        This path already exists in the target prefix, and it won't be removed by
        an uninstall action in this transaction. The path appears to be coming from
        the package '%(colliding_linked_dist)s', which is already installed in the prefix.
        """
        )
        if context.path_conflict == PathConflict.prevent:
            message += (
                "If you'd like to proceed anyway, re-run the command with "
                "the `--clobber` flag.\n."
            )
        super().__init__(
            message,
            context.path_conflict,
            target_path=target_path,
            colliding_dist_being_linked=colliding_dist_being_linked,
            colliding_linked_dist=colliding_linked_dist,
        )


class UnknownPackageClobberError(ClobberError):
    def __init__(self, target_path, colliding_dist_being_linked, context):
        message = dals(
            """
        The package '%(colliding_dist_being_linked)s' cannot be installed due to a
        path collision for '%(target_path)s'.
        This path already exists in the target prefix, and it won't be removed
        by an uninstall action in this transaction. The path is one that conda
        doesn't recognize. It may have been created by another package manager.
        """
        )
        if context.path_conflict == PathConflict.prevent:
            message += (
                "If you'd like to proceed anyway, re-run the command with "
                "the `--clobber` flag.\n."
            )
        super().__init__(
            message,
            context.path_conflict,
            target_path=target_path,
            colliding_dist_being_linked=colliding_dist_being_linked,
        )


class SharedLinkPathClobberError(ClobberError):
    def __init__(self, target_path, incompatible_package_dists, context):
        message = dals(
            """
        This transaction has incompatible packages due to a shared path.
          packages: %(incompatible_packages)s
          path: '%(target_path)s'
        """
        )
        if context.path_conflict == PathConflict.prevent:
            message += (
                "If you'd like to proceed anyway, re-run the command with "
                "the `--clobber` flag.\n."
            )
        super().__init__(
            message,
            context.path_conflict,
            target_path=target_path,
            incompatible_packages=", ".join(str(d) for d in incompatible_package_dists),
        )


class CommandNotFoundError(CondaError):
    def __init__(self, command):
        activate_commands = {
            "activate",
            "deactivate",
            "run",
        }
        conda_commands = {
            "clean",
            "config",
            "create",
            "--help",  # https://github.com/conda/conda/issues/11585
            "info",
            "install",
            "list",
            "package",
            "remove",
            "search",
            "uninstall",
            "update",
            "upgrade",
        }
        build_commands = {
            "build",
            "convert",
            "develop",
            "index",
            "inspect",
            "metapackage",
            "render",
            "skeleton",
        }
        from .cli.main import init_loggers

        init_loggers()
        if command in activate_commands:
            # TODO: Point users to a page at conda-docs, which explains this context in more detail
            builder = [
                "Your shell has not been properly configured to use 'conda %(command)s'."
            ]
            if on_win:
                builder.append(
                    dals(
                        """
                If using 'conda %(command)s' from a batch script, change your
                invocation to 'CALL conda.bat %(command)s'.
                """
                    )
                )
            builder.append(
                dals(
                    """
            To initialize your shell, run

                $ conda init <SHELL_NAME>

            Currently supported shells are:%(supported_shells)s

            See 'conda init --help' for more information and options.

            IMPORTANT: You may need to close and restart your shell after running 'conda init'.
            """
                )
                % {
                    "supported_shells": dashlist(COMPATIBLE_SHELLS),
                }
            )
            message = "\n".join(builder)
        elif command in build_commands:
            message = "To use 'conda %(command)s', install conda-build."
        else:
            from difflib import get_close_matches

            from .cli.find_commands import find_commands

            message = "No command 'conda %(command)s'."
            choices = (
                activate_commands
                | conda_commands
                | build_commands
                | set(find_commands())
            )
            close = get_close_matches(command, choices)
            if close:
                message += f"\nDid you mean 'conda {close[0]}'?"
        super().__init__(message, command=command)


class PathNotFoundError(CondaError, OSError):
    def __init__(self, path):
        message = "%(path)s"
        super().__init__(message, path=path)


class DirectoryNotFoundError(CondaError):
    def __init__(self, path):
        message = "%(path)s"
        super().__init__(message, path=path)


class EnvironmentLocationNotFound(CondaError):
    def __init__(self, location):
        message = "Not a conda environment: %(location)s"
        super().__init__(message, location=location)


class EnvironmentNameNotFound(CondaError):
    def __init__(self, environment_name):
        message = dals(
            """
        Could not find conda environment: %(environment_name)s
        You can list all discoverable environments with `conda info --envs`.
        """
        )
        super().__init__(message, environment_name=environment_name)


class NoBaseEnvironmentError(CondaError):
    def __init__(self):
        message = dals(
            """
        This conda installation has no default base environment. Use
        'conda create' to create new environments and 'conda activate' to
        activate environments.
        """
        )
        super().__init__(message)


class DirectoryNotACondaEnvironmentError(CondaError):
    def __init__(self, target_directory):
        message = dals(
            """
        The target directory exists, but it is not a conda environment.
        Use 'conda create' to convert the directory to a conda environment.
          target directory: %(target_directory)s
        """
        )
        super().__init__(message, target_directory=target_directory)


class CondaEnvironmentError(CondaError, EnvironmentError):
    def __init__(self, message, *args):
        msg = f"{message}"
        super().__init__(msg, *args)


class DryRunExit(CondaExitZero):
    def __init__(self):
        msg = "Dry run. Exiting."
        super().__init__(msg)


class CondaSystemExit(CondaExitZero, SystemExit):
    def __init__(self, *args):
        msg = " ".join(str(arg) for arg in self.args)
        super().__init__(msg)


class PaddingError(CondaError):
    def __init__(self, dist, placeholder, placeholder_length):
        msg = (
            "Placeholder of length '%d' too short in package %s.\n"
            "The package must be rebuilt with conda-build > 2.0."
            % (placeholder_length, dist)
        )
        super().__init__(msg)


class LinkError(CondaError):
    def __init__(self, message):
        super().__init__(message)


class CondaOSError(CondaError, OSError):
    def __init__(self, message, **kwargs):
        msg = f"{message}"
        super().__init__(msg, **kwargs)


class ProxyError(CondaError):
    def __init__(self):
        message = dals(
            """
        Conda cannot proceed due to an error in your proxy configuration.
        Check for typos and other configuration errors in any '.netrc' file in your home directory,
        any environment variables ending in '_PROXY', and any other system-wide proxy
        configuration settings.
        """
        )
        super().__init__(message)


class CondaIOError(CondaError, IOError):
    def __init__(self, message, *args):
        msg = f"{message}"
        super().__init__(msg)


class CondaFileIOError(CondaIOError):
    def __init__(self, filepath, message, *args):
        self.filepath = filepath

        msg = f"'{filepath}'. {message}"
        super().__init__(msg, *args)


class CondaKeyError(CondaError, KeyError):
    def __init__(self, key, message, *args):
        self.key = key
        self.msg = f"{key!r}: {message}"
        super().__init__(self.msg, *args)


class ChannelError(CondaError):
    pass


class ChannelNotAllowed(ChannelError):
    def __init__(self, channel):
        channel = Channel(channel)
        channel_name = channel.name
        channel_url = maybe_unquote(channel.base_url)
        message = dals(
            """
        Channel not included in allowlist:
          channel name: %(channel_name)s
          channel url: %(channel_url)s
        """
        )
        super().__init__(message, channel_url=channel_url, channel_name=channel_name)


class UnavailableInvalidChannel(ChannelError):
    status_code: str | int

    def __init__(
        self, channel, status_code, response: requests.models.Response | None = None
    ):
        # parse channel
        channel = Channel(channel)
        channel_name = channel.name
        channel_url = maybe_unquote(channel.base_url)

        # define hardcoded/default reason/message
        reason = getattr(response, "reason", None)
        message = dals(
            """
            The channel is not accessible or is invalid.

            You will need to adjust your conda configuration to proceed.
            Use `conda config --show channels` to view your configuration's current state,
            and use `conda config --show-sources` to view config file locations.
            """
        )
        if channel.scheme == "file":
            url = join_url(channel.location, channel.name)
            message += dedent(
                f"""
                As of conda 4.3, a valid channel must contain a `noarch/repodata.json` and
                associated `noarch/repodata.json.bz2` file, even if `noarch/repodata.json` is
                empty. Use `conda index {url}`, or create `noarch/repodata.json`
                and associated `noarch/repodata.json.bz2`.
                """
            )

        # if response includes a valid json body we prefer the reason/message defined there
        try:
            body = response.json()
        except (AttributeError, JSONDecodeError):
            body = {}
        else:
            reason = body.get("reason", None) or reason
            message = body.get("message", None) or message

        # standardize arguments
        status_code = status_code or "000"
        reason = reason or "UNAVAILABLE OR INVALID"
        if isinstance(reason, str):
            reason = reason.upper()

        self.status_code = status_code

        super().__init__(
            f"HTTP {status_code} {reason} for channel {channel_name} <{channel_url}>\n\n{message}",
            channel_name=channel_name,
            channel_url=channel_url,
            status_code=status_code,
            reason=reason,
            response_details=stringify(response, content_max_len=1024) or "",
            json=body,
        )


class OperationNotAllowed(CondaError):
    def __init__(self, message):
        super().__init__(message)


class CondaImportError(CondaError, ImportError):
    def __init__(self, message):
        msg = f"{message}"
        super().__init__(msg)


class ParseError(CondaError):
    def __init__(self, message):
        msg = f"{message}"
        super().__init__(msg)


class CouldntParseError(ParseError):
    def __init__(self, reason):
        self.reason = reason
        super().__init__(self.args[0])


class ChecksumMismatchError(CondaError):
    def __init__(
        self, url, target_full_path, checksum_type, expected_checksum, actual_checksum
    ):
        message = dals(
            """
        Conda detected a mismatch between the expected content and downloaded content
        for url '%(url)s'.
          download saved to: %(target_full_path)s
          expected %(checksum_type)s: %(expected_checksum)s
          actual %(checksum_type)s: %(actual_checksum)s
        """
        )
        url = maybe_unquote(url)
        super().__init__(
            message,
            url=url,
            target_full_path=target_full_path,
            checksum_type=checksum_type,
            expected_checksum=expected_checksum,
            actual_checksum=actual_checksum,
        )


class PackageNotInstalledError(CondaError):
    def __init__(self, prefix, package_name):
        message = dals(
            """
        Package is not installed in prefix.
          prefix: %(prefix)s
          package name: %(package_name)s
        """
        )
        super().__init__(message, prefix=prefix, package_name=package_name)


class CondaHTTPError(CondaError):
    def __init__(
        self,
        message,
        url,
        status_code,
        reason,
        elapsed_time,
        response=None,
        caused_by=None,
    ):
        # if response includes a valid json body we prefer the reason/message defined there
        try:
            body = response.json()
        except (AttributeError, JSONDecodeError):
            body = {}
        else:
            reason = body.get("reason", None) or reason
            message = body.get("message", None) or message

        # standardize arguments
        url = maybe_unquote(url)
        status_code = status_code or "000"
        reason = reason or "CONNECTION FAILED"
        if isinstance(reason, str):
            reason = reason.upper()
        elapsed_time = elapsed_time or "-"
        if isinstance(elapsed_time, timedelta):
            elapsed_time = str(elapsed_time).split(":", 1)[-1]

        # extract CF-RAY
        try:
            cf_ray = response.headers["CF-RAY"]
        except (AttributeError, KeyError):
            cf_ray = ""
        else:
            cf_ray = f"CF-RAY: {cf_ray}\n"

        super().__init__(
            dals(
                f"""
                HTTP {status_code} {reason} for url <{url}>
                Elapsed: {elapsed_time}
                {cf_ray}
                """
            )
            # since message may include newlines don't include in f-string/dals above
            + message,
            url=url,
            status_code=status_code,
            reason=reason,
            elapsed_time=elapsed_time,
            response_details=stringify(response, content_max_len=1024) or "",
            json=body,
            caused_by=caused_by,
        )


class CondaSSLError(CondaError):
    pass


class AuthenticationError(CondaError):
    pass


class PackagesNotFoundError(CondaError):
    def __init__(self, packages, channel_urls=()):
        format_list = lambda iterable: "  - " + "\n  - ".join(str(x) for x in iterable)

        if channel_urls:
            message = dals(
                """
            The following packages are not available from current channels:

            %(packages_formatted)s

            Current channels:

            %(channels_formatted)s

            To search for alternate channels that may provide the conda package you're
            looking for, navigate to

                https://anaconda.org

            and use the search bar at the top of the page.
            """
            )
            from .base.context import context

            if context.use_only_tar_bz2:
                message += dals(
                    """
                Note: 'use_only_tar_bz2' is enabled. This might be omitting some
                packages from the index. Set this option to 'false' and retry.
                """
                )
            packages_formatted = format_list(packages)
            channels_formatted = format_list(channel_urls)
        else:
            message = dals(
                """
            The following packages are missing from the target environment:
            %(packages_formatted)s
            """
            )
            packages_formatted = format_list(packages)
            channels_formatted = ()

        super().__init__(
            message,
            packages=packages,
            packages_formatted=packages_formatted,
            channel_urls=channel_urls,
            channels_formatted=channels_formatted,
        )


class UnsatisfiableError(CondaError):
    """An exception to report unsatisfiable dependencies.

    Args:
        bad_deps: a list of tuples of objects (likely MatchSpecs).
        chains: (optional) if True, the tuples are interpreted as chains
            of dependencies, from top level to bottom. If False, the tuples
            are interpreted as simple lists of conflicting specs.

    Returns:
        Raises an exception with a formatted message detailing the
        unsatisfiable specifications.
    """

    def _format_chain_str(self, bad_deps):
        chains = {}
        for dep in sorted(bad_deps, key=len, reverse=True):
            dep1 = [s.partition(" ") for s in dep[1:]]
            key = (dep[0],) + tuple(v[0] for v in dep1)
            vals = ("",) + tuple(v[2] for v in dep1)
            found = False
            for key2, csets in chains.items():
                if key2[: len(key)] == key:
                    for cset, val in zip(csets, vals):
                        cset.add(val)
                    found = True
            if not found:
                chains[key] = [{val} for val in vals]
        for key, csets in chains.items():
            deps = []
            for name, cset in zip(key, csets):
                if "" not in cset:
                    pass
                elif len(cset) == 1:
                    cset.clear()
                else:
                    cset.remove("")
                    cset.add("*")
                if name[0] == "@":
                    name = "feature:" + name[1:]
                deps.append(
                    "{} {}".format(name, "|".join(sorted(cset))) if cset else name
                )
            chains[key] = " -> ".join(deps)
        return [chains[key] for key in sorted(chains.keys())]

    def __init__(self, bad_deps, chains=True, strict=False):
        from .models.match_spec import MatchSpec

        messages = {
            "python": dals(
                """

The following specifications were found
to be incompatible with the existing python installation in your environment:

Specifications:\n{specs}

Your python: {ref}

If python is on the left-most side of the chain, that's the version you've asked for.
When python appears to the right, that indicates that the thing on the left is somehow
not available for the python version you are constrained to. Note that conda will not
change your python version to a different minor version unless you explicitly specify
that.

        """
            ),
            "request_conflict_with_history": dals(
                """

The following specifications were found to be incompatible with a past
explicit spec that is not an explicit spec in this operation ({ref}):\n{specs}

                    """
            ),
            "direct": dals(
                """

The following specifications were found to be incompatible with each other:
                    """
            ),
            "virtual_package": dals(
                """

The following specifications were found to be incompatible with your system:\n{specs}

Your installed version is: {ref}
"""
            ),
        }

        msg = ""
        self.unsatisfiable = []
        if len(bad_deps) == 0:
            msg += """
Did not find conflicting dependencies. If you would like to know which
packages conflict ensure that you have enabled unsatisfiable hints.

conda config --set unsatisfiable_hints True
            """
        else:
            for class_name, dep_class in bad_deps.items():
                if dep_class:
                    _chains = []
                    if class_name == "direct":
                        msg += messages["direct"]
                        last_dep_entry = {d[0][-1].name for d in dep_class}
                        dep_constraint_map = {}
                        for dep in dep_class:
                            if dep[0][-1].name in last_dep_entry:
                                if not dep_constraint_map.get(dep[0][-1].name):
                                    dep_constraint_map[dep[0][-1].name] = []
                                dep_constraint_map[dep[0][-1].name].append(dep[0])
                        msg += "\nOutput in format: Requested package -> Available versions"
                        for dep, chain in dep_constraint_map.items():
                            if len(chain) > 1:
                                msg += f"\n\nPackage {dep} conflicts for:\n"
                                msg += "\n".join(
                                    [" -> ".join([str(i) for i in c]) for c in chain]
                                )
                                self.unsatisfiable += [
                                    tuple(entries) for entries in chain
                                ]
                    else:
                        for dep_chain, installed_blocker in dep_class:
                            # Remove any target values from the MatchSpecs, convert to strings
                            dep_chain = [
                                str(MatchSpec(dep, target=None)) for dep in dep_chain
                            ]
                            _chains.append(dep_chain)

                        if _chains:
                            _chains = self._format_chain_str(_chains)
                        else:
                            _chains = [", ".join(c) for c in _chains]
                        msg += messages[class_name].format(
                            specs=dashlist(_chains), ref=installed_blocker
                        )
        if strict:
            msg += (
                "\nNote that strict channel priority may have removed "
                "packages required for satisfiability."
            )

        super().__init__(msg)


class RemoveError(CondaError):
    def __init__(self, message):
        msg = f"{message}"
        super().__init__(msg)


class DisallowedPackageError(CondaError):
    def __init__(self, package_ref, **kwargs):
        from .models.records import PackageRecord

        package_ref = PackageRecord.from_objects(package_ref)
        message = (
            "The package '%(dist_str)s' is disallowed by configuration.\n"
            "See 'conda config --show disallowed_packages'."
        )
        super().__init__(
            message, package_ref=package_ref, dist_str=package_ref.dist_str(), **kwargs
        )


class SpecsConfigurationConflictError(CondaError):
    def __init__(self, requested_specs, pinned_specs, prefix):
        message = dals(
            """
        Requested specs conflict with configured specs.
          requested specs: {requested_specs_formatted}
          pinned specs: {pinned_specs_formatted}
        Use 'conda config --show-sources' to look for 'pinned_specs' and 'track_features'
        configuration parameters.  Pinned specs may also be defined in the file
        {pinned_specs_path}.
        """
        ).format(
            requested_specs_formatted=dashlist(requested_specs, 4),
            pinned_specs_formatted=dashlist(pinned_specs, 4),
            pinned_specs_path=join(prefix, "conda-meta", "pinned"),
        )
        super().__init__(
            message,
            requested_specs=requested_specs,
            pinned_specs=pinned_specs,
            prefix=prefix,
        )


class CondaIndexError(CondaError, IndexError):
    def __init__(self, message):
        msg = f"{message}"
        super().__init__(msg)


class CondaValueError(CondaError, ValueError):
    def __init__(self, message, *args, **kwargs):
        super().__init__(message, *args, **kwargs)


class CyclicalDependencyError(CondaError, ValueError):
    def __init__(self, packages_with_cycles, **kwargs):
        from .models.records import PackageRecord

        packages_with_cycles = tuple(
            PackageRecord.from_objects(p) for p in packages_with_cycles
        )
        message = f"Cyclic dependencies exist among these items: {dashlist(p.dist_str() for p in packages_with_cycles)}"
        super().__init__(message, packages_with_cycles=packages_with_cycles, **kwargs)


class CorruptedEnvironmentError(CondaError):
    def __init__(self, environment_location, corrupted_file, **kwargs):
        message = dals(
            """
        The target environment has been corrupted. Corrupted environments most commonly
        occur when the conda process is force-terminated while in an unlink-link
        transaction.
          environment location: %(environment_location)s
          corrupted file: %(corrupted_file)s
        """
        )
        super().__init__(
            message,
            environment_location=environment_location,
            corrupted_file=corrupted_file,
            **kwargs,
        )


class CondaHistoryError(CondaError):
    def __init__(self, message):
        msg = f"{message}"
        super().__init__(msg)


class CondaUpgradeError(CondaError):
    def __init__(self, message):
        msg = f"{message}"
        super().__init__(msg)


class CondaVerificationError(CondaError):
    def __init__(self, message):
        super().__init__(message)


class SafetyError(CondaError):
    def __init__(self, message):
        super().__init__(message)


class CondaMemoryError(CondaError, MemoryError):
    def __init__(self, caused_by, **kwargs):
        message = "The conda process ran out of memory. Increase system memory and/or try again."
        super().__init__(message, caused_by=caused_by, **kwargs)


class NotWritableError(CondaError, OSError):
    def __init__(self, path, errno, **kwargs):
        kwargs.update(
            {
                "path": path,
                "errno": errno,
            }
        )
        if on_win:
            message = dals(
                """
            The current user does not have write permissions to a required path.
              path: %(path)s
            """
            )
        else:
            message = dals(
                """
            The current user does not have write permissions to a required path.
              path: %(path)s
              uid: %(uid)s
              gid: %(gid)s

            If you feel that permissions on this path are set incorrectly, you can manually
            change them by executing

              $ sudo chown %(uid)s:%(gid)s %(path)s

            In general, it's not advisable to use 'sudo conda'.
            """
            )
            kwargs.update(
                {
                    "uid": os.geteuid(),
                    "gid": os.getegid(),
                }
            )
        super().__init__(message, **kwargs)
        self.errno = errno


class NoWritableEnvsDirError(CondaError):
    def __init__(self, envs_dirs, **kwargs):
        message = f"No writeable envs directories configured.{dashlist(envs_dirs)}"
        super().__init__(message, envs_dirs=envs_dirs, **kwargs)


class NoWritablePkgsDirError(CondaError):
    def __init__(self, pkgs_dirs, **kwargs):
        message = f"No writeable pkgs directories configured.{dashlist(pkgs_dirs)}"
        super().__init__(message, pkgs_dirs=pkgs_dirs, **kwargs)


class EnvironmentNotWritableError(CondaError):
    def __init__(self, environment_location, **kwargs):
        kwargs.update(
            {
                "environment_location": environment_location,
            }
        )
        if on_win:
            message = dals(
                """
            The current user does not have write permissions to the target environment.
              environment location: %(environment_location)s
            """
            )
        else:
            message = dals(
                """
            The current user does not have write permissions to the target environment.
              environment location: %(environment_location)s
              uid: %(uid)s
              gid: %(gid)s
            """
            )
            kwargs.update(
                {
                    "uid": os.geteuid(),
                    "gid": os.getegid(),
                }
            )
        super().__init__(message, **kwargs)


class CondaDependencyError(CondaError):
    def __init__(self, message):
        super().__init__(message)


class BinaryPrefixReplacementError(CondaError):
    def __init__(
        self, path, placeholder, new_prefix, original_data_length, new_data_length
    ):
        message = dals(
            """
        Refusing to replace mismatched data length in binary file.
          path: %(path)s
          placeholder: %(placeholder)s
          new prefix: %(new_prefix)s
          original data Length: %(original_data_length)d
          new data length: %(new_data_length)d
        """
        )
        kwargs = {
            "path": path,
            "placeholder": placeholder,
            "new_prefix": new_prefix,
            "original_data_length": original_data_length,
            "new_data_length": new_data_length,
        }
        super().__init__(message, **kwargs)


class InvalidSpec(CondaError, ValueError):
    def __init__(self, message: str, **kwargs):
        super().__init__(message, **kwargs)


class InvalidVersionSpec(InvalidSpec):
    def __init__(self, invalid_spec: str, details: str):
        message = "Invalid version '%(invalid_spec)s': %(details)s"
        super().__init__(message, invalid_spec=invalid_spec, details=details)


class InvalidMatchSpec(InvalidSpec):
    def __init__(self, invalid_spec: str, details: str):
        message = "Invalid spec '%(invalid_spec)s': %(details)s"
        super().__init__(message, invalid_spec=invalid_spec, details=details)


class EncodingError(CondaError):
    def __init__(self, caused_by, **kwargs):
        message = (
            dals(
                """
        A unicode encoding or decoding error has occurred.
        Python 2 is the interpreter under which conda is running in your base environment.
        Replacing your base environment with one having Python 3 may help resolve this issue.
        If you still have a need for Python 2 environments, consider using 'conda create'
        and 'conda activate'.  For example:

            $ conda create -n py2 python=2
            $ conda activate py2

        Error details: %r

        """
            )
            % caused_by
        )
        super().__init__(message, caused_by=caused_by, **kwargs)


class NoSpaceLeftError(CondaError):
    def __init__(self, caused_by, **kwargs):
        message = "No space left on devices."
        super().__init__(message, caused_by=caused_by, **kwargs)


class CondaEnvException(CondaError):
    def __init__(self, message, *args, **kwargs):
        msg = f"{message}"
        super().__init__(msg, *args, **kwargs)


class EnvironmentFileNotFound(CondaEnvException):
    def __init__(self, filename, *args, **kwargs):
        msg = f"'{filename}' file not found"
        self.filename = filename
        super().__init__(msg, *args, **kwargs)


class EnvironmentFileExtensionNotValid(CondaEnvException):
    def __init__(self, filename, *args, **kwargs):
        msg = f"'{filename}' file extension must be one of '.txt', '.yaml' or '.yml'"
        self.filename = filename
        super().__init__(msg, *args, **kwargs)


class EnvironmentFileEmpty(CondaEnvException):
    def __init__(self, filename, *args, **kwargs):
        self.filename = filename
        msg = f"'{filename}' is empty"
        super().__init__(msg, *args, **kwargs)


class EnvironmentFileNotDownloaded(CondaError):
    def __init__(self, username, packagename, *args, **kwargs):
        msg = f"{username}/{packagename} file not downloaded"
        self.username = username
        self.packagename = packagename
        super().__init__(msg, *args, **kwargs)


class SpecNotFound(CondaError):
    def __init__(self, msg, *args, **kwargs):
        super().__init__(msg, *args, **kwargs)


class PluginError(CondaError):
    pass


def maybe_raise(error, context):
    if isinstance(error, CondaMultiError):
        groups = groupby(lambda e: isinstance(e, ClobberError), error.errors)
        clobber_errors = groups.get(True, ())
        groups = groupby(lambda e: isinstance(e, SafetyError), groups.get(False, ()))
        safety_errors = groups.get(True, ())
        other_errors = groups.get(False, ())

        if (
            (safety_errors and context.safety_checks == SafetyChecks.enabled)
            or (
                clobber_errors
                and context.path_conflict == PathConflict.prevent
                and not context.clobber
            )
            or other_errors
        ):
            raise error
        elif (safety_errors and context.safety_checks == SafetyChecks.warn) or (
            clobber_errors
            and context.path_conflict == PathConflict.warn
            and not context.clobber
        ):
            print_conda_exception(error)

    elif isinstance(error, ClobberError):
        if context.path_conflict == PathConflict.prevent and not context.clobber:
            raise error
        elif context.path_conflict == PathConflict.warn and not context.clobber:
            print_conda_exception(error)

    elif isinstance(error, SafetyError):
        if context.safety_checks == SafetyChecks.enabled:
            raise error
        elif context.safety_checks == SafetyChecks.warn:
            print_conda_exception(error)

    else:
        raise error


def print_conda_exception(exc_val, exc_tb=None):
    from .base.context import context

    rc = getattr(exc_val, "return_code", None)
    if context.debug or (not isinstance(exc_val, DryRunExit) and context.info):
        print(_format_exc(exc_val, exc_tb), file=sys.stderr)
    elif context.json:
        if isinstance(exc_val, DryRunExit):
            return
        logger = getLogger("conda.stdout" if rc else "conda.stderr")
        exc_json = json.dumps(
            exc_val.dump_map(), indent=2, sort_keys=True, cls=EntityEncoder
        )
        logger.info(f"{exc_json}\n")
    else:
        stderrlog = getLogger("conda.stderr")
        stderrlog.error("\n%r\n", exc_val)
        # An alternative which would allow us not to reload sys with newly setdefaultencoding()
        # is to not use `%r`, e.g.:
        # Still, not being able to use `%r` seems too great a price to pay.
        # stderrlog.error("\n" + exc_val.__repr__() + \n")


def _format_exc(exc_val=None, exc_tb=None):
    if exc_val is None:
        exc_type, exc_val, exc_tb = sys.exc_info()
    else:
        exc_type = type(exc_val)
    if exc_tb:
        formatted_exception = format_exception(exc_type, exc_val, exc_tb)
    else:
        formatted_exception = format_exception_only(exc_type, exc_val)
    return "".join(formatted_exception)


class InvalidInstaller(Exception):
    def __init__(self, name):
        msg = f"Unable to load installer for {name}"
        super().__init__(msg)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Collection of conda's high-level APIs."""

from .base.constants import DepsModifier as _DepsModifier
from .base.constants import UpdateModifier as _UpdateModifier
from .base.context import context
from .common.constants import NULL
from .core.package_cache_data import PackageCacheData as _PackageCacheData
from .core.prefix_data import PrefixData as _PrefixData
from .core.subdir_data import SubdirData as _SubdirData
from .models.channel import Channel

#: Flags to enable alternate handling of dependencies.
DepsModifier = _DepsModifier

#: Flags to enable alternate handling for updates of existing packages in the environment.
UpdateModifier = _UpdateModifier


class Solver:
    """
    **Beta** While in beta, expect both major and minor changes across minor releases.

    A high-level API to conda's solving logic. Three public methods are provided to access a
    solution in various forms.

      * :meth:`solve_final_state`
      * :meth:`solve_for_diff`
      * :meth:`solve_for_transaction`

    """

    def __init__(
        self, prefix, channels, subdirs=(), specs_to_add=(), specs_to_remove=()
    ):
        """
        **Beta**

        Args:
            prefix (str):
                The conda prefix / environment location for which the :class:`Solver`
                is being instantiated.
            channels (Sequence[:class:`Channel`]):
                A prioritized list of channels to use for the solution.
            subdirs (Sequence[str]):
                A prioritized list of subdirs to use for the solution.
            specs_to_add (set[:class:`MatchSpec`]):
                The set of package specs to add to the prefix.
            specs_to_remove (set[:class:`MatchSpec`]):
                The set of package specs to remove from the prefix.

        """
        solver_backend = context.plugin_manager.get_cached_solver_backend()
        self._internal = solver_backend(
            prefix, channels, subdirs, specs_to_add, specs_to_remove
        )

    def solve_final_state(
        self,
        update_modifier=NULL,
        deps_modifier=NULL,
        prune=NULL,
        ignore_pinned=NULL,
        force_remove=NULL,
    ):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Gives the final, solved state of the environment.

        Args:
            deps_modifier (DepsModifier):
                An optional flag indicating special solver handling for dependencies. The
                default solver behavior is to be as conservative as possible with dependency
                updates (in the case the dependency already exists in the environment), while
                still ensuring all dependencies are satisfied.  Options include
                * NO_DEPS
                * ONLY_DEPS
                * UPDATE_DEPS
                * UPDATE_DEPS_ONLY_DEPS
                * FREEZE_INSTALLED
            prune (bool):
                If ``True``, the solution will not contain packages that were
                previously brought into the environment as dependencies but are no longer
                required as dependencies and are not user-requested.
            ignore_pinned (bool):
                If ``True``, the solution will ignore pinned package configuration
                for the prefix.
            force_remove (bool):
                Forces removal of a package without removing packages that depend on it.

        Returns:
            tuple[PackageRef]:
                In sorted dependency order from roots to leaves, the package references for
                the solved state of the environment.

        """
        return self._internal.solve_final_state(
            update_modifier, deps_modifier, prune, ignore_pinned, force_remove
        )

    def solve_for_diff(
        self,
        update_modifier=NULL,
        deps_modifier=NULL,
        prune=NULL,
        ignore_pinned=NULL,
        force_remove=NULL,
        force_reinstall=False,
    ):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Gives the package references to remove from an environment, followed by
        the package references to add to an environment.

        Args:
            deps_modifier (DepsModifier):
                See :meth:`solve_final_state`.
            prune (bool):
                See :meth:`solve_final_state`.
            ignore_pinned (bool):
                See :meth:`solve_final_state`.
            force_remove (bool):
                See :meth:`solve_final_state`.
            force_reinstall (bool):
                For requested specs_to_add that are already satisfied in the environment,
                instructs the solver to remove the package and spec from the environment,
                and then add it back--possibly with the exact package instance modified,
                depending on the spec exactness.

        Returns:
            tuple[PackageRef], tuple[PackageRef]:
                A two-tuple of PackageRef sequences.  The first is the group of packages to
                remove from the environment, in sorted dependency order from leaves to roots.
                The second is the group of packages to add to the environment, in sorted
                dependency order from roots to leaves.

        """
        return self._internal.solve_for_diff(
            update_modifier,
            deps_modifier,
            prune,
            ignore_pinned,
            force_remove,
            force_reinstall,
        )

    def solve_for_transaction(
        self,
        update_modifier=NULL,
        deps_modifier=NULL,
        prune=NULL,
        ignore_pinned=NULL,
        force_remove=NULL,
        force_reinstall=False,
    ):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Gives an UnlinkLinkTransaction instance that can be used to execute the solution
        on an environment.

        Args:
            deps_modifier (DepsModifier):
                See :meth:`solve_final_state`.
            prune (bool):
                See :meth:`solve_final_state`.
            ignore_pinned (bool):
                See :meth:`solve_final_state`.
            force_remove (bool):
                See :meth:`solve_final_state`.
            force_reinstall (bool):
                See :meth:`solve_for_diff`.

        Returns:
            UnlinkLinkTransaction:

        """
        return self._internal.solve_for_transaction(
            update_modifier,
            deps_modifier,
            prune,
            ignore_pinned,
            force_remove,
            force_reinstall,
        )


class SubdirData:
    """
    **Beta** While in beta, expect both major and minor changes across minor releases.

    High-level management and usage of repodata.json for subdirs.
    """

    def __init__(self, channel):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Args:
            channel (str or Channel):
                The target subdir for the instance. Must either be a url that includes a subdir
                or a :obj:`Channel` that includes a subdir. e.g.:
                    * 'https://repo.anaconda.com/pkgs/main/linux-64'
                    * Channel('https://repo.anaconda.com/pkgs/main/linux-64')
                    * Channel('conda-forge/osx-64')
        """
        channel = Channel(channel)
        assert channel.subdir
        self._internal = _SubdirData(channel)

    def query(self, package_ref_or_match_spec):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Run a query against this specific instance of repodata.

        Args:
            package_ref_or_match_spec (PackageRef or MatchSpec or str):
                Either an exact :obj:`PackageRef` to match against, or a :obj:`MatchSpec`
                query object.  A :obj:`str` will be turned into a :obj:`MatchSpec` automatically.

        Returns:
            tuple[PackageRecord]

        """
        return tuple(self._internal.query(package_ref_or_match_spec))

    @staticmethod
    def query_all(package_ref_or_match_spec, channels=None, subdirs=None):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Run a query against all repodata instances in channel/subdir matrix.

        Args:
            package_ref_or_match_spec (PackageRef or MatchSpec or str):
                Either an exact :obj:`PackageRef` to match against, or a :obj:`MatchSpec`
                query object.  A :obj:`str` will be turned into a :obj:`MatchSpec` automatically.
            channels (Iterable[Channel or str] or None):
                An iterable of urls for channels or :obj:`Channel` objects. If None, will fall
                back to context.channels.
            subdirs (Iterable[str] or None):
                If None, will fall back to context.subdirs.

        Returns:
            tuple[PackageRecord]

        """
        return tuple(
            _SubdirData.query_all(package_ref_or_match_spec, channels, subdirs)
        )

    def iter_records(self):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Returns:
            Iterable[PackageRecord]: A generator over all records contained in the repodata.json
                instance.  Warning: this is a generator that is exhausted on first use.

        """
        return self._internal.iter_records()

    def reload(self):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Update the instance with new information. Backing information (i.e. repodata.json)
        is lazily downloaded/loaded on first use by the other methods of this class. You
        should only use this method if you are *sure* you have outdated data.

        Returns:
            SubdirData

        """
        self._internal = self._internal.reload()
        return self


class PackageCacheData:
    """
    **Beta** While in beta, expect both major and minor changes across minor releases.

    High-level management and usage of package caches.
    """

    def __init__(self, pkgs_dir):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Args:
            pkgs_dir (str):
        """
        self._internal = _PackageCacheData(pkgs_dir)

    def get(self, package_ref, default=NULL):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Args:
            package_ref (PackageRef):
                A :obj:`PackageRef` instance representing the key for the
                :obj:`PackageCacheRecord` being sought.
            default: The default value to return if the record does not exist. If not
                specified and no record exists, :exc:`KeyError` is raised.

        Returns:
            PackageCacheRecord

        """
        return self._internal.get(package_ref, default)

    def query(self, package_ref_or_match_spec):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Run a query against this specific package cache instance.

        Args:
            package_ref_or_match_spec (PackageRef or MatchSpec or str):
                Either an exact :obj:`PackageRef` to match against, or a :obj:`MatchSpec`
                query object.  A :obj:`str` will be turned into a :obj:`MatchSpec` automatically.

        Returns:
            tuple[PackageCacheRecord]

        """
        return tuple(self._internal.query(package_ref_or_match_spec))

    @staticmethod
    def query_all(package_ref_or_match_spec, pkgs_dirs=None):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Run a query against all package caches.

        Args:
            package_ref_or_match_spec (PackageRef or MatchSpec or str):
                Either an exact :obj:`PackageRef` to match against, or a :obj:`MatchSpec`
                query object.  A :obj:`str` will be turned into a :obj:`MatchSpec` automatically.
            pkgs_dirs (Iterable[str] or None):
                If None, will fall back to context.pkgs_dirs.

        Returns:
            tuple[PackageCacheRecord]

        """
        return tuple(_PackageCacheData.query_all(package_ref_or_match_spec, pkgs_dirs))

    def iter_records(self):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Returns:
            Iterable[PackageCacheRecord]: A generator over all records contained in the package
                cache instance.  Warning: this is a generator that is exhausted on first use.

        """
        return self._internal.iter_records()

    @property
    def is_writable(self):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Indicates if the package cache location is writable or read-only.

        Returns:
            bool

        """
        return self._internal.is_writable

    @staticmethod
    def first_writable(pkgs_dirs=None):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Get an instance object for the first writable package cache.

        Args:
            pkgs_dirs (Iterable[str]):
                If None, will fall back to context.pkgs_dirs.

        Returns:
            PackageCacheData:
                An instance for the first writable package cache.

        """
        return PackageCacheData(_PackageCacheData.first_writable(pkgs_dirs).pkgs_dir)

    def reload(self):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Update the instance with new information. Backing information (i.e. contents of
        the pkgs_dir) is lazily loaded on first use by the other methods of this class. You
        should only use this method if you are *sure* you have outdated data.

        Returns:
            PackageCacheData

        """
        self._internal = self._internal.reload()
        return self


class PrefixData:
    """
    **Beta** While in beta, expect both major and minor changes across minor releases.

    High-level management and usage of conda environment prefixes.
    """

    def __init__(self, prefix_path):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Args:
            prefix_path (str):
        """
        self._internal = _PrefixData(prefix_path)

    def get(self, package_ref, default=NULL):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Args:
            package_ref (PackageRef):
                A :obj:`PackageRef` instance representing the key for the
                :obj:`PrefixRecord` being sought.
            default: The default value to return if the record does not exist. If not
                specified and no record exists, :exc:`KeyError` is raised.

        Returns:
            PrefixRecord

        """
        return self._internal.get(package_ref.name, default)

    def query(self, package_ref_or_match_spec):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Run a query against this specific prefix instance.

        Args:
            package_ref_or_match_spec (PackageRef or MatchSpec or str):
                Either an exact :obj:`PackageRef` to match against, or a :obj:`MatchSpec`
                query object.  A :obj:`str` will be turned into a :obj:`MatchSpec` automatically.

        Returns:
            tuple[PrefixRecord]

        """
        return tuple(self._internal.query(package_ref_or_match_spec))

    def iter_records(self):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Returns:
            Iterable[PrefixRecord]: A generator over all records contained in the prefix.
                Warning: this is a generator that is exhausted on first use.

        """
        return self._internal.iter_records()

    @property
    def is_writable(self):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Indicates if the prefix is writable or read-only.

        Returns:
            bool or None:
                True if the prefix is writable.  False if read-only.  None if the prefix
                does not exist as a conda environment.

        """
        return self._internal.is_writable

    def reload(self):
        """
        **Beta** While in beta, expect both major and minor changes across minor releases.

        Update the instance with new information. Backing information (i.e. contents of
        the conda-meta directory) is lazily loaded on first use by the other methods of this
        class. You should only use this method if you are *sure* you have outdated data.

        Returns:
            PrefixData

        """
        self._internal = self._internal.reload()
        return self


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""OS-agnostic, system-level binary package manager."""

import os
import sys
from json import JSONEncoder
from os.path import abspath, dirname

try:
    from ._version import __version__
except ImportError:
    # _version.py is only created after running `pip install`
    try:
        from setuptools_scm import get_version

        __version__ = get_version(root="..", relative_to=__file__)
    except (ImportError, OSError, LookupError):
        # ImportError: setuptools_scm isn't installed
        # OSError: git isn't installed
        # LookupError: setuptools_scm unable to detect version
        # Conda abides by CEP-8 which specifies using CalVer, so the dev version is:
        #     YY.MM.MICRO.devN+gHASH[.dirty]
        __version__ = "0.0.0.dev0+placeholder"


try:
    from frozendict import frozendict
except ImportError:
    from ._vendor.frozendict import frozendict

__all__ = (
    "__name__",
    "__version__",
    "__author__",
    "__email__",
    "__license__",
    "__summary__",
    "__url__",
    "CONDA_PACKAGE_ROOT",
    "CondaError",
    "CondaMultiError",
    "CondaExitZero",
    "conda_signal_handler",
    "__copyright__",
)

__name__ = "conda"
__author__ = "Anaconda, Inc."
__email__ = "conda@continuum.io"
__license__ = "BSD-3-Clause"
__copyright__ = "Copyright (c) 2012, Anaconda, Inc."
__summary__ = __doc__
__url__ = "https://github.com/conda/conda"

if os.getenv("CONDA_ROOT") is None:
    os.environ["CONDA_ROOT"] = sys.prefix

#: The conda package directory.
CONDA_PACKAGE_ROOT = abspath(dirname(__file__))
#: The path within which to find the conda package.
#:
#: If `conda` is statically installed this is the site-packages. If `conda` is an editable install
#: or otherwise uninstalled this is the git repo.
CONDA_SOURCE_ROOT = dirname(CONDA_PACKAGE_ROOT)


class CondaError(Exception):
    return_code = 1
    reportable = False  # Exception may be reported to core maintainers

    def __init__(self, message, caused_by=None, **kwargs):
        self.message = message
        self._kwargs = kwargs
        self._caused_by = caused_by
        super().__init__(message)

    def __repr__(self):
        return f"{self.__class__.__name__}: {self}"

    def __str__(self):
        try:
            return str(self.message % self._kwargs)
        except Exception:
            debug_message = "\n".join(
                (
                    "class: " + self.__class__.__name__,
                    "message:",
                    self.message,
                    "kwargs:",
                    str(self._kwargs),
                    "",
                )
            )
            print(debug_message, file=sys.stderr)
            raise

    def dump_map(self):
        result = {k: v for k, v in vars(self).items() if not k.startswith("_")}
        result.update(
            exception_type=str(type(self)),
            exception_name=self.__class__.__name__,
            message=str(self),
            error=repr(self),
            caused_by=repr(self._caused_by),
            **self._kwargs,
        )
        return result


class CondaMultiError(CondaError):
    def __init__(self, errors):
        self.errors = errors
        super().__init__(None)

    def __repr__(self):
        errs = []
        for e in self.errors:
            if isinstance(e, EnvironmentError) and not isinstance(e, CondaError):
                errs.append(str(e))
            else:
                # We avoid Python casting this back to a str()
                # by using e.__repr__() instead of repr(e)
                # https://github.com/scrapy/cssselect/issues/34
                errs.append(e.__repr__())
        res = "\n".join(errs)
        return res

    def __str__(self):
        return "\n".join(str(e) for e in self.errors) + "\n"

    def dump_map(self):
        return dict(
            exception_type=str(type(self)),
            exception_name=self.__class__.__name__,
            errors=tuple(error.dump_map() for error in self.errors),
            error="Multiple Errors Encountered.",
        )

    def contains(self, exception_class):
        return any(isinstance(e, exception_class) for e in self.errors)


class CondaExitZero(CondaError):
    return_code = 0


ACTIVE_SUBPROCESSES = set()


def conda_signal_handler(signum, frame):
    # This function is in the base __init__.py so that it can be monkey-patched by other code
    #   if downstream conda users so choose.  The biggest danger of monkey-patching is that
    #   unlink/link transactions don't get rolled back if interrupted mid-transaction.
    for p in ACTIVE_SUBPROCESSES:
        if p.poll() is None:
            p.send_signal(signum)

    from .exceptions import CondaSignalInterrupt

    raise CondaSignalInterrupt(signum)


def _default(self, obj):
    if isinstance(obj, frozendict):
        return dict(obj)
    if hasattr(obj, "to_json"):
        return obj.to_json()
    return _default.default(obj)


_default.default = JSONEncoder().default
JSONEncoder.default = _default


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Tools interfacing with conda's history file."""

from __future__ import annotations

import codecs
import logging
import os
import re
import sys
import time
import warnings
from ast import literal_eval
from errno import EACCES, EPERM, EROFS
from itertools import islice
from operator import itemgetter
from os.path import isdir, isfile, join
from textwrap import dedent

from . import __version__ as CONDA_VERSION
from .auxlib.ish import dals
from .base.constants import DEFAULTS_CHANNEL_NAME
from .base.context import context
from .common.compat import ensure_text_type, open
from .common.iterators import groupby_to_dict as groupby
from .common.path import paths_equal
from .core.prefix_data import PrefixData
from .exceptions import CondaHistoryError, NotWritableError
from .gateways.disk.update import touch
from .models.dist import dist_str_to_quad
from .models.match_spec import MatchSpec
from .models.version import VersionOrder, version_relation_re

log = logging.getLogger(__name__)


class CondaHistoryWarning(Warning):
    pass


def write_head(fo):
    fo.write("==> {} <==\n".format(time.strftime("%Y-%m-%d %H:%M:%S")))
    fo.write("# cmd: {}\n".format(" ".join(ensure_text_type(s) for s in sys.argv)))
    fo.write(
        "# conda version: {}\n".format(".".join(islice(CONDA_VERSION.split("."), 3)))
    )


def is_diff(content):
    return any(s.startswith(("-", "+")) for s in content)


def pretty_diff(diff):
    added = {}
    removed = {}
    for s in diff:
        fn = s[1:]
        name, version, _, channel = dist_str_to_quad(fn)
        if channel != DEFAULTS_CHANNEL_NAME:
            version += f" ({channel})"
        if s.startswith("-"):
            removed[name.lower()] = version
        elif s.startswith("+"):
            added[name.lower()] = version
    changed = set(added) & set(removed)
    for name in sorted(changed):
        yield f" {name}  {{{removed[name]} -> {added[name]}}}"
    for name in sorted(set(removed) - changed):
        yield f"-{name}-{removed[name]}"
    for name in sorted(set(added) - changed):
        yield f"+{name}-{added[name]}"


def pretty_content(content):
    if is_diff(content):
        return pretty_diff(content)
    else:
        return iter(sorted(content))


class History:
    com_pat = re.compile(r"#\s*cmd:\s*(.+)")
    spec_pat = re.compile(r"#\s*(\w+)\s*specs:\s*(.+)?")
    conda_v_pat = re.compile(r"#\s*conda version:\s*(.+)")

    def __init__(self, prefix):
        self.prefix = prefix
        self.meta_dir = join(prefix, "conda-meta")
        self.path = join(self.meta_dir, "history")

    def __enter__(self):
        self.init_log_file()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.update()

    def init_log_file(self):
        touch(self.path, True)

    def file_is_empty(self):
        return os.stat(self.path).st_size == 0

    def update(self) -> None:
        """Update the history file (creating a new one if necessary)."""
        try:
            try:
                last = set(self.get_state())
            except CondaHistoryError as e:
                warnings.warn(f"Error in {self.path}: {e}", CondaHistoryWarning)
                return
            pd = PrefixData(self.prefix)
            curr = {prefix_rec.dist_str() for prefix_rec in pd.iter_records()}
            self.write_changes(last, curr)
        except OSError as e:
            if e.errno in (EACCES, EPERM, EROFS):
                raise NotWritableError(self.path, e.errno)
            else:
                raise

    def parse(self) -> list[tuple[str, set[str], list[str]]]:
        """Parse the history file.

        Return a list of tuples(datetime strings, set of distributions/diffs, comments).

        Comments appearing before the first section header (e.g. ``==> 2024-01-01 00:00:00 <==``)
        in the history file will be ignored.
        """
        res = []
        if not isfile(self.path):
            return res
        sep_pat = re.compile(r"==>\s*(.+?)\s*<==")
        with open(self.path) as f:
            lines = f.read().splitlines()
        for line in lines:
            line = line.strip()
            if not line:
                continue
            m = sep_pat.match(line)
            if m:
                res.append((m.group(1), set(), []))
            elif line.startswith("#") and res:
                res[-1][2].append(line)
            elif res:
                res[-1][1].add(line)
        return res

    @staticmethod
    def _parse_old_format_specs_string(specs_string):
        """
        Parse specifications string that use conda<4.5 syntax.

        Examples
        --------
          - "param >=1.5.1,<2.0'"
          - "python>=3.5.1,jupyter >=1.0.0,<2.0,matplotlib >=1.5.1,<2.0"
        """
        specs = []
        for spec in specs_string.split(","):
            # If the spec starts with a version qualifier, then it actually belongs to the
            # previous spec. But don't try to join if there was no previous spec.
            if version_relation_re.match(spec) and specs:
                specs[-1] = ",".join([specs[-1], spec])
            else:
                specs.append(spec)
        return specs

    @classmethod
    def _parse_comment_line(cls, line):
        """
        Parse comment lines in the history file.

        These lines can be of command type or action type.

        Examples
        --------
          - "# cmd: /scratch/mc3/bin/conda install -c conda-forge param>=1.5.1,<2.0"
          - "# install specs: python>=3.5.1,jupyter >=1.0.0,<2.0,matplotlib >=1.5.1,<2.0"
        """
        item = {}
        m = cls.com_pat.match(line)
        if m:
            argv = m.group(1).split()
            if argv[0].endswith("conda"):
                argv[0] = "conda"
            item["cmd"] = argv

        m = cls.conda_v_pat.match(line)
        if m:
            item["conda_version"] = m.group(1)

        m = cls.spec_pat.match(line)
        if m:
            action, specs_string = m.groups()
            specs_string = specs_string or ""
            item["action"] = action

            if specs_string.startswith("["):
                specs = literal_eval(specs_string)
            elif "[" not in specs_string:
                specs = History._parse_old_format_specs_string(specs_string)

            specs = [spec for spec in specs if spec and not spec.endswith("@")]

            if specs and action in ("update", "install", "create"):
                item["update_specs"] = item["specs"] = specs
            elif specs and action in ("remove", "uninstall"):
                item["remove_specs"] = item["specs"] = specs
            elif specs and action in ("neutered",):
                item["neutered_specs"] = item["specs"] = specs

        return item

    def get_user_requests(self):
        """Return a list of user requested items.

        Each item is a dict with the following keys:
        'date': the date and time running the command
        'cmd': a list of argv of the actual command which was run
        'action': install/remove/update
        'specs': the specs being used
        """
        res = []
        for dt, unused_cont, comments in self.parse():
            item = {"date": dt}
            for line in comments:
                comment_items = self._parse_comment_line(line)
                item.update(comment_items)

            if "cmd" in item:
                res.append(item)

            dists = groupby(itemgetter(0), unused_cont)
            item["unlink_dists"] = dists.get("-", ())
            item["link_dists"] = dists.get("+", ())

        conda_versions_from_history = tuple(
            x["conda_version"] for x in res if "conda_version" in x
        )
        if conda_versions_from_history and not context.allow_conda_downgrades:
            minimum_conda_version = sorted(
                conda_versions_from_history, key=VersionOrder
            )[-1]
            minimum_major_minor = ".".join(islice(minimum_conda_version.split("."), 2))
            current_major_minor = ".".join(islice(CONDA_VERSION.split("."), 2))
            if VersionOrder(current_major_minor) < VersionOrder(minimum_major_minor):
                message = dals(
                    """
                This environment has previously been operated on by a conda version that's newer
                than the conda currently being used. A newer version of conda is required.
                  target environment location: %(target_prefix)s
                  current conda version: %(conda_version)s
                  minimum conda version: %(minimum_version)s
                """
                ) % {
                    "target_prefix": self.prefix,
                    "conda_version": CONDA_VERSION,
                    "minimum_version": minimum_major_minor,
                }
                if not paths_equal(self.prefix, context.root_prefix):
                    message += dedent(
                        """
                    Update conda and try again.
                        $ conda install -p "%(base_prefix)s" "conda>=%(minimum_version)s"
                    """
                    ) % {
                        "base_prefix": context.root_prefix,
                        "minimum_version": minimum_major_minor,
                    }
                message += dedent(
                    """
                To work around this restriction, one can also set the config parameter
                'allow_conda_downgrades' to False at their own risk.
                """
                )

                # TODO: we need to rethink this.  It's fine as a warning to try to get users
                #    to avoid breaking their system.  However, right now it is preventing
                #    normal conda operation after downgrading conda.
                # raise CondaUpgradeError(message)

        return res

    def get_requested_specs_map(self):
        # keys are package names and values are specs
        spec_map = {}
        for request in self.get_user_requests():
            remove_specs = (MatchSpec(spec) for spec in request.get("remove_specs", ()))
            for spec in remove_specs:
                spec_map.pop(spec.name, None)
            update_specs = (MatchSpec(spec) for spec in request.get("update_specs", ()))
            spec_map.update((s.name, s) for s in update_specs)
            # here is where the neutering takes effect, overriding past values
            neutered_specs = (
                MatchSpec(spec) for spec in request.get("neutered_specs", ())
            )
            spec_map.update((s.name, s) for s in neutered_specs)

        # Conda hasn't always been good about recording when specs have been removed from
        # environments.  If the package isn't installed in the current environment, then we
        # shouldn't try to force it here.
        prefix_recs = {_.name for _ in PrefixData(self.prefix).iter_records()}
        return {name: spec for name, spec in spec_map.items() if name in prefix_recs}

    def construct_states(self):
        """Return a list of tuples(datetime strings, set of distributions)."""
        res = []
        cur = set()
        for dt, cont, unused_com in self.parse():
            if not is_diff(cont):
                cur = cont
            else:
                for s in cont:
                    if s.startswith("-"):
                        cur.discard(s[1:])
                    elif s.startswith("+"):
                        cur.add(s[1:])
                    else:
                        raise CondaHistoryError(f"Did not expect: {s}")
            res.append((dt, cur.copy()))
        return res

    def get_state(self, rev=-1):
        """Return the state, i.e. the set of distributions, for a given revision.

        Defaults to latest (which is the same as the current state when
        the log file is up-to-date).

        Returns a list of dist_strs.
        """
        states = self.construct_states()
        if not states:
            return set()
        times, pkgs = zip(*states)
        return pkgs[rev]

    def print_log(self):
        for i, (date, content, unused_com) in enumerate(self.parse()):
            print("%s  (rev %d)" % (date, i))
            for line in pretty_content(content):
                print(f"    {line}")
            print()

    def object_log(self):
        result = []
        for i, (date, content, unused_com) in enumerate(self.parse()):
            # Based on Mateusz's code; provides more details about the
            # history event
            event = {
                "date": date,
                "rev": i,
                "install": [],
                "remove": [],
                "upgrade": [],
                "downgrade": [],
            }
            added = {}
            removed = {}
            if is_diff(content):
                for pkg in content:
                    name, version, build, channel = dist_str_to_quad(pkg[1:])
                    if pkg.startswith("+"):
                        added[name.lower()] = (version, build, channel)
                    elif pkg.startswith("-"):
                        removed[name.lower()] = (version, build, channel)

                changed = set(added) & set(removed)
                for name in sorted(changed):
                    old = removed[name]
                    new = added[name]
                    details = {
                        "old": "-".join((name,) + old),
                        "new": "-".join((name,) + new),
                    }

                    if new > old:
                        event["upgrade"].append(details)
                    else:
                        event["downgrade"].append(details)

                for name in sorted(set(removed) - changed):
                    event["remove"].append("-".join((name,) + removed[name]))

                for name in sorted(set(added) - changed):
                    event["install"].append("-".join((name,) + added[name]))
            else:
                for pkg in sorted(content):
                    event["install"].append(pkg)
            result.append(event)
        return result

    def write_changes(self, last_state, current_state):
        if not isdir(self.meta_dir):
            os.makedirs(self.meta_dir)
        with codecs.open(self.path, mode="ab", encoding="utf-8") as fo:
            write_head(fo)
            for fn in sorted(last_state - current_state):
                fo.write(f"-{fn}\n")
            for fn in sorted(current_state - last_state):
                fo.write(f"+{fn}\n")

    def write_specs(self, remove_specs=(), update_specs=(), neutered_specs=()):
        remove_specs = [str(MatchSpec(s)) for s in remove_specs]
        update_specs = [str(MatchSpec(s)) for s in update_specs]
        neutered_specs = [str(MatchSpec(s)) for s in neutered_specs]
        if any((update_specs, remove_specs, neutered_specs)):
            with codecs.open(self.path, mode="ab", encoding="utf-8") as fh:
                if remove_specs:
                    fh.write(f"# remove specs: {remove_specs}\n")
                if update_specs:
                    fh.write(f"# update specs: {update_specs}\n")
                if neutered_specs:
                    fh.write(f"# neutered specs: {neutered_specs}\n")


if __name__ == "__main__":
    from pprint import pprint

    # Don't use in context manager mode---it augments the history every time
    h = History(sys.prefix)
    pprint(h.get_user_requests())
    print(h.get_requested_specs_map())


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Conda activate and deactivate logic.

Implementation for all shell interface logic exposed via
`conda shell.* [activate|deactivate|reactivate|hook|commands]`. This includes a custom argument
parser, an abstract shell class, and special path handling for Windows.

See conda.cli.main.main_sourced for the entry point into this module.
"""

from __future__ import annotations

import abc
import json
import ntpath
import os
import posixpath
import re
import sys
from logging import getLogger
from os.path import (
    abspath,
    basename,
    dirname,
    exists,
    expanduser,
    expandvars,
    isdir,
    join,
)
from pathlib import Path
from shutil import which
from subprocess import run
from textwrap import dedent
from typing import TYPE_CHECKING

# Since we have to have configuration context here, anything imported by
#   conda.base.context is fair game, but nothing more.
from . import CONDA_PACKAGE_ROOT, CONDA_SOURCE_ROOT
from .auxlib.compat import Utf8NamedTemporaryFile
from .base.constants import (
    CONDA_ENV_VARS_UNSET_VAR,
    PACKAGE_ENV_VARS_DIR,
    PREFIX_STATE_FILE,
)
from .base.context import ROOT_ENV_NAME, context, locate_prefix_by_name
from .common.compat import FILESYSTEM_ENCODING, on_win
from .common.path import paths_equal
from .deprecations import deprecated

if TYPE_CHECKING:
    from collections.abc import Callable, Iterable

log = getLogger(__name__)


class _Activator(metaclass=abc.ABCMeta):
    # Activate and deactivate have three tasks
    #   1. Set and unset environment variables
    #   2. Execute/source activate.d/deactivate.d scripts
    #   3. Update the command prompt
    #
    # Shells should also use 'reactivate' following conda's install, update, and
    #   remove/uninstall commands.
    #
    # All core logic is in build_activate() or build_deactivate(), and is independent of
    # shell type.  Each returns a map containing the keys:
    #   export_vars
    #   unset_var
    #   activate_scripts
    #   deactivate_scripts
    #
    # The value of the CONDA_PROMPT_MODIFIER environment variable holds conda's contribution
    #   to the command prompt.
    #
    # To implement support for a new shell, ideally one would only need to add shell-specific
    # information to the __init__ method of this class.

    # The following instance variables must be defined by each implementation.
    pathsep_join: str
    sep: str
    path_conversion: Callable[
        [str | Iterable[str] | None], str | tuple[str, ...] | None
    ]
    script_extension: str
    #: temporary file's extension, None writes to stdout instead
    tempfile_extension: str | None
    command_join: str

    unset_var_tmpl: str
    export_var_tmpl: str
    set_var_tmpl: str
    run_script_tmpl: str

    hook_source_path: Path | None

    def __init__(self, arguments=None):
        self._raw_arguments = arguments

    def get_export_unset_vars(self, export_metavars=True, **kwargs):
        """
        :param export_metavars: whether to export `conda_exe_vars` meta variables.
        :param kwargs: environment variables to export.
            .. if you pass and set any other variable to None, then it
            emits it to the dict with a value of None.

        :return: A dict of env vars to export ordered the same way as kwargs.
            And a list of env vars to unset.
        """
        unset_vars = []
        export_vars = {}

        # split provided environment variables into exports vs unsets
        for name, value in kwargs.items():
            if value is None:
                if context.envvars_force_uppercase:
                    unset_vars.append(name.upper())
                else:
                    unset_vars.append(name)

            else:
                if context.envvars_force_uppercase:
                    export_vars[name.upper()] = value
                else:
                    export_vars[name] = value

        if export_metavars:
            # split meta variables into exports vs unsets
            for name, value in context.conda_exe_vars_dict.items():
                if value is None:
                    if context.envvars_force_uppercase:
                        unset_vars.append(name.upper())
                    else:
                        unset_vars.append(name)
                elif "/" in value or "\\" in value:
                    if context.envvars_force_uppercase:
                        export_vars[name.upper()] = self.path_conversion(value)
                    else:
                        export_vars[name] = self.path_conversion(value)
                else:
                    if context.envvars_force_uppercase:
                        export_vars[name.upper()] = value
                    else:
                        export_vars[name] = value
        else:
            # unset all meta variables
            unset_vars.extend(context.conda_exe_vars_dict)

        return export_vars, unset_vars

    @deprecated(
        "24.9",
        "25.3",
        addendum="Use `conda.activate._Activator.get_export_unset_vars` instead.",
    )
    def add_export_unset_vars(self, export_vars, unset_vars, **kwargs):
        new_export_vars, new_unset_vars = self.get_export_unset_vars(**kwargs)
        return {
            {**(export_vars or {}), **new_export_vars},
            [*(unset_vars or []), *new_unset_vars],
        }

    @deprecated("24.9", "25.3", addendum="For testing only. Moved to test suite.")
    def get_scripts_export_unset_vars(self, **kwargs) -> tuple[str, str]:
        export_vars, unset_vars = self.get_export_unset_vars(**kwargs)
        return (
            self.command_join.join(
                self.export_var_tmpl % (k, v) for k, v in (export_vars or {}).items()
            ),
            self.command_join.join(
                self.unset_var_tmpl % (k) for k in (unset_vars or [])
            ),
        )

    def _finalize(self, commands, ext):
        commands = (*commands, "")  # add terminating newline
        if ext is None:
            return self.command_join.join(commands)
        elif ext:
            with Utf8NamedTemporaryFile("w+", suffix=ext, delete=False) as tf:
                # the default mode is 'w+b', and universal new lines don't work in that mode
                # command_join should account for that
                tf.write(self.command_join.join(commands))
            return tf.name
        else:
            raise NotImplementedError()

    def activate(self):
        if self.stack:
            builder_result = self.build_stack(self.env_name_or_prefix)
        else:
            builder_result = self.build_activate(self.env_name_or_prefix)
        return self._finalize(
            self._yield_commands(builder_result), self.tempfile_extension
        )

    def deactivate(self):
        return self._finalize(
            self._yield_commands(self.build_deactivate()), self.tempfile_extension
        )

    def reactivate(self):
        return self._finalize(
            self._yield_commands(self.build_reactivate()), self.tempfile_extension
        )

    def hook(self, auto_activate_base: bool | None = None) -> str:
        builder: list[str] = []
        if preamble := self._hook_preamble():
            builder.append(preamble)
        if self.hook_source_path:
            builder.append(self.hook_source_path.read_text())
        if (
            auto_activate_base is None
            and context.auto_activate_base
            or auto_activate_base
        ):
            builder.append("conda activate base\n")
        postamble = self._hook_postamble()
        if postamble is not None:
            builder.append(postamble)
        return "\n".join(builder)

    def execute(self):
        # return value meant to be written to stdout
        self._parse_and_set_args(self._raw_arguments)
        return getattr(self, self.command)()

    def commands(self):
        """
        Returns a list of possible subcommands that are valid
        immediately following `conda` at the command line.
        This method is generally only used by tab-completion.
        """
        # Import locally to reduce impact on initialization time.
        from .cli.conda_argparse import find_builtin_commands, generate_parser
        from .cli.find_commands import find_commands

        # return value meant to be written to stdout
        # Hidden commands to provide metadata to shells.
        return "\n".join(
            sorted(
                find_builtin_commands(generate_parser()) + tuple(find_commands(True))
            )
        )

    @abc.abstractmethod
    def _hook_preamble(self) -> str | None:
        # must be implemented in subclass
        raise NotImplementedError

    def _hook_postamble(self) -> str | None:
        return None

    def _parse_and_set_args(self, arguments):
        def raise_invalid_command_error(actual_command=None):
            from .exceptions import ArgumentError

            message = (
                "'activate', 'deactivate', 'hook', 'commands', or 'reactivate' "
                "command must be given"
            )
            if actual_command:
                message += f". Instead got '{actual_command}'."
            raise ArgumentError(message)

        if arguments is None or len(arguments) < 1:
            raise_invalid_command_error()

        command, *arguments = arguments
        help_flags = ("-h", "--help", "/?")
        non_help_args = tuple(arg for arg in arguments if arg not in help_flags)
        help_requested = len(arguments) != len(non_help_args)
        remainder_args = list(arg for arg in non_help_args if arg and arg != command)

        if not command:
            raise_invalid_command_error()
        elif help_requested:
            from .exceptions import ActivateHelp, DeactivateHelp, GenericHelp

            help_classes = {
                "activate": ActivateHelp(),
                "deactivate": DeactivateHelp(),
                "hook": GenericHelp("hook"),
                "commands": GenericHelp("commands"),
                "reactivate": GenericHelp("reactivate"),
            }
            raise help_classes[command]
        elif command not in (
            "activate",
            "deactivate",
            "reactivate",
            "hook",
            "commands",
        ):
            raise_invalid_command_error(actual_command=command)

        if command.endswith("activate") or command == "hook":
            try:
                dev_idx = remainder_args.index("--dev")
            except ValueError:
                context.dev = False
            else:
                del remainder_args[dev_idx]
                context.dev = True

        if command == "activate":
            self.stack = context.auto_stack and context.shlvl <= context.auto_stack
            try:
                stack_idx = remainder_args.index("--stack")
            except ValueError:
                stack_idx = -1
            try:
                no_stack_idx = remainder_args.index("--no-stack")
            except ValueError:
                no_stack_idx = -1
            if stack_idx >= 0 and no_stack_idx >= 0:
                from .exceptions import ArgumentError

                raise ArgumentError(
                    "cannot specify both --stack and --no-stack to " + command
                )
            if stack_idx >= 0:
                self.stack = True
                del remainder_args[stack_idx]
            if no_stack_idx >= 0:
                self.stack = False
                del remainder_args[no_stack_idx]
            if len(remainder_args) > 1:
                from .exceptions import ArgumentError

                raise ArgumentError(
                    command
                    + " does not accept more than one argument:\n"
                    + str(remainder_args)
                    + "\n"
                )
            self.env_name_or_prefix = remainder_args and remainder_args[0] or "base"

        else:
            if remainder_args:
                from .exceptions import ArgumentError

                raise ArgumentError(
                    f"{command} does not accept arguments\nremainder_args: {remainder_args}\n"
                )

        self.command = command

    def _yield_commands(self, cmds_dict):
        for key, value in sorted(cmds_dict.get("export_path", {}).items()):
            yield self.export_var_tmpl % (key, value)

        for script in cmds_dict.get("deactivate_scripts", ()):
            yield self.run_script_tmpl % script

        for key in cmds_dict.get("unset_vars", ()):
            yield self.unset_var_tmpl % key

        for key, value in cmds_dict.get("set_vars", {}).items():
            yield self.set_var_tmpl % (key, value)

        for key, value in cmds_dict.get("export_vars", {}).items():
            yield self.export_var_tmpl % (key, value)

        for script in cmds_dict.get("activate_scripts", ()):
            yield self.run_script_tmpl % script

    def build_activate(self, env_name_or_prefix):
        return self._build_activate_stack(env_name_or_prefix, False)

    def build_stack(self, env_name_or_prefix):
        return self._build_activate_stack(env_name_or_prefix, True)

    def _build_activate_stack(self, env_name_or_prefix, stack):
        # get environment prefix
        if re.search(r"\\|/", env_name_or_prefix):
            prefix = expand(env_name_or_prefix)
            if not isdir(join(prefix, "conda-meta")):
                from .exceptions import EnvironmentLocationNotFound

                raise EnvironmentLocationNotFound(prefix)
        elif env_name_or_prefix in (ROOT_ENV_NAME, "root"):
            prefix = context.root_prefix
        else:
            prefix = locate_prefix_by_name(env_name_or_prefix)

        # get prior shlvl and prefix
        old_conda_shlvl = int(os.getenv("CONDA_SHLVL", "").strip() or 0)
        old_conda_prefix = os.getenv("CONDA_PREFIX")

        # if the prior active prefix is this prefix we are actually doing a reactivate
        if old_conda_prefix == prefix and old_conda_shlvl > 0:
            return self.build_reactivate()

        activate_scripts = self._get_activate_scripts(prefix)
        conda_shlvl = old_conda_shlvl + 1
        conda_default_env = self._default_env(prefix)
        conda_prompt_modifier = self._prompt_modifier(prefix, conda_default_env)
        env_vars = {
            name: value
            for name, value in self._get_environment_env_vars(prefix).items()
            if value != CONDA_ENV_VARS_UNSET_VAR
        }

        # get clobbered environment variables
        clobber_vars = set(env_vars).intersection(os.environ)
        overwritten_clobber_vars = [
            clobber_var
            for clobber_var in clobber_vars
            if os.getenv(clobber_var) != env_vars[clobber_var]
        ]
        if overwritten_clobber_vars:
            print(
                "WARNING: overwriting environment variables set in the machine",
                file=sys.stderr,
            )
            print(f"overwriting variable {overwritten_clobber_vars}", file=sys.stderr)
        for name in clobber_vars:
            env_vars[f"__CONDA_SHLVL_{old_conda_shlvl}_{name}"] = os.getenv(name)

        if old_conda_shlvl == 0:
            export_vars, unset_vars = self.get_export_unset_vars(
                path=self.pathsep_join(self._add_prefix_to_path(prefix)),
                conda_prefix=prefix,
                conda_shlvl=conda_shlvl,
                conda_default_env=conda_default_env,
                conda_prompt_modifier=conda_prompt_modifier,
                **env_vars,
            )
            deactivate_scripts = ()
        elif stack:
            export_vars, unset_vars = self.get_export_unset_vars(
                path=self.pathsep_join(self._add_prefix_to_path(prefix)),
                conda_prefix=prefix,
                conda_shlvl=conda_shlvl,
                conda_default_env=conda_default_env,
                conda_prompt_modifier=conda_prompt_modifier,
                **env_vars,
                **{
                    f"CONDA_PREFIX_{old_conda_shlvl}": old_conda_prefix,
                    f"CONDA_STACKED_{conda_shlvl}": "true",
                },
            )
            deactivate_scripts = ()
        else:
            export_vars, unset_vars = self.get_export_unset_vars(
                path=self.pathsep_join(
                    self._replace_prefix_in_path(old_conda_prefix, prefix)
                ),
                conda_prefix=prefix,
                conda_shlvl=conda_shlvl,
                conda_default_env=conda_default_env,
                conda_prompt_modifier=conda_prompt_modifier,
                **env_vars,
                **{
                    f"CONDA_PREFIX_{old_conda_shlvl}": old_conda_prefix,
                },
            )
            deactivate_scripts = self._get_deactivate_scripts(old_conda_prefix)

        set_vars = {}
        if context.changeps1:
            self._update_prompt(set_vars, conda_prompt_modifier)

        return {
            "unset_vars": unset_vars,
            "set_vars": set_vars,
            "export_vars": export_vars,
            "deactivate_scripts": deactivate_scripts,
            "activate_scripts": activate_scripts,
        }

    def build_deactivate(self):
        self._deactivate = True
        # query environment
        old_conda_prefix = os.getenv("CONDA_PREFIX")
        old_conda_shlvl = int(os.getenv("CONDA_SHLVL", "").strip() or 0)
        if not old_conda_prefix or old_conda_shlvl < 1:
            # no active environment, so cannot deactivate; do nothing
            return {
                "unset_vars": (),
                "set_vars": {},
                "export_vars": {},
                "deactivate_scripts": (),
                "activate_scripts": (),
            }
        deactivate_scripts = self._get_deactivate_scripts(old_conda_prefix)
        old_conda_environment_env_vars = self._get_environment_env_vars(
            old_conda_prefix
        )

        new_conda_shlvl = old_conda_shlvl - 1
        set_vars = {}
        if old_conda_shlvl == 1:
            new_path = self.pathsep_join(
                self._remove_prefix_from_path(old_conda_prefix)
            )
            # You might think that you can remove the CONDA_EXE vars with export_metavars=False
            # here so that "deactivate means deactivate" but you cannot since the conda shell
            # scripts still refer to them and they only set them once at the top. We could change
            # that though, the conda() shell function could set them instead of doing it at the
            # top.  This would be *much* cleaner. I personally cannot abide that I have
            # deactivated conda and anything at all in my env still references it (apart from the
            # shell script, we need something I suppose!)
            export_vars, unset_vars = self.get_export_unset_vars(
                conda_prefix=None,
                conda_shlvl=new_conda_shlvl,
                conda_default_env=None,
                conda_prompt_modifier=None,
            )
            conda_prompt_modifier = ""
            activate_scripts = ()
            export_path = {
                "PATH": new_path,
            }
        else:
            assert old_conda_shlvl > 1
            new_prefix = os.getenv("CONDA_PREFIX_%d" % new_conda_shlvl)
            conda_default_env = self._default_env(new_prefix)
            conda_prompt_modifier = self._prompt_modifier(new_prefix, conda_default_env)
            new_conda_environment_env_vars = self._get_environment_env_vars(new_prefix)

            old_prefix_stacked = "CONDA_STACKED_%d" % old_conda_shlvl in os.environ
            new_path = ""

            unset_vars = ["CONDA_PREFIX_%d" % new_conda_shlvl]
            if old_prefix_stacked:
                new_path = self.pathsep_join(
                    self._remove_prefix_from_path(old_conda_prefix)
                )
                unset_vars.append("CONDA_STACKED_%d" % old_conda_shlvl)
            else:
                new_path = self.pathsep_join(
                    self._replace_prefix_in_path(old_conda_prefix, new_prefix)
                )

            export_vars, unset_vars2 = self.get_export_unset_vars(
                conda_prefix=new_prefix,
                conda_shlvl=new_conda_shlvl,
                conda_default_env=conda_default_env,
                conda_prompt_modifier=conda_prompt_modifier,
                **new_conda_environment_env_vars,
            )
            unset_vars += unset_vars2
            export_path = {
                "PATH": new_path,
            }
            activate_scripts = self._get_activate_scripts(new_prefix)

        if context.changeps1:
            self._update_prompt(set_vars, conda_prompt_modifier)

        for env_var in old_conda_environment_env_vars.keys():
            unset_vars.append(env_var)
            save_var = f"__CONDA_SHLVL_{new_conda_shlvl}_{env_var}"
            if save_value := os.getenv(save_var):
                export_vars[env_var] = save_value
        return {
            "unset_vars": unset_vars,
            "set_vars": set_vars,
            "export_vars": export_vars,
            "export_path": export_path,
            "deactivate_scripts": deactivate_scripts,
            "activate_scripts": activate_scripts,
        }

    def build_reactivate(self):
        self._reactivate = True
        conda_prefix = os.getenv("CONDA_PREFIX")
        conda_shlvl = int(os.getenv("CONDA_SHLVL", "").strip() or 0)
        if not conda_prefix or conda_shlvl < 1:
            # no active environment, so cannot reactivate; do nothing
            return {
                "unset_vars": (),
                "set_vars": {},
                "export_vars": {},
                "deactivate_scripts": (),
                "activate_scripts": (),
            }
        conda_default_env = os.getenv(
            "CONDA_DEFAULT_ENV", self._default_env(conda_prefix)
        )
        new_path = self.pathsep_join(
            self._replace_prefix_in_path(conda_prefix, conda_prefix)
        )
        set_vars = {}
        conda_prompt_modifier = self._prompt_modifier(conda_prefix, conda_default_env)
        if context.changeps1:
            self._update_prompt(set_vars, conda_prompt_modifier)

        env_vars_to_unset = ()
        env_vars_to_export = {
            "PATH": new_path,
            "CONDA_SHLVL": conda_shlvl,
            "CONDA_PROMPT_MODIFIER": self._prompt_modifier(
                conda_prefix, conda_default_env
            ),
        }
        conda_environment_env_vars = self._get_environment_env_vars(conda_prefix)
        for k, v in conda_environment_env_vars.items():
            if v == CONDA_ENV_VARS_UNSET_VAR:
                env_vars_to_unset = env_vars_to_unset + (k,)
            else:
                env_vars_to_export[k] = v
        # environment variables are set only to aid transition from conda 4.3 to conda 4.4
        return {
            "unset_vars": env_vars_to_unset,
            "set_vars": set_vars,
            "export_vars": env_vars_to_export,
            "deactivate_scripts": self._get_deactivate_scripts(conda_prefix),
            "activate_scripts": self._get_activate_scripts(conda_prefix),
        }

    def _get_starting_path_list(self):
        # For isolation, running the conda test suite *without* env. var. inheritance
        # every so often is a good idea. We should probably make this a pytest fixture
        # along with one that tests both hardlink-only and copy-only, but before that
        # conda's testsuite needs to be a lot faster!
        clean_paths = {
            "darwin": "/usr/bin:/bin:/usr/sbin:/sbin",
            # You may think 'let us do something more clever here and interpolate
            # `%windir%`' but the point here is the the whole env. is cleaned out
            "win32": "C:\\Windows\\system32;"
            "C:\\Windows;"
            "C:\\Windows\\System32\\Wbem;"
            "C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\",
        }
        path = os.getenv(
            "PATH",
            clean_paths[sys.platform] if sys.platform in clean_paths else "/usr/bin",
        )
        path_split = path.split(os.pathsep)
        return path_split

    @deprecated.argument("24.9", "25.3", "extra_library_bin")
    def _get_path_dirs(self, prefix):
        if on_win:  # pragma: unix no cover
            yield prefix.rstrip("\\")

            # We need to stat(2) for possible environments because
            # tests can't be told where to look!
            #
            # mingw-w64 is a legacy variant used by m2w64-* packages
            #
            # We could include clang32 and mingw32 variants
            variants = []
            for variant in ["ucrt64", "clang64", "mingw64", "clangarm64"]:
                path = self.sep.join((prefix, "Library", variant))

                # MSYS2 /c/
                # cygwin /cygdrive/c/
                if re.match("^(/[A-Za-z]/|/cygdrive/[A-Za-z]/).*", prefix):
                    path = unix_path_to_native(path, prefix)

                if isdir(path):
                    variants.append(variant)

            if len(variants) > 1:
                print(
                    f"WARNING: {prefix}: {variants} MSYS2 envs exist: please check your dependencies",
                    file=sys.stderr,
                )
                print(
                    f"WARNING: conda list -n {self._default_env(prefix)}",
                    file=sys.stderr,
                )

            if variants:
                yield self.sep.join((prefix, "Library", variants[0], "bin"))

            yield self.sep.join((prefix, "Library", "mingw-w64", "bin"))
            yield self.sep.join((prefix, "Library", "usr", "bin"))
            yield self.sep.join((prefix, "Library", "bin"))
            yield self.sep.join((prefix, "Scripts"))
            yield self.sep.join((prefix, "bin"))
        else:
            yield self.sep.join((prefix, "bin"))

    def _add_prefix_to_path(self, prefix, starting_path_dirs=None):
        prefix = self.path_conversion(prefix)
        if starting_path_dirs is None:
            path_list = list(self.path_conversion(self._get_starting_path_list()))
        else:
            path_list = list(self.path_conversion(starting_path_dirs))

        # If this is the first time we're activating an environment, we need to ensure that
        # the condabin directory is included in the path list.
        # Under normal conditions, if the shell hook is working correctly, this should
        # never trigger.
        old_conda_shlvl = int(os.getenv("CONDA_SHLVL", "").strip() or 0)
        if not old_conda_shlvl and not any(p.endswith("condabin") for p in path_list):
            condabin_dir = self.path_conversion(join(context.conda_prefix, "condabin"))
            path_list.insert(0, condabin_dir)

        path_list[0:0] = list(self.path_conversion(self._get_path_dirs(prefix)))
        return tuple(path_list)

    def _remove_prefix_from_path(self, prefix, starting_path_dirs=None):
        return self._replace_prefix_in_path(prefix, None, starting_path_dirs)

    def _replace_prefix_in_path(self, old_prefix, new_prefix, starting_path_dirs=None):
        old_prefix = self.path_conversion(old_prefix)
        new_prefix = self.path_conversion(new_prefix)
        if starting_path_dirs is None:
            path_list = list(self.path_conversion(self._get_starting_path_list()))
        else:
            path_list = list(self.path_conversion(starting_path_dirs))

        def index_of_path(paths, test_path):
            for q, path in enumerate(paths):
                if paths_equal(path, test_path):
                    return q
            return None

        if old_prefix is not None:
            prefix_dirs = tuple(self._get_path_dirs(old_prefix))
            first_idx = index_of_path(path_list, prefix_dirs[0])
            if first_idx is None:
                first_idx = 0
            else:
                prefix_dirs_idx = len(prefix_dirs) - 1
                last_idx = None
                while last_idx is None and prefix_dirs_idx > -1:
                    last_idx = index_of_path(path_list, prefix_dirs[prefix_dirs_idx])
                    if last_idx is None:
                        print(
                            f"Did not find path entry {prefix_dirs[prefix_dirs_idx]}",
                            file=sys.stderr,
                        )
                    prefix_dirs_idx = prefix_dirs_idx - 1
                # this compensates for an extra Library/bin dir entry from the interpreter on
                #     windows.  If that entry isn't being added, it should have no effect.
                library_bin_dir = self.path_conversion(
                    self.sep.join((sys.prefix, "Library", "bin"))
                )
                if path_list[last_idx + 1] == library_bin_dir:
                    last_idx += 1
                del path_list[first_idx : last_idx + 1]
        else:
            first_idx = 0

        if new_prefix is not None:
            path_list[first_idx:first_idx] = list(self._get_path_dirs(new_prefix))

        return tuple(path_list)

    def _update_prompt(self, set_vars, conda_prompt_modifier):
        pass

    def _default_env(self, prefix):
        if paths_equal(prefix, context.root_prefix):
            return "base"
        return basename(prefix) if basename(dirname(prefix)) == "envs" else prefix

    def _prompt_modifier(self, prefix, conda_default_env):
        if context.changeps1:
            # Get current environment and prompt stack
            env_stack = []
            prompt_stack = []
            old_shlvl = int(os.getenv("CONDA_SHLVL", "0").rstrip())
            for i in range(1, old_shlvl + 1):
                if i == old_shlvl:
                    env_i = self._default_env(os.getenv("CONDA_PREFIX", ""))
                else:
                    env_i = self._default_env(
                        os.getenv(f"CONDA_PREFIX_{i}", "").rstrip()
                    )
                stacked_i = bool(os.getenv(f"CONDA_STACKED_{i}", "").rstrip())
                env_stack.append(env_i)
                if not stacked_i:
                    prompt_stack = prompt_stack[0:-1]
                prompt_stack.append(env_i)

            # Modify prompt stack according to pending operation
            deactivate = getattr(self, "_deactivate", False)
            reactivate = getattr(self, "_reactivate", False)
            if deactivate:
                prompt_stack = prompt_stack[0:-1]
                env_stack = env_stack[0:-1]
                stacked = bool(os.getenv(f"CONDA_STACKED_{old_shlvl}", "").rstrip())
                if not stacked and env_stack:
                    prompt_stack.append(env_stack[-1])
            elif reactivate:
                pass
            else:
                stack = getattr(self, "stack", False)
                if not stack:
                    prompt_stack = prompt_stack[0:-1]
                prompt_stack.append(conda_default_env)

            conda_stacked_env = ",".join(prompt_stack[::-1])

            return context.env_prompt.format(
                default_env=conda_default_env,
                stacked_env=conda_stacked_env,
                prefix=prefix,
                name=basename(prefix),
            )
        else:
            return ""

    def _get_activate_scripts(self, prefix):
        _script_extension = self.script_extension
        se_len = -len(_script_extension)
        try:
            paths = (
                entry.path
                for entry in os.scandir(join(prefix, "etc", "conda", "activate.d"))
            )
        except OSError:
            return ()
        return self.path_conversion(
            sorted(p for p in paths if p[se_len:] == _script_extension)
        )

    def _get_deactivate_scripts(self, prefix):
        _script_extension = self.script_extension
        se_len = -len(_script_extension)
        try:
            paths = (
                entry.path
                for entry in os.scandir(join(prefix, "etc", "conda", "deactivate.d"))
            )
        except OSError:
            return ()
        return self.path_conversion(
            sorted((p for p in paths if p[se_len:] == _script_extension), reverse=True)
        )

    def _get_environment_env_vars(self, prefix):
        env_vars_file = join(prefix, PREFIX_STATE_FILE)
        pkg_env_var_dir = join(prefix, PACKAGE_ENV_VARS_DIR)
        env_vars = {}

        # First get env vars from packages
        if exists(pkg_env_var_dir):
            for pkg_env_var_path in sorted(
                entry.path for entry in os.scandir(pkg_env_var_dir)
            ):
                with open(pkg_env_var_path) as f:
                    env_vars.update(json.loads(f.read()))

        # Then get env vars from environment specification
        if exists(env_vars_file):
            with open(env_vars_file) as f:
                prefix_state = json.loads(f.read())
                prefix_state_env_vars = prefix_state.get("env_vars", {})
                dup_vars = [
                    ev for ev in env_vars.keys() if ev in prefix_state_env_vars.keys()
                ]
                for dup in dup_vars:
                    print(
                        "WARNING: duplicate env vars detected. Vars from the environment "
                        "will overwrite those from packages",
                        file=sys.stderr,
                    )
                    print(f"variable {dup} duplicated", file=sys.stderr)
                env_vars.update(prefix_state_env_vars)

        return env_vars


def expand(path):
    return abspath(expanduser(expandvars(path)))


def ensure_binary(value):
    try:
        return value.encode("utf-8")
    except AttributeError:  # pragma: no cover
        # AttributeError: '<>' object has no attribute 'encode'
        # In this case assume already binary type and do nothing
        return value


def ensure_fs_path_encoding(value):
    try:
        return value.decode(FILESYSTEM_ENCODING)
    except AttributeError:
        return value


class _Cygpath:
    @classmethod
    def nt_to_posix(cls, paths: str) -> str:
        return cls.RE_UNIX.sub(cls.translate_unix, paths).replace(
            ntpath.pathsep, posixpath.pathsep
        )

    RE_UNIX = re.compile(
        r"""
        (?P<drive>[A-Za-z]:)?
        (?P<path>[\/\\]+(?:[^:*?\"<>|;]+[\/\\]*)*)
        """,
        flags=re.VERBOSE,
    )

    @staticmethod
    def translate_unix(match: re.Match) -> str:
        return "/" + (
            ((match.group("drive") or "").lower() + match.group("path"))
            .replace("\\", "/")
            .replace(":", "")  # remove drive letter delimiter
            .replace("//", "/")
            .rstrip("/")
        )

    @classmethod
    def posix_to_nt(cls, paths: str, prefix: str) -> str:
        if posixpath.sep not in paths:
            # nothing to translate
            return paths

        if posixpath.pathsep in paths:
            return ntpath.pathsep.join(
                cls.posix_to_nt(path, prefix) for path in paths.split(posixpath.pathsep)
            )
        path = paths

        # Reverting a Unix path means unpicking MSYS2/Cygwin
        # conventions -- in order!
        # 1. drive letter forms:
        #      /x/here/there - MSYS2
        #      /cygdrive/x/here/there - Cygwin
        #    transformed to X:\here\there -- note the uppercase drive letter!
        # 2. either:
        #    a. mount forms:
        #         //here/there
        #       transformed to \\here\there
        #    b. root filesystem forms:
        #         /here/there
        #       transformed to {prefix}\Library\here\there
        # 3. anything else

        # continue performing substitutions until a match is found
        path, subs = cls.RE_DRIVE.subn(cls.translation_drive, path)
        if not subs:
            path, subs = cls.RE_MOUNT.subn(cls.translation_mount, path)
        if not subs:
            path, _ = cls.RE_ROOT.subn(
                lambda match: cls.translation_root(match, prefix), path
            )

        return re.sub(r"/+", r"\\", path)

    RE_DRIVE = re.compile(
        r"""
        ^
        (/cygdrive)?
        /(?P<drive>[A-Za-z])
        (/+(?P<path>.*)?)?
        $
        """,
        flags=re.VERBOSE,
    )

    @staticmethod
    def translation_drive(match: re.Match) -> str:
        drive = match.group("drive").upper()
        path = match.group("path") or ""
        return f"{drive}:\\{path}"

    RE_MOUNT = re.compile(
        r"""
        ^
        //(
            (?P<mount>[^/]+)
            (?P<path>/+.*)?
        )?
        $
        """,
        flags=re.VERBOSE,
    )

    @staticmethod
    def translation_mount(match: re.Match) -> str:
        mount = match.group("mount") or ""
        path = match.group("path") or ""
        return f"\\\\{mount}{path}"

    RE_ROOT = re.compile(
        r"""
        ^
        (?P<path>/[^:]*)
        $
        """,
        flags=re.VERBOSE,
    )

    @staticmethod
    def translation_root(match: re.Match, prefix: str) -> str:
        path = match.group("path")
        return f"{prefix}\\Library{path}"


def native_path_to_unix(
    paths: str | Iterable[str] | None,
) -> str | tuple[str, ...] | None:
    if paths is None:
        return None
    elif not on_win:
        return path_identity(paths)

    # short-circuit if we don't get any paths
    paths = paths if isinstance(paths, str) else tuple(paths)
    if not paths:
        return "." if isinstance(paths, str) else ()

    # on windows, uses cygpath to convert windows native paths to posix paths

    # It is very easy to end up with a bash in one place and a cygpath in another due to e.g.
    # using upstream MSYS2 bash, but with a conda env that does not have bash but does have
    # cygpath.  When this happens, we have two different virtual POSIX machines, rooted at
    # different points in the Windows filesystem.  We do our path conversions with one and
    # expect the results to work with the other.  It does not.

    bash = which("bash")
    cygpath = str(Path(bash).parent / "cygpath") if bash else "cygpath"
    joined = paths if isinstance(paths, str) else ntpath.pathsep.join(paths)

    try:
        # if present, use cygpath to convert paths since its more reliable
        unix_path = run(
            [cygpath, "--unix", "--path", joined],
            text=True,
            capture_output=True,
            check=True,
        ).stdout.strip()
    except FileNotFoundError:
        # fallback logic when cygpath is not available
        # i.e. conda without anything else installed
        log.warning("cygpath is not available, fallback to manual path conversion")

        unix_path = _Cygpath.nt_to_posix(joined)
    except Exception as err:
        log.error("Unexpected cygpath error (%s)", err)
        raise

    if isinstance(paths, str):
        return unix_path
    elif not unix_path:
        return ()
    else:
        return tuple(unix_path.split(posixpath.pathsep))


def unix_path_to_native(
    paths: str | Iterable[str] | None, prefix: str
) -> str | tuple[str, ...] | None:
    if paths is None:
        return None
    elif not on_win:
        return path_identity(paths)

    # short-circuit if we don't get any paths
    paths = paths if isinstance(paths, str) else tuple(paths)
    if not paths:
        return "." if isinstance(paths, str) else ()

    # on windows, uses cygpath to convert posix paths to windows native paths

    # It is very easy to end up with a bash in one place and a cygpath in another due to e.g.
    # using upstream MSYS2 bash, but with a conda env that does not have bash but does have
    # cygpath.  When this happens, we have two different virtual POSIX machines, rooted at
    # different points in the Windows filesystem.  We do our path conversions with one and
    # expect the results to work with the other.  It does not.

    bash = which("bash")
    cygpath = str(Path(bash).parent / "cygpath") if bash else "cygpath"
    joined = paths if isinstance(paths, str) else posixpath.pathsep.join(paths)

    try:
        # if present, use cygpath to convert paths since its more reliable
        win_path = run(
            [cygpath, "--windows", "--path", joined],
            text=True,
            capture_output=True,
            check=True,
        ).stdout.strip()
    except FileNotFoundError:
        # fallback logic when cygpath is not available
        # i.e. conda without anything else installed
        log.warning("cygpath is not available, fallback to manual path conversion")

        # The conda prefix can be in a drive letter form
        prefix = _Cygpath.posix_to_nt(prefix, prefix)

        win_path = _Cygpath.posix_to_nt(joined, prefix)
    except Exception as err:
        log.error("Unexpected cygpath error (%s)", err)
        raise

    if isinstance(paths, str):
        return win_path
    elif not win_path:
        return ()
    else:
        return tuple(win_path.split(ntpath.pathsep))


def path_identity(paths: str | Iterable[str] | None) -> str | tuple[str, ...] | None:
    if paths is None:
        return None
    elif isinstance(paths, str):
        return os.path.normpath(paths)
    else:
        return tuple(os.path.normpath(path) for path in paths)


def backslash_to_forwardslash(
    paths: str | Iterable[str] | None,
) -> str | tuple[str, ...] | None:
    if paths is None:
        return None
    elif isinstance(paths, str):
        return paths.replace("\\", "/")
    else:
        return tuple([path.replace("\\", "/") for path in paths])


class PosixActivator(_Activator):
    pathsep_join = ":".join
    sep = "/"
    path_conversion = staticmethod(native_path_to_unix)
    script_extension = ".sh"
    tempfile_extension = None  # output to stdout
    command_join = "\n"

    unset_var_tmpl = "unset %s"
    export_var_tmpl = "export %s='%s'"
    set_var_tmpl = "%s='%s'"
    run_script_tmpl = '. "%s"'

    hook_source_path = Path(
        CONDA_PACKAGE_ROOT,
        "shell",
        "etc",
        "profile.d",
        "conda.sh",
    )

    def _update_prompt(self, set_vars, conda_prompt_modifier):
        ps1 = os.getenv("PS1", "")
        if "POWERLINE_COMMAND" in ps1:
            # Defer to powerline (https://github.com/powerline/powerline) if it's in use.
            return
        current_prompt_modifier = os.getenv("CONDA_PROMPT_MODIFIER")
        if current_prompt_modifier:
            ps1 = re.sub(re.escape(current_prompt_modifier), r"", ps1)
        # Because we're using single-quotes to set shell variables, we need to handle the
        # proper escaping of single quotes that are already part of the string.
        # Best solution appears to be https://stackoverflow.com/a/1250279
        ps1 = ps1.replace("'", "'\"'\"'")
        set_vars.update(
            {
                "PS1": conda_prompt_modifier + ps1,
            }
        )

    def _hook_preamble(self) -> str:
        result = []
        for key, value in context.conda_exe_vars_dict.items():
            if value is None:
                # Using `unset_var_tmpl` would cause issues for people running
                # with shell flag -u set (error on unset).
                result.append(self.export_var_tmpl % (key, ""))
            elif on_win and ("/" in value or "\\" in value):
                result.append(f'''export {key}="$(cygpath '{value}')"''')
            else:
                result.append(self.export_var_tmpl % (key, value))
        return "\n".join(result) + "\n"


class CshActivator(_Activator):
    pathsep_join = ":".join
    sep = "/"
    path_conversion = staticmethod(native_path_to_unix)
    script_extension = ".csh"
    tempfile_extension = None  # output to stdout
    command_join = ";\n"

    unset_var_tmpl = "unsetenv %s"
    export_var_tmpl = 'setenv %s "%s"'
    set_var_tmpl = "set %s='%s'"
    run_script_tmpl = 'source "%s"'

    hook_source_path = Path(
        CONDA_PACKAGE_ROOT,
        "shell",
        "etc",
        "profile.d",
        "conda.csh",
    )

    def _update_prompt(self, set_vars, conda_prompt_modifier):
        prompt = os.getenv("prompt", "")
        current_prompt_modifier = os.getenv("CONDA_PROMPT_MODIFIER")
        if current_prompt_modifier:
            prompt = re.sub(re.escape(current_prompt_modifier), r"", prompt)
        set_vars.update(
            {
                "prompt": conda_prompt_modifier + prompt,
            }
        )

    def _hook_preamble(self) -> str:
        if on_win:
            return dedent(
                f"""
                setenv CONDA_EXE `cygpath {context.conda_exe}`
                setenv _CONDA_ROOT `cygpath {context.conda_prefix}`
                setenv _CONDA_EXE `cygpath {context.conda_exe}`
                setenv CONDA_PYTHON_EXE `cygpath {sys.executable}`
                """
            ).strip()
        else:
            return dedent(
                f"""
                setenv CONDA_EXE "{context.conda_exe}"
                setenv _CONDA_ROOT "{context.conda_prefix}"
                setenv _CONDA_EXE "{context.conda_exe}"
                setenv CONDA_PYTHON_EXE "{sys.executable}"
                """
            ).strip()


class XonshActivator(_Activator):
    pathsep_join = ";".join if on_win else ":".join
    sep = "/"
    path_conversion = staticmethod(
        backslash_to_forwardslash if on_win else path_identity
    )
    # 'scripts' really refer to de/activation scripts, not scripts in the language per se
    # xonsh can piggy-back activation scripts from other languages depending on the platform
    script_extension = ".bat" if on_win else ".sh"
    tempfile_extension = None  # output to stdout
    command_join = "\n"

    unset_var_tmpl = "del $%s"
    export_var_tmpl = "$%s = '%s'"
    # TODO: determine if different than export_var_tmpl
    set_var_tmpl = "$%s = '%s'"
    run_script_tmpl = (
        'source-cmd --suppress-skip-message "%s"'
        if on_win
        else 'source-bash --suppress-skip-message -n "%s"'
    )

    hook_source_path = Path(CONDA_PACKAGE_ROOT, "shell", "conda.xsh")

    def _hook_preamble(self) -> str:
        return f'$CONDA_EXE = "{self.path_conversion(context.conda_exe)}"'


class CmdExeActivator(_Activator):
    pathsep_join = ";".join
    sep = "\\"
    path_conversion = staticmethod(path_identity)
    script_extension = ".bat"
    tempfile_extension = ".bat"
    command_join = "\n"

    unset_var_tmpl = "@SET %s="
    export_var_tmpl = '@SET "%s=%s"'
    # TODO: determine if different than export_var_tmpl
    set_var_tmpl = '@SET "%s=%s"'
    run_script_tmpl = '@CALL "%s"'

    hook_source_path = None

    def _hook_preamble(self) -> None:
        # TODO: cmd.exe doesn't get a hook function? Or do we need to do something different?
        #       Like, for cmd.exe only, put a special directory containing only conda.bat on PATH?
        pass


class FishActivator(_Activator):
    pathsep_join = '" "'.join
    sep = "/"
    path_conversion = staticmethod(native_path_to_unix)
    script_extension = ".fish"
    tempfile_extension = None  # output to stdout
    command_join = ";\n"

    unset_var_tmpl = "set -e %s"
    export_var_tmpl = 'set -gx %s "%s"'
    set_var_tmpl = 'set -g %s "%s"'
    run_script_tmpl = 'source "%s"'

    hook_source_path = Path(
        CONDA_PACKAGE_ROOT,
        "shell",
        "etc",
        "fish",
        "conf.d",
        "conda.fish",
    )

    def _hook_preamble(self) -> str:
        if on_win:
            return dedent(
                f"""
                set -gx CONDA_EXE (cygpath "{context.conda_exe}")
                set _CONDA_ROOT (cygpath "{context.conda_prefix}")
                set _CONDA_EXE (cygpath "{context.conda_exe}")
                set -gx CONDA_PYTHON_EXE (cygpath "{sys.executable}")
                """
            ).strip()
        else:
            return dedent(
                f"""
                set -gx CONDA_EXE "{context.conda_exe}"
                set _CONDA_ROOT "{context.conda_prefix}"
                set _CONDA_EXE "{context.conda_exe}"
                set -gx CONDA_PYTHON_EXE "{sys.executable}"
                """
            ).strip()


class PowerShellActivator(_Activator):
    pathsep_join = ";".join if on_win else ":".join
    sep = "\\" if on_win else "/"
    path_conversion = staticmethod(path_identity)
    script_extension = ".ps1"
    tempfile_extension = None  # output to stdout
    command_join = "\n"

    unset_var_tmpl = '$Env:%s = ""'
    export_var_tmpl = '$Env:%s = "%s"'
    set_var_tmpl = '$Env:%s = "%s"'
    run_script_tmpl = '. "%s"'

    hook_source_path = Path(
        CONDA_PACKAGE_ROOT,
        "shell",
        "condabin",
        "conda-hook.ps1",
    )

    def _hook_preamble(self) -> str:
        if context.dev:
            return dedent(
                f"""
                $Env:PYTHONPATH = "{CONDA_SOURCE_ROOT}"
                $Env:CONDA_EXE = "{sys.executable}"
                $Env:_CE_M = "-m"
                $Env:_CE_CONDA = "conda"
                $Env:_CONDA_ROOT = "{CONDA_PACKAGE_ROOT}"
                $Env:_CONDA_EXE = "{context.conda_exe}"
                $CondaModuleArgs = @{{ChangePs1 = ${context.changeps1}}}
                """
            ).strip()
        else:
            return dedent(
                f"""
                $Env:CONDA_EXE = "{context.conda_exe}"
                $Env:_CE_M = ""
                $Env:_CE_CONDA = ""
                $Env:_CONDA_ROOT = "{context.conda_prefix}"
                $Env:_CONDA_EXE = "{context.conda_exe}"
                $CondaModuleArgs = @{{ChangePs1 = ${context.changeps1}}}
                """
            ).strip()

    def _hook_postamble(self) -> str:
        return "Remove-Variable CondaModuleArgs"


class JSONFormatMixin(_Activator):
    """Returns the necessary values for activation as JSON, so that tools can use them."""

    pathsep_join = list
    tempfile_extension = None  # output to stdout
    command_join = list

    def _hook_preamble(self):
        if context.dev:
            return {
                "PYTHONPATH": CONDA_SOURCE_ROOT,
                "CONDA_EXE": sys.executable,
                "_CE_M": "-m",
                "_CE_CONDA": "conda",
                "_CONDA_ROOT": CONDA_PACKAGE_ROOT,
                "_CONDA_EXE": context.conda_exe,
            }
        else:
            return {
                "CONDA_EXE": context.conda_exe,
                "_CE_M": "",
                "_CE_CONDA": "",
                "_CONDA_ROOT": context.conda_prefix,
                "_CONDA_EXE": context.conda_exe,
            }

    @deprecated(
        "24.9",
        "25.3",
        addendum="Use `conda.activate._Activator.get_export_unset_vars` instead.",
    )
    def get_scripts_export_unset_vars(self, **kwargs):
        export_vars, unset_vars = self.get_export_unset_vars(**kwargs)
        return export_vars or {}, unset_vars or []

    def _finalize(self, commands, ext):
        merged = {}
        for _cmds in commands:
            merged.update(_cmds)

        commands = merged
        if ext is None:
            return json.dumps(commands, indent=2)
        elif ext:
            with Utf8NamedTemporaryFile("w+", suffix=ext, delete=False) as tf:
                # the default mode is 'w+b', and universal new lines don't work in that mode
                # command_join should account for that
                json.dump(commands, tf, indent=2)
            return tf.name
        else:
            raise NotImplementedError()

    def _yield_commands(self, cmds_dict):
        # TODO: _Is_ defining our own object shape here any better than
        # just dumping the `cmds_dict`?
        path = cmds_dict.get("export_path", {})
        export_vars = cmds_dict.get("export_vars", {})
        # treat PATH specially
        if "PATH" in export_vars:
            new_path = path.get("PATH", [])
            new_path.extend(export_vars.pop("PATH"))
            path["PATH"] = new_path

        yield {
            "path": path,
            "vars": {
                "export": export_vars,
                "unset": cmds_dict.get("unset_vars", ()),
                "set": cmds_dict.get("set_vars", {}),
            },
            "scripts": {
                "activate": cmds_dict.get("activate_scripts", ()),
                "deactivate": cmds_dict.get("deactivate_scripts", ()),
            },
        }


activator_map: dict[str, type[_Activator]] = {
    "posix": PosixActivator,
    "ash": PosixActivator,
    "bash": PosixActivator,
    "dash": PosixActivator,
    "zsh": PosixActivator,
    "csh": CshActivator,
    "tcsh": CshActivator,
    "xonsh": XonshActivator,
    "cmd.exe": CmdExeActivator,
    "fish": FishActivator,
    "powershell": PowerShellActivator,
}

formatter_map = {
    "json": JSONFormatMixin,
}


def _build_activator_cls(shell):
    """Dynamically construct the activator class.

    Detect the base activator and any number of formatters (appended using '+' to the base name).
    For example, `posix+json` (as in `conda shell.posix+json activate`) would use the
    `PosixActivator` base class and add the `JSONFormatMixin`.
    """
    shell_etc = shell.split("+")
    activator, formatters = shell_etc[0], shell_etc[1:]

    bases = [activator_map[activator]]
    for f in formatters:
        bases.append(formatter_map[f])

    cls = type("Activator", tuple(reversed(bases)), {})
    return cls


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
This file should hold most string literals and magic numbers used throughout the code base.
The exception is if a literal is specifically meant to be private to and isolated within a module.
Think of this as a "more static" source of configuration information.

Another important source of "static" configuration is conda/models/enums.py.
"""

import struct
from enum import Enum, EnumMeta
from os.path import join

from ..common.compat import on_win, six_with_metaclass

PREFIX_PLACEHOLDER = (
    "/opt/anaconda1anaconda2"
    # this is intentionally split into parts, such that running
    # this program on itself will leave it unchanged
    "anaconda3"
)

machine_bits = 8 * struct.calcsize("P")

APP_NAME = "conda"

if on_win:  # pragma: no cover
    SEARCH_PATH = (
        "C:/ProgramData/conda/.condarc",
        "C:/ProgramData/conda/condarc",
        "C:/ProgramData/conda/condarc.d",
    )
else:
    SEARCH_PATH = (
        "/etc/conda/.condarc",
        "/etc/conda/condarc",
        "/etc/conda/condarc.d/",
        "/var/lib/conda/.condarc",
        "/var/lib/conda/condarc",
        "/var/lib/conda/condarc.d/",
    )

SEARCH_PATH += (
    "$CONDA_ROOT/.condarc",
    "$CONDA_ROOT/condarc",
    "$CONDA_ROOT/condarc.d/",
    "$XDG_CONFIG_HOME/conda/.condarc",
    "$XDG_CONFIG_HOME/conda/condarc",
    "$XDG_CONFIG_HOME/conda/condarc.d/",
    "~/.config/conda/.condarc",
    "~/.config/conda/condarc",
    "~/.config/conda/condarc.d/",
    "~/.conda/.condarc",
    "~/.conda/condarc",
    "~/.conda/condarc.d/",
    "~/.condarc",
    "$CONDA_PREFIX/.condarc",
    "$CONDA_PREFIX/condarc",
    "$CONDA_PREFIX/condarc.d/",
    "$CONDARC",
)

DEFAULT_CHANNEL_ALIAS = "https://conda.anaconda.org"
CONDA_HOMEPAGE_URL = "https://conda.io"
ERROR_UPLOAD_URL = "https://conda.io/conda-post/unexpected-error"
DEFAULTS_CHANNEL_NAME = "defaults"

KNOWN_SUBDIRS = PLATFORM_DIRECTORIES = (
    "noarch",
    "emscripten-wasm32",
    "wasi-wasm32",
    "freebsd-64",
    "linux-32",
    "linux-64",
    "linux-aarch64",
    "linux-armv6l",
    "linux-armv7l",
    "linux-ppc64",
    "linux-ppc64le",
    "linux-riscv64",
    "linux-s390x",
    "osx-64",
    "osx-arm64",
    "win-32",
    "win-64",
    "win-arm64",
    "zos-z",
)

RECOGNIZED_URL_SCHEMES = ("http", "https", "ftp", "s3", "file")


DEFAULT_CHANNELS_UNIX = (
    "https://repo.anaconda.com/pkgs/main",
    "https://repo.anaconda.com/pkgs/r",
)

DEFAULT_CHANNELS_WIN = (
    "https://repo.anaconda.com/pkgs/main",
    "https://repo.anaconda.com/pkgs/r",
    "https://repo.anaconda.com/pkgs/msys2",
)

DEFAULT_CUSTOM_CHANNELS = {
    "pkgs/pro": "https://repo.anaconda.com",
}

DEFAULT_CHANNELS = DEFAULT_CHANNELS_WIN if on_win else DEFAULT_CHANNELS_UNIX

ROOT_ENV_NAME = "base"
UNUSED_ENV_NAME = "unused-env-name"

ROOT_NO_RM = (
    "python",
    "pycosat",
    "ruamel.yaml",
    "conda",
    "openssl",
    "requests",
)

DEFAULT_AGGRESSIVE_UPDATE_PACKAGES = (
    "ca-certificates",
    "certifi",
    "openssl",
)

if on_win:  # pragma: no cover
    COMPATIBLE_SHELLS = (
        "bash",
        "cmd.exe",
        "fish",
        "tcsh",
        "xonsh",
        "zsh",
        "powershell",
    )
else:
    COMPATIBLE_SHELLS = (
        "bash",
        "fish",
        "tcsh",
        "xonsh",
        "zsh",
        "powershell",
    )


# Maximum priority, reserved for packages we really want to remove
MAX_CHANNEL_PRIORITY = 10000

CONDA_PACKAGE_EXTENSION_V1 = ".tar.bz2"
CONDA_PACKAGE_EXTENSION_V2 = ".conda"
CONDA_PACKAGE_EXTENSIONS = (
    CONDA_PACKAGE_EXTENSION_V2,
    CONDA_PACKAGE_EXTENSION_V1,
)
CONDA_PACKAGE_PARTS = tuple(f"{ext}.part" for ext in CONDA_PACKAGE_EXTENSIONS)
CONDA_TARBALL_EXTENSION = CONDA_PACKAGE_EXTENSION_V1  # legacy support for conda-build
CONDA_TEMP_EXTENSION = ".c~"
CONDA_TEMP_EXTENSIONS = (CONDA_TEMP_EXTENSION, ".trash")
CONDA_LOGS_DIR = ".logs"

UNKNOWN_CHANNEL = "<unknown>"
REPODATA_FN = "repodata.json"

#: Default name of the notices file on the server we look for
NOTICES_FN = "notices.json"

#: Name of cache file where read notice IDs are stored
NOTICES_CACHE_FN = "notices.cache"

#: Determines the subdir for notices cache
NOTICES_CACHE_SUBDIR = "notices"

#: Determines the subdir for notices cache
NOTICES_DECORATOR_DISPLAY_INTERVAL = 86400  # in seconds

DRY_RUN_PREFIX = "Dry run action:"
PREFIX_NAME_DISALLOWED_CHARS = {"/", " ", ":", "#"}


class SafetyChecks(Enum):
    disabled = "disabled"
    warn = "warn"
    enabled = "enabled"

    def __str__(self):
        return self.value


class PathConflict(Enum):
    clobber = "clobber"
    warn = "warn"
    prevent = "prevent"

    def __str__(self):
        return self.value


class DepsModifier(Enum):
    """Flags to enable alternate handling of dependencies."""

    NOT_SET = "not_set"  # default
    NO_DEPS = "no_deps"
    ONLY_DEPS = "only_deps"

    def __str__(self):
        return self.value


class UpdateModifier(Enum):
    SPECS_SATISFIED_SKIP_SOLVE = "specs_satisfied_skip_solve"
    FREEZE_INSTALLED = (
        "freeze_installed"  # freeze is a better name for --no-update-deps
    )
    UPDATE_DEPS = "update_deps"
    UPDATE_SPECS = "update_specs"  # default
    UPDATE_ALL = "update_all"
    # TODO: add REINSTALL_ALL, see https://github.com/conda/conda/issues/6247 and https://github.com/conda/conda/issues/3149  # NOQA

    def __str__(self):
        return self.value


class ChannelPriorityMeta(EnumMeta):
    def __call__(cls, value, *args, **kwargs):
        try:
            return super().__call__(value, *args, **kwargs)
        except ValueError:
            if isinstance(value, str):
                from ..auxlib.type_coercion import typify

                value = typify(value)
            if value is True:
                value = "flexible"
            elif value is False:
                value = cls.DISABLED
            return super().__call__(value, *args, **kwargs)


class ValueEnum(Enum):
    """Subclass of enum that returns the value of the enum as its str representation"""

    def __str__(self):
        return f"{self.value}"


class ChannelPriority(six_with_metaclass(ChannelPriorityMeta, ValueEnum)):
    __name__ = "ChannelPriority"

    STRICT = "strict"
    # STRICT_OR_FLEXIBLE = 'strict_or_flexible'  # TODO: consider implementing if needed
    FLEXIBLE = "flexible"
    DISABLED = "disabled"


class SatSolverChoice(ValueEnum):
    PYCOSAT = "pycosat"
    PYCRYPTOSAT = "pycryptosat"
    PYSAT = "pysat"


#: The name of the default solver, currently "libmamba"
DEFAULT_SOLVER = "libmamba"
CLASSIC_SOLVER = "classic"


class NoticeLevel(ValueEnum):
    CRITICAL = "critical"
    WARNING = "warning"
    INFO = "info"


# Magic files for permissions determination
PACKAGE_CACHE_MAGIC_FILE = "urls.txt"
PREFIX_MAGIC_FILE = join("conda-meta", "history")

PREFIX_STATE_FILE = join("conda-meta", "state")
PACKAGE_ENV_VARS_DIR = join("etc", "conda", "env_vars.d")
CONDA_ENV_VARS_UNSET_VAR = "***unset***"


# TODO: should be frozendict(), but I don't want to import frozendict from auxlib here.
NAMESPACES_MAP = {  # base package name, namespace
    "python": "python",
    "r": "r",
    "r-base": "r",
    "mro-base": "r",
    "erlang": "erlang",
    "java": "java",
    "openjdk": "java",
    "julia": "julia",
    "latex": "latex",
    "lua": "lua",
    "nodejs": "js",
    "perl": "perl",
    "php": "php",
    "ruby": "ruby",
    "m2-base": "m2",
    "msys2-conda-epoch": "m2w64",
}

NAMESPACE_PACKAGE_NAMES = frozenset(NAMESPACES_MAP)
NAMESPACES = frozenset(NAMESPACES_MAP.values())

# Namespace arbiters of uniqueness
#  global: some repository established by Anaconda, Inc. and conda-forge
#  python: https://pypi.org/simple
#  r: https://cran.r-project.org/web/packages/available_packages_by_name.html
#  erlang: https://hex.pm/packages
#  java: https://repo1.maven.org/maven2/
#  julia: https://pkg.julialang.org/
#  latex: https://ctan.org/pkg
#  lua: https://luarocks.org/m/root
#  js: https://docs.npmjs.com/misc/registry
#  pascal: ???
#  perl: https://www.cpan.org/modules/01modules.index.html
#  php: https://packagist.org/
#  ruby: https://rubygems.org/gems
#  clojure: https://clojars.org/


# Not all python namespace packages are registered on PyPI. If a package
# contains files in site-packages, it probably belongs in the python namespace.


# Indicates whether or not external plugins (i.e., plugins that aren't shipped
# with conda) are enabled
NO_PLUGINS = False


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Conda's global configuration object.

The context aggregates all configuration files, environment variables, and command line arguments
into one global stateful object to be used across all of conda.
"""

from __future__ import annotations

import logging
import os
import platform
import struct
import sys
from collections import defaultdict
from contextlib import contextmanager
from errno import ENOENT
from functools import cached_property, lru_cache
from itertools import chain
from os.path import abspath, exists, expanduser, isdir, isfile, join
from os.path import split as path_split
from typing import TYPE_CHECKING, Mapping

from boltons.setutils import IndexedSet

from .. import CONDA_SOURCE_ROOT
from .. import __version__ as CONDA_VERSION
from ..auxlib.decorators import memoizedproperty
from ..auxlib.ish import dals
from ..common._os.linux import linux_get_libc_version
from ..common.compat import NoneType, on_win
from ..common.configuration import (
    Configuration,
    ConfigurationLoadError,
    ConfigurationType,
    EnvRawParameter,
    MapParameter,
    ParameterLoader,
    PrimitiveParameter,
    SequenceParameter,
    ValidationError,
    unique_sequence_map,
)
from ..common.constants import TRACE
from ..common.iterators import unique
from ..common.path import expand, paths_equal
from ..common.url import has_scheme, path_to_url, split_scheme_auth_token
from ..deprecations import deprecated
from .constants import (
    APP_NAME,
    DEFAULT_AGGRESSIVE_UPDATE_PACKAGES,
    DEFAULT_CHANNEL_ALIAS,
    DEFAULT_CHANNELS,
    DEFAULT_CHANNELS_UNIX,
    DEFAULT_CHANNELS_WIN,
    DEFAULT_CUSTOM_CHANNELS,
    DEFAULT_SOLVER,
    DEFAULTS_CHANNEL_NAME,
    ERROR_UPLOAD_URL,
    KNOWN_SUBDIRS,
    NO_PLUGINS,
    PREFIX_MAGIC_FILE,
    PREFIX_NAME_DISALLOWED_CHARS,
    REPODATA_FN,
    ROOT_ENV_NAME,
    SEARCH_PATH,
    ChannelPriority,
    DepsModifier,
    PathConflict,
    SafetyChecks,
    SatSolverChoice,
    UpdateModifier,
)

try:
    from frozendict import frozendict
except ImportError:
    from .._vendor.frozendict import frozendict

if TYPE_CHECKING:
    from pathlib import Path
    from typing import Literal

    from ..common.configuration import Parameter, RawParameter
    from ..plugins.manager import CondaPluginManager

try:
    os.getcwd()
except OSError as e:
    if e.errno == ENOENT:
        # FileNotFoundError can occur when cwd has been deleted out from underneath the process.
        # To resolve #6584, let's go with setting cwd to sys.prefix, and see how far we get.
        os.chdir(sys.prefix)
    else:
        raise

log = logging.getLogger(__name__)

_platform_map = {
    "freebsd13": "freebsd",
    "linux2": "linux",
    "linux": "linux",
    "darwin": "osx",
    "win32": "win",
    "zos": "zos",
}
non_x86_machines = {
    "armv6l",
    "armv7l",
    "aarch64",
    "arm64",
    "ppc64",
    "ppc64le",
    "riscv64",
    "s390x",
}
_arch_names = {
    32: "x86",
    64: "x86_64",
}

user_rc_path = abspath(expanduser("~/.condarc"))
sys_rc_path = join(sys.prefix, ".condarc")


def user_data_dir(  # noqa: F811
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    roaming: bool = False,
):
    # Defer platformdirs import to reduce import time for conda activate.
    global user_data_dir
    try:
        from platformdirs import user_data_dir
    except ImportError:  # pragma: no cover
        from .._vendor.appdirs import user_data_dir
    return user_data_dir(appname, appauthor=appauthor, version=version, roaming=roaming)


def mockable_context_envs_dirs(root_writable, root_prefix, _envs_dirs):
    if root_writable:
        fixed_dirs = [
            join(root_prefix, "envs"),
            join("~", ".conda", "envs"),
        ]
    else:
        fixed_dirs = [
            join("~", ".conda", "envs"),
            join(root_prefix, "envs"),
        ]
    if on_win:
        fixed_dirs.append(join(user_data_dir(APP_NAME, APP_NAME), "envs"))
    return tuple(IndexedSet(expand(path) for path in (*_envs_dirs, *fixed_dirs)))


def channel_alias_validation(value):
    if value and not has_scheme(value):
        return f"channel_alias value '{value}' must have scheme/protocol."
    return True


def default_python_default():
    ver = sys.version_info
    return "%d.%d" % (ver.major, ver.minor)


def default_python_validation(value):
    if value:
        if len(value) >= 3 and value[1] == ".":
            try:
                value = float(value)
                if 2.0 <= value < 4.0:
                    return True
            except ValueError:  # pragma: no cover
                pass
    else:
        # Set to None or '' meaning no python pinning
        return True

    return f"default_python value '{value}' not of the form '[23].[0-9][0-9]?' or ''"


def ssl_verify_validation(value):
    if isinstance(value, str):
        if sys.version_info < (3, 10) and value == "truststore":
            return "`ssl_verify: truststore` is only supported on Python 3.10 or later"
        elif value != "truststore" and not exists(value):
            return (
                f"ssl_verify value '{value}' must be a boolean, a path to a "
                "certificate bundle file, a path to a directory containing "
                "certificates of trusted CAs, or 'truststore' to use the "
                "operating system certificate store."
            )
    return True


class Context(Configuration):
    add_pip_as_python_dependency = ParameterLoader(PrimitiveParameter(True))
    allow_conda_downgrades = ParameterLoader(PrimitiveParameter(False))
    # allow cyclical dependencies, or raise
    allow_cycles = ParameterLoader(PrimitiveParameter(True))
    allow_softlinks = ParameterLoader(PrimitiveParameter(False))
    auto_update_conda = ParameterLoader(
        PrimitiveParameter(True), aliases=("self_update",)
    )
    auto_activate_base = ParameterLoader(PrimitiveParameter(True))
    auto_stack = ParameterLoader(PrimitiveParameter(0))
    notify_outdated_conda = ParameterLoader(PrimitiveParameter(True))
    clobber = ParameterLoader(PrimitiveParameter(False))
    changeps1 = ParameterLoader(PrimitiveParameter(True))
    env_prompt = ParameterLoader(PrimitiveParameter("({default_env}) "))
    create_default_packages = ParameterLoader(
        SequenceParameter(PrimitiveParameter("", element_type=str))
    )
    register_envs = ParameterLoader(PrimitiveParameter(True))
    default_python = ParameterLoader(
        PrimitiveParameter(
            default_python_default(),
            element_type=(str, NoneType),
            validation=default_python_validation,
        )
    )
    download_only = ParameterLoader(PrimitiveParameter(False))
    enable_private_envs = ParameterLoader(PrimitiveParameter(False))
    force_32bit = ParameterLoader(PrimitiveParameter(False))
    non_admin_enabled = ParameterLoader(PrimitiveParameter(True))
    pip_interop_enabled = ParameterLoader(PrimitiveParameter(False))

    # multithreading in various places
    _default_threads = ParameterLoader(
        PrimitiveParameter(0, element_type=int), aliases=("default_threads",)
    )
    # download repodata
    _repodata_threads = ParameterLoader(
        PrimitiveParameter(0, element_type=int), aliases=("repodata_threads",)
    )
    # download packages
    _fetch_threads = ParameterLoader(
        PrimitiveParameter(0, element_type=int), aliases=("fetch_threads",)
    )
    _verify_threads = ParameterLoader(
        PrimitiveParameter(0, element_type=int), aliases=("verify_threads",)
    )
    # this one actually defaults to 1 - that is handled in the property below
    _execute_threads = ParameterLoader(
        PrimitiveParameter(0, element_type=int), aliases=("execute_threads",)
    )

    # Safety & Security
    _aggressive_update_packages = ParameterLoader(
        SequenceParameter(
            PrimitiveParameter("", element_type=str), DEFAULT_AGGRESSIVE_UPDATE_PACKAGES
        ),
        aliases=("aggressive_update_packages",),
    )
    safety_checks = ParameterLoader(PrimitiveParameter(SafetyChecks.warn))
    extra_safety_checks = ParameterLoader(PrimitiveParameter(False))
    _signing_metadata_url_base = ParameterLoader(
        PrimitiveParameter(None, element_type=(str, NoneType)),
        aliases=("signing_metadata_url_base",),
    )
    path_conflict = ParameterLoader(PrimitiveParameter(PathConflict.clobber))

    pinned_packages = ParameterLoader(
        SequenceParameter(
            PrimitiveParameter("", element_type=str), string_delimiter="&"
        )
    )  # TODO: consider a different string delimiter  # NOQA
    disallowed_packages = ParameterLoader(
        SequenceParameter(
            PrimitiveParameter("", element_type=str), string_delimiter="&"
        ),
        aliases=("disallow",),
    )
    rollback_enabled = ParameterLoader(PrimitiveParameter(True))
    track_features = ParameterLoader(
        SequenceParameter(PrimitiveParameter("", element_type=str))
    )
    use_index_cache = ParameterLoader(PrimitiveParameter(False))

    separate_format_cache = ParameterLoader(PrimitiveParameter(False))

    _root_prefix = ParameterLoader(
        PrimitiveParameter(""), aliases=("root_dir", "root_prefix")
    )
    _envs_dirs = ParameterLoader(
        SequenceParameter(
            PrimitiveParameter("", element_type=str), string_delimiter=os.pathsep
        ),
        aliases=("envs_dirs", "envs_path"),
        expandvars=True,
    )
    _pkgs_dirs = ParameterLoader(
        SequenceParameter(PrimitiveParameter("", str)),
        aliases=("pkgs_dirs",),
        expandvars=True,
    )
    _subdir = ParameterLoader(PrimitiveParameter(""), aliases=("subdir",))
    _subdirs = ParameterLoader(
        SequenceParameter(PrimitiveParameter("", str)), aliases=("subdirs",)
    )

    local_repodata_ttl = ParameterLoader(
        PrimitiveParameter(1, element_type=(bool, int))
    )
    # number of seconds to cache repodata locally
    #   True/1: respect Cache-Control max-age header
    #   False/0: always fetch remote repodata (HTTP 304 responses respected)

    # remote connection details
    ssl_verify = ParameterLoader(
        PrimitiveParameter(
            True, element_type=(str, bool), validation=ssl_verify_validation
        ),
        aliases=("verify_ssl",),
        expandvars=True,
    )
    client_ssl_cert = ParameterLoader(
        PrimitiveParameter(None, element_type=(str, NoneType)),
        aliases=("client_cert",),
        expandvars=True,
    )
    client_ssl_cert_key = ParameterLoader(
        PrimitiveParameter(None, element_type=(str, NoneType)),
        aliases=("client_cert_key",),
        expandvars=True,
    )
    proxy_servers = ParameterLoader(
        MapParameter(PrimitiveParameter(None, (str, NoneType))), expandvars=True
    )
    remote_connect_timeout_secs = ParameterLoader(PrimitiveParameter(9.15))
    remote_read_timeout_secs = ParameterLoader(PrimitiveParameter(60.0))
    remote_max_retries = ParameterLoader(PrimitiveParameter(3))
    remote_backoff_factor = ParameterLoader(PrimitiveParameter(1))

    add_anaconda_token = ParameterLoader(
        PrimitiveParameter(True), aliases=("add_binstar_token",)
    )

    _reporters = ParameterLoader(
        SequenceParameter(MapParameter(PrimitiveParameter("", element_type=str))),
        aliases=("reporters",),
    )

    ####################################################
    #               Channel Configuration              #
    ####################################################
    allow_non_channel_urls = ParameterLoader(PrimitiveParameter(False))
    _channel_alias = ParameterLoader(
        PrimitiveParameter(DEFAULT_CHANNEL_ALIAS, validation=channel_alias_validation),
        aliases=("channel_alias",),
        expandvars=True,
    )
    channel_priority = ParameterLoader(PrimitiveParameter(ChannelPriority.FLEXIBLE))
    _channels = ParameterLoader(
        SequenceParameter(
            PrimitiveParameter("", element_type=str), default=(DEFAULTS_CHANNEL_NAME,)
        ),
        aliases=(
            "channels",
            "channel",
        ),
        expandvars=True,
    )  # channel for args.channel
    channel_settings = ParameterLoader(
        SequenceParameter(MapParameter(PrimitiveParameter("", element_type=str)))
    )
    _custom_channels = ParameterLoader(
        MapParameter(PrimitiveParameter("", element_type=str), DEFAULT_CUSTOM_CHANNELS),
        aliases=("custom_channels",),
        expandvars=True,
    )
    _custom_multichannels = ParameterLoader(
        MapParameter(SequenceParameter(PrimitiveParameter("", element_type=str))),
        aliases=("custom_multichannels",),
        expandvars=True,
    )
    _default_channels = ParameterLoader(
        SequenceParameter(PrimitiveParameter("", element_type=str), DEFAULT_CHANNELS),
        aliases=("default_channels",),
        expandvars=True,
    )
    _migrated_channel_aliases = ParameterLoader(
        SequenceParameter(PrimitiveParameter("", element_type=str)),
        aliases=("migrated_channel_aliases",),
    )
    migrated_custom_channels = ParameterLoader(
        MapParameter(PrimitiveParameter("", element_type=str)), expandvars=True
    )  # TODO: also take a list of strings
    override_channels_enabled = ParameterLoader(PrimitiveParameter(True))
    show_channel_urls = ParameterLoader(
        PrimitiveParameter(None, element_type=(bool, NoneType))
    )
    use_local = ParameterLoader(PrimitiveParameter(False))
    allowlist_channels = ParameterLoader(
        SequenceParameter(PrimitiveParameter("", element_type=str)),
        aliases=("whitelist_channels",),
        expandvars=True,
    )
    restore_free_channel = ParameterLoader(PrimitiveParameter(False))
    repodata_fns = ParameterLoader(
        SequenceParameter(
            PrimitiveParameter("", element_type=str),
            ("current_repodata.json", REPODATA_FN),
        )
    )
    _use_only_tar_bz2 = ParameterLoader(
        PrimitiveParameter(None, element_type=(bool, NoneType)),
        aliases=("use_only_tar_bz2",),
    )

    always_softlink = ParameterLoader(PrimitiveParameter(False), aliases=("softlink",))
    always_copy = ParameterLoader(PrimitiveParameter(False), aliases=("copy",))
    always_yes = ParameterLoader(
        PrimitiveParameter(None, element_type=(bool, NoneType)), aliases=("yes",)
    )
    _debug = ParameterLoader(PrimitiveParameter(False), aliases=["debug"])
    _trace = ParameterLoader(PrimitiveParameter(False), aliases=["trace"])
    dev = ParameterLoader(PrimitiveParameter(False))
    dry_run = ParameterLoader(PrimitiveParameter(False))
    error_upload_url = ParameterLoader(PrimitiveParameter(ERROR_UPLOAD_URL))
    force = ParameterLoader(PrimitiveParameter(False))
    json = ParameterLoader(PrimitiveParameter(False))
    offline = ParameterLoader(PrimitiveParameter(False))
    quiet = ParameterLoader(PrimitiveParameter(False))
    ignore_pinned = ParameterLoader(PrimitiveParameter(False))
    report_errors = ParameterLoader(
        PrimitiveParameter(None, element_type=(bool, NoneType))
    )
    shortcuts = ParameterLoader(PrimitiveParameter(True))
    number_channel_notices = ParameterLoader(PrimitiveParameter(5, element_type=int))
    shortcuts = ParameterLoader(PrimitiveParameter(True))
    shortcuts_only = ParameterLoader(
        SequenceParameter(PrimitiveParameter("", element_type=str)), expandvars=True
    )
    _verbosity = ParameterLoader(
        PrimitiveParameter(0, element_type=int), aliases=("verbose", "verbosity")
    )
    experimental = ParameterLoader(SequenceParameter(PrimitiveParameter("", str)))
    no_lock = ParameterLoader(PrimitiveParameter(False))
    repodata_use_zst = ParameterLoader(PrimitiveParameter(True))
    envvars_force_uppercase = ParameterLoader(PrimitiveParameter(True))

    ####################################################
    #               Solver Configuration               #
    ####################################################
    deps_modifier = ParameterLoader(PrimitiveParameter(DepsModifier.NOT_SET))
    update_modifier = ParameterLoader(PrimitiveParameter(UpdateModifier.UPDATE_SPECS))
    sat_solver = ParameterLoader(PrimitiveParameter(SatSolverChoice.PYCOSAT))
    solver_ignore_timestamps = ParameterLoader(PrimitiveParameter(False))
    solver = ParameterLoader(
        PrimitiveParameter(DEFAULT_SOLVER),
        aliases=("experimental_solver",),
    )

    # # CLI-only
    # no_deps = ParameterLoader(PrimitiveParameter(NULL, element_type=(type(NULL), bool)))
    # # CLI-only
    # only_deps = ParameterLoader(PrimitiveParameter(NULL, element_type=(type(NULL), bool)))
    #
    # freeze_installed = ParameterLoader(PrimitiveParameter(False))
    # update_deps = ParameterLoader(PrimitiveParameter(False), aliases=('update_dependencies',))
    # update_specs = ParameterLoader(PrimitiveParameter(False))
    # update_all = ParameterLoader(PrimitiveParameter(False))

    force_remove = ParameterLoader(PrimitiveParameter(False))
    force_reinstall = ParameterLoader(PrimitiveParameter(False))

    target_prefix_override = ParameterLoader(PrimitiveParameter(""))

    unsatisfiable_hints = ParameterLoader(PrimitiveParameter(True))
    unsatisfiable_hints_check_depth = ParameterLoader(PrimitiveParameter(2))

    # conda_build
    bld_path = ParameterLoader(PrimitiveParameter(""))
    anaconda_upload = ParameterLoader(
        PrimitiveParameter(None, element_type=(bool, NoneType)),
        aliases=("binstar_upload",),
    )
    _croot = ParameterLoader(PrimitiveParameter(""), aliases=("croot",))
    _conda_build = ParameterLoader(
        MapParameter(PrimitiveParameter("", element_type=str)),
        aliases=("conda-build", "conda_build"),
    )

    ####################################################
    #               Plugin Configuration               #
    ####################################################

    no_plugins = ParameterLoader(PrimitiveParameter(NO_PLUGINS))

    def __init__(self, search_path=None, argparse_args=None, **kwargs):
        super().__init__(argparse_args=argparse_args)

        self._set_search_path(
            SEARCH_PATH if search_path is None else search_path,
            # for proper search_path templating when --name/--prefix is used
            CONDA_PREFIX=determine_target_prefix(self, argparse_args),
        )
        self._set_env_vars(APP_NAME)
        self._set_argparse_args(argparse_args)

    def post_build_validation(self):
        errors = []
        if self.client_ssl_cert_key and not self.client_ssl_cert:
            error = ValidationError(
                "client_ssl_cert",
                self.client_ssl_cert,
                "<<merged>>",
                "'client_ssl_cert' is required when 'client_ssl_cert_key' "
                "is defined",
            )
            errors.append(error)
        if self.always_copy and self.always_softlink:
            error = ValidationError(
                "always_copy",
                self.always_copy,
                "<<merged>>",
                "'always_copy' and 'always_softlink' are mutually exclusive. "
                "Only one can be set to 'True'.",
            )
            errors.append(error)
        return errors

    @property
    def plugin_manager(self) -> CondaPluginManager:
        """
        This is the preferred way of accessing the ``PluginManager`` object for this application
        and is located here to avoid problems with cyclical imports elsewhere in the code.
        """
        from ..plugins.manager import get_plugin_manager

        return get_plugin_manager()

    @cached_property
    def plugins(self) -> PluginConfig:
        """
        Preferred way of accessing settings introduced by the settings plugin hook
        """
        self.plugin_manager.load_settings()
        return PluginConfig(self.raw_data)

    @property
    def conda_build_local_paths(self):
        # does file system reads to make sure paths actually exist
        return tuple(
            unique(
                full_path
                for full_path in (
                    expand(d)
                    for d in (
                        self._croot,
                        self.bld_path,
                        self.conda_build.get("root-dir"),
                        join(self.root_prefix, "conda-bld"),
                        "~/conda-bld",
                    )
                    if d
                )
                if isdir(full_path)
            )
        )

    @property
    def conda_build_local_urls(self):
        return tuple(path_to_url(p) for p in self.conda_build_local_paths)

    @property
    def croot(self):
        """This is where source caches and work folders live"""
        if self._croot:
            return abspath(expanduser(self._croot))
        elif self.bld_path:
            return abspath(expanduser(self.bld_path))
        elif "root-dir" in self.conda_build:
            return abspath(expanduser(self.conda_build["root-dir"]))
        elif self.root_writable:
            return join(self.root_prefix, "conda-bld")
        else:
            return expand("~/conda-bld")

    @property
    def local_build_root(self):
        return self.croot

    @property
    def conda_build(self):
        # conda-build needs its config map to be mutable
        try:
            return self.__conda_build
        except AttributeError:
            self.__conda_build = __conda_build = dict(self._conda_build)
            return __conda_build

    @property
    def arch_name(self):
        m = platform.machine()
        if m in non_x86_machines:
            return m
        else:
            return _arch_names[self.bits]

    @property
    def platform(self):
        return _platform_map.get(sys.platform, "unknown")

    @property
    def default_threads(self) -> int | None:
        return self._default_threads or None

    @property
    def repodata_threads(self) -> int | None:
        return self._repodata_threads or self.default_threads

    @property
    def fetch_threads(self) -> int | None:
        """
        If both are not overriden (0), return experimentally-determined value of 5
        """
        if self._fetch_threads == 0 and self._default_threads == 0:
            return 5
        return self._fetch_threads or self.default_threads

    @property
    def verify_threads(self) -> int | None:
        if self._verify_threads:
            threads = self._verify_threads
        elif self.default_threads:
            threads = self.default_threads
        else:
            threads = 1
        return threads

    @property
    def execute_threads(self):
        if self._execute_threads:
            threads = self._execute_threads
        elif self.default_threads:
            threads = self.default_threads
        else:
            threads = 1
        return threads

    @property
    def subdir(self):
        if self._subdir:
            return self._subdir
        return self._native_subdir()

    @lru_cache(maxsize=None)
    def _native_subdir(self):
        m = platform.machine()
        if m in non_x86_machines:
            return f"{self.platform}-{m}"
        elif self.platform == "zos":
            return "zos-z"
        else:
            return "%s-%d" % (self.platform, self.bits)

    @property
    def subdirs(self):
        return self._subdirs or (self.subdir, "noarch")

    @memoizedproperty
    def known_subdirs(self):
        return frozenset((*KNOWN_SUBDIRS, *self.subdirs))

    @property
    def bits(self):
        if self.force_32bit:
            return 32
        else:
            return 8 * struct.calcsize("P")

    @property
    @deprecated(
        "24.3",
        "24.9",
        addendum="Please use `conda.base.context.context.root_prefix` instead.",
    )
    def root_dir(self) -> os.PathLike:
        # root_dir is an alias for root_prefix, we prefer the name "root_prefix"
        # because it is more consistent with other names
        return self.root_prefix

    @property
    def root_writable(self):
        # rather than using conda.gateways.disk.test.prefix_is_writable
        # let's shortcut and assume the root prefix exists
        path = join(self.root_prefix, PREFIX_MAGIC_FILE)
        if isfile(path):
            try:
                fh = open(path, "a+")
            except OSError as e:
                log.debug(e)
                return False
            else:
                fh.close()
                return True
        return False

    @property
    def envs_dirs(self):
        return mockable_context_envs_dirs(
            self.root_writable, self.root_prefix, self._envs_dirs
        )

    @property
    def pkgs_dirs(self):
        if self._pkgs_dirs:
            return tuple(IndexedSet(expand(p) for p in self._pkgs_dirs))
        else:
            cache_dir_name = "pkgs32" if context.force_32bit else "pkgs"
            fixed_dirs = (
                self.root_prefix,
                join("~", ".conda"),
            )
            if on_win:
                fixed_dirs += (user_data_dir(APP_NAME, APP_NAME),)
            return tuple(
                IndexedSet(expand(join(p, cache_dir_name)) for p in (fixed_dirs))
            )

    @memoizedproperty
    def trash_dir(self):
        # TODO: this inline import can be cleaned up by moving pkgs_dir write detection logic
        from ..core.package_cache_data import PackageCacheData

        pkgs_dir = PackageCacheData.first_writable().pkgs_dir
        trash_dir = join(pkgs_dir, ".trash")
        from ..gateways.disk.create import mkdir_p

        mkdir_p(trash_dir)
        return trash_dir

    @property
    def default_prefix(self):
        if self.active_prefix:
            return self.active_prefix
        _default_env = os.getenv("CONDA_DEFAULT_ENV")
        if _default_env in (None, ROOT_ENV_NAME, "root"):
            return self.root_prefix
        elif os.sep in _default_env:
            return abspath(_default_env)
        else:
            for envs_dir in self.envs_dirs:
                default_prefix = join(envs_dir, _default_env)
                if isdir(default_prefix):
                    return default_prefix
        return join(self.envs_dirs[0], _default_env)

    @property
    def active_prefix(self):
        return os.getenv("CONDA_PREFIX")

    @property
    def shlvl(self):
        return int(os.getenv("CONDA_SHLVL", -1))

    @property
    def aggressive_update_packages(self):
        from ..models.match_spec import MatchSpec

        return tuple(MatchSpec(s) for s in self._aggressive_update_packages)

    @property
    def target_prefix(self):
        # used for the prefix that is the target of the command currently being executed
        # different from the active prefix, which is sometimes given by -p or -n command line flags
        return determine_target_prefix(self)

    @memoizedproperty
    def root_prefix(self):
        if self._root_prefix:
            return abspath(expanduser(self._root_prefix))
        else:
            return self.conda_prefix

    @property
    def conda_prefix(self):
        return abspath(sys.prefix)

    @property
    @deprecated(
        "23.9",
        "24.9",
        addendum="Please use `conda.base.context.context.conda_exe_vars_dict` instead",
    )
    def conda_exe(self):
        bin_dir = "Scripts" if on_win else "bin"
        exe = "conda.exe" if on_win else "conda"
        return join(self.conda_prefix, bin_dir, exe)

    @property
    def av_data_dir(self):
        """Where critical artifact verification data (e.g., various public keys) can be found."""
        # TODO (AV): Find ways to make this user configurable?
        return join(self.conda_prefix, "etc", "conda")

    @property
    def signing_metadata_url_base(self):
        """Base URL for artifact verification signing metadata (*.root.json, key_mgr.json)."""
        if self._signing_metadata_url_base:
            return self._signing_metadata_url_base
        else:
            return None

    @property
    def conda_exe_vars_dict(self):
        """
        The vars can refer to each other if necessary since the dict is ordered.
        None means unset it.
        """
        if context.dev:
            return {
                "CONDA_EXE": sys.executable,
                # do not confuse with os.path.join, we are joining paths with ; or : delimiters
                "PYTHONPATH": os.pathsep.join(
                    (CONDA_SOURCE_ROOT, os.environ.get("PYTHONPATH", ""))
                ),
                "_CE_M": "-m",
                "_CE_CONDA": "conda",
                "CONDA_PYTHON_EXE": sys.executable,
            }
        else:
            bin_dir = "Scripts" if on_win else "bin"
            exe = "conda.exe" if on_win else "conda"
            # I was going to use None to indicate a variable to unset, but that gets tricky with
            # error-on-undefined.
            return {
                "CONDA_EXE": os.path.join(sys.prefix, bin_dir, exe),
                "_CE_M": "",
                "_CE_CONDA": "",
                "CONDA_PYTHON_EXE": sys.executable,
            }

    @memoizedproperty
    def channel_alias(self):
        from ..models.channel import Channel

        location, scheme, auth, token = split_scheme_auth_token(self._channel_alias)
        return Channel(scheme=scheme, auth=auth, location=location, token=token)

    @property
    def migrated_channel_aliases(self):
        from ..models.channel import Channel

        return tuple(
            Channel(scheme=scheme, auth=auth, location=location, token=token)
            for location, scheme, auth, token in (
                split_scheme_auth_token(c) for c in self._migrated_channel_aliases
            )
        )

    @property
    def prefix_specified(self):
        return (
            self._argparse_args.get("prefix") is not None
            or self._argparse_args.get("name") is not None
        )

    @memoizedproperty
    def default_channels(self):
        # the format for 'default_channels' is a list of strings that either
        #   - start with a scheme
        #   - are meant to be prepended with channel_alias
        return self.custom_multichannels[DEFAULTS_CHANNEL_NAME]

    @memoizedproperty
    def custom_multichannels(self):
        from ..models.channel import Channel

        if (
            not on_win
            and self.subdir.startswith("win-")
            and self._default_channels == DEFAULT_CHANNELS_UNIX
        ):
            default_channels = list(DEFAULT_CHANNELS_WIN)
        else:
            default_channels = list(self._default_channels)

        if self.restore_free_channel:
            default_channels.insert(1, "https://repo.anaconda.com/pkgs/free")

        reserved_multichannel_urls = {
            DEFAULTS_CHANNEL_NAME: default_channels,
            "local": self.conda_build_local_urls,
        }
        reserved_multichannels = {
            name: tuple(
                Channel.make_simple_channel(self.channel_alias, url) for url in urls
            )
            for name, urls in reserved_multichannel_urls.items()
        }
        custom_multichannels = {
            name: tuple(
                Channel.make_simple_channel(self.channel_alias, url) for url in urls
            )
            for name, urls in self._custom_multichannels.items()
        }
        return {
            name: channels
            for name, channels in (
                *custom_multichannels.items(),
                *reserved_multichannels.items(),  # order maters, reserved overrides custom
            )
        }

    @memoizedproperty
    def custom_channels(self):
        from ..models.channel import Channel

        return {
            channel.name: channel
            for channel in (
                *chain.from_iterable(
                    channel for channel in self.custom_multichannels.values()
                ),
                *(
                    Channel.make_simple_channel(self.channel_alias, url, name)
                    for name, url in self._custom_channels.items()
                ),
            )
        }

    @property
    def channels(self):
        local_add = ("local",) if self.use_local else ()
        if (
            self._argparse_args
            and "override_channels" in self._argparse_args
            and self._argparse_args["override_channels"]
        ):
            if not self.override_channels_enabled:
                from ..exceptions import OperationNotAllowed

                raise OperationNotAllowed("Overriding channels has been disabled.")
            elif not (
                self._argparse_args
                and "channel" in self._argparse_args
                and self._argparse_args["channel"]
            ):
                from ..exceptions import ArgumentError

                raise ArgumentError(
                    "At least one -c / --channel flag must be supplied when using "
                    "--override-channels."
                )
            else:
                return tuple(IndexedSet((*local_add, *self._argparse_args["channel"])))

        # add 'defaults' channel when necessary if --channel is given via the command line
        if self._argparse_args and "channel" in self._argparse_args:
            # TODO: it's args.channel right now, not channels
            argparse_channels = tuple(self._argparse_args["channel"] or ())
            # Add condition to make sure that sure that we add the 'defaults'
            # channel only when no channels are defined in condarc
            # We needs to get the config_files and then check that they
            # don't define channels
            channel_in_config_files = any(
                "channels" in context.raw_data[rc_file].keys()
                for rc_file in self.config_files
            )
            if argparse_channels and not channel_in_config_files:
                return tuple(
                    IndexedSet((*local_add, *argparse_channels, DEFAULTS_CHANNEL_NAME))
                )

        return tuple(IndexedSet((*local_add, *self._channels)))

    @property
    def config_files(self):
        return tuple(
            path
            for path in context.collect_all()
            if path not in ("envvars", "cmd_line")
        )

    @property
    def use_only_tar_bz2(self):
        # we avoid importing this at the top to avoid PATH issues.  Ensure that this
        #    is only called when use_only_tar_bz2 is first called.
        import conda_package_handling.api

        use_only_tar_bz2 = False
        if self._use_only_tar_bz2 is None:
            if self._argparse_args and "use_only_tar_bz2" in self._argparse_args:
                use_only_tar_bz2 &= self._argparse_args["use_only_tar_bz2"]
        return (
            (
                hasattr(conda_package_handling.api, "libarchive_enabled")
                and not conda_package_handling.api.libarchive_enabled
            )
            or self._use_only_tar_bz2
            or use_only_tar_bz2
        )

    @property
    def binstar_upload(self):
        # backward compatibility for conda-build
        return self.anaconda_upload

    @property
    def trace(self) -> bool:
        """Alias for context.verbosity >=4."""
        return self.verbosity >= 4

    @property
    def debug(self) -> bool:
        """Alias for context.verbosity >=3."""
        return self.verbosity >= 3

    @property
    def info(self) -> bool:
        """Alias for context.verbosity >=2."""
        return self.verbosity >= 2

    @property
    def verbose(self) -> bool:
        """Alias for context.verbosity >=1."""
        return self.verbosity >= 1

    @property
    def verbosity(self) -> int:
        """Verbosity level.

        For cleaner and readable code it is preferable to use the following alias properties:
            context.trace
            context.debug
            context.info
            context.verbose
            context.log_level
        """
        #                   0  logging.WARNING, standard output
        #           -v    = 1  logging.WARNING, detailed output
        #           -vv   = 2  logging.INFO
        # --debug = -vvv  = 3  logging.DEBUG
        # --trace = -vvvv = 4  conda.gateways.logging.TRACE
        if self._trace:
            return 4
        elif self._debug:
            return 3
        else:
            return self._verbosity

    @property
    def log_level(self) -> int:
        """Map context.verbosity to logging level."""
        if 4 < self.verbosity:
            return logging.NOTSET  # 0
        elif 3 < self.verbosity <= 4:
            return TRACE  # 5
        elif 2 < self.verbosity <= 3:
            return logging.DEBUG  # 10
        elif 1 < self.verbosity <= 2:
            return logging.INFO  # 20
        else:
            return logging.WARNING  # 30

    def solver_user_agent(self):
        user_agent = f"solver/{self.solver}"
        try:
            solver_backend = self.plugin_manager.get_cached_solver_backend()
            # Solver.user_agent has to be a static or class method
            user_agent += f" {solver_backend.user_agent()}"
        except Exception as exc:
            log.debug(
                "User agent could not be fetched from solver class '%s'.",
                self.solver,
                exc_info=exc,
            )
        return user_agent

    @memoizedproperty
    def user_agent(self):
        builder = [f"conda/{CONDA_VERSION} requests/{self.requests_version}"]
        builder.append("{}/{}".format(*self.python_implementation_name_version))
        builder.append("{}/{}".format(*self.platform_system_release))
        builder.append("{}/{}".format(*self.os_distribution_name_version))
        if self.libc_family_version[0]:
            builder.append("{}/{}".format(*self.libc_family_version))
        if self.solver != "classic":
            builder.append(self.solver_user_agent())
        return " ".join(builder)

    @contextmanager
    def _override(self, key, value):
        """
        TODO: This might be broken in some ways. Unsure what happens if the `old`
        value is a property and gets set to a new value. Or if the new value
        overrides the validation logic on the underlying ParameterLoader instance.

        Investigate and implement in a safer way.
        """
        old = getattr(self, key)
        setattr(self, key, value)
        try:
            yield
        finally:
            setattr(self, key, old)

    @memoizedproperty
    def requests_version(self):
        # used in User-Agent as "requests/<version>"
        # if unable to detect a version we expect "requests/unknown"
        try:
            from requests import __version__ as requests_version
        except ImportError as err:
            # ImportError: requests is not installed
            log.error("Unable to import requests: %s", err)
            requests_version = "unknown"
        except Exception as err:
            log.error("Error importing requests: %s", err)
            requests_version = "unknown"
        return requests_version

    @memoizedproperty
    def python_implementation_name_version(self):
        # CPython, Jython
        # '2.7.14'
        return platform.python_implementation(), platform.python_version()

    @memoizedproperty
    def platform_system_release(self):
        # tuple of system name and release version
        #
        # `uname -s` Linux, Windows, Darwin, Java
        #
        # `uname -r`
        # '17.4.0' for macOS
        # '10' or 'NT' for Windows
        return platform.system(), platform.release()

    @memoizedproperty
    def os_distribution_name_version(self):
        # tuple of os distribution name and version
        # e.g.
        #   'debian', '9'
        #   'OSX', '10.13.6'
        #   'Windows', '10.0.17134'
        platform_name = self.platform_system_release[0]
        if platform_name == "Linux":
            try:
                try:
                    import distro
                except ImportError:
                    from .._vendor import distro

                distinfo = distro.id(), distro.version(best=True)
            except Exception as e:
                log.debug("%r", e, exc_info=True)
                distinfo = ("Linux", "unknown")
            distribution_name, distribution_version = distinfo[0], distinfo[1]
        elif platform_name == "Darwin":
            distribution_name = "OSX"
            distribution_version = platform.mac_ver()[0]
        else:
            distribution_name = platform.system()
            distribution_version = platform.version()
        return distribution_name, distribution_version

    @memoizedproperty
    def libc_family_version(self):
        # tuple of lic_family and libc_version
        # None, None if not on Linux
        libc_family, libc_version = linux_get_libc_version()
        return libc_family, libc_version

    @property
    @deprecated("24.3", "24.9")
    def cpu_flags(self):
        # DANGER: This is rather slow
        info = _get_cpu_info()
        return info["flags"]

    @memoizedproperty
    @unique_sequence_map(unique_key="backend")
    def reporters(self) -> tuple[Mapping[str, str]]:
        """
        Determine the value of reporters based on other settings and the ``self._reporters``
        value itself.
        """
        if not self._reporters:
            return (
                {
                    "backend": "json" if self.json else "console",
                    "output": "stdout",
                    "verbosity": self.verbosity,
                    "quiet": self.quiet,
                },
            )

        return self._reporters

    @property
    def category_map(self):
        return {
            "Channel Configuration": (
                "channels",
                "channel_alias",
                "channel_settings",
                "default_channels",
                "override_channels_enabled",
                "allowlist_channels",
                "custom_channels",
                "custom_multichannels",
                "migrated_channel_aliases",
                "migrated_custom_channels",
                "add_anaconda_token",
                "allow_non_channel_urls",
                "restore_free_channel",
                "repodata_fns",
                "use_only_tar_bz2",
                "repodata_threads",
                "fetch_threads",
                "experimental",
                "no_lock",
                "repodata_use_zst",
            ),
            "Basic Conda Configuration": (  # TODO: Is there a better category name here?
                "envs_dirs",
                "pkgs_dirs",
                "default_threads",
            ),
            "Network Configuration": (
                "client_ssl_cert",
                "client_ssl_cert_key",
                "local_repodata_ttl",
                "offline",
                "proxy_servers",
                "remote_connect_timeout_secs",
                "remote_max_retries",
                "remote_backoff_factor",
                "remote_read_timeout_secs",
                "ssl_verify",
            ),
            "Solver Configuration": (
                "aggressive_update_packages",
                "auto_update_conda",
                "channel_priority",
                "create_default_packages",
                "disallowed_packages",
                "force_reinstall",
                "pinned_packages",
                "pip_interop_enabled",
                "track_features",
                "solver",
            ),
            "Package Linking and Install-time Configuration": (
                "allow_softlinks",
                "always_copy",
                "always_softlink",
                "path_conflict",
                "rollback_enabled",
                "safety_checks",
                "extra_safety_checks",
                "signing_metadata_url_base",
                "shortcuts",
                "shortcuts_only",
                "non_admin_enabled",
                "separate_format_cache",
                "verify_threads",
                "execute_threads",
            ),
            "Conda-build Configuration": (
                "bld_path",
                "croot",
                "anaconda_upload",
                "conda_build",
            ),
            "Output, Prompt, and Flow Control Configuration": (
                "always_yes",
                "auto_activate_base",
                "auto_stack",
                "changeps1",
                "env_prompt",
                "json",
                "notify_outdated_conda",
                "quiet",
                "report_errors",
                "show_channel_urls",
                "verbosity",
                "unsatisfiable_hints",
                "unsatisfiable_hints_check_depth",
                "number_channel_notices",
                "envvars_force_uppercase",
            ),
            "CLI-only": (
                "deps_modifier",
                "update_modifier",
                "force",
                "force_remove",
                "clobber",
                "dry_run",
                "download_only",
                "ignore_pinned",
                "use_index_cache",
                "use_local",
            ),
            "Hidden and Undocumented": (
                "allow_cycles",  # allow cyclical dependencies, or raise
                "allow_conda_downgrades",
                "add_pip_as_python_dependency",
                "debug",
                "trace",
                "dev",
                "default_python",
                "enable_private_envs",
                "error_upload_url",  # should remain undocumented
                "force_32bit",
                "root_prefix",
                "sat_solver",
                "solver_ignore_timestamps",
                "subdir",
                "subdirs",
                # https://conda.io/docs/config.html#disable-updating-of-dependencies-update-dependencies # NOQA
                # I don't think this documentation is correct any longer. # NOQA
                "target_prefix_override",
                # used to override prefix rewriting, for e.g. building docker containers or RPMs  # NOQA
                "register_envs",
                # whether to add the newly created prefix to ~/.conda/environments.txt
                "reporters",
            ),
            "Plugin Configuration": ("no_plugins",),
        }

    def get_descriptions(self):
        return self.description_map

    @memoizedproperty
    def description_map(self):
        return frozendict(
            add_anaconda_token=dals(
                """
                In conjunction with the anaconda command-line client (installed with
                `conda install anaconda-client`), and following logging into an Anaconda
                Server API site using `anaconda login`, automatically apply a matching
                private token to enable access to private packages and channels.
                """
            ),
            # add_pip_as_python_dependency=dals(
            #     """
            #     Add pip, wheel and setuptools as dependencies of python. This ensures pip,
            #     wheel and setuptools will always be installed any time python is installed.
            #     """
            # ),
            aggressive_update_packages=dals(
                """
                A list of packages that, if installed, are always updated to the latest possible
                version.
                """
            ),
            allow_non_channel_urls=dals(
                """
                Warn, but do not fail, when conda detects a channel url is not a valid channel.
                """
            ),
            allow_softlinks=dals(
                """
                When allow_softlinks is True, conda uses hard-links when possible, and soft-links
                (symlinks) when hard-links are not possible, such as when installing on a
                different filesystem than the one that the package cache is on. When
                allow_softlinks is False, conda still uses hard-links when possible, but when it
                is not possible, conda copies files. Individual packages can override
                this setting, specifying that certain files should never be soft-linked (see the
                no_link option in the build recipe documentation).
                """
            ),
            always_copy=dals(
                """
                Register a preference that files be copied into a prefix during install rather
                than hard-linked.
                """
            ),
            always_softlink=dals(
                """
                Register a preference that files be soft-linked (symlinked) into a prefix during
                install rather than hard-linked. The link source is the 'pkgs_dir' package cache
                from where the package is being linked. WARNING: Using this option can result in
                corruption of long-lived conda environments. Package caches are *caches*, which
                means there is some churn and invalidation. With this option, the contents of
                environments can be switched out (or erased) via operations on other environments.
                """
            ),
            always_yes=dals(
                """
                Automatically choose the 'yes' option whenever asked to proceed with a conda
                operation, such as when running `conda install`.
                """
            ),
            anaconda_upload=dals(
                """
                Automatically upload packages built with conda build to anaconda.org.
                """
            ),
            auto_activate_base=dals(
                """
                Automatically activate the base environment during shell initialization.
                """
            ),
            auto_update_conda=dals(
                """
                Automatically update conda when a newer or higher priority version is detected.
                """
            ),
            auto_stack=dals(
                """
                Implicitly use --stack when using activate if current level of nesting
                (as indicated by CONDA_SHLVL environment variable) is less than or equal to
                specified value. 0 or false disables automatic stacking, 1 or true enables
                it for one level.
                """
            ),
            bld_path=dals(
                """
                The location where conda-build will put built packages. Same as 'croot', but
                'croot' takes precedence when both are defined. Also used in construction of the
                'local' multichannel.
                """
            ),
            changeps1=dals(
                """
                When using activate, change the command prompt ($PS1) to include the
                activated environment.
                """
            ),
            channel_alias=dals(
                """
                The prepended url location to associate with channel names.
                """
            ),
            channel_priority=dals(
                """
                Accepts values of 'strict', 'flexible', and 'disabled'. The default value
                is 'flexible'. With strict channel priority, packages in lower priority channels
                are not considered if a package with the same name appears in a higher
                priority channel. With flexible channel priority, the solver may reach into
                lower priority channels to fulfill dependencies, rather than raising an
                unsatisfiable error. With channel priority disabled, package version takes
                precedence, and the configured priority of channels is used only to break ties.
                In previous versions of conda, this parameter was configured as either True or
                False. True is now an alias to 'flexible'.
                """
            ),
            channels=dals(
                """
                The list of conda channels to include for relevant operations.
                """
            ),
            channel_settings=dals(
                """
                A list of mappings that allows overriding certain settings for a single channel.
                Each list item should include at least the "channel" key and the setting you would
                like to override.
                """
            ),
            client_ssl_cert=dals(
                """
                A path to a single file containing a private key and certificate (e.g. .pem
                file). Alternately, use client_ssl_cert_key in conjunction with client_ssl_cert
                for individual files.
                """
            ),
            client_ssl_cert_key=dals(
                """
                Used in conjunction with client_ssl_cert for a matching key file.
                """
            ),
            # clobber=dals(
            #     """
            #     Allow clobbering of overlapping file paths within packages, and suppress
            #     related warnings. Overrides the path_conflict configuration value when
            #     set to 'warn' or 'prevent'.
            #     """
            # ),
            # TODO: add shortened link to docs for conda_build at See https://conda.io/docs/user-guide/configuration/use-condarc.html#conda-build-configuration  # NOQA
            conda_build=dals(
                """
                General configuration parameters for conda-build.
                """
            ),
            # TODO: This is a bad parameter name. Consider an alternate.
            create_default_packages=dals(
                """
                Packages that are by default added to a newly created environments.
                """
            ),
            croot=dals(
                """
                The location where conda-build will put built packages. Same as 'bld_path', but
                'croot' takes precedence when both are defined. Also used in construction of the
                'local' multichannel.
                """
            ),
            custom_channels=dals(
                """
                A map of key-value pairs where the key is a channel name and the value is
                a channel location. Channels defined here override the default
                'channel_alias' value. The channel name (key) is not included in the channel
                location (value).  For example, to override the location of the 'conda-forge'
                channel where the url to repodata is
                https://anaconda-repo.dev/packages/conda-forge/linux-64/repodata.json, add an
                entry 'conda-forge: https://anaconda-repo.dev/packages'.
                """
            ),
            custom_multichannels=dals(
                """
                A multichannel is a metachannel composed of multiple channels. The two reserved
                multichannels are 'defaults' and 'local'. The 'defaults' multichannel is
                customized using the 'default_channels' parameter. The 'local'
                multichannel is a list of file:// channel locations where conda-build stashes
                successfully-built packages.  Other multichannels can be defined with
                custom_multichannels, where the key is the multichannel name and the value is
                a list of channel names and/or channel urls.
                """
            ),
            default_channels=dals(
                """
                The list of channel names and/or urls used for the 'defaults' multichannel.
                """
            ),
            # default_python=dals(
            #     """
            #     specifies the default major & minor version of Python to be used when
            #     building packages with conda-build. Also used to determine the major
            #     version of Python (2/3) to be used in new environments. Defaults to
            #     the version used by conda itself.
            #     """
            # ),
            default_threads=dals(
                """
                Threads to use by default for parallel operations.  Default is None,
                which allows operations to choose themselves.  For more specific
                control, see the other *_threads parameters:
                    * repodata_threads - for fetching/loading repodata
                    * verify_threads - for verifying package contents in transactions
                    * execute_threads - for carrying out the unlinking and linking steps
                """
            ),
            disallowed_packages=dals(
                """
                Package specifications to disallow installing. The default is to allow
                all packages.
                """
            ),
            download_only=dals(
                """
                Solve an environment and ensure package caches are populated, but exit
                prior to unlinking and linking packages into the prefix
                """
            ),
            envs_dirs=dals(
                """
                The list of directories to search for named environments. When creating a new
                named environment, the environment will be placed in the first writable
                location.
                """
            ),
            env_prompt=dals(
                """
                Template for prompt modification based on the active environment. Currently
                supported template variables are '{prefix}', '{name}', and '{default_env}'.
                '{prefix}' is the absolute path to the active environment. '{name}' is the
                basename of the active environment prefix. '{default_env}' holds the value
                of '{name}' if the active environment is a conda named environment ('-n'
                flag), or otherwise holds the value of '{prefix}'. Templating uses python's
                str.format() method.
                """
            ),
            execute_threads=dals(
                """
                Threads to use when performing the unlink/link transaction.  When not set,
                defaults to 1.  This step is pretty strongly I/O limited, and you may not
                see much benefit here.
                """
            ),
            fetch_threads=dals(
                """
                Threads to use when downloading packages.  When not set,
                defaults to None, which uses the default ThreadPoolExecutor behavior.
                """
            ),
            force_reinstall=dals(
                """
                Ensure that any user-requested package for the current operation is uninstalled
                and reinstalled, even if that package already exists in the environment.
                """
            ),
            # force=dals(
            #     """
            #     Override any of conda's objections and safeguards for installing packages and
            #     potentially breaking environments. Also re-installs the package, even if the
            #     package is already installed. Implies --no-deps.
            #     """
            # ),
            # force_32bit=dals(
            #     """
            #     CONDA_FORCE_32BIT should only be used when running conda-build (in order
            #     to build 32-bit packages on a 64-bit system).  We don't want to mention it
            #     in the documentation, because it can mess up a lot of things.
            #     """
            # ),
            json=dals(
                """
                Ensure all output written to stdout is structured json.
                """
            ),
            local_repodata_ttl=dals(
                """
                For a value of False or 0, always fetch remote repodata (HTTP 304 responses
                respected). For a value of True or 1, respect the HTTP Cache-Control max-age
                header. Any other positive integer values is the number of seconds to locally
                cache repodata before checking the remote server for an update.
                """
            ),
            migrated_channel_aliases=dals(
                """
                A list of previously-used channel_alias values. Useful when switching between
                different Anaconda Repository instances.
                """
            ),
            migrated_custom_channels=dals(
                """
                A map of key-value pairs where the key is a channel name and the value is
                the previous location of the channel.
                """
            ),
            # no_deps=dals(
            #     """
            #     Do not install, update, remove, or change dependencies. This WILL lead to broken
            #     environments and inconsistent behavior. Use at your own risk.
            #     """
            # ),
            no_plugins=dals(
                """
                Disable all currently-registered plugins, except built-in conda plugins.
                """
            ),
            non_admin_enabled=dals(
                """
                Allows completion of conda's create, install, update, and remove operations, for
                non-privileged (non-root or non-administrator) users.
                """
            ),
            notify_outdated_conda=dals(
                """
                Notify if a newer version of conda is detected during a create, install, update,
                or remove operation.
                """
            ),
            offline=dals(
                """
                Restrict conda to cached download content and file:// based urls.
                """
            ),
            override_channels_enabled=dals(
                """
                Permit use of the --override-channels command-line flag.
                """
            ),
            path_conflict=dals(
                """
                The method by which conda handle's conflicting/overlapping paths during a
                create, install, or update operation. The value must be one of 'clobber',
                'warn', or 'prevent'. The '--clobber' command-line flag or clobber
                configuration parameter overrides path_conflict set to 'prevent'.
                """
            ),
            pinned_packages=dals(
                """
                A list of package specs to pin for every environment resolution.
                This parameter is in BETA, and its behavior may change in a future release.
                """
            ),
            pip_interop_enabled=dals(
                """
                Allow the conda solver to interact with non-conda-installed python packages.
                """
            ),
            pkgs_dirs=dals(
                """
                The list of directories where locally-available packages are linked from at
                install time. Packages not locally available are downloaded and extracted
                into the first writable directory.
                """
            ),
            proxy_servers=dals(
                """
                A mapping to enable proxy settings. Keys can be either (1) a scheme://hostname
                form, which will match any request to the given scheme and exact hostname, or
                (2) just a scheme, which will match requests to that scheme. Values are are
                the actual proxy server, and are of the form
                'scheme://[user:password@]host[:port]'. The optional 'user:password' inclusion
                enables HTTP Basic Auth with your proxy.
                """
            ),
            quiet=dals(
                """
                Disable progress bar display and other output.
                """
            ),
            reporters=dals(
                """
                A list of mappings that allow the configuration of one or more output streams
                (e.g. stdout or file).
                """
            ),
            remote_connect_timeout_secs=dals(
                """
                The number seconds conda will wait for your client to establish a connection
                to a remote url resource.
                """
            ),
            remote_max_retries=dals(
                """
                The maximum number of retries each HTTP connection should attempt.
                """
            ),
            remote_backoff_factor=dals(
                """
                The factor determines the time HTTP connection should wait for attempt.
                """
            ),
            remote_read_timeout_secs=dals(
                """
                Once conda has connected to a remote resource and sent an HTTP request, the
                read timeout is the number of seconds conda will wait for the server to send
                a response.
                """
            ),
            repodata_threads=dals(
                """
                Threads to use when downloading and reading repodata.  When not set,
                defaults to None, which uses the default ThreadPoolExecutor behavior.
                """
            ),
            report_errors=dals(
                """
                Opt in, or opt out, of automatic error reporting to core maintainers. Error
                reports are anonymous, with only the error stack trace and information given
                by `conda info` being sent.
                """
            ),
            restore_free_channel=dals(
                """"
                Add the "free" channel back into defaults, behind "main" in priority. The "free"
                channel was removed from the collection of default channels in conda 4.7.0.
                """
            ),
            rollback_enabled=dals(
                """
                Should any error occur during an unlink/link transaction, revert any disk
                mutations made to that point in the transaction.
                """
            ),
            safety_checks=dals(
                """
                Enforce available safety guarantees during package installation.
                The value must be one of 'enabled', 'warn', or 'disabled'.
                """
            ),
            separate_format_cache=dals(
                """
                Treat .tar.bz2 files as different from .conda packages when
                filenames are otherwise similar. This defaults to False, so
                that your package cache doesn't churn when rolling out the new
                package format. If you'd rather not assume that a .tar.bz2 and
                .conda from the same place represent the same content, set this
                to True.
                """
            ),
            extra_safety_checks=dals(
                """
                Spend extra time validating package contents.  Currently, runs sha256 verification
                on every file within each package during installation.
                """
            ),
            signing_metadata_url_base=dals(
                """
                Base URL for obtaining trust metadata updates (i.e., the `*.root.json` and
                `key_mgr.json` files) used to verify metadata and (eventually) package signatures.
                """
            ),
            shortcuts=dals(
                """
                Allow packages to create OS-specific shortcuts (e.g. in the Windows Start
                Menu) at install time.
                """
            ),
            shortcuts_only=dals(
                """
                Create shortcuts only for the specified package names.
                """
            ),
            show_channel_urls=dals(
                """
                Show channel URLs when displaying what is going to be downloaded.
                """
            ),
            ssl_verify=dals(
                """
                Conda verifies SSL certificates for HTTPS requests, just like a web
                browser. By default, SSL verification is enabled, and conda operations will
                fail if a required url's certificate cannot be verified. Setting ssl_verify to
                False disables certification verification. The value for ssl_verify can also
                be (1) a path to a CA bundle file, (2) a path to a directory containing
                certificates of trusted CA, or (3) 'truststore' to use the
                operating system certificate store.
                """
            ),
            track_features=dals(
                """
                A list of features that are tracked by default. An entry here is similar to
                adding an entry to the create_default_packages list.
                """
            ),
            repodata_fns=dals(
                """
                Specify filenames for repodata fetching. The default is ('current_repodata.json',
                'repodata.json'), which tries a subset of the full index containing only the
                latest version for each package, then falls back to repodata.json.  You may
                want to specify something else to use an alternate index that has been reduced
                somehow.
                """
            ),
            use_index_cache=dals(
                """
                Use cache of channel index files, even if it has expired.
                """
            ),
            use_only_tar_bz2=dals(
                """
                A boolean indicating that only .tar.bz2 conda packages should be downloaded.
                This is forced to True if conda-build is installed and older than 3.18.3,
                because older versions of conda break when conda feeds it the new file format.
                """
            ),
            verbosity=dals(
                """
                Sets output log level. 0 is warn. 1 is info. 2 is debug. 3 is trace.
                """
            ),
            verify_threads=dals(
                """
                Threads to use when performing the transaction verification step.  When not set,
                defaults to 1.
                """
            ),
            allowlist_channels=dals(
                """
                The exclusive list of channels allowed to be used on the system. Use of any
                other channels will result in an error. If conda-build channels are to be
                allowed, along with the --use-local command line flag, be sure to include the
                'local' channel in the list. If the list is empty or left undefined, no
                channel exclusions will be enforced.
                """
            ),
            unsatisfiable_hints=dals(
                """
                A boolean to determine if conda should find conflicting packages in the case
                of a failed install.
                """
            ),
            unsatisfiable_hints_check_depth=dals(
                """
                An integer that specifies how many levels deep to search for unsatisfiable
                dependencies. If this number is 1 it will complete the unsatisfiable hints
                fastest (but perhaps not the most complete). The higher this number, the
                longer the generation of the unsat hint will take. Defaults to 3.
                """
            ),
            solver=dals(
                """
                A string to choose between the different solver logics implemented in
                conda. A solver logic takes care of turning your requested packages into a
                list of specs to add and/or remove from a given environment, based on their
                dependencies and specified constraints.
                """
            ),
            number_channel_notices=dals(
                """
                Sets the number of channel notices to be displayed when running commands
                the "install", "create", "update", "env create", and "env update" . Defaults
                to 5. In order to completely suppress channel notices, set this to 0.
                """
            ),
            experimental=dals(
                """
                List of experimental features to enable.
                """
            ),
            no_lock=dals(
                """
                Disable index cache lock (defaults to enabled).
                """
            ),
            repodata_use_zst=dals(
                """
                Disable check for `repodata.json.zst`; use `repodata.json` only.
                """
            ),
            envvars_force_uppercase=dals(
                """
                Force uppercase for new environment variable names. Defaults to True.
                """
            ),
        )


def reset_context(search_path=SEARCH_PATH, argparse_args=None):
    global context

    # reset plugin config params
    remove_all_plugin_settings()

    context.__init__(search_path, argparse_args)
    context.__dict__.pop("_Context__conda_build", None)
    from ..models.channel import Channel

    Channel._reset_state()
    # need to import here to avoid circular dependency
    return context


@contextmanager
def fresh_context(env=None, search_path=SEARCH_PATH, argparse_args=None, **kwargs):
    if env or kwargs:
        old_env = os.environ.copy()
        os.environ.update(env or {})
        os.environ.update(kwargs)
    yield reset_context(search_path=search_path, argparse_args=argparse_args)
    if env or kwargs:
        os.environ.clear()
        os.environ.update(old_env)
        reset_context()


class ContextStackObject:
    def __init__(self, search_path=SEARCH_PATH, argparse_args=None):
        self.set_value(search_path, argparse_args)

    def set_value(self, search_path=SEARCH_PATH, argparse_args=None):
        self.search_path = search_path
        self.argparse_args = argparse_args

    def apply(self):
        reset_context(self.search_path, self.argparse_args)


class ContextStack:
    def __init__(self):
        self._stack = [ContextStackObject() for _ in range(3)]
        self._stack_idx = 0
        self._last_search_path = None
        self._last_argparse_args = None

    def push(self, search_path, argparse_args):
        self._stack_idx += 1
        old_len = len(self._stack)
        if self._stack_idx >= old_len:
            self._stack.extend([ContextStackObject() for _ in range(old_len)])
        self._stack[self._stack_idx].set_value(search_path, argparse_args)
        self.apply()

    def apply(self):
        if (
            self._last_search_path != self._stack[self._stack_idx].search_path
            or self._last_argparse_args != self._stack[self._stack_idx].argparse_args
        ):
            # Expensive:
            self._stack[self._stack_idx].apply()
            self._last_search_path = self._stack[self._stack_idx].search_path
            self._last_argparse_args = self._stack[self._stack_idx].argparse_args

    def pop(self):
        self._stack_idx -= 1
        self._stack[self._stack_idx].apply()

    def replace(self, search_path, argparse_args):
        self._stack[self._stack_idx].set_value(search_path, argparse_args)
        self._stack[self._stack_idx].apply()


context_stack = ContextStack()


def stack_context(pushing, search_path=SEARCH_PATH, argparse_args=None):
    if pushing:
        # Fast
        context_stack.push(search_path, argparse_args)
    else:
        # Slow
        context_stack.pop()


# Default means "The configuration when there are no condarc files present". It is
# all the settings and defaults that are built in to the code and *not* the default
# value of search_path=SEARCH_PATH. It means search_path=().
def stack_context_default(pushing, argparse_args=None):
    return stack_context(pushing, search_path=(), argparse_args=argparse_args)


def replace_context(pushing=None, search_path=SEARCH_PATH, argparse_args=None):
    # pushing arg intentionally not used here, but kept for API compatibility
    return context_stack.replace(search_path, argparse_args)


def replace_context_default(pushing=None, argparse_args=None):
    # pushing arg intentionally not used here, but kept for API compatibility
    return context_stack.replace(search_path=(), argparse_args=argparse_args)


# Tests that want to only declare 'I support the project-wide default for how to
# manage stacking of contexts'. Tests that are known to be careful with context
# can use `replace_context_default` which might be faster, though it should
# be a stated goal to set conda_tests_ctxt_mgmt_def_pol to replace_context_default
# and not to stack_context_default.
conda_tests_ctxt_mgmt_def_pol = replace_context_default


@deprecated("24.3", "24.9")
@lru_cache(maxsize=None)
def _get_cpu_info():
    # DANGER: This is rather slow
    from .._vendor.cpuinfo import get_cpu_info

    return frozendict(get_cpu_info())


def env_name(prefix):
    # counter part to `locate_prefix_by_name()` below
    if not prefix:
        return None
    if paths_equal(prefix, context.root_prefix):
        return ROOT_ENV_NAME
    maybe_envs_dir, maybe_name = path_split(prefix)
    for envs_dir in context.envs_dirs:
        if paths_equal(envs_dir, maybe_envs_dir):
            return maybe_name
    return prefix


def locate_prefix_by_name(name, envs_dirs=None):
    """Find the location of a prefix given a conda env name.  If the location does not exist, an
    error is raised.
    """
    assert name
    if name in (ROOT_ENV_NAME, "root"):
        return context.root_prefix
    if envs_dirs is None:
        envs_dirs = context.envs_dirs
    for envs_dir in envs_dirs:
        if not isdir(envs_dir):
            continue
        prefix = join(envs_dir, name)
        if isdir(prefix):
            return abspath(prefix)

    from ..exceptions import EnvironmentNameNotFound

    raise EnvironmentNameNotFound(name)


def validate_prefix_name(prefix_name: str, ctx: Context, allow_base=True) -> str:
    """Run various validations to make sure prefix_name is valid"""
    from ..exceptions import CondaValueError

    if PREFIX_NAME_DISALLOWED_CHARS.intersection(prefix_name):
        raise CondaValueError(
            dals(
                f"""
                Invalid environment name: {prefix_name!r}
                Characters not allowed: {PREFIX_NAME_DISALLOWED_CHARS}
                If you are specifying a path to an environment, the `-p`
                flag should be used instead.
                """
            )
        )

    if prefix_name in (ROOT_ENV_NAME, "root"):
        if allow_base:
            return ctx.root_prefix
        else:
            raise CondaValueError(
                "Use of 'base' as environment name is not allowed here."
            )

    else:
        from ..exceptions import EnvironmentNameNotFound

        try:
            return locate_prefix_by_name(prefix_name)
        except EnvironmentNameNotFound:
            return join(_first_writable_envs_dir(), prefix_name)


def determine_target_prefix(ctx, args=None):
    """Get the prefix to operate in.  The prefix may not yet exist.

    Args:
        ctx: the context of conda
        args: the argparse args from the command line

    Returns: the prefix
    Raises: CondaEnvironmentNotFoundError if the prefix is invalid
    """
    argparse_args = args or ctx._argparse_args
    try:
        prefix_name = argparse_args.name
    except AttributeError:
        prefix_name = None
    try:
        prefix_path = argparse_args.prefix
    except AttributeError:
        prefix_path = None

    if prefix_name is not None and not prefix_name.strip():  # pragma: no cover
        from ..exceptions import ArgumentError

        raise ArgumentError("Argument --name requires a value.")

    if prefix_path is not None and not prefix_path.strip():  # pragma: no cover
        from ..exceptions import ArgumentError

        raise ArgumentError("Argument --prefix requires a value.")

    if prefix_name is None and prefix_path is None:
        return ctx.default_prefix
    elif prefix_path is not None:
        return expand(prefix_path)
    else:
        return validate_prefix_name(prefix_name, ctx=ctx)


def _first_writable_envs_dir():
    # Calling this function will *create* an envs directory if one does not already
    # exist. Any caller should intend to *use* that directory for *writing*, not just reading.
    for envs_dir in context.envs_dirs:
        if envs_dir == os.devnull:
            continue

        # The magic file being used here could change in the future.  Don't write programs
        # outside this code base that rely on the presence of this file.
        # This value is duplicated in conda.gateways.disk.create.create_envs_directory().
        envs_dir_magic_file = join(envs_dir, ".conda_envs_dir_test")

        if isfile(envs_dir_magic_file):
            try:
                open(envs_dir_magic_file, "a").close()
                return envs_dir
            except OSError:
                log.log(TRACE, "Tried envs_dir but not writable: %s", envs_dir)
        else:
            from ..gateways.disk.create import create_envs_directory

            was_created = create_envs_directory(envs_dir)
            if was_created:
                return envs_dir

    from ..exceptions import NoWritableEnvsDirError

    raise NoWritableEnvsDirError(context.envs_dirs)


def get_plugin_config_data(
    data: dict[Path, dict[str, RawParameter]],
) -> dict[Path, dict[str, RawParameter]]:
    """
    This is used to move everything under the key "plugins" from the provided dictionary
    to the top level of the returned dictionary. The returned dictionary is then passed
    to :class:`PluginConfig`.
    """
    new_data = defaultdict(dict)

    for source, config in data.items():
        if plugin_data := config.get("plugins"):
            plugin_data_value = plugin_data.value(None)

            if not isinstance(plugin_data_value, Mapping):
                continue

            for param_name, raw_param in plugin_data_value.items():
                new_data[source][param_name] = raw_param

        elif source == EnvRawParameter.source:
            for env_var, raw_param in config.items():
                if env_var.startswith("plugins_"):
                    _, param_name = env_var.split("plugins_")
                    new_data[source][param_name] = raw_param

    return new_data


class PluginConfig(metaclass=ConfigurationType):
    """
    Class used to hold settings for conda plugins.

    The object created by this class should only be accessed via
    :class:`conda.base.context.Context.plugins`.

    When this class is updated via the :func:`add_plugin_setting` function it adds new setting
    properties which can be accessed later via the context object.

    We currently call that function in
    :meth:`conda.plugins.manager.CondaPluginManager.load_settings`.
    because ``CondaPluginManager`` has access to all registered plugin settings via the settings
    plugin hook.
    """

    def __init__(self, data):
        self._cache_ = {}
        self.raw_data = get_plugin_config_data(data)


def add_plugin_setting(name: str, parameter: Parameter, aliases: tuple[str, ...] = ()):
    """
    Adds a setting to the :class:`PluginConfig` class
    """
    PluginConfig.parameter_names = PluginConfig.parameter_names + (name,)
    loader = ParameterLoader(parameter, aliases=aliases)
    name = loader._set_name(name)
    setattr(PluginConfig, name, loader)


def remove_all_plugin_settings() -> None:
    """
    Removes all attached settings from the :class:`PluginConfig` class
    """
    for name in PluginConfig.parameter_names:
        try:
            delattr(PluginConfig, name)
        except AttributeError:
            continue

    PluginConfig.parameter_names = tuple()


try:
    context = Context((), None)
except ConfigurationLoadError as e:  # pragma: no cover
    print(repr(e), file=sys.stderr)
    # Exception handler isn't loaded so use sys.exit
    sys.exit(1)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Base exceptions."""

from ..deprecations import deprecated

deprecated.module("24.3", "24.9", addendum="Nothing to import.")


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Code in ``conda.base`` is the lowest level of the application stack.  It is loaded and executed
virtually every time the application is executed. Any code within, and any of its imports, must
be highly performant.

Conda modules importable from ``conda.base`` are

- ``conda._vendor``
- ``conda.base``
- ``conda.common``

Modules prohibited from importing ``conda.base`` are:

- ``conda._vendor``
- ``conda.common``

All other ``conda`` modules may import from ``conda.base``.
"""


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Topological sorting implementation."""

from functools import reduce as _reduce
from logging import getLogger

log = getLogger(__name__)


def _toposort(data):
    """Dependencies are expressed as a dictionary whose keys are items
    and whose values are a set of dependent items. Output is a list of
    sets in topological order. The first set consists of items with no
    dependences, each subsequent set consists of items that depend upon
    items in the preceding sets.
    """
    # Special case empty input.
    if len(data) == 0:
        return

    # Ignore self dependencies.
    for k, v in data.items():
        v.discard(k)
    # Find all items that don't depend on anything.
    extra_items_in_deps = _reduce(set.union, data.values()) - set(data.keys())
    # Add empty dependences where needed.
    data.update({item: set() for item in extra_items_in_deps})
    while True:
        ordered = sorted({item for item, dep in data.items() if len(dep) == 0})
        if not ordered:
            break

        for item in ordered:
            yield item
            data.pop(item, None)

        for dep in sorted(data.values()):
            dep -= set(ordered)

    if len(data) != 0:
        from ..exceptions import CondaValueError

        msg = "Cyclic dependencies exist among these items: {}"
        raise CondaValueError(msg.format(" -> ".join(repr(x) for x in data.keys())))


def pop_key(data):
    """
    Pop an item from the graph that has the fewest dependencies in the case of a tie
    The winners will be sorted alphabetically
    """
    items = sorted(data.items(), key=lambda item: (len(item[1]), item[0]))
    key = items[0][0]

    data.pop(key)

    for dep in data.values():
        dep.discard(key)

    return key


def _safe_toposort(data):
    """Dependencies are expressed as a dictionary whose keys are items
    and whose values are a set of dependent items. Output is a list of
    sets in topological order. The first set consists of items with no
    dependencies, each subsequent set consists of items that depend upon
    items in the preceding sets.
    """
    # Special case empty input.
    if len(data) == 0:
        return

    t = _toposort(data)

    while True:
        try:
            value = next(t)
            yield value
        except ValueError as err:
            log.debug(err.args[0])

            if not data:
                return  # pragma: nocover

            yield pop_key(data)

            t = _toposort(data)

            continue
        except StopIteration:
            return


def toposort(data, safe=True):
    data = {k: set(v) for k, v in data.items()}

    if "python" in data:
        # Special case: Remove circular dependency between python and pip,
        # to ensure python is always installed before anything that needs it.
        # For more details:
        # - https://github.com/conda/conda/issues/1152
        # - https://github.com/conda/conda/pull/1154
        # - https://github.com/conda/conda-build/issues/401
        # - https://github.com/conda/conda/pull/1614
        data["python"].discard("pip")

    if safe:
        return list(_safe_toposort(data))
    else:
        return list(_toposort(data))


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Replacements for parts of the toolz library."""

from __future__ import annotations

import collections
import itertools
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from typing import Any, Generator, Sequence


def groupby_to_dict(keyfunc, sequence):
    """A `toolz`-style groupby implementation.

    Returns a dictionary of { key: [group] } instead of iterators.
    """
    result = collections.defaultdict(list)
    for key, group in itertools.groupby(sequence, keyfunc):
        result[key].extend(group)
    return dict(result)


def unique(sequence: Sequence[Any]) -> Generator[Any, None, None]:
    """A `toolz` inspired `unique` implementation.

    Returns a generator of unique elements in the sequence
    """
    seen: set[Any] = set()
    yield from (
        # seen.add always returns None so we will always return element
        seen.add(element) or element
        for element in sequence
        # only pass along novel elements
        if element not in seen
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Intercept signals and handle them gracefully."""

import signal
import threading
from contextlib import contextmanager
from logging import getLogger

log = getLogger(__name__)

INTERRUPT_SIGNALS = (
    "SIGABRT",
    "SIGINT",
    "SIGTERM",
    "SIGQUIT",
    "SIGBREAK",
)


def get_signal_name(signum):
    """
    Examples:
        >>> from signal import SIGINT
        >>> get_signal_name(SIGINT)
        'SIGINT'

    """
    return next(
        (
            k
            for k, v in signal.__dict__.items()
            if v == signum and k.startswith("SIG") and not k.startswith("SIG_")
        ),
        None,
    )


@contextmanager
def signal_handler(handler):
    # TODO: test and fix windows
    #   https://danielkaes.wordpress.com/2009/06/04/how-to-catch-kill-events-with-python/
    _thread_local = threading.local()
    _thread_local.previous_handlers = []
    for signame in INTERRUPT_SIGNALS:
        sig = getattr(signal, signame, None)
        if sig:
            log.debug("registering handler for %s", signame)
            try:
                prev_handler = signal.signal(sig, handler)
                _thread_local.previous_handlers.append((sig, prev_handler))
            except ValueError as e:  # pragma: no cover
                # ValueError: signal only works in main thread
                log.debug("%r", e)
    try:
        yield
    finally:
        standard_handlers = signal.SIG_IGN, signal.SIG_DFL
        for sig, previous_handler in _thread_local.previous_handlers:
            if callable(previous_handler) or previous_handler in standard_handlers:
                log.debug("de-registering handler for %s", sig)
                signal.signal(sig, previous_handler)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Common I/O utilities."""

import json
import logging
import os
import signal
import sys
from collections import defaultdict
from concurrent.futures import Executor, Future, ThreadPoolExecutor, _base, as_completed
from concurrent.futures.thread import _WorkItem
from contextlib import contextmanager
from enum import Enum
from errno import EPIPE, ESHUTDOWN
from functools import partial, wraps
from io import BytesIO, StringIO
from itertools import cycle
from logging import CRITICAL, WARN, Formatter, StreamHandler, getLogger
from os.path import dirname, isdir, isfile, join
from threading import Event, Lock, RLock, Thread
from time import sleep, time

from ..auxlib.decorators import memoizemethod
from ..auxlib.logz import NullHandler
from ..auxlib.type_coercion import boolify
from .compat import encode_environment, on_win
from .constants import NULL
from .path import expand

log = getLogger(__name__)
IS_INTERACTIVE = hasattr(sys.stdout, "isatty") and sys.stdout.isatty()


class DeltaSecondsFormatter(Formatter):
    """
    Logging formatter with additional attributes for run time logging.

    Attributes:
      `delta_secs`:
        Elapsed seconds since last log/format call (or creation of logger).
      `relative_created_secs`:
        Like `relativeCreated`, time relative to the initialization of the
        `logging` module but conveniently scaled to seconds as a `float` value.
    """

    def __init__(self, fmt=None, datefmt=None):
        self.prev_time = time()
        super().__init__(fmt=fmt, datefmt=datefmt)

    def format(self, record):
        now = time()
        prev_time = self.prev_time
        self.prev_time = max(self.prev_time, now)
        record.delta_secs = now - prev_time
        record.relative_created_secs = record.relativeCreated / 1000
        return super().format(record)


if boolify(os.environ.get("CONDA_TIMED_LOGGING")):
    _FORMATTER = DeltaSecondsFormatter(
        "%(relative_created_secs) 7.2f %(delta_secs) 7.2f "
        "%(levelname)s %(name)s:%(funcName)s(%(lineno)d): %(message)s"
    )
else:
    _FORMATTER = Formatter(
        "%(levelname)s %(name)s:%(funcName)s(%(lineno)d): %(message)s"
    )


def dashlist(iterable, indent=2):
    return "".join("\n" + " " * indent + "- " + str(x) for x in iterable)


class ContextDecorator:
    """Base class for a context manager class (implementing __enter__() and __exit__()) that also
    makes it a decorator.
    """

    # TODO: figure out how to improve this pattern so e.g. swallow_broken_pipe doesn't have to be instantiated  # NOQA

    def __call__(self, f):
        @wraps(f)
        def decorated(*args, **kwds):
            with self:
                return f(*args, **kwds)

        return decorated


class SwallowBrokenPipe(ContextDecorator):
    # Ignore BrokenPipeError and errors related to stdout or stderr being
    # closed by a downstream program.

    def __enter__(self):
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        if (
            exc_val
            and isinstance(exc_val, EnvironmentError)
            and getattr(exc_val, "errno", None)
            and exc_val.errno in (EPIPE, ESHUTDOWN)
        ):
            return True


swallow_broken_pipe = SwallowBrokenPipe()


class CaptureTarget(Enum):
    """Constants used for contextmanager captured.

    Used similarly like the constants PIPE, STDOUT for stdlib's subprocess.Popen.
    """

    STRING = -1
    STDOUT = -2


@contextmanager
def env_vars(var_map=None, callback=None, stack_callback=None):
    if var_map is None:
        var_map = {}

    new_var_map = encode_environment(var_map)
    saved_vars = {}
    for name, value in new_var_map.items():
        saved_vars[name] = os.environ.get(name, NULL)
        os.environ[name] = value
    try:
        if callback:
            callback()
        if stack_callback:
            stack_callback(True)
        yield
    finally:
        for name, value in saved_vars.items():
            if value is NULL:
                del os.environ[name]
            else:
                os.environ[name] = value
        if callback:
            callback()
        if stack_callback:
            stack_callback(False)


@contextmanager
def env_var(name, value, callback=None, stack_callback=None):
    # Maybe, but in env_vars, not here:
    #    from .compat import ensure_fs_path_encoding
    #    d = dict({name: ensure_fs_path_encoding(value)})
    d = {name: value}
    with env_vars(d, callback=callback, stack_callback=stack_callback) as es:
        yield es


@contextmanager
def env_unmodified(callback=None):
    with env_vars(callback=callback) as es:
        yield es


@contextmanager
def captured(stdout=CaptureTarget.STRING, stderr=CaptureTarget.STRING):
    r"""Capture outputs of sys.stdout and sys.stderr.

    If stdout is STRING, capture sys.stdout as a string,
    if stdout is None, do not capture sys.stdout, leaving it untouched,
    otherwise redirect sys.stdout to the file-like object given by stdout.

    Behave correspondingly for stderr with the exception that if stderr is STDOUT,
    redirect sys.stderr to stdout target and set stderr attribute of yielded object to None.

    .. code-block:: pycon

       >>> from conda.common.io import captured
       >>> with captured() as c:
       ...     print("hello world!")
       ...
       >>> c.stdout
       'hello world!\n'

    Args:
        stdout: capture target for sys.stdout, one of STRING, None, or file-like object
        stderr: capture target for sys.stderr, one of STRING, STDOUT, None, or file-like object

    Yields:
        CapturedText: has attributes stdout, stderr which are either strings, None or the
            corresponding file-like function argument.
    """

    def write_wrapper(self, to_write):
        # NOTE: This function is not thread-safe.  Using within multi-threading may cause spurious
        # behavior of not returning sys.stdout and sys.stderr back to their 'proper' state
        # This may have to deal with a *lot* of text.
        if hasattr(self, "mode") and "b" in self.mode:
            wanted = bytes
        elif isinstance(self, BytesIO):
            wanted = bytes
        else:
            wanted = str
        if not isinstance(to_write, wanted):
            if hasattr(to_write, "decode"):
                decoded = to_write.decode("utf-8")
                self.old_write(decoded)
            elif hasattr(to_write, "encode"):
                b = to_write.encode("utf-8")
                self.old_write(b)
        else:
            self.old_write(to_write)

    class CapturedText:
        pass

    # sys.stdout.write(u'unicode out')
    # sys.stdout.write(bytes('bytes out', encoding='utf-8'))
    # sys.stdout.write(str('str out'))
    saved_stdout, saved_stderr = sys.stdout, sys.stderr
    if stdout == CaptureTarget.STRING:
        outfile = StringIO()
        outfile.old_write = outfile.write
        outfile.write = partial(write_wrapper, outfile)
        sys.stdout = outfile
    else:
        outfile = stdout
        if outfile is not None:
            sys.stdout = outfile
    if stderr == CaptureTarget.STRING:
        errfile = StringIO()
        errfile.old_write = errfile.write
        errfile.write = partial(write_wrapper, errfile)
        sys.stderr = errfile
    elif stderr == CaptureTarget.STDOUT:
        sys.stderr = errfile = outfile
    else:
        errfile = stderr
        if errfile is not None:
            sys.stderr = errfile
    c = CapturedText()
    log.debug("overtaking stderr and stdout")
    try:
        yield c
    finally:
        if stdout == CaptureTarget.STRING:
            c.stdout = outfile.getvalue()
        else:
            c.stdout = outfile
        if stderr == CaptureTarget.STRING:
            c.stderr = errfile.getvalue()
        elif stderr == CaptureTarget.STDOUT:
            c.stderr = None
        else:
            c.stderr = errfile
        sys.stdout, sys.stderr = saved_stdout, saved_stderr
        log.debug("stderr and stdout yielding back")


@contextmanager
def argv(args_list):
    saved_args = sys.argv
    sys.argv = args_list
    try:
        yield
    finally:
        sys.argv = saved_args


@contextmanager
def _logger_lock():
    logging._acquireLock()
    try:
        yield
    finally:
        logging._releaseLock()


@contextmanager
def disable_logger(logger_name):
    logr = getLogger(logger_name)
    _lvl, _dsbld, _prpgt = logr.level, logr.disabled, logr.propagate
    null_handler = NullHandler()
    with _logger_lock():
        logr.addHandler(null_handler)
        logr.setLevel(CRITICAL + 1)
        logr.disabled, logr.propagate = True, False
    try:
        yield
    finally:
        with _logger_lock():
            logr.removeHandler(null_handler)  # restore list logr.handlers
            logr.level, logr.disabled = _lvl, _dsbld
            logr.propagate = _prpgt


@contextmanager
def stderr_log_level(level, logger_name=None):
    logr = getLogger(logger_name)
    _hndlrs, _lvl, _dsbld, _prpgt = (
        logr.handlers,
        logr.level,
        logr.disabled,
        logr.propagate,
    )
    handler = StreamHandler(sys.stderr)
    handler.name = "stderr"
    handler.setLevel(level)
    handler.setFormatter(_FORMATTER)
    with _logger_lock():
        logr.setLevel(level)
        logr.handlers, logr.disabled, logr.propagate = [], False, False
        logr.addHandler(handler)
        logr.setLevel(level)
    try:
        yield
    finally:
        with _logger_lock():
            logr.handlers, logr.level, logr.disabled = _hndlrs, _lvl, _dsbld
            logr.propagate = _prpgt


def attach_stderr_handler(
    level=WARN,
    logger_name=None,
    propagate=False,
    formatter=None,
    filters=None,
):
    """Attach a new `stderr` handler to the given logger and configure both.

    This function creates a new StreamHandler that writes to `stderr` and attaches it
    to the logger given by `logger_name` (which maybe `None`, in which case the root
    logger is used). If the logger already has a handler by the name of `stderr`, it is
    removed first.

    The given `level` is set **for the handler**, not for the logger; however, this
    function also sets the level of the given logger to the minimum of its current
    effective level and the new handler level, ensuring that the handler will receive the
    required log records, while minimizing the number of unnecessary log events. It also
    sets the loggers `propagate` property according to the `propagate` argument.
    The `formatter` argument can be used to set the formatter of the handler.
    """
    # get old stderr logger
    logr = getLogger(logger_name)
    old_stderr_handler = next(
        (handler for handler in logr.handlers if handler.name == "stderr"), None
    )

    # create new stderr logger
    new_stderr_handler = StreamHandler(sys.stderr)
    new_stderr_handler.name = "stderr"
    new_stderr_handler.setLevel(level)
    new_stderr_handler.setFormatter(formatter or _FORMATTER)
    for filter_ in filters or ():
        new_stderr_handler.addFilter(filter_)

    # do the switch
    with _logger_lock():
        if old_stderr_handler:
            logr.removeHandler(old_stderr_handler)
        logr.addHandler(new_stderr_handler)
        if level < logr.getEffectiveLevel():
            logr.setLevel(level)
        logr.propagate = propagate


def timeout(timeout_secs, func, *args, default_return=None, **kwargs):
    """Enforce a maximum time for a callable to complete.
    Not yet implemented on Windows.
    """
    if on_win:
        # Why does Windows have to be so difficult all the time? Kind of gets old.
        # Guess we'll bypass Windows timeouts for now.
        try:
            return func(*args, **kwargs)
        except KeyboardInterrupt:  # pragma: no cover
            return default_return
    else:

        class TimeoutException(Exception):
            pass

        def interrupt(signum, frame):
            raise TimeoutException()

        signal.signal(signal.SIGALRM, interrupt)
        signal.alarm(timeout_secs)

        try:
            ret = func(*args, **kwargs)
            signal.alarm(0)
            return ret
        except (TimeoutException, KeyboardInterrupt):  # pragma: no cover
            return default_return


class Spinner:
    """
    Args:
        message (str):
            A message to prefix the spinner with. The string ': ' is automatically appended.
        enabled (bool):
            If False, usage is a no-op.
        json (bool):
           If True, will not output non-json to stdout.

    """

    # spinner_cycle = cycle("")
    spinner_cycle = cycle("/-\\|")

    def __init__(self, message, enabled=True, json=False, fail_message="failed\n"):
        self.message = message
        self.enabled = enabled
        self.json = json

        self._stop_running = Event()
        self._spinner_thread = Thread(target=self._start_spinning)
        self._indicator_length = len(next(self.spinner_cycle)) + 1
        self.fh = sys.stdout
        self.show_spin = enabled and not json and IS_INTERACTIVE
        self.fail_message = fail_message

    def start(self):
        if self.show_spin:
            self._spinner_thread.start()
        elif not self.json:
            self.fh.write("...working... ")
            self.fh.flush()

    def stop(self):
        if self.show_spin:
            self._stop_running.set()
            self._spinner_thread.join()
            self.show_spin = False

    def _start_spinning(self):
        try:
            while not self._stop_running.is_set():
                self.fh.write(next(self.spinner_cycle) + " ")
                self.fh.flush()
                sleep(0.10)
                self.fh.write("\b" * self._indicator_length)
        except OSError as e:
            if e.errno in (EPIPE, ESHUTDOWN):
                self.stop()
            else:
                raise

    @swallow_broken_pipe
    def __enter__(self):
        if not self.json:
            sys.stdout.write(f"{self.message}: ")
            sys.stdout.flush()
        self.start()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.stop()
        if not self.json:
            with swallow_broken_pipe:
                if exc_type or exc_val:
                    sys.stdout.write(self.fail_message)
                else:
                    sys.stdout.write("done\n")
                sys.stdout.flush()


class ProgressBar:
    @classmethod
    def get_lock(cls):
        # Used only for --json (our own sys.stdout.write/flush calls).
        if not hasattr(cls, "_lock"):
            cls._lock = RLock()
        return cls._lock

    def __init__(
        self, description, enabled=True, json=False, position=None, leave=True
    ):
        """
        Args:
            description (str):
                The name of the progress bar, shown on left side of output.
            enabled (bool):
                If False, usage is a no-op.
            json (bool):
                If true, outputs json progress to stdout rather than a progress bar.
                Currently, the json format assumes this is only used for "fetch", which
                maintains backward compatibility with conda 4.3 and earlier behavior.
        """
        self.description = description
        self.enabled = enabled
        self.json = json

        if json:
            pass
        elif enabled:
            if IS_INTERACTIVE:
                bar_format = "{desc}{bar} | {percentage:3.0f}% "
                try:
                    self.pbar = self._tqdm(
                        desc=description,
                        bar_format=bar_format,
                        ascii=True,
                        total=1,
                        file=sys.stdout,
                        position=position,
                        leave=leave,
                    )
                except OSError as e:
                    if e.errno in (EPIPE, ESHUTDOWN):
                        self.enabled = False
                    else:
                        raise
            else:
                self.pbar = None
                sys.stdout.write(f"{description} ...working...")

    def update_to(self, fraction):
        try:
            if self.enabled:
                if self.json:
                    with self.get_lock():
                        sys.stdout.write(
                            f'{{"fetch":"{self.description}","finished":false,"maxval":1,"progress":{fraction:f}}}\n\0'
                        )
                elif IS_INTERACTIVE:
                    self.pbar.update(fraction - self.pbar.n)
                elif fraction == 1:
                    sys.stdout.write(" done\n")
        except OSError as e:
            if e.errno in (EPIPE, ESHUTDOWN):
                self.enabled = False
            else:
                raise

    def finish(self):
        self.update_to(1)

    def refresh(self):
        """Force refresh i.e. once 100% has been reached"""
        if self.enabled and not self.json and IS_INTERACTIVE:
            self.pbar.refresh()

    @swallow_broken_pipe
    def close(self):
        if self.enabled:
            if self.json:
                with self.get_lock():
                    sys.stdout.write(
                        f'{{"fetch":"{self.description}","finished":true,"maxval":1,"progress":1}}\n\0'
                    )
                    sys.stdout.flush()
            elif IS_INTERACTIVE:
                self.pbar.close()
            else:
                sys.stdout.write(" done\n")

    @staticmethod
    def _tqdm(*args, **kwargs):
        """Deferred import so it doesn't hit the `conda activate` paths."""
        from tqdm.auto import tqdm

        return tqdm(*args, **kwargs)


# use this for debugging, because ProcessPoolExecutor isn't pdb/ipdb friendly
class DummyExecutor(Executor):
    def __init__(self):
        self._shutdown = False
        self._shutdownLock = Lock()

    def submit(self, fn, *args, **kwargs):
        with self._shutdownLock:
            if self._shutdown:
                raise RuntimeError("cannot schedule new futures after shutdown")

            f = Future()
            try:
                result = fn(*args, **kwargs)
            except BaseException as e:
                f.set_exception(e)
            else:
                f.set_result(result)

            return f

    def map(self, func, *iterables):
        for iterable in iterables:
            for thing in iterable:
                yield func(thing)

    def shutdown(self, wait=True):
        with self._shutdownLock:
            self._shutdown = True


class ThreadLimitedThreadPoolExecutor(ThreadPoolExecutor):
    def __init__(self, max_workers=10):
        super().__init__(max_workers)

    def submit(self, fn, *args, **kwargs):
        """
        This is an exact reimplementation of the `submit()` method on the parent class, except
        with an added `try/except` around `self._adjust_thread_count()`.  So long as there is at
        least one living thread, this thread pool will not throw an exception if threads cannot
        be expanded to `max_workers`.

        In the implementation, we use "protected" attributes from concurrent.futures (`_base`
        and `_WorkItem`). Consider vendoring the whole concurrent.futures library
        as an alternative to these protected imports.

        https://github.com/agronholm/pythonfutures/blob/3.2.0/concurrent/futures/thread.py#L121-L131  # NOQA
        https://github.com/python/cpython/blob/v3.6.4/Lib/concurrent/futures/thread.py#L114-L124
        """
        with self._shutdown_lock:
            if self._shutdown:
                raise RuntimeError("cannot schedule new futures after shutdown")

            f = _base.Future()
            w = _WorkItem(f, fn, args, kwargs)

            self._work_queue.put(w)
            try:
                self._adjust_thread_count()
            except RuntimeError:
                # RuntimeError: can't start new thread
                # See https://github.com/conda/conda/issues/6624
                if len(self._threads) > 0:
                    # It's ok to not be able to start new threads if we already have at least
                    # one thread alive.
                    pass
                else:
                    raise
            return f


as_completed = as_completed


def get_instrumentation_record_file():
    default_record_file = join("~", ".conda", "instrumentation-record.csv")
    return expand(
        os.environ.get("CONDA_INSTRUMENTATION_RECORD_FILE", default_record_file)
    )


class time_recorder(ContextDecorator):  # pragma: no cover
    record_file = get_instrumentation_record_file()
    start_time = None
    total_call_num = defaultdict(int)
    total_run_time = defaultdict(float)

    def __init__(self, entry_name=None, module_name=None):
        self.entry_name = entry_name
        self.module_name = module_name

    def _set_entry_name(self, f):
        if self.entry_name is None:
            if hasattr(f, "__qualname__"):
                entry_name = f.__qualname__
            else:
                entry_name = ":" + f.__name__
            if self.module_name:
                entry_name = ".".join((self.module_name, entry_name))
            self.entry_name = entry_name

    def __call__(self, f):
        self._set_entry_name(f)
        return super().__call__(f)

    def __enter__(self):
        enabled = os.environ.get("CONDA_INSTRUMENTATION_ENABLED")
        if enabled and boolify(enabled):
            self.start_time = time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.start_time:
            entry_name = self.entry_name
            end_time = time()
            run_time = end_time - self.start_time
            self.total_call_num[entry_name] += 1
            self.total_run_time[entry_name] += run_time
            self._ensure_dir()
            with open(self.record_file, "a") as fh:
                fh.write(f"{entry_name},{run_time:f}\n")
            # total_call_num = self.total_call_num[entry_name]
            # total_run_time = self.total_run_time[entry_name]
            # log.debug('%s %9.3f %9.3f %d', entry_name, run_time, total_run_time, total_call_num)

    @classmethod
    def log_totals(cls):
        enabled = os.environ.get("CONDA_INSTRUMENTATION_ENABLED")
        if not (enabled and boolify(enabled)):
            return
        log.info("=== time_recorder total time and calls ===")
        for entry_name in sorted(cls.total_run_time.keys()):
            log.info(
                "TOTAL %9.3f % 9d %s",
                cls.total_run_time[entry_name],
                cls.total_call_num[entry_name],
                entry_name,
            )

    @memoizemethod
    def _ensure_dir(self):
        if not isdir(dirname(self.record_file)):
            os.makedirs(dirname(self.record_file))


def print_instrumentation_data():  # pragma: no cover
    record_file = get_instrumentation_record_file()

    grouped_data = defaultdict(list)
    final_data = {}

    if not isfile(record_file):
        return

    with open(record_file) as fh:
        for line in fh:
            entry_name, total_time = line.strip().split(",")
            grouped_data[entry_name].append(float(total_time))

    for entry_name in sorted(grouped_data):
        all_times = grouped_data[entry_name]
        counts = len(all_times)
        total_time = sum(all_times)
        average_time = total_time / counts
        final_data[entry_name] = {
            "counts": counts,
            "total_time": total_time,
            "average_time": average_time,
        }

    print(json.dumps(final_data, sort_keys=True, indent=2, separators=(",", ": ")))


if __name__ == "__main__":
    print_instrumentation_data()


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""YAML and JSON serialization and deserialization functions."""

import functools
import json
from io import StringIO
from logging import getLogger

import ruamel.yaml as yaml

from ..auxlib.entity import EntityEncoder

log = getLogger(__name__)


# FUTURE: Python 3.9+, replace with functools.cache
@functools.lru_cache(maxsize=None)
def _yaml_round_trip():
    parser = yaml.YAML(typ="rt")
    parser.indent(mapping=2, offset=2, sequence=4)
    return parser


# FUTURE: Python 3.9+, replace with functools.cache
@functools.lru_cache(maxsize=None)
def _yaml_safe():
    parser = yaml.YAML(typ="safe", pure=True)
    parser.indent(mapping=2, offset=2, sequence=4)
    parser.default_flow_style = False
    parser.sort_base_mapping_type_on_output = False
    return parser


def yaml_round_trip_load(string):
    return _yaml_round_trip().load(string)


def yaml_safe_load(string):
    """
    Examples:
        >>> yaml_safe_load("key: value")
        {'key': 'value'}

    """
    return _yaml_safe().load(string)


def yaml_round_trip_dump(object, stream=None):
    """Dump object to string or stream."""
    ostream = stream or StringIO()
    _yaml_round_trip().dump(object, ostream)
    if not stream:
        return ostream.getvalue()


def yaml_safe_dump(object, stream=None):
    """Dump object to string or stream."""
    ostream = stream or StringIO()
    _yaml_safe().dump(object, ostream)
    if not stream:
        return ostream.getvalue()


def json_load(string):
    return json.loads(string)


def json_dump(object):
    return json.dumps(
        object, indent=2, sort_keys=True, separators=(",", ": "), cls=EntityEncoder
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Common path utilities."""

from __future__ import annotations

import os
import re
import subprocess
from functools import lru_cache, reduce
from itertools import accumulate, chain
from logging import getLogger
from os.path import (
    abspath,
    basename,
    expanduser,
    expandvars,
    join,
    normcase,
    split,
    splitext,
)
from typing import TYPE_CHECKING
from urllib.parse import urlsplit

from .. import CondaError
from .compat import on_win

if TYPE_CHECKING:
    from typing import Iterable, Sequence

log = getLogger(__name__)

PATH_MATCH_REGEX = (
    r"\./"  # ./
    r"|\.\."  # ..
    r"|~"  # ~
    r"|/"  # /
    r"|[a-zA-Z]:[/\\]"  # drive letter, colon, forward or backslash
    r"|\\\\"  # windows UNC path
    r"|//"  # windows UNC path
)

# any other extension will be mangled by CondaSession.get() as it tries to find
# channel names from URLs, through strip_pkg_extension()
KNOWN_EXTENSIONS = (".conda", ".tar.bz2", ".json", ".jlap", ".json.zst")


def is_path(value):
    if "://" in value:
        return False
    return re.match(PATH_MATCH_REGEX, value)


def expand(path):
    return abspath(expanduser(expandvars(path)))


def paths_equal(path1, path2):
    """
    Examples:
        >>> paths_equal('/a/b/c', '/a/b/c/d/..')
        True

    """
    if on_win:
        return normcase(abspath(path1)) == normcase(abspath(path2))
    else:
        return abspath(path1) == abspath(path2)


@lru_cache(maxsize=None)
def url_to_path(url):
    """Convert a file:// URL to a path.

    Relative file URLs (i.e. `file:relative/path`) are not supported.
    """
    if is_path(url):
        return url
    if not url.startswith("file://"):  # pragma: no cover
        raise CondaError(
            f"You can only turn absolute file: urls into paths (not {url})"
        )
    _, netloc, path, _, _ = urlsplit(url)
    from .url import percent_decode

    path = percent_decode(path)
    if netloc not in ("", "localhost", "127.0.0.1", "::1"):
        if not netloc.startswith("\\\\"):
            # The only net location potentially accessible is a Windows UNC path
            netloc = "//" + netloc
    else:
        netloc = ""
        # Handle Windows drive letters if present
        if re.match("^/([a-z])[:|]", path, re.I):
            path = path[1] + ":" + path[3:]
    return netloc + path


def tokenized_startswith(test_iterable, startswith_iterable):
    return all(t == sw for t, sw in zip(test_iterable, startswith_iterable))


def get_all_directories(files: Iterable[str]) -> list[tuple[str]]:
    return sorted({tuple(f.split("/")[:-1]) for f in files} - {()})


def get_leaf_directories(files: Iterable[str]) -> Sequence[str]:
    # give this function a list of files, and it will hand back a list of leaf
    # directories to pass to os.makedirs()
    directories = get_all_directories(files)
    if not directories:
        return ()

    leaves = []

    def _process(x, y):
        if not tokenized_startswith(y, x):
            leaves.append(x)
        return y

    last = reduce(_process, directories)

    if not leaves:
        leaves.append(directories[-1])
    elif not tokenized_startswith(last, leaves[-1]):
        leaves.append(last)

    return tuple("/".join(leaf) for leaf in leaves)


def explode_directories(child_directories: Iterable[tuple[str, ...]]) -> set[str]:
    # get all directories including parents
    # child_directories must already be split with os.path.split
    return set(
        chain.from_iterable(
            accumulate(directory, join) for directory in child_directories if directory
        )
    )


def pyc_path(py_path, python_major_minor_version):
    """
    This must not return backslashes on Windows as that will break
    tests and leads to an eventual need to make url_to_path return
    backslashes too and that may end up changing files on disc or
    to the result of comparisons with the contents of them.
    """
    pyver_string = python_major_minor_version.replace(".", "")
    if pyver_string.startswith("2"):
        return py_path + "c"
    else:
        directory, py_file = split(py_path)
        basename_root, extension = splitext(py_file)
        pyc_file = (
            "__pycache__" + "/" + f"{basename_root}.cpython-{pyver_string}{extension}c"
        )
        return "{}{}{}".format(directory, "/", pyc_file) if directory else pyc_file


def missing_pyc_files(python_major_minor_version, files):
    # returns a tuple of tuples, with the inner tuple being the .py file and the missing .pyc file
    py_files = (f for f in files if f.endswith(".py"))
    pyc_matches = (
        (py_file, pyc_path(py_file, python_major_minor_version)) for py_file in py_files
    )
    result = tuple(match for match in pyc_matches if match[1] not in files)
    return result


def parse_entry_point_def(ep_definition):
    cmd_mod, func = ep_definition.rsplit(":", 1)
    command, module = cmd_mod.rsplit("=", 1)
    command, module, func = command.strip(), module.strip(), func.strip()
    return command, module, func


def get_python_short_path(python_version=None):
    if on_win:
        return "python.exe"
    if python_version and "." not in python_version:
        python_version = ".".join(python_version)
    return join("bin", "python%s" % (python_version or ""))


def get_python_site_packages_short_path(python_version):
    if python_version is None:
        return None
    elif on_win:
        return "Lib/site-packages"
    else:
        py_ver = get_major_minor_version(python_version)
        return f"lib/python{py_ver}/site-packages"


_VERSION_REGEX = re.compile(r"[0-9]+\.[0-9]+")


def get_major_minor_version(string, with_dot=True):
    # returns None if not found, otherwise two digits as a string
    # should work for
    #   - 3.5.2
    #   - 27
    #   - bin/python2.7
    #   - lib/python34/site-packages/
    # the last two are dangers because windows doesn't have version information there
    assert isinstance(string, str)
    if string.startswith("lib/python"):
        pythonstr = string.split("/")[1]
        start = len("python")
        if len(pythonstr) < start + 2:
            return None
        maj_min = pythonstr[start], pythonstr[start + 1 :]
    elif string.startswith("bin/python"):
        pythonstr = string.split("/")[1]
        start = len("python")
        if len(pythonstr) < start + 3:
            return None
        assert pythonstr[start + 1] == "."
        maj_min = pythonstr[start], pythonstr[start + 2 :]
    else:
        match = _VERSION_REGEX.match(string)
        if match:
            version = match.group(0).split(".")
            maj_min = version[0], version[1]
        else:
            digits = "".join([c for c in string if c.isdigit()])
            if len(digits) < 2:
                return None
            maj_min = digits[0], digits[1:]

    return ".".join(maj_min) if with_dot else "".join(maj_min)


def get_bin_directory_short_path():
    return "Scripts" if on_win else "bin"


def win_path_ok(path):
    return path.replace("/", "\\") if on_win else path


def win_path_double_escape(path):
    return path.replace("\\", "\\\\") if on_win else path


def win_path_backout(path):
    # replace all backslashes except those escaping spaces
    # if we pass a file url, something like file://\\unc\path\on\win, make sure
    #   we clean that up too
    return re.sub(r"(\\(?! ))", r"/", path).replace(":////", "://")


def ensure_pad(name, pad="_"):
    """

    Examples:
        >>> ensure_pad('conda')
        '_conda_'
        >>> ensure_pad('_conda')
        '__conda_'
        >>> ensure_pad('')
        ''

    """
    if not name or name[0] == name[-1] == pad:
        return name
    else:
        return f"{pad}{name}{pad}"


def is_private_env_name(env_name):
    """

    Examples:
        >>> is_private_env_name("_conda")
        False
        >>> is_private_env_name("_conda_")
        True

    """
    return env_name and env_name[0] == env_name[-1] == "_"


def is_private_env_path(env_path):
    """

    Examples:
        >>> is_private_env_path('/some/path/to/envs/_conda_')
        True
        >>> is_private_env_path('/not/an/envs_dir/_conda_')
        False

    """
    if env_path is not None:
        envs_directory, env_name = split(env_path)
        if basename(envs_directory) != "envs":
            return False
        return is_private_env_name(env_name)
    return False


def right_pad_os_sep(path):
    return path if path.endswith(os.sep) else path + os.sep


def split_filename(path_or_url):
    dn, fn = split(path_or_url)
    return (dn or None, fn) if "." in fn else (path_or_url, None)


def get_python_noarch_target_path(source_short_path, target_site_packages_short_path):
    if source_short_path.startswith("site-packages/"):
        sp_dir = target_site_packages_short_path
        return source_short_path.replace("site-packages", sp_dir, 1)
    elif source_short_path.startswith("python-scripts/"):
        bin_dir = get_bin_directory_short_path()
        return source_short_path.replace("python-scripts", bin_dir, 1)
    else:
        return source_short_path


def win_path_to_unix(path, root_prefix=""):
    # If the user wishes to drive conda from MSYS2 itself while also having
    # msys2 packages in their environment this allows the path conversion to
    # happen relative to the actual shell. The onus is on the user to set
    # CYGPATH to e.g. /usr/bin/cygpath.exe (this will be translated to e.g.
    # (C:\msys32\usr\bin\cygpath.exe by MSYS2) to ensure this one is used.
    if not path:
        return ""

    # rebind to shutil to avoid triggering the deprecation warning
    from shutil import which

    bash = which("bash")
    if bash:
        cygpath = os.environ.get(
            "CYGPATH", os.path.join(os.path.dirname(bash), "cygpath.exe")
        )
    else:
        cygpath = os.environ.get("CYGPATH", "cygpath.exe")
    try:
        path = (
            subprocess.check_output([cygpath, "-up", path])
            .decode("ascii")
            .split("\n")[0]
        )
    except Exception as e:
        log.debug(f"{e!r}", exc_info=True)

        # Convert a path or ;-separated string of paths into a unix representation
        # Does not add cygdrive.  If you need that, set root_prefix to "/cygdrive"
        def _translation(found_path):  # NOQA
            found = (
                found_path.group(1)
                .replace("\\", "/")
                .replace(":", "")
                .replace("//", "/")
            )
            return root_prefix + "/" + found

        path_re = '(?<![:/^a-zA-Z])([a-zA-Z]:[/\\\\]+(?:[^:*?"<>|]+[/\\\\]+)*[^:*?"<>|;/\\\\]+?(?![a-zA-Z]:))'  # noqa
        path = re.sub(path_re, _translation, path).replace(";/", ":/")
    return path


def which(executable):
    """Backwards-compatibility wrapper. Use `shutil.which` directly if possible."""
    from shutil import which

    return which(executable)


def strip_pkg_extension(path: str):
    """
    Examples:
        >>> strip_pkg_extension("/path/_license-1.1-py27_1.tar.bz2")
        ('/path/_license-1.1-py27_1', '.tar.bz2')
        >>> strip_pkg_extension("/path/_license-1.1-py27_1.conda")
        ('/path/_license-1.1-py27_1', '.conda')
        >>> strip_pkg_extension("/path/_license-1.1-py27_1")
        ('/path/_license-1.1-py27_1', None)
    """
    # NOTE: not using CONDA_TARBALL_EXTENSION_V1 or CONDA_TARBALL_EXTENSION_V2 to comply with
    #       import rules and to avoid a global lookup.
    for extension in KNOWN_EXTENSIONS:
        if path.endswith(extension):
            return path[: -len(extension)], extension
    return path, None


def is_package_file(path):
    """
    Examples:
        >>> is_package_file("/path/_license-1.1-py27_1.tar.bz2")
        True
        >>> is_package_file("/path/_license-1.1-py27_1.conda")
        True
        >>> is_package_file("/path/_license-1.1-py27_1")
        False
    """
    # NOTE: not using CONDA_TARBALL_EXTENSION_V1 or CONDA_TARBALL_EXTENSION_V2 to comply with
    #       import rules and to avoid a global lookup.
    return path[-6:] == ".conda" or path[-8:] == ".tar.bz2"


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
A generalized application configuration utility.

Features include:
  - lazy eval
  - merges configuration files
  - parameter type validation, with custom validation
  - parameter aliases

Easily extensible to other source formats, e.g. json and ini

"""

from __future__ import annotations

import copy
import sys
from abc import ABCMeta, abstractmethod
from collections import defaultdict
from collections.abc import Mapping
from enum import Enum, EnumMeta
from functools import wraps
from itertools import chain
from logging import getLogger
from os import environ
from os.path import expandvars
from pathlib import Path
from re import IGNORECASE, VERBOSE, compile
from string import Template
from typing import TYPE_CHECKING

from boltons.setutils import IndexedSet
from ruamel.yaml.comments import CommentedMap, CommentedSeq
from ruamel.yaml.reader import ReaderError
from ruamel.yaml.scanner import ScannerError

from .. import CondaError, CondaMultiError
from ..auxlib.collection import AttrDict, first, last
from ..auxlib.exceptions import ThisShouldNeverHappenError
from ..auxlib.type_coercion import TypeCoercionError, typify, typify_data_structure
from ..common.iterators import unique
from ..deprecations import deprecated
from .compat import isiterable, primitive_types
from .constants import NULL
from .serialize import yaml_round_trip_load

try:
    from frozendict import deepfreeze, frozendict
    from frozendict import getFreezeConversionMap as _getFreezeConversionMap
    from frozendict import register as _register

    if Enum not in _getFreezeConversionMap():
        # leave enums as is, deepfreeze will flatten it into a dict
        # see https://github.com/Marco-Sulla/python-frozendict/issues/98
        _register(Enum, lambda x: x)

    del _getFreezeConversionMap
    del _register
except ImportError:
    from .._vendor.frozendict import frozendict
    from ..auxlib.collection import make_immutable as deepfreeze

if TYPE_CHECKING:
    from re import Match
    from typing import Any, Hashable, Iterable, Sequence

log = getLogger(__name__)

EMPTY_MAP = frozendict()


def pretty_list(iterable, padding="  "):  # TODO: move elsewhere in conda.common
    if not isiterable(iterable):
        iterable = [iterable]
    try:
        return "\n".join(f"{padding}- {item}" for item in iterable)
    except TypeError:
        return pretty_list([iterable], padding)


def pretty_map(dictionary, padding="  "):
    return "\n".join(f"{padding}{key}: {value}" for key, value in dictionary.items())


def expand_environment_variables(unexpanded):
    if isinstance(unexpanded, (str, bytes)):
        return expandvars(unexpanded)
    else:
        return unexpanded


class ConfigurationError(CondaError):
    pass


class ConfigurationLoadError(ConfigurationError):
    def __init__(self, path, message_addition="", **kwargs):
        message = "Unable to load configuration file.\n  path: %(path)s\n"
        super().__init__(message + message_addition, path=path, **kwargs)


class ValidationError(ConfigurationError):
    def __init__(self, parameter_name, parameter_value, source, msg=None, **kwargs):
        self.parameter_name = parameter_name
        self.parameter_value = parameter_value
        self.source = source
        super().__init__(msg, **kwargs)


class MultipleKeysError(ValidationError):
    def __init__(self, source, keys, preferred_key):
        self.source = source
        self.keys = keys
        msg = (
            f"Multiple aliased keys in file {source}:\n"
            f"{pretty_list(keys)}\n"
            f"Must declare only one. Prefer '{preferred_key}'"
        )
        super().__init__(preferred_key, None, source, msg=msg)


class InvalidTypeError(ValidationError):
    def __init__(
        self, parameter_name, parameter_value, source, wrong_type, valid_types, msg=None
    ):
        self.wrong_type = wrong_type
        self.valid_types = valid_types
        if msg is None:
            msg = (
                f"Parameter {parameter_name} = {parameter_value!r} declared in {source} has type {wrong_type}.\n"
                f"Valid types:\n{pretty_list(valid_types)}"
            )
        super().__init__(parameter_name, parameter_value, source, msg=msg)


class CustomValidationError(ValidationError):
    def __init__(self, parameter_name, parameter_value, source, custom_message):
        super().__init__(
            parameter_name,
            parameter_value,
            source,
            msg=(
                f"Parameter {parameter_name} = {parameter_value!r} declared in "
                f"{source} is invalid.\n{custom_message}"
            ),
        )


class MultiValidationError(CondaMultiError, ConfigurationError):
    def __init__(self, errors, *args, **kwargs):
        super().__init__(errors, *args, **kwargs)


def raise_errors(errors):
    if not errors:
        return True
    elif len(errors) == 1:
        raise errors[0]
    else:
        raise MultiValidationError(errors)


class ParameterFlag(Enum):
    final = "final"
    top = "top"
    bottom = "bottom"

    def __str__(self):
        return f"{self.value}"

    @classmethod
    def from_name(cls, name):
        return cls[name]

    @classmethod
    def from_value(cls, value):
        return cls(value)

    @classmethod
    def from_string(cls, string):
        try:
            string = string.strip("!#")
            return cls.from_value(string)
        except (ValueError, AttributeError):
            return None


class RawParameter(metaclass=ABCMeta):
    def __init__(self, source, key, raw_value):
        self.source = source
        self.key = key
        try:
            self._raw_value = raw_value.decode("utf-8")
        except AttributeError:
            # AttributeError: raw_value is not encoded
            self._raw_value = raw_value

    def __repr__(self):
        return str(vars(self))

    @abstractmethod
    def value(self, parameter_obj):
        raise NotImplementedError()

    @abstractmethod
    def keyflag(self):
        raise NotImplementedError()

    @abstractmethod
    def valueflags(self, parameter_obj):
        raise NotImplementedError()

    @classmethod
    def make_raw_parameters(cls, source, from_map):
        if from_map:
            return {key: cls(source, key, from_map[key]) for key in from_map}
        return EMPTY_MAP


class EnvRawParameter(RawParameter):
    source = "envvars"

    def value(self, parameter_obj):
        # note: this assumes that EnvRawParameters will only have flat configuration of either
        # primitive or sequential type
        if hasattr(parameter_obj, "string_delimiter"):
            assert isinstance(self._raw_value, str)
            string_delimiter = getattr(parameter_obj, "string_delimiter")
            # TODO: add stripping of !important, !top, and !bottom
            return tuple(
                EnvRawParameter(EnvRawParameter.source, self.key, v)
                for v in (vv.strip() for vv in self._raw_value.split(string_delimiter))
                if v
            )
        else:
            return self.__important_split_value[0].strip()

    def keyflag(self):
        return ParameterFlag.final if len(self.__important_split_value) >= 2 else None

    def valueflags(self, parameter_obj):
        if hasattr(parameter_obj, "string_delimiter"):
            string_delimiter = getattr(parameter_obj, "string_delimiter")
            # TODO: add stripping of !important, !top, and !bottom
            return tuple("" for _ in self._raw_value.split(string_delimiter))
        else:
            return self.__important_split_value[0].strip()

    @property
    def __important_split_value(self):
        return self._raw_value.split("!important")

    @classmethod
    def make_raw_parameters(cls, appname):
        keystart = f"{appname.upper()}_"
        raw_env = {
            k.replace(keystart, "", 1).lower(): v
            for k, v in environ.items()
            if k.startswith(keystart)
        }
        return super().make_raw_parameters(EnvRawParameter.source, raw_env)


class ArgParseRawParameter(RawParameter):
    source = "cmd_line"

    def value(self, parameter_obj):
        # note: this assumes ArgParseRawParameter will only have flat configuration of either
        # primitive or sequential type
        if isiterable(self._raw_value):
            children_values = []
            for i in range(len(self._raw_value)):
                children_values.append(
                    ArgParseRawParameter(self.source, self.key, self._raw_value[i])
                )
            return tuple(children_values)
        else:
            return deepfreeze(self._raw_value)

    def keyflag(self):
        return None

    def valueflags(self, parameter_obj):
        return None if isinstance(parameter_obj, PrimitiveLoadedParameter) else ()

    @classmethod
    def make_raw_parameters(cls, args_from_argparse):
        return super().make_raw_parameters(
            ArgParseRawParameter.source, args_from_argparse
        )


class YamlRawParameter(RawParameter):
    # this class should encapsulate all direct use of ruamel.yaml in this module

    def __init__(self, source, key, raw_value, key_comment):
        self._key_comment = key_comment
        super().__init__(source, key, raw_value)

        if isinstance(self._raw_value, CommentedSeq):
            value_comments = self._get_yaml_list_comments(self._raw_value)
            self._value_flags = tuple(
                ParameterFlag.from_string(s) for s in value_comments
            )
            children_values = []
            for i in range(len(self._raw_value)):
                children_values.append(
                    YamlRawParameter(
                        self.source, self.key, self._raw_value[i], value_comments[i]
                    )
                )
            self._value = tuple(children_values)
        elif isinstance(self._raw_value, CommentedMap):
            value_comments = self._get_yaml_map_comments(self._raw_value)
            self._value_flags = {
                k: ParameterFlag.from_string(v)
                for k, v in value_comments.items()
                if v is not None
            }
            children_values = {}
            for k, v in self._raw_value.items():
                children_values[k] = YamlRawParameter(
                    self.source, self.key, v, value_comments[k]
                )
            self._value = frozendict(children_values)
        elif isinstance(self._raw_value, primitive_types):
            self._value_flags = None
            self._value = self._raw_value
        else:
            print(type(self._raw_value), self._raw_value, file=sys.stderr)
            raise ThisShouldNeverHappenError()  # pragma: no cover

    def value(self, parameter_obj):
        return self._value

    def keyflag(self):
        return ParameterFlag.from_string(self._key_comment)

    def valueflags(self, parameter_obj):
        return self._value_flags

    @staticmethod
    def _get_yaml_key_comment(commented_dict, key):
        try:
            return commented_dict.ca.items[key][2].value.strip()
        except (AttributeError, KeyError):
            return None

    @classmethod
    def _get_yaml_list_comments(cls, value):
        # value is a ruamel.yaml CommentedSeq, len(value) is the number of lines in the sequence,
        # value.ca is the comment object for the sequence and the comments themselves are stored as
        # a sparse dict
        list_comments = []
        for i in range(len(value)):
            try:
                list_comments.append(cls._get_yaml_list_comment_item(value.ca.items[i]))
            except (AttributeError, IndexError, KeyError, TypeError):
                list_comments.append(None)
        return tuple(list_comments)

    @staticmethod
    def _get_yaml_list_comment_item(item):
        # take the pre_item comment if available
        # if not, take the first post_item comment if available
        if item[0]:
            return item[0].value.strip() or None
        else:
            return item[1][0].value.strip() or None

    @staticmethod
    def _get_yaml_map_comments(value):
        map_comments = {}
        for key in value:
            try:
                map_comments[key] = value.ca.items[key][2].value.strip() or None
            except (AttributeError, KeyError):
                map_comments[key] = None
        return map_comments

    @classmethod
    def make_raw_parameters(cls, source, from_map):
        if from_map:
            return {
                key: cls(
                    source, key, from_map[key], cls._get_yaml_key_comment(from_map, key)
                )
                for key in from_map
            }
        return EMPTY_MAP

    @classmethod
    def make_raw_parameters_from_file(cls, filepath):
        with open(filepath) as fh:
            try:
                yaml_obj = yaml_round_trip_load(fh)
            except ScannerError as err:
                mark = err.problem_mark
                raise ConfigurationLoadError(
                    filepath,
                    "  reason: invalid yaml at line %(line)s, column %(column)s",
                    line=mark.line,
                    column=mark.column,
                )
            except ReaderError as err:
                raise ConfigurationLoadError(
                    filepath,
                    "  reason: invalid yaml at position %(position)s",
                    position=err.position,
                )
            return cls.make_raw_parameters(filepath, yaml_obj) or EMPTY_MAP


class DefaultValueRawParameter(RawParameter):
    """Wraps a default value as a RawParameter, for usage in ParameterLoader."""

    def __init__(self, source, key, raw_value):
        super().__init__(source, key, raw_value)

        if isinstance(self._raw_value, Mapping):
            children_values = {}
            for k, v in self._raw_value.items():
                children_values[k] = DefaultValueRawParameter(self.source, self.key, v)
            self._value = frozendict(children_values)
        elif isiterable(self._raw_value):
            children_values = []
            for i in range(len(self._raw_value)):
                children_values.append(
                    DefaultValueRawParameter(self.source, self.key, self._raw_value[i])
                )
            self._value = tuple(children_values)
        elif isinstance(self._raw_value, ConfigurationObject):
            self._value = self._raw_value
            for attr_name, attr_value in vars(self._raw_value).items():
                self._value.__setattr__(
                    attr_name,
                    DefaultValueRawParameter(self.source, self.key, attr_value),
                )
        elif isinstance(self._raw_value, Enum):
            self._value = self._raw_value
        elif isinstance(self._raw_value, primitive_types):
            self._value = self._raw_value
        else:
            raise ThisShouldNeverHappenError()  # pragma: no cover

    def value(self, parameter_obj):
        return self._value

    def keyflag(self):
        return None

    def valueflags(self, parameter_obj):
        if isinstance(self._raw_value, Mapping):
            return frozendict()
        elif isiterable(self._raw_value):
            return ()
        elif isinstance(self._raw_value, ConfigurationObject):
            return None
        elif isinstance(self._raw_value, Enum):
            return None
        elif isinstance(self._raw_value, primitive_types):
            return None
        else:
            raise ThisShouldNeverHappenError()  # pragma: no cover


@deprecated("24.3", "24.9")
def load_file_configs(search_path: Iterable[Path | str], **kwargs) -> dict[Path, dict]:
    expanded_paths = Configuration._expand_search_path(search_path, **kwargs)
    return dict(Configuration._load_search_path(expanded_paths))


class LoadedParameter(metaclass=ABCMeta):
    # (type) describes the type of parameter
    _type = None
    # (Parameter or type) if the LoadedParameter holds a collection, describes the element held in
    # the collection. if not, describes the primitive type held by the LoadedParameter.
    _element_type = None

    def __init__(self, name, value, key_flag, value_flags, validation=None):
        """
        Represents a Parameter that has been loaded with configuration value.

        Args:
            name (str): name of the loaded parameter
            value (LoadedParameter or primitive): the value of the loaded parameter
            key_flag (ParameterFlag or None): priority flag for the parameter itself
            value_flags (Any or None): priority flags for the parameter values
            validation (callable): Given a parameter value as input, return a boolean indicating
                validity, or alternately return a string describing an invalid value.
        """
        self._name = name
        self.value = value
        self.key_flag = key_flag
        self.value_flags = value_flags
        self._validation = validation

    def __eq__(self, other):
        if type(other) is type(self):
            return self.value == other.value
        return False

    def __hash__(self):
        return hash(self.value)

    def collect_errors(self, instance, typed_value, source="<<merged>>"):
        """
        Validate a LoadedParameter typed value.

        Args:
            instance (Configuration): the instance object used to create the LoadedParameter.
            typed_value (Any): typed value to validate.
            source (str): string description for the source of the typed_value.
        """
        errors = []
        if not isinstance(typed_value, self._type):
            errors.append(
                InvalidTypeError(
                    self._name, typed_value, source, type(self.value), self._type
                )
            )
        elif self._validation is not None:
            result = self._validation(typed_value)
            if result is False:
                errors.append(ValidationError(self._name, typed_value, source))
            elif isinstance(result, str):
                errors.append(
                    CustomValidationError(self._name, typed_value, source, result)
                )
        return errors

    def expand(self):
        """
        Recursively expands any environment values in the Loaded Parameter.

        Returns: LoadedParameter
        """
        # This is similar to conda.auxlib.type_coercion.typify_data_structure
        # It could be DRY-er but that would break SRP.
        if isinstance(self.value, Mapping):
            new_value = type(self.value)((k, v.expand()) for k, v in self.value.items())
        elif isiterable(self.value):
            new_value = type(self.value)(v.expand() for v in self.value)
        elif isinstance(self.value, ConfigurationObject):
            for attr_name, attr_value in vars(self.value).items():
                if isinstance(attr_value, LoadedParameter):
                    self.value.__setattr__(attr_name, attr_value.expand())
            return self.value
        else:
            new_value = expand_environment_variables(self.value)
        self.value = new_value
        return self

    @abstractmethod
    def merge(self, matches):
        """
        Recursively merges matches into one LoadedParameter.

        Args:
            matches (List<LoadedParameter>): list of matches of this parameter.

        Returns: LoadedParameter
        """
        raise NotImplementedError()

    def typify(self, source):
        """
        Recursively types a LoadedParameter.

        Args:
            source (str): string describing the source of the LoadedParameter.

        Returns: a primitive, sequence, or map representing the typed value.
        """
        element_type = self._element_type
        try:
            return LoadedParameter._typify_data_structure(
                self.value, source, element_type
            )
        except TypeCoercionError as e:
            msg = str(e)
            if issubclass(element_type, Enum):
                choices = ", ".join(
                    map("'{}'".format, element_type.__members__.values())
                )
                msg += f"\nValid choices for {self._name}: {choices}"
            raise CustomValidationError(self._name, e.value, source, msg)

    @staticmethod
    def _typify_data_structure(value, source, type_hint=None):
        if isinstance(value, Mapping):
            return type(value)((k, v.typify(source)) for k, v in value.items())
        elif isiterable(value):
            return type(value)(v.typify(source) for v in value)
        elif isinstance(value, ConfigurationObject):
            for attr_name, attr_value in vars(value).items():
                if isinstance(attr_value, LoadedParameter):
                    value.__setattr__(attr_name, attr_value.typify(source))
            return value
        elif (
            isinstance(value, str)
            and isinstance(type_hint, type)
            and issubclass(type_hint, str)
        ):
            # This block is necessary because if we fall through to typify(), we end up calling
            # .strip() on the str, when sometimes we want to preserve preceding and trailing
            # whitespace.
            return type_hint(value)
        else:
            return typify(value, type_hint)

    @staticmethod
    def _match_key_is_important(loaded_parameter):
        return loaded_parameter.key_flag is ParameterFlag.final

    @staticmethod
    def _first_important_matches(matches):
        idx = first(
            enumerate(matches),
            lambda x: LoadedParameter._match_key_is_important(x[1]),
            apply=lambda x: x[0],
        )
        return matches if idx is None else matches[: idx + 1]


class PrimitiveLoadedParameter(LoadedParameter):
    """
    LoadedParameter type that holds a single python primitive value.

    The python primitive types are str, int, float, complex, bool, and NoneType. In addition,
    python 2 has long and unicode types.
    """

    def __init__(
        self, name, element_type, value, key_flag, value_flags, validation=None
    ):
        """
        Args:
            element_type (type or tuple[type]): Type-validation of parameter's value.
            value (primitive value): primitive python value.
        """
        self._type = element_type
        self._element_type = element_type
        super().__init__(name, value, key_flag, value_flags, validation)

    def __eq__(self, other):
        if type(other) is type(self):
            return self.value == other.value
        return False

    def __hash__(self):
        return hash(self.value)

    def merge(self, matches):
        important_match = first(
            matches, LoadedParameter._match_key_is_important, default=None
        )
        if important_match is not None:
            return important_match

        last_match = last(matches, lambda x: x is not None, default=None)
        if last_match is not None:
            return last_match
        raise ThisShouldNeverHappenError()  # pragma: no cover


class MapLoadedParameter(LoadedParameter):
    """LoadedParameter type that holds a map (i.e. dict) of LoadedParameters."""

    _type = frozendict

    def __init__(
        self, name, value, element_type, key_flag, value_flags, validation=None
    ):
        """
        Args:
            value (Mapping): Map of string keys to LoadedParameter values.
            element_type (Parameter): The Parameter type that is held in value.
            value_flags (Mapping): Map of priority value flags.
        """
        self._element_type = element_type
        super().__init__(name, value, key_flag, value_flags, validation)

    def collect_errors(self, instance, typed_value, source="<<merged>>"):
        errors = super().collect_errors(instance, typed_value, self.value)

        # recursively validate the values in the map
        if isinstance(self.value, Mapping):
            for key, value in self.value.items():
                errors.extend(value.collect_errors(instance, typed_value[key], source))
        return errors

    def merge(self, parameters: Sequence[MapLoadedParameter]) -> MapLoadedParameter:
        # get all values up to and including first important_match
        # but if no important_match, then all matches are important_matches
        parameters = LoadedParameter._first_important_matches(parameters)

        # ensure all parameter values are Mappings
        for parameter in parameters:
            if not isinstance(parameter.value, Mapping):
                raise InvalidTypeError(
                    self.name,
                    parameter.value,
                    parameter.source,
                    parameter.value.__class__.__name__,
                    self._type.__name__,
                )

        # map keys with final values,
        # first key has higher precedence than later ones
        final_map = {
            key: value
            for parameter in reversed(parameters)
            for key, value in parameter.value.items()
            if parameter.value_flags.get(key) == ParameterFlag.final
        }

        # map each value by recursively calling merge on any entries with the same key,
        # last key has higher precedence than earlier ones
        grouped_map = {}
        for parameter in parameters:
            for key, value in parameter.value.items():
                grouped_map.setdefault(key, []).append(value)
        merged_map = {
            key: values[0].merge(values) for key, values in grouped_map.items()
        }

        # update merged_map with final_map values
        merged_value = frozendict({**merged_map, **final_map})

        # create new parameter for the merged values
        return MapLoadedParameter(
            self._name,
            merged_value,
            self._element_type,
            self.key_flag,
            self.value_flags,
            validation=self._validation,
        )


class SequenceLoadedParameter(LoadedParameter):
    """LoadedParameter type that holds a sequence (i.e. list) of LoadedParameters."""

    _type = tuple

    def __init__(
        self, name, value, element_type, key_flag, value_flags, validation=None
    ):
        """
        Args:
            value (Sequence): Sequence of LoadedParameter values.
            element_type (Parameter): The Parameter type that is held in the sequence.
            value_flags (Sequence): Sequence of priority value_flags.
        """
        self._element_type = element_type
        super().__init__(name, value, key_flag, value_flags, validation)

    def collect_errors(self, instance, typed_value, source="<<merged>>"):
        errors = super().collect_errors(instance, typed_value, self.value)
        # recursively collect errors on the elements in the sequence
        for idx, element in enumerate(self.value):
            errors.extend(element.collect_errors(instance, typed_value[idx], source))
        return errors

    def merge(self, matches):
        # get matches up to and including first important_match
        # but if no important_match, then all matches are important_matches
        relevant_matches_and_values = tuple(
            (match, match.value)
            for match in LoadedParameter._first_important_matches(matches)
        )
        for match, value in relevant_matches_and_values:
            if not isinstance(value, tuple):
                raise InvalidTypeError(
                    self.name,
                    value,
                    match.source,
                    value.__class__.__name__,
                    self._type.__name__,
                )

        # get individual lines from important_matches that were marked important
        # these will be prepended to the final result
        def get_marked_lines(match, marker):
            return (
                tuple(
                    line
                    for line, flag in zip(match.value, match.value_flags)
                    if flag is marker
                )
                if match
                else ()
            )

        top_lines = chain.from_iterable(
            get_marked_lines(m, ParameterFlag.top)
            for m, _ in relevant_matches_and_values
        )

        # also get lines that were marked as bottom, but reverse the match order so that lines
        # coming earlier will ultimately be last
        bottom_lines = tuple(
            chain.from_iterable(
                get_marked_lines(match, ParameterFlag.bottom)
                for match, _ in reversed(relevant_matches_and_values)
            )
        )

        # now, concat all lines, while reversing the matches
        #   reverse because elements closer to the end of search path take precedence
        all_lines = chain.from_iterable(
            v for _, v in reversed(relevant_matches_and_values)
        )

        # stack top_lines + all_lines, then de-dupe
        top_deduped = tuple(unique((*top_lines, *all_lines)))

        # take the top-deduped lines, reverse them, and concat with reversed bottom_lines
        # this gives us the reverse of the order we want, but almost there
        # NOTE: for a line value marked both top and bottom, the bottom marker will win out
        #       for the top marker to win out, we'd need one additional de-dupe step
        bottom_deduped = tuple(
            unique((*reversed(bottom_lines), *reversed(top_deduped)))
        )
        # just reverse, and we're good to go
        merged_values = tuple(reversed(bottom_deduped))

        return SequenceLoadedParameter(
            self._name,
            merged_values,
            self._element_type,
            self.key_flag,
            self.value_flags,
            validation=self._validation,
        )


class ObjectLoadedParameter(LoadedParameter):
    """LoadedParameter type that holds a mapping (i.e. object) of LoadedParameters."""

    _type = object

    def __init__(
        self, name, value, element_type, key_flag, value_flags, validation=None
    ):
        """
        Args:
            value (Sequence): Object with LoadedParameter fields.
            element_type (object): The Parameter type that is held in the sequence.
            value_flags (Sequence): Sequence of priority value_flags.
        """
        self._element_type = element_type
        super().__init__(name, value, key_flag, value_flags, validation)

    def collect_errors(self, instance, typed_value, source="<<merged>>"):
        errors = super().collect_errors(instance, typed_value, self.value)

        # recursively validate the values in the object fields
        if isinstance(self.value, ConfigurationObject):
            for key, value in vars(self.value).items():
                if isinstance(value, LoadedParameter):
                    errors.extend(
                        value.collect_errors(instance, typed_value[key], source)
                    )
        return errors

    def merge(
        self, parameters: Sequence[ObjectLoadedParameter]
    ) -> ObjectLoadedParameter:
        # get all parameters up to and including first important_match
        # but if no important_match, then all parameters are important_matches
        parameters = LoadedParameter._first_important_matches(parameters)

        # map keys with final values,
        # first key has higher precedence than later ones
        final_map = {
            key: value
            for parameter in reversed(parameters)
            for key, value in vars(parameter.value).items()
            if (
                isinstance(value, LoadedParameter)
                and parameter.value_flags.get(key) == ParameterFlag.final
            )
        }

        # map each value by recursively calling merge on any entries with the same key,
        # last key has higher precedence than earlier ones
        grouped_map = {}
        for parameter in parameters:
            for key, value in vars(parameter.value).items():
                grouped_map.setdefault(key, []).append(value)
        merged_map = {
            key: values[0].merge(values) for key, values in grouped_map.items()
        }

        # update merged_map with final_map values
        merged_value = copy.deepcopy(self._element_type)
        for key, value in {**merged_map, **final_map}.items():
            merged_value.__setattr__(key, value)

        # create new parameter for the merged values
        return ObjectLoadedParameter(
            self._name,
            merged_value,
            self._element_type,
            self.key_flag,
            self.value_flags,
            validation=self._validation,
        )


class ConfigurationObject:
    """Dummy class to mark whether a Python object has config parameters within."""


class Parameter(metaclass=ABCMeta):
    # (type) describes the type of parameter
    _type = None
    # (Parameter or type) if the Parameter is holds a collection, describes the element held in
    # the collection. if not, describes the primitive type held by the Parameter.
    _element_type = None

    def __init__(self, default, validation=None):
        """
        The Parameter class represents an unloaded configuration parameter, holding type, default
        and validation information until the parameter is loaded with a configuration.

        Args:
            default (Any): the typed, python representation default value given if the Parameter
                is not found in a Configuration.
            validation (callable): Given a parameter value as input, return a boolean indicating
                validity, or alternately return a string describing an invalid value.
        """
        self._default = default
        self._validation = validation

    @property
    def default(self):
        """Returns a DefaultValueRawParameter that wraps the actual default value."""
        wrapped_default = DefaultValueRawParameter("default", "default", self._default)
        return self.load("default", wrapped_default)

    def get_all_matches(self, name, names, instance):
        """
        Finds all matches of a Parameter in a Configuration instance

        Args:
            name (str): canonical name of the parameter to search for
            names (tuple(str)): alternative aliases of the parameter
            instance (Configuration): instance of the configuration to search within

        Returns (List(RawParameter)): matches of the parameter found in the configuration.
        """
        matches = []
        multikey_exceptions = []
        for filepath, raw_parameters in instance.raw_data.items():
            match, error = ParameterLoader.raw_parameters_from_single_source(
                name, names, raw_parameters
            )
            if match is not None:
                matches.append(match)
            if error:
                multikey_exceptions.append(error)
        return matches, multikey_exceptions

    @abstractmethod
    def load(self, name, match):
        """
        Loads a Parameter with the value in a RawParameter.

        Args:
            name (str): name of the parameter to pass through
            match (RawParameter): the value of the RawParameter match

        Returns a LoadedParameter
        """
        raise NotImplementedError()

    def typify(self, name, source, value):
        element_type = self._element_type
        try:
            return typify_data_structure(value, element_type)
        except TypeCoercionError as e:
            msg = str(e)
            if issubclass(element_type, Enum):
                choices = ", ".join(
                    map("'{}'".format, element_type.__members__.values())
                )
                msg += f"\nValid choices for {name}: {choices}"
            raise CustomValidationError(name, e.value, source, msg)


class PrimitiveParameter(Parameter):
    """
    Parameter type for a Configuration class that holds a single python primitive value.

    The python primitive types are str, int, float, complex, bool, and NoneType. In addition,
    python 2 has long and unicode types.
    """

    def __init__(self, default, element_type=None, validation=None):
        """
        Args:
            default (primitive value): default value if the Parameter is not found.
            element_type (type or tuple[type]): Type-validation of parameter's value. If None,
                type(default) is used.
        """
        self._type = type(default) if element_type is None else element_type
        self._element_type = self._type
        super().__init__(default, validation)

    def load(self, name, match):
        return PrimitiveLoadedParameter(
            name,
            self._type,
            match.value(self._element_type),
            match.keyflag(),
            match.valueflags(self._element_type),
            validation=self._validation,
        )


class MapParameter(Parameter):
    """Parameter type for a Configuration class that holds a map (i.e. dict) of Parameters."""

    _type = frozendict

    def __init__(self, element_type, default=frozendict(), validation=None):
        """
        Args:
            element_type (Parameter): The Parameter type held in the MapParameter.
            default (Mapping):  The parameter's default value. If None, will be an empty dict.
        """
        self._element_type = element_type
        default = default and frozendict(default) or frozendict()
        super().__init__(default, validation=validation)

    def get_all_matches(self, name, names, instance):
        # it also config settings like `proxy_servers: ~`
        matches, exceptions = super().get_all_matches(name, names, instance)
        matches = tuple(m for m in matches if m._raw_value is not None)
        return matches, exceptions

    def load(self, name, match):
        value = match.value(self._element_type)
        if value is None:
            return MapLoadedParameter(
                name,
                frozendict(),
                self._element_type,
                match.keyflag(),
                frozendict(),
                validation=self._validation,
            )

        if not isinstance(value, Mapping):
            raise InvalidTypeError(
                name, value, match.source, value.__class__.__name__, self._type.__name__
            )

        loaded_map = {}
        for key, child_value in match.value(self._element_type).items():
            loaded_child_value = self._element_type.load(name, child_value)
            loaded_map[key] = loaded_child_value

        return MapLoadedParameter(
            name,
            frozendict(loaded_map),
            self._element_type,
            match.keyflag(),
            match.valueflags(self._element_type),
            validation=self._validation,
        )


class SequenceParameter(Parameter):
    """Parameter type for a Configuration class that holds a sequence (i.e. list) of Parameters."""

    _type = tuple

    def __init__(self, element_type, default=(), validation=None, string_delimiter=","):
        """
        Args:
            element_type (Parameter): The Parameter type that is held in the sequence.
            default (Sequence): default value, empty tuple if not given.
            string_delimiter (str): separation string used to parse string into sequence.
        """
        self._element_type = element_type
        self.string_delimiter = string_delimiter
        super().__init__(default, validation)

    def get_all_matches(self, name, names, instance):
        # this is necessary to handle argparse `action="append"`, which can't be set to a
        #   default value of NULL
        # it also config settings like `channels: ~`
        matches, exceptions = super().get_all_matches(name, names, instance)
        matches = tuple(m for m in matches if m._raw_value is not None)
        return matches, exceptions

    def load(self, name, match):
        value = match.value(self)
        if value is None:
            return SequenceLoadedParameter(
                name,
                (),
                self._element_type,
                match.keyflag(),
                (),
                validation=self._validation,
            )

        if not isiterable(value):
            raise InvalidTypeError(
                name, value, match.source, value.__class__.__name__, self._type.__name__
            )

        loaded_sequence = []
        for child_value in value:
            loaded_child_value = self._element_type.load(name, child_value)
            loaded_sequence.append(loaded_child_value)

        return SequenceLoadedParameter(
            name,
            tuple(loaded_sequence),
            self._element_type,
            match.keyflag(),
            match.valueflags(self._element_type),
            validation=self._validation,
        )


class ObjectParameter(Parameter):
    """Parameter type for a Configuration class that holds an object with Parameter fields."""

    _type = object

    def __init__(self, element_type, default=ConfigurationObject(), validation=None):
        """
        Args:
            element_type (object): The object type with parameter fields held in ObjectParameter.
            default (Sequence): default value, empty tuple if not given.
        """
        self._element_type = element_type
        super().__init__(default, validation)

    def get_all_matches(self, name, names, instance):
        # it also config settings like `proxy_servers: ~`
        matches, exceptions = super().get_all_matches(name, names, instance)
        matches = tuple(m for m in matches if m._raw_value is not None)
        return matches, exceptions

    def load(self, name, match):
        value = match.value(self._element_type)
        if value is None:
            return ObjectLoadedParameter(
                name,
                None,
                self._element_type,
                match.keyflag(),
                None,
                validation=self._validation,
            )

        if not isinstance(value, (Mapping, ConfigurationObject)):
            raise InvalidTypeError(
                name, value, match.source, value.__class__.__name__, self._type.__name__
            )

        # for a default object, extract out the instance variables
        if isinstance(value, ConfigurationObject):
            value = vars(value)

        object_parameter_attrs = {
            attr_name: parameter_type
            for attr_name, parameter_type in vars(self._element_type).items()
            if isinstance(parameter_type, Parameter) and attr_name in value.keys()
        }

        # recursively load object fields
        loaded_attrs = {}
        for attr_name, parameter_type in object_parameter_attrs.items():
            raw_child_value = value.get(attr_name)
            loaded_child_value = parameter_type.load(name, raw_child_value)
            loaded_attrs[attr_name] = loaded_child_value

        # copy object and replace Parameter with LoadedParameter fields
        object_copy = copy.deepcopy(self._element_type)
        for attr_name, loaded_child_parameter in loaded_attrs.items():
            object_copy.__setattr__(attr_name, loaded_child_parameter)

        return ObjectLoadedParameter(
            name,
            object_copy,
            self._element_type,
            match.keyflag(),
            match.valueflags(self._element_type),
            validation=self._validation,
        )


class ParameterLoader:
    """
    ParameterLoader class contains the top level logic needed to load a parameter from start to
    finish.
    """

    def __init__(self, parameter_type, aliases=(), expandvars=False):
        """
        Args:
            parameter_type (Parameter): the type of Parameter that is stored in the loader.
            aliases (tuple(str)): alternative aliases for the Parameter
            expandvars (bool): whether or not to recursively expand environmental variables.
        """
        self._name = None
        self._names = None
        self.type = parameter_type
        self.aliases = aliases
        self._expandvars = expandvars

    def _set_name(self, name):
        # this is an explicit method, and not a descriptor/setter
        # it's meant to be called by the Configuration metaclass
        self._name = name
        _names = frozenset(x for x in chain(self.aliases, (name,)))
        self._names = _names
        return name

    @property
    def name(self):
        if self._name is None:
            # The Configuration metaclass should call the `_set_name` method.
            raise ThisShouldNeverHappenError()  # pragma: no cover
        return self._name

    @property
    def names(self):
        if self._names is None:
            # The Configuration metaclass should call the `_set_name` method.
            raise ThisShouldNeverHappenError()  # pragma: no cover
        return self._names

    def __get__(self, instance, instance_type):
        # strategy is "extract and merge," which is actually just map and reduce
        # extract matches from each source in SEARCH_PATH
        # then merge matches together
        if self.name in instance._cache_:
            return instance._cache_[self.name]

        # step 1/2: load config and find top level matches
        raw_matches, errors = self.type.get_all_matches(self.name, self.names, instance)

        # step 3: parse RawParameters into LoadedParameters
        matches = [self.type.load(self.name, match) for match in raw_matches]

        # step 4: merge matches
        merged = matches[0].merge(matches) if matches else self.type.default

        # step 5: typify
        # We need to expand any environment variables before type casting.
        # Otherwise e.g. `my_bool_var: $BOOL` with BOOL=True would raise a TypeCoercionError.
        expanded = merged.expand() if self._expandvars else merged
        try:
            result = expanded.typify("<<merged>>")
        except CustomValidationError as e:
            errors.append(e)
        else:
            errors.extend(expanded.collect_errors(instance, result, "<<merged>>"))
        raise_errors(errors)
        instance._cache_[self.name] = result
        return result

    def _raw_parameters_from_single_source(self, raw_parameters):
        return ParameterLoader.raw_parameters_from_single_source(
            self.name, self.names, raw_parameters
        )

    @staticmethod
    def raw_parameters_from_single_source(name, names, raw_parameters):
        # while supporting parameter name aliases, we enforce that only one definition is given
        # per data source
        keys = names & frozenset(raw_parameters.keys())
        matches = {key: raw_parameters[key] for key in keys}
        numkeys = len(keys)
        if numkeys == 0:
            return None, None
        elif numkeys == 1:
            return next(iter(matches.values())), None
        elif name in keys:
            return matches[name], MultipleKeysError(
                raw_parameters[next(iter(keys))].source, keys, name
            )
        else:
            return None, MultipleKeysError(
                raw_parameters[next(iter(keys))].source, keys, name
            )


class ConfigurationType(type):
    """metaclass for Configuration"""

    def __init__(cls, name, bases, attr):
        super().__init__(name, bases, attr)

        # call _set_name for each parameter
        cls.parameter_names = tuple(
            p._set_name(name)
            for name, p in cls.__dict__.items()
            if isinstance(p, ParameterLoader)
        )


CONDARC_FILENAMES = (".condarc", "condarc")
YAML_EXTENSIONS = (".yml", ".yaml")
_RE_CUSTOM_EXPANDVARS = compile(
    rf"""
    # delimiter and a Python identifier
    \$(?P<named>{Template.idpattern}) |

    # delimiter and a braced identifier
    \${{(?P<braced>{Template.idpattern})}} |

    # delimiter padded identifier
    %(?P<padded>{Template.idpattern})%
    """,
    flags=IGNORECASE | VERBOSE,
)


def custom_expandvars(
    template: str, mapping: Mapping[str, Any] = {}, /, **kwargs
) -> str:
    """Expand variables in a string.

    Inspired by `string.Template` and modified to mirror `os.path.expandvars` functionality
    allowing custom variables without mutating `os.environ`.

    Expands POSIX and Windows CMD environment variables as follows:

    - $VARIABLE  value of VARIABLE
    - ${VARIABLE}  value of VARIABLE
    - %VARIABLE%  value of VARIABLE

    Invalid substitutions are left as-is:

    - $MISSING  $MISSING
    - ${MISSING}  ${MISSING}
    - %MISSING%  %MISSING%
    - $$  $$
    - %%  %%
    - $  $
    - %  %
    """
    mapping = {**mapping, **kwargs}

    def convert(match: Match):
        return str(
            mapping.get(
                match.group("named") or match.group("braced") or match.group("padded"),
                match.group(),  # fallback to the original string
            )
        )

    return _RE_CUSTOM_EXPANDVARS.sub(convert, template)


class Configuration(metaclass=ConfigurationType):
    def __init__(self, search_path=(), app_name=None, argparse_args=None, **kwargs):
        # Currently, __init__ does a **full** disk reload of all files.
        # A future improvement would be to cache files that are already loaded.
        self.raw_data = {}
        self._cache_ = {}
        self._reset_callbacks = IndexedSet()
        self._validation_errors = defaultdict(list)

        self._set_search_path(search_path, **kwargs)
        self._set_env_vars(app_name)
        self._set_argparse_args(argparse_args)

    @staticmethod
    def _expand_search_path(
        search_path: Iterable[Path | str],
        **kwargs,
    ) -> Iterable[Path]:
        for search in search_path:
            # use custom_expandvars instead of os.path.expandvars so additional variables can be
            # passed in without mutating os.environ
            if isinstance(search, Path):
                path = search
            else:
                template = custom_expandvars(search, environ, **kwargs)
                path = Path(template).expanduser()

            if path.is_file() and (
                path.name in CONDARC_FILENAMES or path.suffix in YAML_EXTENSIONS
            ):
                yield path
            elif path.is_dir():
                yield from (
                    subpath
                    for subpath in sorted(path.iterdir())
                    if subpath.is_file() and subpath.suffix in YAML_EXTENSIONS
                )

    @classmethod
    def _load_search_path(
        cls,
        search_path: Iterable[Path],
    ) -> Iterable[tuple[Path, dict]]:
        for path in search_path:
            try:
                yield path, YamlRawParameter.make_raw_parameters_from_file(path)
            except ConfigurationLoadError as err:
                log.warning(
                    "Ignoring configuration file (%s) due to error:\n%s",
                    path,
                    err,
                )

    def _set_search_path(self, search_path: Iterable[Path | str], **kwargs):
        self._search_path = IndexedSet(self._expand_search_path(search_path, **kwargs))

        self._set_raw_data(dict(self._load_search_path(self._search_path)))

        self._reset_cache()
        return self

    def _set_env_vars(self, app_name=None):
        self._app_name = app_name

        # remove existing source so "insert" order is correct
        source = EnvRawParameter.source
        if source in self.raw_data:
            del self.raw_data[source]

        if app_name:
            self.raw_data[source] = EnvRawParameter.make_raw_parameters(app_name)

        self._reset_cache()
        return self

    def _set_argparse_args(self, argparse_args):
        # the argparse_args we store internally in this class as self._argparse_args
        #   will be a mapping type, not a non-`dict` object like argparse_args is natively
        if hasattr(argparse_args, "__dict__"):
            # the argparse_args from argparse will be an object with a __dict__ attribute
            #   and not a mapping type like this method will turn it into
            items = vars(argparse_args).items()
        elif not argparse_args:
            # argparse_args can be initialized as `None`
            items = ()
        else:
            # we're calling this method with argparse_args that are a mapping type, likely
            #   already having been processed by this method before
            items = argparse_args.items()

        self._argparse_args = argparse_args = AttrDict(
            {k: v for k, v in items if v is not NULL}
        )

        # remove existing source so "insert" order is correct
        source = ArgParseRawParameter.source
        if source in self.raw_data:
            del self.raw_data[source]

        self.raw_data[source] = ArgParseRawParameter.make_raw_parameters(argparse_args)

        self._reset_cache()
        return self

    def _set_raw_data(self, raw_data: Mapping[Hashable, dict]):
        self.raw_data.update(raw_data)
        self._reset_cache()
        return self

    def _reset_cache(self):
        self._cache_ = {}
        for callback in self._reset_callbacks:
            callback()
        return self

    def register_reset_callaback(self, callback):
        self._reset_callbacks.add(callback)

    def check_source(self, source):
        # this method ends up duplicating much of the logic of Parameter.__get__
        # I haven't yet found a way to make it more DRY though
        typed_values = {}
        validation_errors = []
        raw_parameters = self.raw_data[source]
        for key in self.parameter_names:
            parameter = self.__class__.__dict__[key]
            match, multikey_error = parameter._raw_parameters_from_single_source(
                raw_parameters
            )
            if multikey_error:
                validation_errors.append(multikey_error)

            if match is not None:
                loaded_parameter = parameter.type.load(key, match)
                # untyped_value = loaded_parameter.value
                # if untyped_value is None:
                #     if isinstance(parameter, SequenceLoadedParameter):
                #         untyped_value = ()
                #     elif isinstance(parameter, MapLoadedParameter):
                #         untyped_value = {}
                try:
                    typed_value = loaded_parameter.typify(match.source)
                except CustomValidationError as e:
                    validation_errors.append(e)
                else:
                    collected_errors = loaded_parameter.collect_errors(
                        self, typed_value, match.source
                    )
                    if collected_errors:
                        validation_errors.extend(collected_errors)
                    else:
                        typed_values[match.key] = typed_value
            else:
                # this situation will happen if there is a multikey_error and none of the
                # matched keys is the primary key
                pass
        return typed_values, validation_errors

    def validate_all(self):
        validation_errors = list(
            chain.from_iterable(
                self.check_source(source)[1] for source in self.raw_data
            )
        )
        raise_errors(validation_errors)
        self.validate_configuration()

    @staticmethod
    def _collect_validation_error(func, *args, **kwargs):
        try:
            func(*args, **kwargs)
        except ConfigurationError as e:
            return (e.errors if hasattr(e, "errors") else e,)
        return ()

    def validate_configuration(self):
        errors = chain.from_iterable(
            Configuration._collect_validation_error(getattr, self, name)
            for name in self.parameter_names
        )
        post_errors = self.post_build_validation()
        raise_errors(tuple(chain.from_iterable((errors, post_errors))))

    def post_build_validation(self):
        return ()

    def collect_all(self):
        typed_values = {}
        validation_errors = {}
        for source in self.raw_data:
            typed_values[source], validation_errors[source] = self.check_source(source)
        raise_errors(tuple(chain.from_iterable(validation_errors.values())))
        return {k: v for k, v in typed_values.items() if v}

    def describe_parameter(self, parameter_name):
        # TODO, in Parameter base class, rename element_type to value_type
        if parameter_name not in self.parameter_names:
            parameter_name = "_" + parameter_name
        parameter_loader = self.__class__.__dict__[parameter_name]
        parameter = parameter_loader.type
        assert isinstance(parameter, Parameter)

        # dedupe leading underscore from name
        name = parameter_loader.name.lstrip("_")
        aliases = tuple(alias for alias in parameter_loader.aliases if alias != name)

        description = self.get_descriptions().get(name, "")
        et = parameter._element_type
        if type(et) == EnumMeta:  # noqa: E721
            et = [et]
        if not isiterable(et):
            et = [et]

        if isinstance(parameter._element_type, Parameter):
            element_types = tuple(
                _et.__class__.__name__.lower().replace("parameter", "") for _et in et
            )
        else:
            element_types = tuple(_et.__name__ for _et in et)

        details = {
            "parameter_type": parameter.__class__.__name__.lower().replace(
                "parameter", ""
            ),
            "name": name,
            "aliases": aliases,
            "element_types": element_types,
            "default_value": parameter.default.typify("<<describe>>"),
            "description": description.replace("\n", " ").strip(),
        }
        if isinstance(parameter, SequenceParameter):
            details["string_delimiter"] = parameter.string_delimiter
        return details

    def list_parameters(self):
        return tuple(sorted(name.lstrip("_") for name in self.parameter_names))

    def typify_parameter(self, parameter_name, value, source):
        # return a tuple with correct parameter name and typed-value
        if parameter_name not in self.parameter_names:
            parameter_name = "_" + parameter_name
        parameter_loader = self.__class__.__dict__[parameter_name]
        parameter = parameter_loader.type
        assert isinstance(parameter, Parameter)

        return parameter.typify(parameter_name, source, value)

    def get_descriptions(self):
        raise NotImplementedError()


def unique_sequence_map(*, unique_key: str):
    """
    Used to validate properties on :class:`Configuration` subclasses defined as a
    ``SequenceParameter(MapParameter())`` where the map contains a single key that
    should be regarded as unique. This decorator will handle removing duplicates and
    merging to a single sequence.
    """

    def inner_wrap(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            sequence_map = func(*args, **kwargs)
            new_sequence_mapping = {}

            for mapping in sequence_map:
                unique_key_value = mapping.get(unique_key)

                if unique_key_value is None:
                    log.error(
                        f'Configuration: skipping {mapping} for "{func.__name__}"; unique key '
                        f'"{unique_key}" not present on mapping'
                    )
                    continue

                if unique_key_value in new_sequence_mapping:
                    log.error(
                        f'Configuration: skipping {mapping} for "{func.__name__}"; value '
                        f'"{unique_key_value}" already present'
                    )
                    continue

                new_sequence_mapping[unique_key_value] = mapping

            return tuple(new_sequence_mapping.values())

        return wrapper

    return inner_wrap


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
The basic idea to nest logical expressions is instead of trying to denest
things via distribution, we add new variables. So if we have some logical
expression expr, we replace it with x and add expr <-> x to the clauses,
where x is a new variable, and expr <-> x is recursively evaluated in the
same way, so that the final clauses are ORs of atoms.

To use this, create a new Clauses object with the max var, for instance, if you
already have [[1, 2, -3]], you would use C = Clause(3).  All functions return
a new literal, which represents that function, or True or False if the expression
can be resolved fully. They may also add new clauses to C.clauses, which
will then be delivered to the SAT solver.

All functions take atoms as arguments (an atom is an integer, representing a
literal or a negated literal, or boolean constants True or False; that is,
it is the callers' responsibility to do the conversion of expressions
recursively. This is done because we do not have data structures
representing the various logical classes, only atoms.

The polarity argument can be set to True or False if you know that the literal
being used will only be used in the positive or the negative, respectively
(e.g., you will only use x, not -x).  This will generate fewer clauses. It
is probably best if you do not take advantage of this directly, but rather
through the Require and Prevent functions.

"""

from itertools import chain

from ._logic import FALSE, TRUE
from ._logic import Clauses as _Clauses

# TODO: We may want to turn the user-facing {TRUE,FALSE} values into an Enum and
#       hide the _logic.{TRUE,FALSE} values as an implementation detail.
#       We then have to handle the {TRUE,FALSE} -> _logic.{TRUE,FALSE} conversion
#       in Clauses._convert and the inverse _logic.{TRUE,FALSE} -> {TRUE,FALSE}
#       conversion in Clauses._eval.
TRUE = TRUE
FALSE = FALSE

PycoSatSolver = "pycosat"
PyCryptoSatSolver = "pycryptosat"
PySatSolver = "pysat"


class Clauses:
    def __init__(self, m=0, sat_solver=PycoSatSolver):
        self.names = {}
        self.indices = {}
        self._clauses = _Clauses(m=m, sat_solver_str=sat_solver)

    @property
    def m(self):
        return self._clauses.m

    @property
    def unsat(self):
        return self._clauses.unsat

    def get_clause_count(self):
        return self._clauses.get_clause_count()

    def as_list(self):
        return self._clauses.as_list()

    def _check_variable(self, variable):
        if 0 < abs(variable) <= self.m:
            return variable
        raise ValueError(f"SAT variable out of bounds: {variable} (max_var: {self.m})")

    def _check_literal(self, literal):
        if literal in {TRUE, FALSE}:
            return literal
        return self._check_variable(literal)

    def add_clause(self, clause):
        self._clauses.add_clause(map(self._check_variable, self._convert(clause)))

    def add_clauses(self, clauses):
        for clause in clauses:
            self.add_clause(clause)

    def name_var(self, m, name):
        self._check_literal(m)
        nname = "!" + name
        self.names[name] = m
        self.names[nname] = -m
        if m not in {TRUE, FALSE} and m not in self.indices:
            self.indices[m] = name
            self.indices[-m] = nname
        return m

    def new_var(self, name=None):
        m = self._clauses.new_var()
        if name:
            self.name_var(m, name)
        return m

    def from_name(self, name):
        return self.names.get(name)

    def from_index(self, m):
        return self.indices.get(m)

    def _assign(self, vals, name=None):
        x = self._clauses.assign(vals)
        if not name:
            return x
        if vals in {TRUE, FALSE}:
            x = self._clauses.new_var()
            self._clauses.add_clause((x,) if vals else (-x,))
        return self.name_var(x, name)

    def _convert(self, x):
        if isinstance(x, (tuple, list)):
            return type(x)(map(self._convert, x))
        if isinstance(x, int):
            return self._check_literal(x)
        name = x
        try:
            return self.names[name]
        except KeyError:
            raise ValueError(f"Unregistered SAT variable name: {name}")

    def _eval(self, func, args, no_literal_args, polarity, name):
        args = self._convert(args)
        if name is False:
            self._clauses.Eval(func, args + no_literal_args, polarity)
            return None
        vals = func(*(args + no_literal_args), polarity=polarity)
        return self._assign(vals, name)

    def Prevent(self, what, *args):
        return what.__get__(self, Clauses)(*args, polarity=False, name=False)

    def Require(self, what, *args):
        return what.__get__(self, Clauses)(*args, polarity=True, name=False)

    def Not(self, x, polarity=None, name=None):
        return self._eval(self._clauses.Not, (x,), (), polarity, name)

    def And(self, f, g, polarity=None, name=None):
        return self._eval(self._clauses.And, (f, g), (), polarity, name)

    def Or(self, f, g, polarity=None, name=None):
        return self._eval(self._clauses.Or, (f, g), (), polarity, name)

    def Xor(self, f, g, polarity=None, name=None):
        return self._eval(self._clauses.Xor, (f, g), (), polarity, name)

    def ITE(self, c, t, f, polarity=None, name=None):
        """If c Then t Else f.

        In this function, if any of c, t, or f are True and False the resulting
        expression is resolved.
        """
        return self._eval(self._clauses.ITE, (c, t, f), (), polarity, name)

    def All(self, iter, polarity=None, name=None):
        return self._eval(self._clauses.All, (iter,), (), polarity, name)

    def Any(self, vals, polarity=None, name=None):
        return self._eval(self._clauses.Any, (list(vals),), (), polarity, name)

    def AtMostOne_NSQ(self, vals, polarity=None, name=None):
        return self._eval(
            self._clauses.AtMostOne_NSQ, (list(vals),), (), polarity, name
        )

    def AtMostOne_BDD(self, vals, polarity=None, name=None):
        return self._eval(
            self._clauses.AtMostOne_BDD, (list(vals),), (), polarity, name
        )

    def AtMostOne(self, vals, polarity=None, name=None):
        vals = list(vals)
        nv = len(vals)
        if nv < 5 - (polarity is not True):
            what = self.AtMostOne_NSQ
        else:
            what = self.AtMostOne_BDD
        return self._eval(what, (vals,), (), polarity, name)

    def ExactlyOne_NSQ(self, vals, polarity=None, name=None):
        return self._eval(
            self._clauses.ExactlyOne_NSQ, (list(vals),), (), polarity, name
        )

    def ExactlyOne_BDD(self, vals, polarity=None, name=None):
        return self._eval(
            self._clauses.ExactlyOne_BDD, (list(vals),), (), polarity, name
        )

    def ExactlyOne(self, vals, polarity=None, name=None):
        vals = list(vals)
        nv = len(vals)
        if nv < 2:
            what = self.ExactlyOne_NSQ
        else:
            what = self.ExactlyOne_BDD
        return self._eval(what, (vals,), (), polarity, name)

    def LinearBound(self, equation, lo, hi, preprocess=True, polarity=None, name=None):
        if not isinstance(equation, dict):
            # in case of duplicate literal -> coefficient mappings, always take the last one
            equation = {named_lit: coeff for coeff, named_lit in equation}
        named_literals = list(equation.keys())
        coefficients = list(equation.values())
        return self._eval(
            self._clauses.LinearBound,
            (named_literals,),
            (coefficients, lo, hi, preprocess),
            polarity,
            name,
        )

    def sat(self, additional=None, includeIf=False, names=False, limit=0):
        """
        Calculate a SAT solution for the current clause set.

        Returned is the list of those solutions.  When the clauses are
        unsatisfiable, an empty list is returned.

        """
        if self.unsat:
            return None
        if not self.m:
            return set() if names else []
        if additional:
            additional = (tuple(self.names.get(c, c) for c in cc) for cc in additional)
        solution = self._clauses.sat(
            additional=additional, includeIf=includeIf, limit=limit
        )
        if solution is None:
            return None
        if names:
            return {
                nm
                for nm in (self.indices.get(s) for s in solution)
                if nm and nm[0] != "!"
            }
        return solution

    def itersolve(self, constraints=None, m=None):
        exclude = []
        if m is None:
            m = self.m
        while True:
            # We don't use pycosat.itersolve because it is more
            # important to limit the number of terms added to the
            # exclusion list, in our experience. Once we update
            # pycosat to do this, this can use it.
            sol = self.sat(chain(constraints, exclude))
            if sol is None:
                return
            yield sol
            exclude.append([-k for k in sol if -m <= k <= m])

    def minimize(self, objective, bestsol=None, trymax=False):
        if not isinstance(objective, dict):
            # in case of duplicate literal -> coefficient mappings, always take the last one
            objective = {named_lit: coeff for coeff, named_lit in objective}
        literals = self._convert(list(objective.keys()))
        coeffs = list(objective.values())

        return self._clauses.minimize(literals, coeffs, bestsol=bestsol, trymax=trymax)


def minimal_unsatisfiable_subset(clauses, sat, explicit_specs):
    """
    Given a set of clauses, find a minimal unsatisfiable subset (an
    unsatisfiable core)

    A set is a minimal unsatisfiable subset if no proper subset is
    unsatisfiable.  A set of clauses may have many minimal unsatisfiable
    subsets of different sizes.

    sat should be a function that takes a tuple of clauses and returns True if
    the clauses are satisfiable and False if they are not.  The algorithm will
    work with any order-reversing function (reversing the order of subset and
    the order False < True), that is, any function where (A <= B) iff (sat(B)
    <= sat(A)), where A <= B means A is a subset of B and False < True).

    """
    working_set = set()
    found_conflicts = set()

    if sat(explicit_specs, True) is None:
        found_conflicts = set(explicit_specs)
    else:
        # we succeeded, so we'll add the spec to our future constraints
        working_set = set(explicit_specs)

    for spec in set(clauses) - working_set:
        if (
            sat(
                working_set
                | {
                    spec,
                },
                True,
            )
            is None
        ):
            found_conflicts.add(spec)
        else:
            # we succeeded, so we'll add the spec to our future constraints
            working_set.add(spec)

    return found_conflicts


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Common constants."""

from ..auxlib import NULL

# Use this NULL object when needing to distinguish a value from None
# For example, when parsing json, you may need to determine if a json key was given and set
#   to null, or the key didn't exist at all.  There could be a bit of potential confusion here,
#   because in python null == None, while here I'm defining NULL to mean 'not defined'.
NULL = NULL

# Custom "trace" logging level for output more verbose than debug logs (logging.DEBUG == 10).
TRACE = 5


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
import sys
from array import array
from itertools import combinations
from logging import DEBUG, getLogger

from .constants import TRACE

log = getLogger(__name__)


TRUE = sys.maxsize
FALSE = -TRUE


class _ClauseList:
    """Storage for the CNF clauses, represented as a list of tuples of ints."""

    def __init__(self):
        self._clause_list = []
        # Methods append and extend are directly bound for performance reasons,
        # to avoid call overhead and lookups.
        self.append = self._clause_list.append
        self.extend = self._clause_list.extend

    def get_clause_count(self):
        """Return number of stored clauses."""
        return len(self._clause_list)

    def save_state(self):
        """
        Get state information to be able to revert temporary additions of
        supplementary clauses.  _ClauseList: state is simply the number of clauses.
        """
        return len(self._clause_list)

    def restore_state(self, saved_state):
        """
        Restore state saved via `save_state`.
        Removes clauses that were added after the state has been saved.
        """
        len_clauses = saved_state
        self._clause_list[len_clauses:] = []

    def as_list(self):
        """Return clauses as a list of tuples of ints."""
        return self._clause_list

    def as_array(self):
        """Return clauses as a flat int array, each clause being terminated by 0."""
        clause_array = array("i")
        for c in self._clause_list:
            clause_array.extend(c)
            clause_array.append(0)
        return clause_array


class _ClauseArray:
    """
    Storage for the CNF clauses, represented as a flat int array.
    Each clause is terminated by int(0).
    """

    def __init__(self):
        self._clause_array = array("i")
        # Methods append and extend are directly bound for performance reasons,
        # to avoid call overhead and lookups.
        self._array_append = self._clause_array.append
        self._array_extend = self._clause_array.extend

    def extend(self, clauses):
        for clause in clauses:
            self.append(clause)

    def append(self, clause):
        self._array_extend(clause)
        self._array_append(0)

    def get_clause_count(self):
        """
        Return number of stored clauses.
        This is an O(n) operation since we don't store the number of clauses
        explicitly due to performance reasons (Python interpreter overhead in
        self.append).
        """
        return self._clause_array.count(0)

    def save_state(self):
        """
        Get state information to be able to revert temporary additions of
        supplementary clauses. _ClauseArray: state is the length of the int
        array, NOT number of clauses.
        """
        return len(self._clause_array)

    def restore_state(self, saved_state):
        """
        Restore state saved via `save_state`.
        Removes clauses that were added after the state has been saved.
        """
        len_clause_array = saved_state
        self._clause_array[len_clause_array:] = array("i")

    def as_list(self):
        """Return clauses as a list of tuples of ints."""
        clause = []
        for v in self._clause_array:
            if v == 0:
                yield tuple(clause)
                clause.clear()
            else:
                clause.append(v)

    def as_array(self):
        """Return clauses as a flat int array, each clause being terminated by 0."""
        return self._clause_array


class _SatSolver:
    """Simple wrapper to call a SAT solver given a _ClauseList/_ClauseArray instance."""

    def __init__(self, **run_kwargs):
        self._run_kwargs = run_kwargs or {}
        self._clauses = _ClauseList()
        # Bind some methods of _clauses to reduce lookups and call overhead.
        self.add_clause = self._clauses.append
        self.add_clauses = self._clauses.extend

    def get_clause_count(self):
        return self._clauses.get_clause_count()

    def as_list(self):
        return self._clauses.as_list()

    def save_state(self):
        return self._clauses.save_state()

    def restore_state(self, saved_state):
        return self._clauses.restore_state(saved_state)

    def run(self, m, **kwargs):
        run_kwargs = self._run_kwargs.copy()
        run_kwargs.update(kwargs)
        solver = self.setup(m, **run_kwargs)
        sat_solution = self.invoke(solver)
        solution = self.process_solution(sat_solution)
        return solution

    def setup(self, m, **kwargs):
        """Create a solver instance, add the clauses to it, and return it."""
        raise NotImplementedError()

    def invoke(self, solver):
        """Start the actual SAT solving and return the calculated solution."""
        raise NotImplementedError()

    def process_solution(self, sat_solution):
        """
        Process the solution returned by self.invoke.
        Returns a list of satisfied variables or None if no solution is found.
        """
        raise NotImplementedError()


class _PycoSatSolver(_SatSolver):
    def setup(self, m, limit=0, **kwargs):
        from pycosat import itersolve

        # NOTE: The iterative solving isn't actually used here, we just call
        #       itersolve to separate setup from the actual run.
        return itersolve(self._clauses.as_list(), vars=m, prop_limit=limit)
        # If we add support for passing the clauses as an integer stream to the
        # solvers, we could also use self._clauses.as_array like this:
        # return itersolve(self._clauses.as_array(), vars=m, prop_limit=limit)

    def invoke(self, iter_sol):
        try:
            sat_solution = next(iter_sol)
        except StopIteration:
            sat_solution = "UNSAT"
        del iter_sol
        return sat_solution

    def process_solution(self, sat_solution):
        if sat_solution in ("UNSAT", "UNKNOWN"):
            return None
        return sat_solution


class _PyCryptoSatSolver(_SatSolver):
    def setup(self, m, threads=1, **kwargs):
        from pycryptosat import Solver

        solver = Solver(threads=threads)
        solver.add_clauses(self._clauses.as_list())
        return solver

    def invoke(self, solver):
        sat, sat_solution = solver.solve()
        if not sat:
            sat_solution = None
        return sat_solution

    def process_solution(self, solution):
        if not solution:
            return None
        # The first element of the solution is always None.
        solution = [i for i, b in enumerate(solution) if b]
        return solution


class _PySatSolver(_SatSolver):
    def setup(self, m, **kwargs):
        from pysat.solvers import Glucose4

        solver = Glucose4()
        solver.append_formula(self._clauses.as_list())
        return solver

    def invoke(self, solver):
        if not solver.solve():
            sat_solution = None
        else:
            sat_solution = solver.get_model()
        solver.delete()
        return sat_solution

    def process_solution(self, sat_solution):
        if sat_solution is None:
            solution = None
        else:
            solution = sat_solution
        return solution


_sat_solver_str_to_cls = {
    "pycosat": _PycoSatSolver,
    "pycryptosat": _PyCryptoSatSolver,
    "pysat": _PySatSolver,
}

_sat_solver_cls_to_str = {cls: string for string, cls in _sat_solver_str_to_cls.items()}


# Code that uses special cases (generates no clauses) is in ADTs/FEnv.h in
# minisatp. Code that generates clauses is in Hardware_clausify.cc (and are
# also described in the paper, "Translating Pseudo-Boolean Constraints into
# SAT," En and Srensson).
class Clauses:
    def __init__(self, m=0, sat_solver_str=_sat_solver_cls_to_str[_PycoSatSolver]):
        self.unsat = False
        self.m = m

        try:
            sat_solver_cls = _sat_solver_str_to_cls[sat_solver_str]
        except KeyError:
            raise NotImplementedError(f"Unknown SAT solver: {sat_solver_str}")
        self._sat_solver = sat_solver_cls()

        # Bind some methods of _sat_solver to reduce lookups and call overhead.
        self.add_clause = self._sat_solver.add_clause
        self.add_clauses = self._sat_solver.add_clauses

    def get_clause_count(self):
        return self._sat_solver.get_clause_count()

    def as_list(self):
        return self._sat_solver.as_list()

    def new_var(self):
        m = self.m + 1
        self.m = m
        return m

    def assign(self, vals):
        if isinstance(vals, tuple):
            x = self.new_var()
            self.add_clauses((-x,) + y for y in vals[0])
            self.add_clauses((x,) + y for y in vals[1])
            return x
        return vals

    def Combine(self, args, polarity):
        if any(v == FALSE for v in args):
            return FALSE
        args = [v for v in args if v != TRUE]
        nv = len(args)
        if nv == 0:
            return TRUE
        if nv == 1:
            return args[0]
        if all(isinstance(v, tuple) for v in args):
            return (sum((v[0] for v in args), []), sum((v[1] for v in args), []))
        else:
            return self.All(map(self.assign, args), polarity)

    def Eval(self, func, args, polarity):
        saved_state = self._sat_solver.save_state()
        vals = func(*args, polarity=polarity)
        # eval without assignment:
        if isinstance(vals, tuple):
            self.add_clauses(vals[0])
            self.add_clauses(vals[1])
        elif vals not in {TRUE, FALSE}:
            self.add_clause((vals if polarity else -vals,))
        else:
            self._sat_solver.restore_state(saved_state)
            self.unsat = self.unsat or (vals == TRUE) != polarity

    def Prevent(self, func, *args):
        self.Eval(func, args, polarity=False)

    def Require(self, func, *args):
        self.Eval(func, args, polarity=True)

    def Not(self, x, polarity=None, add_new_clauses=False):
        return -x

    def And(self, f, g, polarity, add_new_clauses=False):
        if f == FALSE or g == FALSE:
            return FALSE
        if f == TRUE:
            return g
        if g == TRUE:
            return f
        if f == g:
            return f
        if f == -g:
            return FALSE
        if g < f:
            f, g = g, f
        if add_new_clauses:
            # This is equivalent to running self.assign(pval, nval) on
            # the (pval, nval) tuple we return below. Duplicating the code here
            # is an important performance tweak to avoid the costly generator
            # expressions and tuple additions in self.assign.
            x = self.new_var()
            if polarity in (True, None):
                self.add_clauses(
                    [
                        (
                            -x,
                            f,
                        ),
                        (
                            -x,
                            g,
                        ),
                    ]
                )
            if polarity in (False, None):
                self.add_clauses([(x, -f, -g)])
            return x
        pval = [(f,), (g,)] if polarity in (True, None) else []
        nval = [(-f, -g)] if polarity in (False, None) else []
        return pval, nval

    def Or(self, f, g, polarity, add_new_clauses=False):
        if f == TRUE or g == TRUE:
            return TRUE
        if f == FALSE:
            return g
        if g == FALSE:
            return f
        if f == g:
            return f
        if f == -g:
            return TRUE
        if g < f:
            f, g = g, f
        if add_new_clauses:
            x = self.new_var()
            if polarity in (True, None):
                self.add_clauses([(-x, f, g)])
            if polarity in (False, None):
                self.add_clauses(
                    [
                        (
                            x,
                            -f,
                        ),
                        (
                            x,
                            -g,
                        ),
                    ]
                )
            return x
        pval = [(f, g)] if polarity in (True, None) else []
        nval = [(-f,), (-g,)] if polarity in (False, None) else []
        return pval, nval

    def Xor(self, f, g, polarity, add_new_clauses=False):
        if f == FALSE:
            return g
        if f == TRUE:
            return self.Not(g, polarity, add_new_clauses=add_new_clauses)
        if g == FALSE:
            return f
        if g == TRUE:
            return -f
        if f == g:
            return FALSE
        if f == -g:
            return TRUE
        if g < f:
            f, g = g, f
        if add_new_clauses:
            x = self.new_var()
            if polarity in (True, None):
                self.add_clauses([(-x, f, g), (-x, -f, -g)])
            if polarity in (False, None):
                self.add_clauses([(x, -f, g), (x, f, -g)])
            return x
        pval = [(f, g), (-f, -g)] if polarity in (True, None) else []
        nval = [(-f, g), (f, -g)] if polarity in (False, None) else []
        return pval, nval

    def ITE(self, c, t, f, polarity, add_new_clauses=False):
        if c == TRUE:
            return t
        if c == FALSE:
            return f
        if t == TRUE:
            return self.Or(c, f, polarity, add_new_clauses=add_new_clauses)
        if t == FALSE:
            return self.And(-c, f, polarity, add_new_clauses=add_new_clauses)
        if f == FALSE:
            return self.And(c, t, polarity, add_new_clauses=add_new_clauses)
        if f == TRUE:
            return self.Or(t, -c, polarity, add_new_clauses=add_new_clauses)
        if t == c:
            return self.Or(c, f, polarity, add_new_clauses=add_new_clauses)
        if t == -c:
            return self.And(-c, f, polarity, add_new_clauses=add_new_clauses)
        if f == c:
            return self.And(c, t, polarity, add_new_clauses=add_new_clauses)
        if f == -c:
            return self.Or(t, -c, polarity, add_new_clauses=add_new_clauses)
        if t == f:
            return t
        if t == -f:
            return self.Xor(c, f, polarity, add_new_clauses=add_new_clauses)
        if t < f:
            t, f, c = f, t, -c
        # Basically, c ? t : f is equivalent to (c AND t) OR (NOT c AND f)
        # The third clause in each group is redundant but assists the unit
        # propagation in the SAT solver.
        if add_new_clauses:
            x = self.new_var()
            if polarity in (True, None):
                self.add_clauses([(-x, -c, t), (-x, c, f), (-x, t, f)])
            if polarity in (False, None):
                self.add_clauses([(x, -c, -t), (x, c, -f), (x, -t, -f)])
            return x
        pval = [(-c, t), (c, f), (t, f)] if polarity in (True, None) else []
        nval = [(-c, -t), (c, -f), (-t, -f)] if polarity in (False, None) else []
        return pval, nval

    def All(self, iter, polarity=None):
        vals = set()
        for v in iter:
            if v == TRUE:
                continue
            if v == FALSE or -v in vals:
                return FALSE
            vals.add(v)
        nv = len(vals)
        if nv == 0:
            return TRUE
        elif nv == 1:
            return next(v for v in vals)
        pval = [(v,) for v in vals] if polarity in (True, None) else []
        nval = [tuple(-v for v in vals)] if polarity in (False, None) else []
        return pval, nval

    def Any(self, iter, polarity):
        vals = set()
        for v in iter:
            if v == FALSE:
                continue
            elif v == TRUE or -v in vals:
                return TRUE
            vals.add(v)
        nv = len(vals)
        if nv == 0:
            return FALSE
        elif nv == 1:
            return next(v for v in vals)
        pval = [tuple(vals)] if polarity in (True, None) else []
        nval = [(-v,) for v in vals] if polarity in (False, None) else []
        return pval, nval

    def AtMostOne_NSQ(self, vals, polarity):
        combos = []
        for v1, v2 in combinations(map(self.Not, vals), 2):
            combos.append(self.Or(v1, v2, polarity))
        return self.Combine(combos, polarity)

    def AtMostOne_BDD(self, vals, polarity=None):
        lits = list(vals)
        coeffs = [1] * len(lits)
        return self.LinearBound(lits, coeffs, 0, 1, True, polarity)

    def ExactlyOne_NSQ(self, vals, polarity):
        vals = list(vals)
        v1 = self.AtMostOne_NSQ(vals, polarity)
        v2 = self.Any(vals, polarity)
        return self.Combine((v1, v2), polarity)

    def ExactlyOne_BDD(self, vals, polarity):
        lits = list(vals)
        coeffs = [1] * len(lits)
        return self.LinearBound(lits, coeffs, 1, 1, True, polarity)

    def LB_Preprocess(self, lits, coeffs):
        equation = []
        offset = 0
        for coeff, lit in zip(coeffs, lits):
            if lit == TRUE:
                offset += coeff
                continue
            if lit == FALSE or coeff == 0:
                continue
            if coeff < 0:
                offset += coeff
                coeff, lit = -coeff, -lit
            equation.append((coeff, lit))
        coeffs, lits = tuple(zip(*sorted(equation))) or ((), ())
        return lits, coeffs, offset

    def BDD(self, lits, coeffs, nterms, lo, hi, polarity):
        # The equation (coeffs x lits) is sorted in
        # order of increasing coefficients.
        # Then we take advantage of the following recurrence:
        #                l      <= S + cN xN <= u
        #  => IF xN THEN l - cN <= S         <= u - cN
        #           ELSE l      <= S         <= u
        # we use memoization to prune common subexpressions
        total = sum(c for c in coeffs[:nterms])
        target = (nterms - 1, 0, total)
        call_stack = [target]
        ret = {}
        call_stack_append = call_stack.append
        call_stack_pop = call_stack.pop
        ret_get = ret.get
        ITE = self.ITE

        csum = 0
        while call_stack:
            ndx, csum, total = call_stack[-1]
            lower_limit = lo - csum
            upper_limit = hi - csum
            if lower_limit <= 0 and upper_limit >= total:
                ret[call_stack_pop()] = TRUE
                continue
            if lower_limit > total or upper_limit < 0:
                ret[call_stack_pop()] = FALSE
                continue
            LA = lits[ndx]
            LC = coeffs[ndx]
            ndx -= 1
            total -= LC
            hi_key = (ndx, csum if LA < 0 else csum + LC, total)
            thi = ret_get(hi_key)
            if thi is None:
                call_stack_append(hi_key)
                continue
            lo_key = (ndx, csum + LC if LA < 0 else csum, total)
            tlo = ret_get(lo_key)
            if tlo is None:
                call_stack_append(lo_key)
                continue
            # NOTE: The following ITE call is _the_ hotspot of the Python-side
            # computations for the overall minimization run. For performance we
            # avoid calling self.assign here via add_new_clauses=True.
            # If we want to translate parts of the code to a compiled language,
            # self.BDD (+ its downward call stack) is the prime candidate!
            ret[call_stack_pop()] = ITE(
                abs(LA), thi, tlo, polarity, add_new_clauses=True
            )
        return ret[target]

    def LinearBound(self, lits, coeffs, lo, hi, preprocess, polarity):
        if preprocess:
            lits, coeffs, offset = self.LB_Preprocess(lits, coeffs)
            lo -= offset
            hi -= offset
        nterms = len(coeffs)
        if nterms and coeffs[-1] > hi:
            nprune = sum(c > hi for c in coeffs)
            log.log(
                TRACE, "Eliminating %d/%d terms for bound violation", nprune, nterms
            )
            nterms -= nprune
        else:
            nprune = 0
        # Tighten bounds
        total = sum(c for c in coeffs[:nterms])
        if preprocess:
            lo = max([lo, 0])
            hi = min([hi, total])
        if lo > hi:
            return FALSE
        if nterms == 0:
            res = TRUE if lo == 0 else FALSE
        else:
            res = self.BDD(lits, coeffs, nterms, lo, hi, polarity)
        if nprune:
            prune = self.All([-a for a in lits[nterms:]], polarity)
            res = self.Combine((res, prune), polarity)
        return res

    def _run_sat(self, m, limit=0):
        if log.isEnabledFor(DEBUG):
            log.debug("Invoking SAT with clause count: %s", self.get_clause_count())
        solution = self._sat_solver.run(m, limit=limit)
        return solution

    def sat(self, additional=None, includeIf=False, limit=0):
        """
        Calculate a SAT solution for the current clause set.

        Returned is the list of those solutions.  When the clauses are
        unsatisfiable, an empty list is returned.

        """
        if self.unsat:
            return None
        if not self.m:
            return []
        saved_state = self._sat_solver.save_state()
        if additional:

            def preproc(eqs):
                def preproc_(cc):
                    for c in cc:
                        if c == FALSE:
                            continue
                        yield c
                        if c == TRUE:
                            break

                for cc in eqs:
                    cc = tuple(preproc_(cc))
                    if not cc:
                        yield cc
                        break
                    if cc[-1] != TRUE:
                        yield cc

            additional = list(preproc(additional))
            if additional:
                if not additional[-1]:
                    return None
                self.add_clauses(additional)
        solution = self._run_sat(self.m, limit=limit)
        if additional and (solution is None or not includeIf):
            self._sat_solver.restore_state(saved_state)
        return solution

    def minimize(self, lits, coeffs, bestsol=None, trymax=False):
        """
        Minimize the objective function given by (coeff, integer) pairs in
        zip(coeffs, lits).
        The actual minimization is multiobjective: first, we minimize the
        largest active coefficient value, then we minimize the sum.
        """
        if bestsol is None or len(bestsol) < self.m:
            log.debug("Clauses added, recomputing solution")
            bestsol = self.sat()
        if bestsol is None or self.unsat:
            log.debug("Constraints are unsatisfiable")
            return bestsol, sum(abs(c) for c in coeffs) + 1 if coeffs else 1
        if not coeffs:
            log.debug("Empty objective, trivial solution")
            return bestsol, 0

        lits, coeffs, offset = self.LB_Preprocess(lits, coeffs)
        maxval = max(coeffs)

        def peak_val(sol, objective_dict):
            return max(objective_dict.get(s, 0) for s in sol)

        def sum_val(sol, objective_dict):
            return sum(objective_dict.get(s, 0) for s in sol)

        lo = 0
        try0 = 0
        for peak in (True, False) if maxval > 1 else (False,):
            if peak:
                log.log(TRACE, "Beginning peak minimization")
                objval = peak_val
            else:
                log.log(TRACE, "Beginning sum minimization")
                objval = sum_val

            objective_dict = {a: c for c, a in zip(coeffs, lits)}
            bestval = objval(bestsol, objective_dict)

            # If we got lucky and the initial solution is optimal, we still
            # need to generate the constraints at least once
            hi = bestval
            m_orig = self.m
            if log.isEnabledFor(DEBUG):
                # This is only used for the log message below.
                nz = self.get_clause_count()
            saved_state = self._sat_solver.save_state()
            if trymax and not peak:
                try0 = hi - 1

            log.log(TRACE, "Initial range (%d,%d)", lo, hi)
            while True:
                if try0 is None:
                    mid = (lo + hi) // 2
                else:
                    mid = try0
                if peak:
                    prevent = tuple(a for c, a in zip(coeffs, lits) if c > mid)
                    require = tuple(a for c, a in zip(coeffs, lits) if lo <= c <= mid)
                    self.Prevent(self.Any, prevent)
                    if require:
                        self.Require(self.Any, require)
                else:
                    self.Require(self.LinearBound, lits, coeffs, lo, mid, False)

                if log.isEnabledFor(DEBUG):
                    log.log(
                        TRACE,
                        "Bisection attempt: (%d,%d), (%d+%d) clauses",
                        lo,
                        mid,
                        nz,
                        self.get_clause_count() - nz,
                    )
                newsol = self.sat()
                if newsol is None:
                    lo = mid + 1
                    log.log(TRACE, "Bisection failure, new range=(%d,%d)", lo, hi)
                    if lo > hi:
                        # FIXME: This is not supposed to happen!
                        # TODO: Investigate and fix the cause.
                        break
                    # If this was a failure of the first test after peak minimization,
                    # then it means that the peak minimizer is "tight" and we don't need
                    # any further constraints.
                else:
                    done = lo == mid
                    bestsol = newsol
                    bestval = objval(newsol, objective_dict)
                    hi = bestval
                    log.log(TRACE, "Bisection success, new range=(%d,%d)", lo, hi)
                    if done:
                        break
                self.m = m_orig
                # Since we only ever _add_ clauses and only remove then via
                # restore_state, it's fine to test on equality only.
                if self._sat_solver.save_state() != saved_state:
                    self._sat_solver.restore_state(saved_state)
                self.unsat = False
                try0 = None

            log.debug("Final %s objective: %d" % ("peak" if peak else "sum", bestval))
            if bestval == 0:
                break
            elif peak:
                # Now that we've minimized the peak value, we can drop any terms
                # with coefficients larger than this. Furthermore, since we know
                # at least one peak will be active, our lower bound for the sum
                # equals the peak.
                lits = [a for c, a in zip(coeffs, lits) if c <= bestval]
                coeffs = [c for c in coeffs if c <= bestval]
                try0 = sum_val(bestsol, objective_dict)
                lo = bestval
            else:
                log.debug("New peak objective: %d" % peak_val(bestsol, objective_dict))

        return bestsol, bestval


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Common decorators."""

import os
from functools import wraps

from ..deprecations import deprecated

deprecated.module("24.3", "24.9")


@deprecated("24.3", "24.9")
def env_override(envvar_name, convert_empty_to_none=False):
    """Override the return value of the decorated function with an environment variable.

    If convert_empty_to_none is true, if the value of the environment variable
    is the empty string, a None value will be returned.
    """

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            value = os.environ.get(envvar_name, None)

            if value is not None:
                if value == "" and convert_empty_to_none:
                    return None
                else:
                    return value
            else:
                return func(*args, **kwargs)

        return wrapper

    return decorator


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Code in ``conda.common`` is not conda-specific.  Technically, it sits *aside* the application
stack and not *within* the stack.  It is able to stand independently on its own.
The *only* allowed imports of conda code in ``conda.common`` modules are imports of other
``conda.common`` modules and imports from ``conda._vendor``.

If objects are needed from other parts of conda, they should be passed directly as arguments to
functions and methods.
"""


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Common URL utilities."""

import codecs
import re
import socket
from collections import namedtuple
from functools import lru_cache
from getpass import getpass
from os.path import abspath, expanduser
from urllib.parse import (  # noqa: F401
    ParseResult,
    quote,
    quote_plus,
    unquote,
    unquote_plus,
)
from urllib.parse import urlparse as _urlparse
from urllib.parse import urlunparse as _urlunparse  # noqa: F401

from .compat import on_win
from .path import split_filename, strip_pkg_extension


def hex_octal_to_int(ho):
    ho = ord(ho.upper())
    o0 = ord("0")
    o9 = ord("9")
    oA = ord("A")
    oF = ord("F")
    res = (
        ho - o0
        if ho >= o0 and ho <= o9
        else (ho - oA + 10)
        if ho >= oA and ho <= oF
        else None
    )
    return res


@lru_cache(maxsize=None)
def percent_decode(path):
    # This is not fast so avoid when we can.
    if "%" not in path:
        return path
    ranges = []
    for m in re.finditer(r"(%[0-9A-F]{2})", path, flags=re.IGNORECASE):
        ranges.append((m.start(), m.end()))
    if not len(ranges):
        return path

    # Sorry! Correctness is more important than speed at the moment.
    # Should use a map + lambda eventually.
    result = b""
    skips = 0
    for i, c in enumerate(path):
        if skips > 0:
            skips -= 1
            continue
        c = c.encode("ascii")
        emit = c
        if c == b"%":
            for r in ranges:
                if i == r[0]:
                    import struct

                    emit = struct.pack(
                        "B",
                        hex_octal_to_int(path[i + 1]) * 16
                        + hex_octal_to_int(path[i + 2]),
                    )
                    skips = 2
                    break
        if emit:
            result += emit
    return codecs.utf_8_decode(result)[0]


file_scheme = "file://"

# Keeping this around for now, need to combine with the same function in conda/common/path.py
"""
def url_to_path(url):
    assert url.startswith(file_scheme), "{} is not a file-scheme URL".format(url)
    decoded = percent_decode(url[len(file_scheme):])
    if decoded.startswith('/') and decoded[2] == ':':
        # A Windows path.
        decoded.replace('/', '\\')
    return decoded
"""


@lru_cache(maxsize=None)
def path_to_url(path):
    if not path:
        raise ValueError(f"Not allowed: {path!r}")
    if path.startswith(file_scheme):
        try:
            path.decode("ascii")
        except UnicodeDecodeError:
            raise ValueError(
                f"Non-ascii not allowed for things claiming to be URLs: {path!r}"
            )
        return path
    path = abspath(expanduser(path)).replace("\\", "/")
    # We do not use urljoin here because we want to take our own
    # *very* explicit control of how paths get encoded into URLs.
    #   We should not follow any RFCs on how to encode and decode
    # them, we just need to make sure we can represent them in a
    # way that will not cause problems for whatever amount of
    # urllib processing we *do* need to do on them (which should
    # be none anyway, but I doubt that is the case). I have gone
    # for ASCII and % encoding of everything not alphanumeric or
    # not in `!'()*-._/:`. This should be pretty save.
    #
    # To avoid risking breaking the internet, this code only runs
    # for `file://` URLs.
    #
    percent_encode_chars = "!'()*-._/\\:"
    percent_encode = lambda s: "".join(
        [f"%{ord(c):02X}", c][c < "{" and c.isalnum() or c in percent_encode_chars]
        for c in s
    )
    if any(ord(char) >= 128 for char in path):
        path = percent_encode(
            path.decode("unicode-escape")
            if hasattr(path, "decode")
            else bytes(path, "utf-8").decode("unicode-escape")
        )

    # https://blogs.msdn.microsoft.com/ie/2006/12/06/file-uris-in-windows/
    if len(path) > 1 and path[1] == ":":
        path = file_scheme + "/" + path
    else:
        path = file_scheme + path
    return path


url_attrs = (
    "scheme",
    "path",
    "query",
    "fragment",
    "username",
    "password",
    "hostname",
    "port",
)


class Url(namedtuple("Url", url_attrs)):
    """
    Object used to represent a Url. The string representation of this object is a url string.

    This object was inspired by the urllib3 implementation as it gives you a way to construct
    URLs from various parts. The motivation behind this object was making something that is
    interoperable with built the `urllib.parse.urlparse` function and has more features than
    the built-in `ParseResult` object.
    """

    def __new__(
        cls,
        scheme=None,
        path=None,
        query=None,
        fragment=None,
        username=None,
        password=None,
        hostname=None,
        port=None,
    ):
        if path and not path.startswith("/"):
            path = "/" + path
        if scheme:
            scheme = scheme.lower()
        if hostname:
            hostname = hostname.lower()
        return super().__new__(
            cls, scheme, path, query, fragment, username, password, hostname, port
        )

    @property
    def auth(self):
        if self.username and self.password:
            return f"{self.username}:{self.password}"
        elif self.username:
            return self.username

    @property
    def netloc(self):
        if self.port:
            return f"{self.hostname}:{self.port}"
        return self.hostname

    def __str__(self):
        scheme, path, query, fragment, username, password, hostname, port = self
        url = ""

        if scheme:
            url += f"{scheme}://"
        if password and username:
            url += f"{username}:{password}@"
        if hostname:
            url += hostname
        if port:
            url += f":{port}"
        if path:
            url += path
        if query:
            url += f"?{query}"
        if fragment:
            url += f"#{fragment}"

        return url

    def as_dict(self) -> dict:
        """Provide a public interface for namedtuple's _asdict"""
        return self._asdict()

    def replace(self, **kwargs) -> "Url":
        """Provide a public interface for namedtuple's _replace"""
        return self._replace(**kwargs)

    @classmethod
    def from_parse_result(cls, parse_result: ParseResult) -> "Url":
        values = {fld: getattr(parse_result, fld, "") for fld in url_attrs}
        return cls(**values)


@lru_cache(maxsize=None)
def urlparse(url: str) -> Url:
    if on_win and url.startswith("file:"):
        url.replace("\\", "/")
    # Allows us to pass in strings like 'example.com:8080/path/1'.
    if not has_scheme(url):
        url = "//" + url
    return Url.from_parse_result(_urlparse(url))


def url_to_s3_info(url):
    """Convert an s3 url to a tuple of bucket and key.

    Examples:
        >>> url_to_s3_info("s3://bucket-name.bucket/here/is/the/key")
        ('bucket-name.bucket', '/here/is/the/key')
    """
    parsed_url = urlparse(url)
    assert parsed_url.scheme == "s3", f"You can only use s3: urls (not {url!r})"
    bucket, key = parsed_url.hostname, parsed_url.path
    return bucket, key


def is_url(url):
    """
    Examples:
        >>> is_url(None)
        False
        >>> is_url("s3://some/bucket")
        True
    """
    if not url:
        return False
    try:
        return urlparse(url).scheme != ""
    except ValueError:
        return False


def is_ipv4_address(string_ip):
    """
    Examples:
        >>> [is_ipv4_address(ip) for ip in ('8.8.8.8', '192.168.10.10', '255.255.255.255')]
        [True, True, True]
        >>> [is_ipv4_address(ip) for ip in ('8.8.8', '192.168.10.10.20', '256.255.255.255', '::1')]
        [False, False, False, False]
    """
    try:
        socket.inet_aton(string_ip)
    except OSError:
        return False
    return string_ip.count(".") == 3


def is_ipv6_address(string_ip):
    """
    Examples:
        >> [is_ipv6_address(ip) for ip in ('::1', '2001:db8:85a3::370:7334', '1234:'*7+'1234')]
        [True, True, True]
        >> [is_ipv6_address(ip) for ip in ('192.168.10.10', '1234:'*8+'1234')]
        [False, False]
    """
    try:
        socket.inet_pton(socket.AF_INET6, string_ip)
    except OSError:
        return False
    return True


def is_ip_address(string_ip):
    """
    Examples:
        >> is_ip_address('192.168.10.10')
        True
        >> is_ip_address('::1')
        True
        >> is_ip_address('www.google.com')
        False
    """
    return is_ipv4_address(string_ip) or is_ipv6_address(string_ip)


def join(*args):
    start = "/" if not args[0] or args[0].startswith("/") else ""
    return start + "/".join(y for y in (x.strip("/") for x in args if x) if y)


join_url = join


def has_scheme(value):
    return re.match(r"[a-z][a-z0-9]{0,11}://", value)


def strip_scheme(url):
    """
    Examples:
        >>> strip_scheme("https://www.conda.io")
        'www.conda.io'
        >>> strip_scheme("s3://some.bucket/plus/a/path.ext")
        'some.bucket/plus/a/path.ext'
    """
    return url.split("://", 1)[-1]


def mask_anaconda_token(url):
    _, token = split_anaconda_token(url)
    return url.replace(token, "<TOKEN>", 1) if token else url


def split_anaconda_token(url):
    """
    Examples:
        >>> split_anaconda_token("https://1.2.3.4/t/tk-123-456/path")
        (u'https://1.2.3.4/path', u'tk-123-456')
        >>> split_anaconda_token("https://1.2.3.4/t//path")
        (u'https://1.2.3.4/path', u'')
        >>> split_anaconda_token("https://some.domain/api/t/tk-123-456/path")
        (u'https://some.domain/api/path', u'tk-123-456')
        >>> split_anaconda_token("https://1.2.3.4/conda/t/tk-123-456/path")
        (u'https://1.2.3.4/conda/path', u'tk-123-456')
        >>> split_anaconda_token("https://1.2.3.4/path")
        (u'https://1.2.3.4/path', None)
        >>> split_anaconda_token("https://10.2.3.4:8080/conda/t/tk-123-45")
        (u'https://10.2.3.4:8080/conda', u'tk-123-45')
    """
    _token_match = re.search(r"/t/([a-zA-Z0-9-]*)", url)
    token = _token_match.groups()[0] if _token_match else None
    cleaned_url = url.replace("/t/" + token, "", 1) if token is not None else url
    return cleaned_url.rstrip("/"), token


def split_platform(known_subdirs, url):
    """

    Examples:
        >>> from conda.base.constants import KNOWN_SUBDIRS
        >>> split_platform(KNOWN_SUBDIRS, "https://1.2.3.4/t/tk-123/linux-ppc64le/path")
        (u'https://1.2.3.4/t/tk-123/path', u'linux-ppc64le')

    """
    _platform_match = _split_platform_re(known_subdirs).search(url)
    platform = _platform_match.groups()[0] if _platform_match else None
    cleaned_url = url.replace("/" + platform, "", 1) if platform is not None else url
    return cleaned_url.rstrip("/"), platform


@lru_cache(maxsize=None)
def _split_platform_re(known_subdirs):
    _platform_match_regex = r"/({})(?:/|$)".format(
        r"|".join(rf"{d}" for d in known_subdirs)
    )
    return re.compile(_platform_match_regex, re.IGNORECASE)


def has_platform(url, known_subdirs):
    url_no_package_name, _ = split_filename(url)
    if not url_no_package_name:
        return None
    maybe_a_platform = url_no_package_name.rsplit("/", 1)[-1]
    return maybe_a_platform in known_subdirs and maybe_a_platform or None


def split_scheme_auth_token(url):
    """
    Examples:
        >>> split_scheme_auth_token("https://u:p@conda.io/t/x1029384756/more/path")
        ('conda.io/more/path', 'https', 'u:p', 'x1029384756')
        >>> split_scheme_auth_token(None)
        (None, None, None, None)
    """
    if not url:
        return None, None, None, None
    cleaned_url, token = split_anaconda_token(url)
    url_parts = urlparse(cleaned_url)
    remainder_url = Url(
        hostname=url_parts.hostname,
        port=url_parts.port,
        path=url_parts.path,
        query=url_parts.query,
    )

    return str(remainder_url), url_parts.scheme, url_parts.auth, token


def split_conda_url_easy_parts(known_subdirs, url):
    # scheme, auth, token, platform, package_filename, host, port, path, query
    cleaned_url, token = split_anaconda_token(url)
    cleaned_url, platform = split_platform(known_subdirs, cleaned_url)
    _, ext = strip_pkg_extension(cleaned_url)
    cleaned_url, package_filename = (
        cleaned_url.rsplit("/", 1)
        if ext and "/" in cleaned_url
        else (cleaned_url, None)
    )

    # TODO: split out namespace using regex
    url_parts = urlparse(cleaned_url)

    return (
        url_parts.scheme,
        url_parts.auth,
        token,
        platform,
        package_filename,
        url_parts.hostname,
        url_parts.port,
        url_parts.path,
        url_parts.query,
    )


@lru_cache(maxsize=None)
def get_proxy_username_and_pass(scheme):
    username = input(f"\n{scheme} proxy username: ")
    passwd = getpass("Password: ")
    return username, passwd


def add_username_and_password(url: str, username: str, password: str) -> str:
    """
    Inserts `username` and `password` into provided `url`

    >>> add_username_and_password('https://anaconda.org', 'TestUser', 'Password')
    'https://TestUser:Password@anaconda.org'
    """
    url = urlparse(url)
    url_with_auth = url.replace(username=username, password=quote(password, safe=""))
    return str(url_with_auth)


def maybe_add_auth(url: str, auth: str, force=False) -> str:
    """Add auth if the url doesn't currently have it.

    By default, does not replace auth if it already exists.  Setting ``force`` to ``True``
    overrides this behavior.

    Examples:
        >>> maybe_add_auth("https://www.conda.io", "user:passwd")
        'https://user:passwd@www.conda.io'
        >>> maybe_add_auth("https://www.conda.io", "")
        'https://www.conda.io'
    """
    if not auth:
        return url

    url_parts = urlparse(url)
    if url_parts.username and url_parts.password and not force:
        return url

    auth_parts = auth.split(":")
    if len(auth_parts) > 1:
        url_parts = url_parts.replace(username=auth_parts[0], password=auth_parts[1])

    return str(url_parts)


def maybe_unquote(url):
    return unquote_plus(remove_auth(url)) if url else url


def remove_auth(url: str) -> str:
    """Remove embedded authentication from URL.

    .. code-block:: pycon

       >>> remove_auth("https://user:password@anaconda.com")
       'https://anaconda.com'
    """
    url = urlparse(url)
    url_no_auth = url.replace(username="", password="")

    return str(url_no_auth)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Common compatiblity code."""
# Try to keep compat small because it's imported by everything
# What is compat, and what isn't?
# If a piece of code is "general" and used in multiple modules, it goes here.
# If it's only used in one module, keep it in that module, preferably near the top.
# This module should contain ONLY stdlib imports.

import builtins
import sys

from ..deprecations import deprecated

on_win = bool(sys.platform == "win32")
on_mac = bool(sys.platform == "darwin")
on_linux = bool(sys.platform == "linux")

FILESYSTEM_ENCODING = sys.getfilesystemencoding()

# Control some tweakables that will be removed finally.
ENCODE_ENVIRONMENT = True


def encode_for_env_var(value) -> str:
    """Environment names and values need to be string."""
    if isinstance(value, str):
        return value
    elif isinstance(value, bytes):
        return value.decode()
    return str(value)


def encode_environment(env):
    if ENCODE_ENVIRONMENT:
        env = {encode_for_env_var(k): encode_for_env_var(v) for k, v in env.items()}
    return env


@deprecated("24.9", "25.3")
def encode_arguments(arguments):
    return arguments


from collections.abc import Iterable


def isiterable(obj):
    return not isinstance(obj, str) and isinstance(obj, Iterable)


# #############################
# other
# #############################

from collections import OrderedDict as odict  # noqa: F401


def open(
    file, mode="r", buffering=-1, encoding=None, errors=None, newline=None, closefd=True
):
    if "b" in mode:
        return builtins.open(
            file,
            str(mode),
            buffering=buffering,
            errors=errors,
            newline=newline,
            closefd=closefd,
        )
    else:
        return builtins.open(
            file,
            str(mode),
            buffering=buffering,
            encoding=encoding or "utf-8",
            errors=errors,
            newline=newline,
            closefd=closefd,
        )


def six_with_metaclass(meta, *bases):
    """Create a base class with a metaclass."""

    # This requires a bit of explanation: the basic idea is to make a dummy
    # metaclass for one level of class instantiation that replaces itself with
    # the actual metaclass.
    class metaclass(type):
        def __new__(cls, name, this_bases, d):
            return meta(name, bases, d)

        @classmethod
        def __prepare__(cls, name, this_bases):
            return meta.__prepare__(name, bases)

    return type.__new__(metaclass, "temporary_class", (), {})


NoneType = type(None)
primitive_types = (str, int, float, complex, bool, NoneType)


def ensure_binary(value):
    try:
        return value.encode("utf-8")
    except AttributeError:  # pragma: no cover
        # AttributeError: '<>' object has no attribute 'encode'
        # In this case assume already binary type and do nothing
        return value


def ensure_text_type(value) -> str:
    try:
        return value.decode("utf-8")
    except AttributeError:  # pragma: no cover
        # AttributeError: '<>' object has no attribute 'decode'
        # In this case assume already text_type and do nothing
        return value
    except UnicodeDecodeError:  # pragma: no cover
        from charset_normalizer import from_bytes

        return str(from_bytes(value).best())
    except UnicodeEncodeError:  # pragma: no cover
        # it's already str, so ignore?
        # not sure, surfaced with tests/models/test_match_spec.py test_tarball_match_specs
        # using py27
        return value


def ensure_unicode(value):
    try:
        return value.decode("unicode_escape")
    except AttributeError:  # pragma: no cover
        # AttributeError: '<>' object has no attribute 'decode'
        # In this case assume already unicode and do nothing
        return value


def ensure_fs_path_encoding(value):
    try:
        return value.encode(FILESYSTEM_ENCODING)
    except AttributeError:
        return value
    except UnicodeEncodeError:
        return value


def ensure_utf8_encoding(value):
    try:
        return value.encode("utf-8")
    except AttributeError:
        return value
    except UnicodeEncodeError:
        return value


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Common disk utilities."""

from contextlib import contextmanager
from os import unlink

from ..auxlib.compat import Utf8NamedTemporaryFile
from ..deprecations import deprecated


@deprecated("24.3", "24.9", addendum="Use `tempfile` instead.")
@contextmanager
def temporary_content_in_file(content, suffix=""):
    # content returns temporary file path with contents
    fh = None
    path = None
    try:
        with Utf8NamedTemporaryFile(mode="w", delete=False, suffix=suffix) as fh:
            path = fh.name
            fh.write(content)
            fh.flush()
            fh.close()
            yield path
    finally:
        if fh is not None:
            fh.close()
        if path is not None:
            unlink(path)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
from __future__ import annotations

import os
from functools import lru_cache
from logging import getLogger
from os.path import exists

from ..compat import on_linux

log = getLogger(__name__)


@lru_cache(maxsize=None)
def linux_get_libc_version() -> tuple[str, str] | tuple[None, None]:
    """If on linux, returns (libc_family, version), otherwise (None, None)."""
    if not on_linux:
        return None, None

    for name in ("CS_GNU_LIBC_VERSION", "CS_GNU_LIBPTHREAD_VERSION"):
        try:
            # check if os.confstr returned None
            if value := os.confstr(name):
                family, version = value.strip().split(" ")
                break
        except ValueError:
            # ValueError: name is not defined in os.confstr_names
            # ValueError: value is not of the form "<family> <version>"
            pass
    else:
        family, version = "glibc", "2.5"
        log.warning(
            "Failed to detect libc family and version, assuming %s/%s",
            family,
            version,
        )

    # NPTL is just the name of the threading library, even though the
    # version refers to that of uClibc. os.readlink() can help to try to
    # figure out a better name instead.
    if family == "NPTL":  # pragma: no cover
        for clib in (
            entry.path for entry in os.scandir("/lib") if entry.name[:7] == "libc.so"
        ):
            clib = os.readlink(clib)
            if exists(clib):
                if clib.startswith("libuClibc"):
                    if version.startswith("0."):
                        family = "uClibc"
                    else:
                        family = "uClibc-ng"
                    break
        else:
            # This could be some other C library; it is unlikely though.
            family = "uClibc"
            log.warning(
                "Failed to detect non-glibc family, assuming %s/%s",
                family,
                version,
            )

    return family, version


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
import os
from logging import getLogger

log = getLogger(__name__)


def get_free_space_on_unix(dir_name):
    st = os.statvfs(dir_name)
    return st.f_bavail * st.f_frsize


def is_admin_on_unix():
    # http://stackoverflow.com/a/1026626/2127762
    return os.geteuid() == 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
from enum import IntEnum
from logging import getLogger

from ..compat import ensure_binary, on_win

log = getLogger(__name__)

if on_win:
    from ctypes import (
        POINTER,
        Structure,
        WinError,
        byref,
        c_char_p,
        c_int,
        c_ulong,
        c_ulonglong,
        c_void_p,
        c_wchar_p,
        pointer,
        sizeof,
        windll,
    )
    from ctypes.wintypes import BOOL, DWORD, HANDLE, HINSTANCE, HKEY, HWND

    PHANDLE = POINTER(HANDLE)
    PDWORD = POINTER(DWORD)
    SEE_MASK_NOCLOSEPROCESS = 0x00000040
    INFINITE = -1

    WaitForSingleObject = windll.kernel32.WaitForSingleObject
    WaitForSingleObject.argtypes = (HANDLE, DWORD)
    WaitForSingleObject.restype = DWORD

    CloseHandle = windll.kernel32.CloseHandle
    CloseHandle.argtypes = (HANDLE,)
    CloseHandle.restype = BOOL

    class ShellExecuteInfo(Structure):
        """
        https://docs.microsoft.com/en-us/windows/desktop/api/shellapi/nf-shellapi-shellexecuteexa
        https://docs.microsoft.com/en-us/windows/desktop/api/shellapi/ns-shellapi-_shellexecuteinfoa
        """

        _fields_ = [
            ("cbSize", DWORD),
            ("fMask", c_ulong),
            ("hwnd", HWND),
            ("lpVerb", c_char_p),
            ("lpFile", c_char_p),
            ("lpParameters", c_char_p),
            ("lpDirectory", c_char_p),
            ("nShow", c_int),
            ("hInstApp", HINSTANCE),
            ("lpIDList", c_void_p),
            ("lpClass", c_char_p),
            ("hKeyClass", HKEY),
            ("dwHotKey", DWORD),
            ("hIcon", HANDLE),
            ("hProcess", HANDLE),
        ]

        def __init__(self, **kwargs):
            Structure.__init__(self)
            self.cbSize = sizeof(self)
            for field_name, field_value in kwargs.items():
                if isinstance(field_value, str):
                    field_value = ensure_binary(field_value)
                setattr(self, field_name, field_value)

    PShellExecuteInfo = POINTER(ShellExecuteInfo)
    ShellExecuteEx = windll.Shell32.ShellExecuteExA
    ShellExecuteEx.argtypes = (PShellExecuteInfo,)
    ShellExecuteEx.restype = BOOL


class SW(IntEnum):
    HIDE = 0
    MAXIMIZE = 3
    MINIMIZE = 6
    RESTORE = 9
    SHOW = 5
    SHOWDEFAULT = 10
    SHOWMAXIMIZED = 3
    SHOWMINIMIZED = 2
    SHOWMINNOACTIVE = 7
    SHOWNA = 8
    SHOWNOACTIVATE = 4
    SHOWNORMAL = 1


class ERROR(IntEnum):
    ZERO = 0
    FILE_NOT_FOUND = 2
    PATH_NOT_FOUND = 3
    BAD_FORMAT = 11
    ACCESS_DENIED = 5
    ASSOC_INCOMPLETE = 27
    DDE_BUSY = 30
    DDE_FAIL = 29
    DDE_TIMEOUT = 28
    DLL_NOT_FOUND = 32
    NO_ASSOC = 31
    OOM = 8
    SHARE = 26


def get_free_space_on_windows(dir_name):
    result = None
    free_bytes = c_ulonglong(0)
    try:
        windll.kernel32.GetDiskFreeSpaceExW(
            c_wchar_p(dir_name),
            None,
            None,
            pointer(free_bytes),
        )
        result = free_bytes.value
    except Exception as e:
        log.info("%r", e)
    return result


def is_admin_on_windows():  # pragma: unix no cover
    # http://stackoverflow.com/a/1026626/2127762
    result = False
    try:
        result = windll.shell32.IsUserAnAdmin() != 0
    except Exception as e:  # pragma: no cover
        log.info("%r", e)
        # result = 'unknown'
    return result


def _wait_and_close_handle(process_handle):
    """Waits until spawned process finishes and closes the handle for it."""
    try:
        WaitForSingleObject(process_handle, INFINITE)
        CloseHandle(process_handle)
    except Exception as e:
        log.info("%r", e)


def run_as_admin(args, wait=True):
    """
    Run command line argument list (`args`) with elevated privileges.

    If `wait` is True, the process will block until completion.

    NOTES:
        - no stdin / stdout / stderr pipe support
        - does not automatically quote arguments (i.e. for paths that may contain spaces)
    See:
    - http://stackoverflow.com/a/19719292/1170370 on 20160407 MCS.
    - msdn.microsoft.com/en-us/library/windows/desktop/bb762153(v=vs.85).aspx
    - https://github.com/ContinuumIO/menuinst/blob/master/menuinst/windows/win_elevate.py
    - https://github.com/saltstack/salt-windows-install/blob/master/deps/salt/python/App/Lib/site-packages/win32/Demos/pipes/runproc.py  # NOQA
    - https://github.com/twonds/twisted/blob/master/twisted/internet/_dumbwin32proc.py
    - https://stackoverflow.com/a/19982092/2127762
    - https://www.codeproject.com/Articles/19165/Vista-UAC-The-Definitive-Guide
    - https://github.com/JustAMan/pyWinClobber/blob/master/win32elevate.py
    """
    arg0 = args[0]
    param_str = " ".join(args[1:] if len(args) > 1 else ())
    hprocess = None
    error_code = None
    try:
        execute_info = ShellExecuteInfo(
            fMask=SEE_MASK_NOCLOSEPROCESS,
            hwnd=None,
            lpVerb="runas",
            lpFile=arg0,
            lpParameters=param_str,
            lpDirectory=None,
            nShow=SW.HIDE,
        )
        successful = ShellExecuteEx(byref(execute_info))
        hprocess = execute_info.hProcess
    except Exception as e:
        successful = False
        error_code = e
        log.info("%r", e)

    if not successful:
        error_code = WinError()
    elif wait:
        _wait_and_close_handle(execute_info.hProcess)

    return hprocess, error_code


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
from logging import getLogger

from ..compat import on_win

if on_win:
    from .windows import get_free_space_on_windows as get_free_space
    from .windows import is_admin_on_windows as is_admin
else:
    from .unix import get_free_space_on_unix as get_free_space  # noqa
    from .unix import is_admin_on_unix as is_admin  # noqa


log = getLogger(__name__)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Common Python package format utilities."""

import platform
import re
import sys
import warnings
from collections import namedtuple
from configparser import ConfigParser
from csv import reader as csv_reader
from email.parser import HeaderParser
from errno import ENOENT
from io import StringIO
from itertools import chain
from logging import getLogger
from os import name as os_name
from os import scandir, strerror
from os.path import basename, dirname, isdir, isfile, join, lexists
from posixpath import normpath as posix_normpath

from ... import CondaError
from ...auxlib.decorators import memoizedproperty
from ..compat import open
from ..iterators import groupby_to_dict as groupby
from ..path import (
    get_major_minor_version,
    get_python_site_packages_short_path,
    pyc_path,
    win_path_ok,
)

try:
    from frozendict import frozendict
except ImportError:
    from ..._vendor.frozendict import frozendict

log = getLogger(__name__)

# TODO: complete this list
PYPI_TO_CONDA = {
    "graphviz": "python-graphviz",
}
# TODO: complete this list
PYPI_CONDA_DEPS = {
    "graphviz": ["graphviz"],  # What version constraints?
}
# This regex can process requirement including or not including name.
# This is useful for parsing, for example, `Python-Version`
PARTIAL_PYPI_SPEC_PATTERN = re.compile(
    r"""
    # Text needs to be stripped and all extra spaces replaced by single spaces
    (?P<name>^[A-Z0-9][A-Z0-9._-]*)?
    \s?
    (\[(?P<extras>.*)\])?
    \s?
    (?P<constraints>\(? \s? ([\w\d<>=!~,\s\.\*+-]*) \s? \)? )?
    \s?
""",
    re.VERBOSE | re.IGNORECASE,
)
PY_FILE_RE = re.compile(r"^[^\t\n\r\f\v]+/site-packages/[^\t\n\r\f\v]+\.py$")
PySpec = namedtuple("PySpec", ["name", "extras", "constraints", "marker", "url"])


class MetadataWarning(Warning):
    pass


# Dist classes
# -----------------------------------------------------------------------------
class PythonDistribution:
    """Base object describing a python distribution based on path to anchor file."""

    MANIFEST_FILES = ()  # Only one is used, but many names available
    REQUIRES_FILES = ()  # Only one is used, but many names available
    MANDATORY_FILES = ()
    ENTRY_POINTS_FILES = ("entry_points.txt",)

    @staticmethod
    def init(prefix_path, anchor_file, python_version):
        if anchor_file.endswith(".egg-link"):
            return PythonEggLinkDistribution(prefix_path, anchor_file, python_version)
        elif ".dist-info" in anchor_file:
            return PythonInstalledDistribution(prefix_path, anchor_file, python_version)
        elif anchor_file.endswith(".egg-info"):
            anchor_full_path = join(prefix_path, win_path_ok(anchor_file))
            sp_reference = basename(anchor_file)
            return PythonEggInfoDistribution(
                anchor_full_path, python_version, sp_reference
            )
        elif ".egg-info" in anchor_file:
            anchor_full_path = join(prefix_path, win_path_ok(dirname(anchor_file)))
            sp_reference = basename(dirname(anchor_file))
            return PythonEggInfoDistribution(
                anchor_full_path, python_version, sp_reference
            )
        elif ".egg" in anchor_file:
            anchor_full_path = join(prefix_path, win_path_ok(dirname(anchor_file)))
            sp_reference = basename(dirname(anchor_file))
            return PythonEggInfoDistribution(
                anchor_full_path, python_version, sp_reference
            )
        else:
            raise NotImplementedError()

    def __init__(self, anchor_full_path, python_version):
        # Don't call PythonDistribution directly. Use the init() static method.
        self.anchor_full_path = anchor_full_path
        self.python_version = python_version

        if anchor_full_path and isfile(anchor_full_path):
            self._metadata_dir_full_path = dirname(anchor_full_path)
        elif anchor_full_path and isdir(anchor_full_path):
            self._metadata_dir_full_path = anchor_full_path
        else:
            raise RuntimeError(f"Path not found: {anchor_full_path}")

        self._check_files()
        self._metadata = PythonDistributionMetadata(anchor_full_path)
        self._provides_file_data = ()
        self._requires_file_data = ()

    def _check_files(self):
        """Check the existence of mandatory files for a given distribution."""
        for fname in self.MANDATORY_FILES:
            if self._metadata_dir_full_path:
                fpath = join(self._metadata_dir_full_path, fname)
                if not isfile(fpath):
                    raise OSError(ENOENT, strerror(ENOENT), fpath)

    def _check_path_data(self, path, checksum, size):
        """Normalizes record data content and format."""
        if checksum:
            assert checksum.startswith("sha256="), (
                self._metadata_dir_full_path,
                path,
                checksum,
            )
            checksum = checksum[7:]
        else:
            checksum = None
        size = int(size) if size else None

        return path, checksum, size

    @staticmethod
    def _parse_requires_file_data(data, global_section="__global__"):
        # https://setuptools.readthedocs.io/en/latest/formats.html#requires-txt
        requires = {}
        lines = [line.strip() for line in data.split("\n") if line]

        if lines and not (lines[0].startswith("[") and lines[0].endswith("]")):
            # Add dummy section for unsectioned items
            lines = [f"[{global_section}]"] + lines

        # Parse sections
        for line in lines:
            if line.startswith("[") and line.endswith("]"):
                section = line.strip()[1:-1]
                requires[section] = []
                continue

            if line.strip():
                requires[section].append(line.strip())

        # Adapt to *standard* requirements (add env markers to requirements)
        reqs = []
        extras = []
        for section, values in requires.items():
            if section == global_section:
                # This is the global section (same as dist_requires)
                reqs.extend(values)
            elif section.startswith(":"):
                # The section is used as a marker
                # Example: ":python_version < '3'"
                marker = section.replace(":", "; ")
                new_values = [v + marker for v in values]
                reqs.extend(new_values)
            else:
                # The section is an extra, i.e. "docs", or "tests"...
                extras.append(section)
                marker = f'; extra == "{section}"'
                new_values = [v + marker for v in values]
                reqs.extend(new_values)

        return frozenset(reqs), extras

    @staticmethod
    def _parse_entries_file_data(data):
        # https://setuptools.readthedocs.io/en/latest/formats.html#entry-points-txt-entry-point-plugin-metadata
        # FIXME: Use pkg_resources which provides API for this?
        entries_data = {}
        config = ConfigParser()
        config.optionxform = lambda x: x  # Avoid lowercasing keys
        try:
            do_read = config.read_file
        except AttributeError:
            do_read = config.readfp
        do_read(StringIO(data))
        for section in config.sections():
            entries_data[section] = dict(config.items(section))

        return entries_data

    def _load_requires_provides_file(self):
        # https://setuptools.readthedocs.io/en/latest/formats.html#requires-txt
        # FIXME: Use pkg_resources which provides API for this?
        requires, extras = None, None
        for fname in self.REQUIRES_FILES:
            fpath = join(self._metadata_dir_full_path, fname)
            if isfile(fpath):
                with open(fpath) as fh:
                    data = fh.read()

                requires, extras = self._parse_requires_file_data(data)
                self._provides_file_data = extras
                self._requires_file_data = requires
                break

        return requires, extras

    @memoizedproperty
    def manifest_full_path(self):
        manifest_full_path = None
        if self._metadata_dir_full_path:
            for fname in self.MANIFEST_FILES:
                manifest_full_path = join(self._metadata_dir_full_path, fname)
                if isfile(manifest_full_path):
                    break
        return manifest_full_path

    def get_paths(self):
        """
        Read the list of installed paths from record or source file.

        Example
        -------
        [(u'skdata/__init__.py', u'sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU', 0),
         (u'skdata/diabetes.py', None, None),
         ...
        ]
        """
        manifest_full_path = self.manifest_full_path
        if manifest_full_path:
            python_version = self.python_version
            sp_dir = get_python_site_packages_short_path(python_version) + "/"
            prepend_metadata_dirname = (
                basename(manifest_full_path) == "installed-files.txt"
            )
            if prepend_metadata_dirname:
                path_prepender = basename(dirname(manifest_full_path)) + "/"
            else:
                path_prepender = ""

            def process_csv_row(reader):
                seen = []
                records = []
                for row in reader:
                    cleaned_path = posix_normpath(f"{sp_dir}{path_prepender}{row[0]}")
                    if len(row) == 3:
                        checksum, size = row[1:]
                        if checksum:
                            assert checksum.startswith("sha256="), (
                                self._metadata_dir_full_path,
                                cleaned_path,
                                checksum,
                            )
                            checksum = checksum[7:]
                        else:
                            checksum = None
                        size = int(size) if size else None
                    else:
                        checksum = size = None
                    if cleaned_path not in seen and row[0]:
                        seen.append(cleaned_path)
                        records.append((cleaned_path, checksum, size))
                return tuple(records)

            csv_delimiter = ","
            with open(manifest_full_path) as csvfile:
                record_reader = csv_reader(csvfile, delimiter=csv_delimiter)
                # format of each record is (path, checksum, size)
                records = process_csv_row(record_reader)
            files_set = {record[0] for record in records}

            _pyc_path, _py_file_re = pyc_path, PY_FILE_RE
            py_ver_mm = get_major_minor_version(python_version, with_dot=False)
            missing_pyc_files = (
                ff
                for ff in (
                    _pyc_path(f, py_ver_mm) for f in files_set if _py_file_re.match(f)
                )
                if ff not in files_set
            )
            records = sorted(
                (*records, *((pf, None, None) for pf in missing_pyc_files))
            )
            return records

        return []

    def get_dist_requirements(self):
        # FIXME: On some packages, requirements are not added to metadata,
        # but on a separate requires.txt, see: python setup.py develop for
        # anaconda-client. This is setuptools behavior.
        # TODO: what is the dependency_links.txt on the same example?
        data = self._metadata.get_dist_requirements()
        if self._requires_file_data:
            data = self._requires_file_data
        elif not data:
            self._load_requires_provides_file()
            data = self._requires_file_data
        return data

    def get_python_requirements(self):
        return self._metadata.get_python_requirements()

    def get_external_requirements(self):
        return self._metadata.get_external_requirements()

    def get_extra_provides(self):
        # FIXME: On some packages, requirements are not added to metadata,
        # but on a separate requires.txt, see: python setup.py develop for
        # anaconda-client. This is setuptools behavior.
        data = self._metadata.get_extra_provides()
        if self._provides_file_data:
            data = self._provides_file_data
        elif not data:
            self._load_requires_provides_file()
            data = self._provides_file_data

        return data

    def get_conda_dependencies(self):
        """
        Process metadata fields providing dependency information.

        This includes normalizing fields, and evaluating environment markers.
        """
        python_spec = "python {}.*".format(".".join(self.python_version.split(".")[:2]))

        def pyspec_to_norm_req(pyspec):
            conda_name = pypi_name_to_conda_name(norm_package_name(pyspec.name))
            return (
                f"{conda_name} {pyspec.constraints}"
                if pyspec.constraints
                else conda_name
            )

        reqs = self.get_dist_requirements()
        pyspecs = tuple(parse_specification(req) for req in reqs)
        marker_groups = groupby(lambda ps: ps.marker.split("==", 1)[0].strip(), pyspecs)
        depends = {pyspec_to_norm_req(pyspec) for pyspec in marker_groups.pop("", ())}
        extras = marker_groups.pop("extra", ())
        execution_context = {
            "python_version": self.python_version,
        }
        depends.update(
            pyspec_to_norm_req(pyspec)
            for pyspec in chain.from_iterable(marker_groups.values())
            if interpret(pyspec.marker, execution_context)
        )
        constrains = {
            pyspec_to_norm_req(pyspec) for pyspec in extras if pyspec.constraints
        }
        depends.add(python_spec)

        return sorted(depends), sorted(constrains)

    def get_optional_dependencies(self):
        raise NotImplementedError

    def get_entry_points(self):
        # TODO: need to add entry points, "exports," and other files that might
        # not be in RECORD
        for fname in self.ENTRY_POINTS_FILES:
            fpath = join(self._metadata_dir_full_path, fname)
            if isfile(fpath):
                with open(fpath) as fh:
                    data = fh.read()
        return self._parse_entries_file_data(data)

    @property
    def name(self):
        return self._metadata.name

    @property
    def norm_name(self):
        return norm_package_name(self.name)

    @property
    def conda_name(self):
        return pypi_name_to_conda_name(self.norm_name)

    @property
    def version(self):
        return self._metadata.version


class PythonInstalledDistribution(PythonDistribution):
    """
    Python distribution installed via distutils.

    Notes
    -----
      - https://www.python.org/dev/peps/pep-0376/
    """

    MANIFEST_FILES = ("RECORD",)
    REQUIRES_FILES = ()
    MANDATORY_FILES = ("METADATA",)
    # FIXME: Do this check? Disabled for tests where only Metadata file is stored
    # MANDATORY_FILES = ('METADATA', 'RECORD', 'INSTALLER')
    ENTRY_POINTS_FILES = ()

    is_manageable = True

    def __init__(self, prefix_path, anchor_file, python_version):
        anchor_full_path = join(prefix_path, win_path_ok(dirname(anchor_file)))
        super().__init__(anchor_full_path, python_version)
        self.sp_reference = basename(dirname(anchor_file))


class PythonEggInfoDistribution(PythonDistribution):
    """
    Python distribution installed via setuptools.

    Notes
    -----
      - http://peak.telecommunity.com/DevCenter/EggFormats
    """

    MANIFEST_FILES = ("installed-files.txt", "SOURCES", "SOURCES.txt")
    REQUIRES_FILES = ("requires.txt", "depends.txt")
    MANDATORY_FILES = ()
    ENTRY_POINTS_FILES = ("entry_points.txt",)

    def __init__(self, anchor_full_path, python_version, sp_reference):
        super().__init__(anchor_full_path, python_version)
        self.sp_reference = sp_reference

    @property
    def is_manageable(self):
        return (
            self.manifest_full_path
            and basename(self.manifest_full_path) == "installed-files.txt"
        )


class PythonEggLinkDistribution(PythonEggInfoDistribution):
    is_manageable = False

    def __init__(self, prefix_path, anchor_file, python_version):
        anchor_full_path = get_dist_file_from_egg_link(anchor_file, prefix_path)
        sp_reference = None  # This can be None in case the egg-info is no longer there
        super().__init__(anchor_full_path, python_version, sp_reference)


# Python distribution/eggs metadata
# -----------------------------------------------------------------------------


class PythonDistributionMetadata:
    """
    Object representing the metada of a Python Distribution given by anchor
    file (or directory) path.

    This metadata is extracted from a single file. Python distributions might
    create additional files that complement this metadata information, but
    that is handled at the python distribution level.

    Notes
    -----
      - https://packaging.python.org/specifications/core-metadata/
      - Metadata 2.1: https://www.python.org/dev/peps/pep-0566/
      - Metadata 2.0: https://www.python.org/dev/peps/pep-0426/ (Withdrawn)
      - Metadata 1.2: https://www.python.org/dev/peps/pep-0345/
      - Metadata 1.1: https://www.python.org/dev/peps/pep-0314/
      - Metadata 1.0: https://www.python.org/dev/peps/pep-0241/
    """

    FILE_NAMES = ("METADATA", "PKG-INFO")

    # Python Packages Metadata 2.1
    # -----------------------------------------------------------------------------
    SINGLE_USE_KEYS = frozendict(
        (
            ("Metadata-Version", "metadata_version"),
            ("Name", "name"),
            ("Version", "version"),
            # ('Summary', 'summary'),
            # ('Description', 'description'),
            # ('Description-Content-Type', 'description_content_type'),
            # ('Keywords', 'keywords'),
            # ('Home-page', 'home_page'),
            # ('Download-URL', 'download_url'),
            # ('Author', 'author'),
            # ('Author-email', 'author_email'),
            # ('Maintainer', 'maintainer'),
            # ('Maintainer-email', 'maintainer_email'),
            ("License", "license"),
            # # Deprecated
            # ('Obsoleted-By', 'obsoleted_by'),  # Note: See 2.0
            # ('Private-Version', 'private_version'),  # Note: See 2.0
        )
    )
    MULTIPLE_USE_KEYS = frozendict(
        (
            ("Platform", "platform"),
            ("Supported-Platform", "supported_platform"),
            # ('Classifier', 'classifier'),
            ("Requires-Dist", "requires_dist"),
            ("Requires-External", "requires_external"),
            ("Requires-Python", "requires_python"),
            # ('Project-URL', 'project_url'),
            ("Provides-Extra", "provides_extra"),
            # ('Provides-Dist', 'provides_dist'),
            # ('Obsoletes-Dist', 'obsoletes_dist'),
            # # Deprecated
            # ('Extension', 'extension'),  # Note: See 2.0
            # ('Obsoletes', 'obsoletes'),
            # ('Provides', 'provides'),
            ("Requires", "requires"),
            # ('Setup-Requires-Dist', 'setup_requires_dist'),  # Note: See 2.0
        )
    )

    def __init__(self, path):
        metadata_path = self._process_path(path, self.FILE_NAMES)
        self._path = path
        self._data = self._read_metadata(metadata_path)

    @staticmethod
    def _process_path(path, metadata_filenames):
        """Find metadata file inside dist-info folder, or check direct file."""
        metadata_path = None
        if path:
            if isdir(path):
                for fname in metadata_filenames:
                    fpath = join(path, fname)
                    if isfile(fpath):
                        metadata_path = fpath
                        break
            elif isfile(path):
                # '<pkg>.egg-info' file contains metadata directly
                filenames = [".egg-info"]
                if metadata_filenames:
                    filenames.extend(metadata_filenames)
                assert any(path.endswith(filename) for filename in filenames)
                metadata_path = path
            else:
                # `path` does not exist
                warnings.warn("Metadata path not found", MetadataWarning)
        else:
            warnings.warn("Metadata path not found", MetadataWarning)

        return metadata_path

    @classmethod
    def _message_to_dict(cls, message):
        """
        Convert the RFC-822 headers data into a dictionary.

        `message` is an email.parser.Message instance.

        The canonical method to transform metadata fields into such a data
        structure is as follows:
          - The original key-value format should be read with
            email.parser.HeaderParser
          - All transformed keys should be reduced to lower case. Hyphens
            should be replaced with underscores, but otherwise should retain
            all other characters
          - The transformed value for any field marked with "(Multiple-use")
            should be a single list containing all the original values for the
            given key
          - The Keywords field should be converted to a list by splitting the
            original value on whitespace characters
          - The message body, if present, should be set to the value of the
            description key.
          - The result should be stored as a string-keyed dictionary.
        """
        new_data = {}

        if message:
            for key, value in message.items():
                if key in cls.MULTIPLE_USE_KEYS:
                    new_key = cls.MULTIPLE_USE_KEYS[key]
                    if new_key not in new_data:
                        new_data[new_key] = [value]
                    else:
                        new_data[new_key].append(value)

                elif key in cls.SINGLE_USE_KEYS:
                    new_key = cls.SINGLE_USE_KEYS[key]
                    new_data[new_key] = value

            # TODO: Handle license later on for convenience

        return new_data

    @classmethod
    def _read_metadata(cls, fpath):
        """Read the original format which is stored as RFC-822 headers."""
        data = {}
        if fpath and isfile(fpath):
            parser = HeaderParser()

            # FIXME: Is this a correct assumption for the encoding?
            # This was needed due to some errors on windows
            with open(fpath) as fp:
                data = parser.parse(fp)

        return cls._message_to_dict(data)

    def _get_multiple_data(self, keys):
        """
        Helper method to get multiple data values by keys.

        Keys is an iterable including the preferred key in order, to include
        values of key that might have been replaced (deprecated), for example
        keys can be ['requires_dist', 'requires'], where the key 'requires' is
        deprecated and replaced by 'requires_dist'.
        """
        data = []
        if self._data:
            for key in keys:
                raw_data = self._data.get(key, [])
                for req in raw_data:
                    data.append(req.strip())

                if data:
                    break

        return frozenset(data)

    def get_dist_requirements(self):
        """
        Changed in version 2.1: The field format specification was relaxed to
        accept the syntax used by popular publishing tools.

        Each entry contains a string naming some other distutils project
        required by this distribution.

        The format of a requirement string contains from one to four parts:
          - A project name, in the same format as the Name: field. The only
            mandatory part.
          - A comma-separated list of extra names. These are defined by the
            required project, referring to specific features which may need
            extra dependencies.
          - A version specifier. Tools parsing the format should accept
            optional parentheses around this, but tools generating it should
            not use parentheses.
          - An environment marker after a semicolon. This means that the
            requirement is only needed in the specified conditions.

        This field may be followed by an environment marker after a semicolon.

        Example
        -------
        frozenset(['pkginfo', 'PasteDeploy', 'zope.interface (>3.5.0)',
                   'pywin32 >1.0; sys_platform == "win32"'])

        Return 'Requires' if 'Requires-Dist' is empty.
        """
        return self._get_multiple_data(["requires_dist", "requires"])

    def get_python_requirements(self):
        """
        New in version 1.2.

        This field specifies the Python version(s) that the distribution is
        guaranteed to be compatible with. Installation tools may look at this
        when picking which version of a project to install.

        The value must be in the format specified in Version specifiers.

        This field may be followed by an environment marker after a semicolon.

        Example
        -------
        frozenset(['>=3', '>2.6,!=3.0.*,!=3.1.*', '~=2.6',
                   '>=3; sys_platform == "win32"'])
        """
        return self._get_multiple_data(["requires_python"])

    def get_external_requirements(self):
        """
        Changed in version 2.1: The field format specification was relaxed to
        accept the syntax used by popular publishing tools.

        Each entry contains a string describing some dependency in the system
        that the distribution is to be used. This field is intended to serve
        as a hint to downstream project maintainers, and has no semantics
        which are meaningful to the distutils distribution.

        The format of a requirement string is a name of an external dependency,
        optionally followed by a version declaration within parentheses.

        This field may be followed by an environment marker after a semicolon.

        Because they refer to non-Python software releases, version numbers for
        this field are not required to conform to the format specified in PEP
        440: they should correspond to the version scheme used by the external
        dependency.

        Notice that theres is no particular rule on the strings to be used!

        Example
        -------
        frozenset(['C', 'libpng (>=1.5)', 'make; sys_platform != "win32"'])
        """
        return self._get_multiple_data(["requires_external"])

    def get_extra_provides(self):
        """
        New in version 2.1.

        A string containing the name of an optional feature. Must be a valid
        Python identifier. May be used to make a dependency conditional on
        hether the optional feature has been requested.

        Example
        -------
        frozenset(['pdf', 'doc', 'test'])
        """
        return self._get_multiple_data(["provides_extra"])

    def get_dist_provides(self):
        """
        New in version 1.2.

        Changed in version 2.1: The field format specification was relaxed to
        accept the syntax used by popular publishing tools.

        Each entry contains a string naming a Distutils project which is
        contained within this distribution. This field must include the project
        identified in the Name field, followed by the version : Name (Version).

        A distribution may provide additional names, e.g. to indicate that
        multiple projects have been bundled together. For instance, source
        distributions of the ZODB project have historically included the
        transaction project, which is now available as a separate distribution.
        Installing such a source distribution satisfies requirements for both
        ZODB and transaction.

        A distribution may also provide a virtual project name, which does
        not correspond to any separately-distributed project: such a name might
        be used to indicate an abstract capability which could be supplied by
        one of multiple projects. E.g., multiple projects might supply RDBMS
        bindings for use by a given ORM: each project might declare that it
        provides ORM-bindings, allowing other projects to depend only on having
        at most one of them installed.

        A version declaration may be supplied and must follow the rules
        described in Version specifiers. The distributions version number
        will be implied if none is specified.

        This field may be followed by an environment marker after a semicolon.

        Return `Provides` in case `Provides-Dist` is empty.
        """
        return self._get_multiple_data(["provides_dist", "provides"])

    def get_dist_obsolete(self):
        """
        New in version 1.2.

        Changed in version 2.1: The field format specification was relaxed to
        accept the syntax used by popular publishing tools.

        Each entry contains a string describing a distutils projects
        distribution which this distribution renders obsolete, meaning that
        the two projects should not be installed at the same time.

        Version declarations can be supplied. Version numbers must be in the
        format specified in Version specifiers [1].

        The most common use of this field will be in case a project name
        changes, e.g. Gorgon 2.3 gets subsumed into Torqued Python 1.0. When
        you install Torqued Python, the Gorgon distribution should be removed.

        This field may be followed by an environment marker after a semicolon.

        Return `Obsoletes` in case `Obsoletes-Dist` is empty.

        Example
        -------
        frozenset(['Gorgon', "OtherProject (<3.0) ; python_version == '2.7'"])

        Notes
        -----
        - [1] https://packaging.python.org/specifications/version-specifiers/
        """
        return self._get_multiple_data(["obsoletes_dist", "obsoletes"])

    def get_classifiers(self):
        """
        Classifiers are described in PEP 301, and the Python Package Index
        publishes a dynamic list of currently defined classifiers.

        This field may be followed by an environment marker after a semicolon.

        Example
        -------
        frozenset(['Development Status :: 4 - Beta',
                   "Environment :: Console (Text Based) ; os_name == "posix"])
        """
        return self._get_multiple_data(["classifier"])

    @property
    def name(self):
        return self._data.get("name")  # TODO: Check for existence?

    @property
    def version(self):
        return self._data.get("version")  # TODO: Check for existence?


# Helper functions
# -----------------------------------------------------------------------------
def norm_package_name(name):
    return name.replace(".", "-").replace("_", "-").lower() if name else ""


def pypi_name_to_conda_name(pypi_name):
    return PYPI_TO_CONDA.get(pypi_name, pypi_name) if pypi_name else ""


def norm_package_version(version):
    """Normalize a version by removing extra spaces and parentheses."""
    if version:
        version = ",".join(v.strip() for v in version.split(",")).strip()

        if version.startswith("(") and version.endswith(")"):
            version = version[1:-1]

        version = "".join(v for v in version if v.strip())
    else:
        version = ""

    return version


def split_spec(spec, sep):
    """Split a spec by separator and return stripped start and end parts."""
    parts = spec.rsplit(sep, 1)
    spec_start = parts[0].strip()
    spec_end = ""
    if len(parts) == 2:
        spec_end = parts[-1].strip()
    return spec_start, spec_end


def parse_specification(spec):
    """
    Parse a requirement from a python distribution metadata and return a
    namedtuple with name, extras, constraints, marker and url components.

    This method does not enforce strict specifications but extracts the
    information which is assumed to be *correct*. As such no errors are raised.

    Example
    -------
    PySpec(name='requests', extras=['security'], constraints='>=3.3.0',
           marker='foo >= 2.7 or bar == 1', url=''])
    """
    name, extras, const = spec, [], ""

    # Remove excess whitespace
    spec = " ".join(p for p in spec.split(" ") if p).strip()

    # Extract marker (Assumes that there can only be one ';' inside the spec)
    spec, marker = split_spec(spec, ";")

    # Extract url (Assumes that there can only be one '@' inside the spec)
    spec, url = split_spec(spec, "@")

    # Find name, extras and constraints
    r = PARTIAL_PYPI_SPEC_PATTERN.match(spec)
    if r:
        # Normalize name
        name = r.group("name")
        name = norm_package_name(name)  # TODO: Do we want this or not?

        # Clean extras
        extras = r.group("extras")
        extras = [e.strip() for e in extras.split(",") if e] if extras else []

        # Clean constraints
        const = r.group("constraints")
        const = "".join(c for c in const.split(" ") if c).strip()
        if const.startswith("(") and const.endswith(")"):
            # Remove parens
            const = const[1:-1]
        const = const.replace("-", ".")

    return PySpec(name=name, extras=extras, constraints=const, marker=marker, url=url)


def get_site_packages_anchor_files(site_packages_path, site_packages_dir):
    """Get all the anchor files for the site packages directory."""
    site_packages_anchor_files = set()
    for entry in scandir(site_packages_path):
        fname = entry.name
        anchor_file = None
        if fname.endswith(".dist-info"):
            anchor_file = "{}/{}/{}".format(site_packages_dir, fname, "RECORD")
        elif fname.endswith(".egg-info"):
            if isfile(join(site_packages_path, fname)):
                anchor_file = f"{site_packages_dir}/{fname}"
            else:
                anchor_file = "{}/{}/{}".format(site_packages_dir, fname, "PKG-INFO")
        elif fname.endswith(".egg"):
            if isdir(join(site_packages_path, fname)):
                anchor_file = "{}/{}/{}/{}".format(
                    site_packages_dir, fname, "EGG-INFO", "PKG-INFO"
                )
            # FIXME: If it is a .egg file, we need to unzip the content to be
            # able. Do this once and leave the directory, and remove the egg
            # (which is a zip file in disguise?)
        elif fname.endswith(".egg-link"):
            anchor_file = f"{site_packages_dir}/{fname}"
        elif fname.endswith(".pth"):
            continue
        else:
            continue

        if anchor_file:
            site_packages_anchor_files.add(anchor_file)

    return site_packages_anchor_files


def get_dist_file_from_egg_link(egg_link_file, prefix_path):
    """Return the egg info file path following an egg link."""
    egg_info_full_path = None

    egg_link_path = join(prefix_path, win_path_ok(egg_link_file))
    try:
        with open(egg_link_path) as fh:
            # See: https://setuptools.readthedocs.io/en/latest/formats.html#egg-links
            # "...Each egg-link file should contain a single file or directory name
            # with no newlines..."
            egg_link_contents = fh.readlines()[0].strip()
    except UnicodeDecodeError:
        from locale import getpreferredencoding

        with open(egg_link_path, encoding=getpreferredencoding()) as fh:
            egg_link_contents = fh.readlines()[0].strip()

    if lexists(egg_link_contents):
        egg_info_fnames = tuple(
            name
            for name in (entry.name for entry in scandir(egg_link_contents))
            if name[-9:] == ".egg-info"
        )
    else:
        egg_info_fnames = ()

    if egg_info_fnames:
        if len(egg_info_fnames) != 1:
            raise CondaError(
                f"Expected exactly one `egg-info` directory in '{egg_link_contents}', via egg-link '{egg_link_file}'."
                f" Instead found: {egg_info_fnames}.  These are often left over from "
                "legacy operations that did not clean up correctly.  Please "
                "remove all but one of these."
            )

        egg_info_full_path = join(egg_link_contents, egg_info_fnames[0])

        if isdir(egg_info_full_path):
            egg_info_full_path = join(egg_info_full_path, "PKG-INFO")

    if egg_info_full_path is None:
        raise OSError(ENOENT, strerror(ENOENT), egg_link_contents)

    return egg_info_full_path


# See: https://bitbucket.org/pypa/distlib/src/34629e41cdff5c29429c7a4d1569ef5508b56929/distlib/util.py?at=default&fileviewer=file-view-default  # NOQA
# ------------------------------------------------------------------------------------------------
def parse_marker(marker_string):
    """
    Parse marker string and return a dictionary containing a marker expression.

    The dictionary will contain keys "op", "lhs" and "rhs" for non-terminals in
    the expression grammar, or strings. A string contained in quotes is to be
    interpreted as a literal string, and a string not contained in quotes is a
    variable (such as os_name).
    """

    def marker_var(remaining):
        # either identifier, or literal string
        m = IDENTIFIER.match(remaining)
        if m:
            result = m.groups()[0]
            remaining = remaining[m.end() :]
        elif not remaining:
            raise SyntaxError("unexpected end of input")
        else:
            q = remaining[0]
            if q not in "'\"":
                raise SyntaxError(f"invalid expression: {remaining}")
            oq = "'\"".replace(q, "")
            remaining = remaining[1:]
            parts = [q]
            while remaining:
                # either a string chunk, or oq, or q to terminate
                if remaining[0] == q:
                    break
                elif remaining[0] == oq:
                    parts.append(oq)
                    remaining = remaining[1:]
                else:
                    m = STRING_CHUNK.match(remaining)
                    if not m:
                        raise SyntaxError(f"error in string literal: {remaining}")
                    parts.append(m.groups()[0])
                    remaining = remaining[m.end() :]
            else:
                s = "".join(parts)
                raise SyntaxError(f"unterminated string: {s}")
            parts.append(q)
            result = "".join(parts)
            remaining = remaining[1:].lstrip()  # skip past closing quote
        return result, remaining

    def marker_expr(remaining):
        if remaining and remaining[0] == "(":
            result, remaining = marker(remaining[1:].lstrip())
            if remaining[0] != ")":
                raise SyntaxError(f"unterminated parenthesis: {remaining}")
            remaining = remaining[1:].lstrip()
        else:
            lhs, remaining = marker_var(remaining)
            while remaining:
                m = MARKER_OP.match(remaining)
                if not m:
                    break
                op = m.groups()[0]
                remaining = remaining[m.end() :]
                rhs, remaining = marker_var(remaining)
                lhs = {"op": op, "lhs": lhs, "rhs": rhs}
            result = lhs
        return result, remaining

    def marker_and(remaining):
        lhs, remaining = marker_expr(remaining)
        while remaining:
            m = AND.match(remaining)
            if not m:
                break
            remaining = remaining[m.end() :]
            rhs, remaining = marker_expr(remaining)
            lhs = {"op": "and", "lhs": lhs, "rhs": rhs}
        return lhs, remaining

    def marker(remaining):
        lhs, remaining = marker_and(remaining)
        while remaining:
            m = OR.match(remaining)
            if not m:
                break
            remaining = remaining[m.end() :]
            rhs, remaining = marker_and(remaining)
            lhs = {"op": "or", "lhs": lhs, "rhs": rhs}
        return lhs, remaining

    return marker(marker_string)


# See:
#   https://bitbucket.org/pypa/distlib/src/34629e41cdff5c29429c7a4d1569ef5508b56929/distlib/util.py?at=default&fileviewer=file-view-default  # NOQA
#   https://bitbucket.org/pypa/distlib/src/34629e41cdff5c29429c7a4d1569ef5508b56929/distlib/markers.py?at=default&fileviewer=file-view-default  # NOQA
# ------------------------------------------------------------------------------------------------
#
# Requirement parsing code as per PEP 508
#
IDENTIFIER = re.compile(r"^([\w\.-]+)\s*")
VERSION_IDENTIFIER = re.compile(r"^([\w\.*+-]+)\s*")
COMPARE_OP = re.compile(r"^(<=?|>=?|={2,3}|[~!]=)\s*")
MARKER_OP = re.compile(r"^((<=?)|(>=?)|={2,3}|[~!]=|in|not\s+in)\s*")
OR = re.compile(r"^or\b\s*")
AND = re.compile(r"^and\b\s*")
NON_SPACE = re.compile(r"(\S+)\s*")
STRING_CHUNK = re.compile(r"([\s\w\.{}()*+#:;,/?!~`@$%^&=|<>\[\]-]+)")


def _is_literal(o):
    if not isinstance(o, str) or not o:
        return False
    return o[0] in "'\""


class Evaluator:
    """This class is used to evaluate marker expressions."""

    operations = {
        "==": lambda x, y: x == y,
        "===": lambda x, y: x == y,
        "~=": lambda x, y: x == y or x > y,
        "!=": lambda x, y: x != y,
        "<": lambda x, y: x < y,
        "<=": lambda x, y: x == y or x < y,
        ">": lambda x, y: x > y,
        ">=": lambda x, y: x == y or x > y,
        "and": lambda x, y: x and y,
        "or": lambda x, y: x or y,
        "in": lambda x, y: x in y,
        "not in": lambda x, y: x not in y,
    }

    def evaluate(self, expr, context):
        """
        Evaluate a marker expression returned by the :func:`parse_requirement`
        function in the specified context.
        """
        if isinstance(expr, str):
            if expr[0] in "'\"":
                result = expr[1:-1]
            else:
                if expr not in context:
                    raise SyntaxError(f"unknown variable: {expr}")
                result = context[expr]
        else:
            assert isinstance(expr, dict)
            op = expr["op"]
            if op not in self.operations:
                raise NotImplementedError(f"op not implemented: {op}")
            elhs = expr["lhs"]
            erhs = expr["rhs"]
            if _is_literal(expr["lhs"]) and _is_literal(expr["rhs"]):
                raise SyntaxError(f"invalid comparison: {elhs} {op} {erhs}")

            lhs = self.evaluate(elhs, context)
            rhs = self.evaluate(erhs, context)
            result = self.operations[op](lhs, rhs)
        return result


# def update_marker_context(python_version):
#     """Update default marker context to include environment python version."""
#     updated_context = DEFAULT_MARKER_CONTEXT.copy()
#     context = {
#         'python_full_version': python_version,
#         'python_version': '.'.join(python_version.split('.')[:2]),
#         'extra': '',
#     }
#     updated_context.update(context)
#     return updated_context


def get_default_marker_context():
    """Return the default context dictionary to use when parsing markers."""

    def format_full_version(info):
        version = f"{info.major}.{info.minor}.{info.micro}"
        kind = info.releaselevel
        if kind != "final":
            version += kind[0] + str(info.serial)
        return version

    if hasattr(sys, "implementation"):
        implementation_version = format_full_version(sys.implementation.version)
        implementation_name = sys.implementation.name
    else:
        implementation_version = "0"
        implementation_name = ""

    # TODO: we can't use this
    result = {
        # See: https://www.python.org/dev/peps/pep-0508/#environment-markers
        "implementation_name": implementation_name,
        "implementation_version": implementation_version,
        "os_name": os_name,
        "platform_machine": platform.machine(),
        "platform_python_implementation": platform.python_implementation(),
        "platform_release": platform.release(),
        "platform_system": platform.system(),
        "platform_version": platform.version(),
        "python_full_version": platform.python_version(),
        "python_version": ".".join(platform.python_version().split(".")[:2]),
        "sys_platform": sys.platform,
        # See: https://www.python.org/dev/peps/pep-0345/#environment-markers
        "os.name": os_name,
        "platform.python_implementation": platform.python_implementation(),
        "platform.version": platform.version(),
        "platform.machine": platform.machine(),
        "sys.platform": sys.platform,
        "extra": "",
    }
    return result


DEFAULT_MARKER_CONTEXT = get_default_marker_context()
evaluator = Evaluator()


# FIXME: Should this raise errors, or fail silently or with a warning?
def interpret(marker, execution_context=None):
    """
    Interpret a marker and return a result depending on environment.

    :param marker: The marker to interpret.
    :type marker: str
    :param execution_context: The context used for name lookup.
    :type execution_context: mapping
    """
    try:
        expr, rest = parse_marker(marker)
    except Exception as e:
        raise SyntaxError(f"Unable to interpret marker syntax: {marker}: {e}")

    if rest and rest[0] != "#":
        raise SyntaxError(f"unexpected trailing data in marker: {marker}: {rest}")

    context = DEFAULT_MARKER_CONTEXT.copy()
    if execution_context:
        context.update(execution_context)

    return evaluator.evaluate(expr, context)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
These helpers were originally defined in tests/test_create.py,
but were refactored here so downstream projects can benefit from
them too.
"""

from __future__ import annotations

import json
import os
import sys
from contextlib import contextmanager
from functools import lru_cache
from logging import getLogger
from os.path import dirname, isdir, join, lexists
from pathlib import Path
from random import sample
from shutil import copyfile, rmtree
from subprocess import check_output
from tempfile import gettempdir
from typing import TYPE_CHECKING
from uuid import uuid4

import pytest

from ..auxlib.compat import Utf8NamedTemporaryFile
from ..auxlib.entity import EntityEncoder
from ..base.constants import PACKAGE_CACHE_MAGIC_FILE
from ..base.context import conda_tests_ctxt_mgmt_def_pol, context, reset_context
from ..cli.conda_argparse import do_call, generate_parser
from ..cli.main import init_loggers
from ..common.compat import on_win
from ..common.io import (
    argv,
    captured,
    dashlist,
    disable_logger,
    env_var,
    stderr_log_level,
)
from ..common.url import path_to_url
from ..core.package_cache_data import PackageCacheData
from ..core.prefix_data import PrefixData
from ..deprecations import deprecated
from ..exceptions import conda_exception_handler
from ..gateways.disk.create import mkdir_p
from ..gateways.disk.delete import rm_rf
from ..gateways.disk.link import link
from ..gateways.disk.update import touch
from ..gateways.logging import DEBUG
from ..models.match_spec import MatchSpec
from ..models.records import PackageRecord
from ..utils import massage_arguments

if TYPE_CHECKING:
    from typing import Iterator

    from ..models.records import PrefixRecord

TEST_LOG_LEVEL = DEBUG
PYTHON_BINARY = "python.exe" if on_win else "bin/python"
BIN_DIRECTORY = "Scripts" if on_win else "bin"
UNICODE_CHARACTERS = ""
# UNICODE_CHARACTERS_RESTRICTED = u""
UNICODE_CHARACTERS_RESTRICTED = "abcdef"
which_or_where = "which" if not on_win else "where"
cp_or_copy = "cp" if not on_win else "copy"
env_or_set = "env" if not on_win else "set"

# UNICODE_CHARACTERS = u"12345678abcdef"
# UNICODE_CHARACTERS_RESTRICTED = UNICODE_CHARACTERS

# When testing for bugs, you may want to change this to a _,
# for example to see if a bug is related to spaces in prefixes.
SPACER_CHARACTER = " "

log = getLogger(__name__)


def escape_for_winpath(p):
    return p.replace("\\", "\\\\")


@lru_cache(maxsize=None)
@deprecated("24.9", "25.3")
def running_a_python_capable_of_unicode_subprocessing():
    name = None
    # try:
    # UNICODE_CHARACTERS + os.sep +
    with Utf8NamedTemporaryFile(
        mode="w", suffix=UNICODE_CHARACTERS + ".bat", delete=False
    ) as batch_file:
        batch_file.write("@echo Hello World\n")
        batch_file.write("@exit 0\n")
        name = batch_file.name
    if name:
        try:
            out = check_output(name, cwd=dirname(name), stderr=None, shell=False)
            out = out.decode("utf-8") if hasattr(out, "decode") else out
            if out.startswith("Hello World"):
                return True
            return False
        except Exception:
            return False
        finally:
            os.unlink(name)
    return False


tmpdir_in_use = None


@pytest.fixture(autouse=True)
@deprecated(
    "24.9",
    "25.3",
    addendum="Use `tmp_path`, `conda.testing.path_factory`, or `conda.testing.tmp_env` instead.",
)
def set_tmpdir(tmpdir):
    global tmpdir_in_use
    if not tmpdir:
        return tmpdir_in_use
    td = tmpdir.strpath
    assert os.sep in td
    tmpdir_in_use = td


@deprecated(
    "24.9",
    "25.3",
    addendum="Use `tmp_path`, `conda.testing.path_factory`, or `conda.testing.tmp_env` instead.",
)
def _get_temp_prefix(name=None, use_restricted_unicode=False):
    tmpdir = tmpdir_in_use or gettempdir()
    capable = running_a_python_capable_of_unicode_subprocessing()

    if not capable or use_restricted_unicode:
        RESTRICTED = UNICODE_CHARACTERS_RESTRICTED
        random_unicode = "".join(sample(RESTRICTED, len(RESTRICTED)))
    else:
        random_unicode = "".join(sample(UNICODE_CHARACTERS, len(UNICODE_CHARACTERS)))
    tmpdir_name = os.environ.get(
        "CONDA_TEST_TMPDIR_NAME",
        (str(uuid4())[:4] + SPACER_CHARACTER + random_unicode)
        if name is None
        else name,
    )
    prefix = join(tmpdir, tmpdir_name)

    # Exit immediately if we cannot use hardlinks, on Windows, we get permissions errors if we use
    # sys.executable so instead use the pdb files.
    src = sys.executable.replace(".exe", ".pdb") if on_win else sys.executable
    dst = os.path.join(tmpdir, os.path.basename(sys.executable))

    try:
        link(src, dst)
    except OSError:
        print(
            f"\nWARNING :: You are testing `conda` with `tmpdir`:-\n           {tmpdir}\n"
            f"           not on the same FS as `sys.prefix`:\n           {sys.prefix}\n"
            "           this will be slow and unlike the majority of end-user installs.\n"
            "           Please pass `--basetemp=<somewhere-else>` instead."
        )
    try:
        rm_rf(dst)
    except Exception as e:
        print(e)
        pass

    return prefix


@deprecated(
    "24.9",
    "25.3",
    addendum="Use `tmp_path`, `conda.testing.path_factory`, or `conda.testing.tmp_env` instead.",
)
def make_temp_prefix(name=None, use_restricted_unicode=False, _temp_prefix=None):
    """
    When the env. you are creating will be used to install Python 2.7 on Windows
    only a restricted amount of Unicode will work, and probably only those chars
    in your current codepage, so the characters in UNICODE_CHARACTERS_RESTRICTED
    should probably be randomly generated from that instead. The problem here is
    that the current codepage needs to be able to handle 'sys.prefix' otherwise
    ntpath will fall over.
    """
    if not _temp_prefix:
        _temp_prefix = _get_temp_prefix(
            name=name, use_restricted_unicode=use_restricted_unicode
        )
    try:
        os.makedirs(_temp_prefix)
    except:
        pass
    assert isdir(_temp_prefix)
    return _temp_prefix


@deprecated(
    "24.9",
    "25.3",
    addendum="Use `tmp_path`, `conda.testing.path_factory`, or `conda.testing.tmp_env` instead.",
)
def FORCE_temp_prefix(name=None, use_restricted_unicode=False):
    _temp_prefix = _get_temp_prefix(
        name=name, use_restricted_unicode=use_restricted_unicode
    )
    rm_rf(_temp_prefix)
    os.makedirs(_temp_prefix)
    assert isdir(_temp_prefix)
    return _temp_prefix


class Commands:
    COMPARE = "compare"
    CONFIG = "config"
    CLEAN = "clean"
    CREATE = "create"
    INFO = "info"
    INSTALL = "install"
    LIST = "list"
    REMOVE = "remove"
    SEARCH = "search"
    UPDATE = "update"
    RUN = "run"


@deprecated("23.9", "25.3", addendum="Use `conda.testing.conda_cli` instead.")
def run_command(command, prefix, *arguments, **kwargs) -> tuple[str, str, int]:
    assert isinstance(arguments, tuple), "run_command() arguments must be tuples"
    arguments = massage_arguments(arguments)

    use_exception_handler = kwargs.get("use_exception_handler", False)
    # These commands require 'dev' mode to be enabled during testing because
    # they end up calling run_script() in link.py and that uses wrapper scripts for e.g. activate.
    # Setting `dev` means that, in these scripts, conda is executed via:
    #   `sys.prefix/bin/python -m conda` (or the Windows equivalent).
    # .. and the source code for `conda` is put on `sys.path` via `PYTHONPATH` (a bit gross but
    # less so than always requiring `cwd` to be the root of the conda source tree in every case).
    # If you do not want this to happen for some test you must pass dev=False as a kwarg, though
    # for nearly all tests, you want to make sure you are running *this* conda and not some old
    # conda (it was random which you'd get depending on the initial values of PATH and PYTHONPATH
    # - and likely more variables - before `dev` came along). Setting CONDA_EXE is not enough
    # either because in the 4.5 days that would just run whatever Python was found first on PATH.
    command_defaults_to_dev = command in (
        Commands.CREATE,
        Commands.INSTALL,
        Commands.REMOVE,
        Commands.RUN,
    )
    dev = kwargs.get("dev", True if command_defaults_to_dev else False)
    debug = kwargs.get("debug_wrapper_scripts", False)

    p = generate_parser()

    if command is Commands.CONFIG:
        arguments.append("--file")
        arguments.append(join(prefix, "condarc"))
    if command in (
        Commands.LIST,
        Commands.COMPARE,
        Commands.CREATE,
        Commands.INSTALL,
        Commands.REMOVE,
        Commands.UPDATE,
        Commands.RUN,
    ):
        arguments.insert(0, "-p")
        arguments.insert(1, prefix)
    if command in (Commands.CREATE, Commands.INSTALL, Commands.REMOVE, Commands.UPDATE):
        arguments.extend(["-y", "-q"])

    arguments.insert(0, command)
    if dev:
        arguments.insert(1, "--dev")
    if debug:
        arguments.insert(1, "--debug-wrapper-scripts")

    # It would be nice at this point to re-use:
    # from ..cli.python_api import run_command as python_api_run_command
    # python_api_run_command
    # .. but that does not support no_capture and probably more stuff.

    args = p.parse_args(arguments)
    context._set_argparse_args(args)
    init_loggers()
    cap_args = () if not kwargs.get("no_capture") else (None, None)
    # list2cmdline is not exact, but it is only informational.
    print(
        "\n\nEXECUTING COMMAND >>> $ conda {}\n\n".format(" ".join(arguments)),
        file=sys.stderr,
    )
    with stderr_log_level(TEST_LOG_LEVEL, "conda"), stderr_log_level(
        TEST_LOG_LEVEL, "requests"
    ):
        with argv(["python_api", *arguments]), captured(*cap_args) as c:
            if use_exception_handler:
                result = conda_exception_handler(do_call, args, p)
            else:
                result = do_call(args, p)
        stdout = c.stdout
        stderr = c.stderr
        print(stdout, file=sys.stdout)
        print(stderr, file=sys.stderr)

    # Unfortunately there are other ways to change context, such as Commands.CREATE --offline.
    # You will probably end up playing whack-a-bug here adding more and more the tuple here.
    if command in (Commands.CONFIG,):
        reset_context([os.path.join(prefix + os.sep, "condarc")], args)
    return stdout, stderr, result


@deprecated("24.9", "25.3", addendum="Use `conda.testing.tmp_env` instead.")
@contextmanager
def make_temp_env(*packages, **kwargs) -> Iterator[str]:
    name = kwargs.pop("name", None)
    use_restricted_unicode = kwargs.pop("use_restricted_unicode", False)

    prefix = kwargs.pop("prefix", None) or _get_temp_prefix(
        name=name, use_restricted_unicode=use_restricted_unicode
    )
    clean_prefix = kwargs.pop("clean_prefix", None)
    if clean_prefix:
        if os.path.exists(prefix):
            rm_rf(prefix)
    if not isdir(prefix):
        make_temp_prefix(name, use_restricted_unicode, prefix)
    with disable_logger("fetch"):
        try:
            # try to clear any config that's been set by other tests
            # CAUTION :: This does not partake in the context stack management code
            #            of env_{var,vars,unmodified} and, when used in conjunction
            #            with that code, this *must* be called first.
            reset_context([os.path.join(prefix + os.sep, "condarc")])
            run_command(Commands.CREATE, prefix, *packages, **kwargs)
            yield prefix
        finally:
            if "CONDA_TEST_SAVE_TEMPS" not in os.environ:
                rmtree(prefix, ignore_errors=True)
            else:
                log.warning(
                    f"CONDA_TEST_SAVE_TEMPS :: retaining make_temp_env {prefix}"
                )


@deprecated("24.9", "25.3", addendum="Use `conda.testing.tmp_pkgs_dir` instead.")
@contextmanager
def make_temp_package_cache() -> Iterator[str]:
    prefix = make_temp_prefix(use_restricted_unicode=on_win)
    pkgs_dir = join(prefix, "pkgs")
    mkdir_p(pkgs_dir)
    touch(join(pkgs_dir, PACKAGE_CACHE_MAGIC_FILE))

    try:
        with env_var(
            "CONDA_PKGS_DIRS",
            pkgs_dir,
            stack_callback=conda_tests_ctxt_mgmt_def_pol,
        ):
            assert context.pkgs_dirs == (pkgs_dir,)
            yield pkgs_dir
    finally:
        rmtree(prefix, ignore_errors=True)
        PackageCacheData._cache_.pop(pkgs_dir, None)


@deprecated("24.9", "25.3", addendum="Use `conda.testing.tmp_channel` instead.")
@contextmanager
def make_temp_channel(packages) -> Iterator[str]:
    package_reqs = [pkg.replace("-", "=") for pkg in packages]
    package_names = [pkg.split("-")[0] for pkg in packages]

    with make_temp_env(*package_reqs) as prefix:
        for package in packages:
            assert package_is_installed(prefix, package.replace("-", "="))
        data = [
            p for p in PrefixData(prefix).iter_records() if p["name"] in package_names
        ]
        run_command(Commands.REMOVE, prefix, *package_names)
        for package in packages:
            assert not package_is_installed(prefix, package.replace("-", "="))

    repodata = {"info": {}, "packages": {}}
    tarfiles = {}
    for package_data in data:
        pkg_data = package_data
        fname = pkg_data["fn"]
        tarfiles[fname] = join(PackageCacheData.first_writable().pkgs_dir, fname)

        pkg_data = pkg_data.dump()
        for field in ("url", "channel", "schannel"):
            pkg_data.pop(field, None)
        repodata["packages"][fname] = PackageRecord(**pkg_data)

    with make_temp_env() as channel:
        subchan = join(channel, context.subdir)
        noarch_dir = join(channel, "noarch")
        channel = path_to_url(channel)
        os.makedirs(subchan)
        os.makedirs(noarch_dir)
        for fname, tar_old_path in tarfiles.items():
            tar_new_path = join(subchan, fname)
            copyfile(tar_old_path, tar_new_path)

        with open(join(subchan, "repodata.json"), "w") as f:
            f.write(json.dumps(repodata, cls=EntityEncoder))
        with open(join(noarch_dir, "repodata.json"), "w") as f:
            f.write(json.dumps({}, cls=EntityEncoder))

        yield channel


@deprecated(
    "24.9", "25.3", addendum="Use `tmp_path` or `conda.testing.path_factory` instead."
)
def create_temp_location() -> str:
    return _get_temp_prefix()


@deprecated(
    "24.9", "25.3", addendum="Use `tmp_path` or `conda.testing.path_factory` instead."
)
@contextmanager
def tempdir() -> Iterator[str]:
    prefix = create_temp_location()
    try:
        os.makedirs(prefix)
        yield prefix
    finally:
        if lexists(prefix):
            rm_rf(prefix)


@deprecated("24.9", "25.3", addendum="Use `conda.base.context.reset_context` instead.")
def reload_config(prefix) -> None:
    prefix_condarc = join(prefix, "condarc")
    reset_context([prefix_condarc])


def package_is_installed(
    prefix: str | os.PathLike | Path,
    spec: str | MatchSpec,
) -> PrefixRecord | None:
    spec = MatchSpec(spec)
    prefix_recs = tuple(PrefixData(str(prefix), pip_interop_enabled=True).query(spec))
    if not prefix_recs:
        return None
    elif len(prefix_recs) > 1:
        raise AssertionError(
            f"Multiple packages installed.{dashlist(prec.dist_str() for prec in prefix_recs)}"
        )
    else:
        return prefix_recs[0]


def get_shortcut_dir(prefix_for_unix=sys.prefix):
    if sys.platform == "win32":
        # On Windows, .nonadmin has been historically created by constructor in sys.prefix
        user_mode = "user" if Path(sys.prefix, ".nonadmin").is_file() else "system"
        try:  # menuinst v2
            from menuinst.platforms.win_utils.knownfolders import dirs_src

            return dirs_src[user_mode]["start"][0]
        except ImportError:  # older menuinst versions; TODO: remove
            try:
                from menuinst.win32 import dirs_src

                return dirs_src[user_mode]["start"][0]
            except ImportError:
                from menuinst.win32 import dirs

                return dirs[user_mode]["start"]
    # on unix, .nonadmin is only created by menuinst v2 as needed on the target prefix
    # it might exist, or might not; if it doesn't, we try to create it
    # see https://github.com/conda/menuinst/issues/150
    non_admin_file = Path(prefix_for_unix, ".nonadmin")
    if non_admin_file.is_file():
        user_mode = "user"
    else:
        try:
            non_admin_file.touch()
        except OSError:
            user_mode = "system"
        else:
            user_mode = "user"
            non_admin_file.unlink()

    if sys.platform == "darwin":
        if user_mode == "user":
            return join(os.environ["HOME"], "Applications")
        return "/Applications"
    if sys.platform == "linux":
        if user_mode == "user":
            return join(os.environ["HOME"], ".local", "share", "applications")
        return "/usr/share/applications"
    raise NotImplementedError(sys.platform)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Helpers for testing the solver."""

from __future__ import annotations

import collections
import functools
import json
import pathlib
from tempfile import TemporaryDirectory

import pytest

from ..base.context import context
from ..core.solve import Solver
from ..exceptions import (
    PackagesNotFoundError,
    ResolvePackageNotFound,
    UnsatisfiableError,
)
from ..models.channel import Channel
from ..models.match_spec import MatchSpec
from ..models.records import PackageRecord
from . import helpers


@functools.lru_cache
def index_packages(num):
    """Get the index data of the ``helpers.get_index_r_*`` helpers."""
    # XXX: get_index_r_X should probably be refactored to avoid loading the environment like this.
    get_index = getattr(helpers, f"get_index_r_{num}")
    index, _ = get_index(context.subdir)
    return list(index.values())


def package_string(record):
    return f"{record.channel.name}::{record.name}-{record.version}-{record.build}"


def package_string_set(packages):
    """Transforms package container in package string set."""
    return {package_string(record) for record in packages}


def package_dict(packages):
    """Transforms package container into a dictionary."""
    return {record.name: record for record in packages}


class SimpleEnvironment:
    """Helper environment object."""

    REPO_DATA_KEYS = (
        "build",
        "build_number",
        "depends",
        "license",
        "md5",
        "name",
        "sha256",
        "size",
        "subdir",
        "timestamp",
        "version",
        "track_features",
        "features",
    )

    def __init__(self, path, solver_class, subdirs=context.subdirs):
        self._path = pathlib.Path(path)
        self._prefix_path = self._path / "prefix"
        self._channels_path = self._path / "channels"
        self._solver_class = solver_class
        self.subdirs = subdirs
        self.installed_packages = []
        # if repo_packages is a list, the packages will be put in a `test` channel
        # if it is a dictionary, it the keys are the channel name and the value
        # the channel packages
        self.repo_packages: list[str] | dict[str, list[str]] = []

    def solver(self, add, remove):
        """Writes ``repo_packages`` to the disk and creates a solver instance."""
        channels = []
        self._write_installed_packages()
        for channel_name, packages in self._channel_packages.items():
            self._write_repo_packages(channel_name, packages)
            channel = Channel(str(self._channels_path / channel_name))
            channels.append(channel)
        return self._solver_class(
            prefix=self._prefix_path,
            subdirs=self.subdirs,
            channels=channels,
            specs_to_add=add,
            specs_to_remove=remove,
        )

    def solver_transaction(self, add=(), remove=(), as_specs=False):
        packages = self.solver(add=add, remove=remove).solve_final_state()
        if as_specs:
            return packages
        return package_string_set(packages)

    def install(self, *specs, as_specs=False):
        return self.solver_transaction(add=specs, as_specs=as_specs)

    def remove(self, *specs, as_specs=False):
        return self.solver_transaction(remove=specs, as_specs=as_specs)

    @property
    def _channel_packages(self):
        """Helper that unfolds the ``repo_packages`` into a dictionary."""
        if isinstance(self.repo_packages, dict):
            return self.repo_packages
        return {"test": self.repo_packages}

    def _package_data(self, record):
        """Turn record into data, to be written in the JSON environment/repo files."""
        data = {
            key: value
            for key, value in vars(record).items()
            if key in self.REPO_DATA_KEYS
        }
        if "subdir" not in data:
            data["subdir"] = context.subdir
        return data

    def _write_installed_packages(self):
        if not self.installed_packages:
            return
        conda_meta = self._prefix_path / "conda-meta"
        conda_meta.mkdir(exist_ok=True, parents=True)
        # write record files
        for record in self.installed_packages:
            record_path = (
                conda_meta / f"{record.name}-{record.version}-{record.build}.json"
            )
            record_data = self._package_data(record)
            record_data["channel"] = record.channel.name
            record_path.write_text(json.dumps(record_data))
        # write history file
        history_path = conda_meta / "history"
        history_path.write_text(
            "\n".join(
                (
                    "==> 2000-01-01 00:00:00 <==",
                    *map(package_string, self.installed_packages),
                )
            )
        )

    def _write_repo_packages(self, channel_name, packages):
        """Write packages to the channel path."""
        # build package data
        package_data = collections.defaultdict(dict)
        for record in packages:
            package_data[record.subdir][record.fn] = self._package_data(record)
        # write repodata
        assert set(self.subdirs).issuperset(set(package_data.keys()))
        for subdir in self.subdirs:
            subdir_path = self._channels_path / channel_name / subdir
            subdir_path.mkdir(parents=True, exist_ok=True)
            subdir_path.joinpath("repodata.json").write_text(
                json.dumps(
                    {
                        "info": {
                            "subdir": subdir,
                        },
                        "packages": package_data.get(subdir, {}),
                    }
                )
            )


def empty_prefix():
    return TemporaryDirectory(prefix="conda-test-repo-")


@pytest.fixture()
def temp_simple_env(solver_class=Solver) -> SimpleEnvironment:
    with empty_prefix() as prefix:
        yield SimpleEnvironment(prefix, solver_class)


class SolverTests:
    """Tests for :py:class:`conda.core.solve.Solver` implementations."""

    @property
    def solver_class(self) -> type[Solver]:
        """Class under test."""
        raise NotImplementedError

    @property
    def tests_to_skip(self):
        return {}  # skip reason -> list of tests to skip

    @pytest.fixture(autouse=True)
    def skip_tests(self, request):
        for reason, skip_list in self.tests_to_skip.items():
            if request.node.name in skip_list:
                pytest.skip(reason)

    @pytest.fixture()
    def env(self):
        with TemporaryDirectory(prefix="conda-test-repo-") as tmpdir:
            self.env = SimpleEnvironment(tmpdir, self.solver_class)
            yield self.env
            self.env = None

    def find_package_in_list(self, packages, **kwargs):
        for record in packages:
            if all(getattr(record, key) == value for key, value in kwargs.items()):
                return record

    def find_package(self, **kwargs):
        if isinstance(self.env.repo_packages, dict):
            if "channel" not in kwargs:
                raise ValueError(
                    "Repo has multiple channels, the `channel` argument must be specified"
                )
            packages = self.env.repo_packages[kwargs["channel"]]
        else:
            packages = self.env.repo_packages
        return self.find_package_in_list(packages, **kwargs)

    def assert_unsatisfiable(self, exc_info, entries):
        """Helper to assert that a :py:class:`conda.exceptions.UnsatisfiableError`
        instance as a the specified set of unsatisfiable specifications.
        """
        assert issubclass(exc_info.type, UnsatisfiableError)
        if exc_info.type is UnsatisfiableError:
            assert (
                sorted(
                    tuple(map(str, entries)) for entries in exc_info.value.unsatisfiable
                )
                == entries
            )

    def test_empty(self, env):
        env.repo_packages = index_packages(1)
        assert env.install() == set()

    def test_iopro_mkl(self, env):
        env.repo_packages = index_packages(1)
        assert env.install("iopro 1.4*", "python 2.7*", "numpy 1.7*") == {
            "test::iopro-1.4.3-np17py27_p0",
            "test::numpy-1.7.1-py27_0",
            "test::openssl-1.0.1c-0",
            "test::python-2.7.5-0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::unixodbc-2.3.1-0",
            "test::zlib-1.2.7-0",
            "test::distribute-0.6.36-py27_1",
            "test::pip-1.3.1-py27_1",
        }

    def test_iopro_nomkl(self, env):
        env.repo_packages = index_packages(1)
        assert env.install(
            "iopro 1.4*", "python 2.7*", "numpy 1.7*", MatchSpec(track_features="mkl")
        ) == {
            "test::iopro-1.4.3-np17py27_p0",
            "test::mkl-rt-11.0-p0",
            "test::numpy-1.7.1-py27_p0",
            "test::openssl-1.0.1c-0",
            "test::python-2.7.5-0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::unixodbc-2.3.1-0",
            "test::zlib-1.2.7-0",
            "test::distribute-0.6.36-py27_1",
            "test::pip-1.3.1-py27_1",
        }

    def test_mkl(self, env):
        env.repo_packages = index_packages(1)
        assert env.install("mkl") == env.install(
            "mkl 11*", MatchSpec(track_features="mkl")
        )

    def test_accelerate(self, env):
        env.repo_packages = index_packages(1)
        assert env.install("accelerate") == env.install(
            "accelerate", MatchSpec(track_features="mkl")
        )

    def test_scipy_mkl(self, env):
        env.repo_packages = index_packages(1)
        records = env.install(
            "scipy",
            "python 2.7*",
            "numpy 1.7*",
            MatchSpec(track_features="mkl"),
            as_specs=True,
        )

        for record in records:
            if record.name in ("numpy", "scipy"):
                assert "mkl" in record.features

        assert "test::numpy-1.7.1-py27_p0" in package_string_set(records)
        assert "test::scipy-0.12.0-np17py27_p0" in package_string_set(records)

    def test_anaconda_nomkl(self, env):
        env.repo_packages = index_packages(1)
        records = env.install("anaconda 1.5.0", "python 2.7*", "numpy 1.7*")
        assert len(records) == 107
        assert "test::scipy-0.12.0-np17py27_0" in records

    def test_pseudo_boolean(self, env):
        env.repo_packages = index_packages(1)
        # The latest version of iopro, 1.5.0, was not built against numpy 1.5
        assert env.install("iopro", "python 2.7*", "numpy 1.5*") == {
            "test::iopro-1.4.3-np15py27_p0",
            "test::numpy-1.5.1-py27_4",
            "test::openssl-1.0.1c-0",
            "test::python-2.7.5-0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::unixodbc-2.3.1-0",
            "test::zlib-1.2.7-0",
            "test::distribute-0.6.36-py27_1",
            "test::pip-1.3.1-py27_1",
        }
        assert env.install(
            "iopro", "python 2.7*", "numpy 1.5*", MatchSpec(track_features="mkl")
        ) == {
            "test::iopro-1.4.3-np15py27_p0",
            "test::mkl-rt-11.0-p0",
            "test::numpy-1.5.1-py27_p4",
            "test::openssl-1.0.1c-0",
            "test::python-2.7.5-0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::unixodbc-2.3.1-0",
            "test::zlib-1.2.7-0",
            "test::distribute-0.6.36-py27_1",
            "test::pip-1.3.1-py27_1",
        }

    def test_unsat_from_r1(self, env):
        env.repo_packages = index_packages(1)

        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("numpy 1.5*", "scipy 0.12.0b1")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("numpy=1.5",),
                ("scipy==0.12.0b1", "numpy[version='1.6.*|1.7.*']"),
            ],
        )

        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("numpy 1.5*", "python 3*")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("numpy=1.5", "nose", "python=3.3"),
                ("numpy=1.5", "python[version='2.6.*|2.7.*']"),
                ("python=3",),
            ],
        )

        with pytest.raises((ResolvePackageNotFound, PackagesNotFoundError)) as exc_info:
            env.install("numpy 1.5*", "numpy 1.6*")
        if exc_info.type is ResolvePackageNotFound:
            assert sorted(map(str, exc_info.value.bad_deps)) == [
                "numpy[version='1.5.*,1.6.*']",
            ]

    def test_unsat_simple(self, env):
        env.repo_packages = [
            helpers.record(name="a", depends=["c >=1,<2"]),
            helpers.record(name="b", depends=["c >=2,<3"]),
            helpers.record(name="c", version="1.0"),
            helpers.record(name="c", version="2.0"),
        ]
        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("a", "b")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("a", "c[version='>=1,<2']"),
                ("b", "c[version='>=2,<3']"),
            ],
        )

    def test_get_dists(self, env):
        env.repo_packages = index_packages(1)
        records = env.install("anaconda 1.4.0")
        assert "test::anaconda-1.4.0-np17py33_0" in records
        assert "test::freetype-2.4.10-0" in records

    def test_unsat_shortest_chain_1(self, env):
        env.repo_packages = [
            helpers.record(name="a", depends=["d", "c <1.3.0"]),
            helpers.record(name="b", depends=["c"]),
            helpers.record(
                name="c",
                version="1.3.6",
            ),
            helpers.record(
                name="c",
                version="1.2.8",
            ),
            helpers.record(name="d", depends=["c >=0.8.0"]),
        ]
        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("c=1.3.6", "a", "b")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("a", "c[version='<1.3.0']"),
                ("a", "d", "c[version='>=0.8.0']"),
                ("b", "c"),
                ("c=1.3.6",),
            ],
        )

    def test_unsat_shortest_chain_2(self, env):
        env.repo_packages = [
            helpers.record(name="a", depends=["d", "c >=0.8.0"]),
            helpers.record(name="b", depends=["c"]),
            helpers.record(
                name="c",
                version="1.3.6",
            ),
            helpers.record(
                name="c",
                version="1.2.8",
            ),
            helpers.record(name="d", depends=["c <1.3.0"]),
        ]
        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("c=1.3.6", "a", "b")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("a", "c[version='>=0.8.0']"),
                ("a", "d", "c[version='<1.3.0']"),
                ("b", "c"),
                ("c=1.3.6",),
            ],
        )

    def test_unsat_shortest_chain_3(self, env):
        env.repo_packages = [
            helpers.record(name="a", depends=["f", "e"]),
            helpers.record(name="b", depends=["c"]),
            helpers.record(
                name="c",
                version="1.3.6",
            ),
            helpers.record(
                name="c",
                version="1.2.8",
            ),
            helpers.record(name="d", depends=["c >=0.8.0"]),
            helpers.record(name="e", depends=["c <1.3.0"]),
            helpers.record(name="f", depends=["d"]),
        ]
        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("c=1.3.6", "a", "b")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("a", "e", "c[version='<1.3.0']"),
                ("b", "c"),
                ("c=1.3.6",),
            ],
        )

    def test_unsat_shortest_chain_4(self, env):
        env.repo_packages = [
            helpers.record(name="a", depends=["py =3.7.1"]),
            helpers.record(name="py_req_1"),
            helpers.record(name="py_req_2"),
            helpers.record(
                name="py", version="3.7.1", depends=["py_req_1", "py_req_2"]
            ),
            helpers.record(
                name="py", version="3.6.1", depends=["py_req_1", "py_req_2"]
            ),
        ]
        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("a", "py=3.6.1")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("a", "py=3.7.1"),
                ("py=3.6.1",),
            ],
        )

    def test_unsat_chain(self, env):
        # a -> b -> c=1.x -> d=1.x
        # e      -> c=2.x -> d=2.x
        env.repo_packages = [
            helpers.record(name="a", depends=["b"]),
            helpers.record(name="b", depends=["c >=1,<2"]),
            helpers.record(name="c", version="1.0", depends=["d >=1,<2"]),
            helpers.record(name="d", version="1.0"),
            helpers.record(name="e", depends=["c >=2,<3"]),
            helpers.record(name="c", version="2.0", depends=["d >=2,<3"]),
            helpers.record(name="d", version="2.0"),
        ]
        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("a", "e")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("a", "b", "c[version='>=1,<2']"),
                ("e", "c[version='>=2,<3']"),
            ],
        )

    def test_unsat_any_two_not_three(self, env):
        # can install any two of a, b and c but not all three
        env.repo_packages = [
            helpers.record(name="a", version="1.0", depends=["d >=1,<2"]),
            helpers.record(name="a", version="2.0", depends=["d >=2,<3"]),
            helpers.record(name="b", version="1.0", depends=["d >=1,<2"]),
            helpers.record(name="b", version="2.0", depends=["d >=3,<4"]),
            helpers.record(name="c", version="1.0", depends=["d >=2,<3"]),
            helpers.record(name="c", version="2.0", depends=["d >=3,<4"]),
            helpers.record(name="d", version="1.0"),
            helpers.record(name="d", version="2.0"),
            helpers.record(name="d", version="3.0"),
        ]
        # a and b can be installed
        installed = env.install("a", "b", as_specs=True)
        assert any(k.name == "a" and k.version == "1.0" for k in installed)
        assert any(k.name == "b" and k.version == "1.0" for k in installed)
        # a and c can be installed
        installed = env.install("a", "c", as_specs=True)
        assert any(k.name == "a" and k.version == "2.0" for k in installed)
        assert any(k.name == "c" and k.version == "1.0" for k in installed)
        # b and c can be installed
        installed = env.install("b", "c", as_specs=True)
        assert any(k.name == "b" and k.version == "2.0" for k in installed)
        assert any(k.name == "c" and k.version == "2.0" for k in installed)
        # a, b and c cannot be installed
        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("a", "b", "c")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("a", "d[version='>=1,<2|>=2,<3']"),
                ("b", "d[version='>=1,<2|>=3,<4']"),
                ("c", "d[version='>=2,<3|>=3,<4']"),
            ],
        )

    def test_unsat_expand_single(self, env):
        env.repo_packages = [
            helpers.record(name="a", depends=["b", "c"]),
            helpers.record(name="b", depends=["d >=1,<2"]),
            helpers.record(name="c", depends=["d >=2,<3"]),
            helpers.record(name="d", version="1.0"),
            helpers.record(name="d", version="2.0"),
        ]
        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("a")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("b", "d[version='>=1,<2']"),
                ("c", "d[version='>=2,<3']"),
            ],
        )

    def test_unsat_missing_dep(self, env):
        env.repo_packages = [
            helpers.record(name="a", depends=["b", "c"]),
            helpers.record(name="b", depends=["c >=2,<3"]),
            helpers.record(name="c", version="1.0"),
        ]
        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("a", "b")
        self.assert_unsatisfiable(
            exc_info,
            [
                ("a", "b"),
                ("b",),
            ],
        )

    def test_nonexistent(self, env):
        with pytest.raises((ResolvePackageNotFound, PackagesNotFoundError)):
            env.install("notarealpackage 2.0*")
        with pytest.raises((ResolvePackageNotFound, PackagesNotFoundError)):
            env.install("numpy 1.5")

    def test_timestamps_and_deps(self, env):
        env.repo_packages = index_packages(1) + [
            helpers.record(
                name="mypackage",
                version="1.0",
                build="hash12_0",
                timestamp=1,
                depends=["libpng 1.2.*"],
            ),
            helpers.record(
                name="mypackage",
                version="1.0",
                build="hash15_0",
                timestamp=0,
                depends=["libpng 1.5.*"],
            ),
        ]
        # libpng 1.2
        records_12 = env.install("libpng 1.2.*", "mypackage")
        assert "test::libpng-1.2.50-0" in records_12
        assert "test::mypackage-1.0-hash12_0" in records_12
        # libpng 1.5
        records_15 = env.install("libpng 1.5.*", "mypackage")
        assert "test::libpng-1.5.13-1" in records_15
        assert "test::mypackage-1.0-hash15_0" in records_15
        # this is testing that previously installed reqs are not disrupted
        # by newer timestamps. regression test of sorts for
        #  https://github.com/conda/conda/issues/6271
        assert (
            env.install("mypackage", *env.install("libpng 1.2.*", as_specs=True))
            == records_12
        )
        assert (
            env.install("mypackage", *env.install("libpng 1.5.*", as_specs=True))
            == records_15
        )
        # unspecified python version should maximize libpng (v1.5),
        # even though it has a lower timestamp
        assert env.install("mypackage") == records_15

    def test_nonexistent_deps(self, env):
        env.repo_packages = index_packages(1) + [
            helpers.record(
                name="mypackage",
                version="1.0",
                depends=["nose", "python 3.3*", "notarealpackage 2.0*"],
            ),
            helpers.record(
                name="mypackage",
                version="1.1",
                depends=["nose", "python 3.3*"],
            ),
            helpers.record(
                name="anotherpackage",
                version="1.0",
                depends=["nose", "mypackage 1.1"],
            ),
            helpers.record(
                name="anotherpackage",
                version="2.0",
                depends=["nose", "mypackage"],
            ),
        ]
        # XXX: missing find_matches and reduced_index
        assert env.install("mypackage") == {
            "test::mypackage-1.1-0",
            "test::nose-1.3.0-py33_0",
            "test::openssl-1.0.1c-0",
            "test::python-3.3.2-0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
            "test::distribute-0.6.36-py33_1",
            "test::pip-1.3.1-py33_1",
        }
        assert env.install("anotherpackage 1.0") == {
            "test::anotherpackage-1.0-0",
            "test::mypackage-1.1-0",
            "test::nose-1.3.0-py33_0",
            "test::openssl-1.0.1c-0",
            "test::python-3.3.2-0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
            "test::distribute-0.6.36-py33_1",
            "test::pip-1.3.1-py33_1",
        }
        assert env.install("anotherpackage") == {
            "test::anotherpackage-2.0-0",
            "test::mypackage-1.1-0",
            "test::nose-1.3.0-py33_0",
            "test::openssl-1.0.1c-0",
            "test::python-3.3.2-0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
            "test::distribute-0.6.36-py33_1",
            "test::pip-1.3.1-py33_1",
        }

        # This time, the latest version is messed up
        env.repo_packages = index_packages(1) + [
            helpers.record(
                name="mypackage",
                version="1.0",
                depends=["nose", "python 3.3*"],
            ),
            helpers.record(
                name="mypackage",
                version="1.1",
                depends=["nose", "python 3.3*", "notarealpackage 2.0*"],
            ),
            helpers.record(
                name="anotherpackage",
                version="1.0",
                depends=["nose", "mypackage 1.0"],
            ),
            helpers.record(
                name="anotherpackage",
                version="2.0",
                depends=["nose", "mypackage"],
            ),
        ]
        # XXX: missing find_matches and reduced_index
        assert env.install("mypackage") == {
            "test::mypackage-1.0-0",
            "test::nose-1.3.0-py33_0",
            "test::openssl-1.0.1c-0",
            "test::python-3.3.2-0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
            "test::distribute-0.6.36-py33_1",
            "test::pip-1.3.1-py33_1",
        }
        # TODO: We need UnsatisfiableError here because mamba does not
        # have more granular exceptions yet.
        with pytest.raises((ResolvePackageNotFound, UnsatisfiableError)):
            env.install("mypackage 1.1")
        assert env.install("anotherpackage 1.0") == {
            "test::anotherpackage-1.0-0",
            "test::mypackage-1.0-0",
            "test::nose-1.3.0-py33_0",
            "test::openssl-1.0.1c-0",
            "test::python-3.3.2-0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
            "test::distribute-0.6.36-py33_1",
            "test::pip-1.3.1-py33_1",
        }

        # If recursive checking is working correctly, this will give
        # anotherpackage 2.0, not anotherpackage 1.0
        assert env.install("anotherpackage") == {
            "test::anotherpackage-2.0-0",
            "test::mypackage-1.0-0",
            "test::nose-1.3.0-py33_0",
            "test::openssl-1.0.1c-0",
            "test::python-3.3.2-0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
            "test::distribute-0.6.36-py33_1",
            "test::pip-1.3.1-py33_1",
        }

    def test_install_package_with_feature(self, env):
        env.repo_packages = index_packages(1) + [
            helpers.record(
                name="mypackage",
                version="1.0",
                depends=["python 3.3*"],
                features="feature",
            ),
            helpers.record(
                name="feature",
                version="1.0",
                depends=["python 3.3*"],
                track_features="feature",
            ),
        ]
        # should not raise
        env.install("mypackage", "feature 1.0")

    def test_unintentional_feature_downgrade(self, env):
        # See https://github.com/conda/conda/issues/6765
        # With the bug in place, this bad build of scipy
        # will be selected for install instead of a later
        # build of scipy 0.11.0.
        good_rec_match = MatchSpec("channel-1::scipy==0.11.0=np17py33_3")
        good_rec = next(
            prec for prec in index_packages(1) if good_rec_match.match(prec)
        )
        bad_deps = tuple(d for d in good_rec.depends if not d.startswith("numpy"))
        bad_rec = PackageRecord.from_objects(
            good_rec,
            channel="test",
            build=good_rec.build.replace("_3", "_x0"),
            build_number=0,
            depends=bad_deps,
            fn=good_rec.fn.replace("_3", "_x0"),
            url=good_rec.url.replace("_3", "_x0"),
        )

        env.repo_packages = index_packages(1) + [bad_rec]
        records = env.install("scipy 0.11.0")
        assert "test::scipy-0.11.0-np17py33_x0" not in records
        assert "test::scipy-0.11.0-np17py33_3" in records

    def test_circular_dependencies(self, env):
        env.repo_packages = index_packages(1) + [
            helpers.record(
                name="package1",
                depends=["package2"],
            ),
            helpers.record(
                name="package2",
                depends=["package1"],
            ),
        ]
        assert (
            env.install("package1", "package2")
            == env.install("package1")
            == env.install("package2")
        )

    def test_irrational_version(self, env):
        env.repo_packages = index_packages(1)
        assert env.install("pytz 2012d", "python 3*") == {
            "test::distribute-0.6.36-py33_1",
            "test::openssl-1.0.1c-0",
            "test::pip-1.3.1-py33_1",
            "test::python-3.3.2-0",
            "test::pytz-2012d-py33_0",
            "test::readline-6.2-0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
        }

    def test_no_features(self, env):
        env.repo_packages = index_packages(1)

        assert env.install("python 2.6*", "numpy 1.6*", "scipy 0.11*") == {
            "test::distribute-0.6.36-py26_1",
            "test::numpy-1.6.2-py26_4",
            "test::openssl-1.0.1c-0",
            "test::pip-1.3.1-py26_1",
            "test::python-2.6.8-6",
            "test::readline-6.2-0",
            "test::scipy-0.11.0-np16py26_3",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
        }
        assert env.install(
            "python 2.6*", "numpy 1.6*", "scipy 0.11*", MatchSpec(track_features="mkl")
        ) == {
            "test::distribute-0.6.36-py26_1",
            "test::mkl-rt-11.0-p0",
            "test::numpy-1.6.2-py26_p4",
            "test::openssl-1.0.1c-0",
            "test::pip-1.3.1-py26_1",
            "test::python-2.6.8-6",
            "test::readline-6.2-0",
            "test::scipy-0.11.0-np16py26_p3",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
        }

        env.repo_packages += [
            helpers.record(
                name="pandas",
                version="0.12.0",
                build="np16py27_0",
                depends=[
                    "dateutil",
                    "numpy 1.6*",
                    "python 2.7*",
                    "pytz",
                ],
            ),
            helpers.record(
                name="numpy",
                version="1.6.2",
                build="py27_p5",
                build_number=0,
                depends=[
                    "mkl-rt 11.0",
                    "python 2.7",
                ],
                features="mkl",
            ),
        ]
        assert env.install("pandas 0.12.0 np16py27_0", "python 2.7*") == {
            "test::dateutil-2.1-py27_1",
            "test::distribute-0.6.36-py27_1",
            "test::numpy-1.6.2-py27_4",
            "test::openssl-1.0.1c-0",
            "test::pandas-0.12.0-np16py27_0",
            "test::pip-1.3.1-py27_1",
            "test::python-2.7.5-0",
            "test::pytz-2013b-py27_0",
            "test::readline-6.2-0",
            "test::six-1.3.0-py27_0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
        }
        assert env.install(
            "pandas 0.12.0 np16py27_0", "python 2.7*", MatchSpec(track_features="mkl")
        ) == {
            "test::dateutil-2.1-py27_1",
            "test::distribute-0.6.36-py27_1",
            "test::mkl-rt-11.0-p0",
            "test::numpy-1.6.2-py27_p4",
            "test::openssl-1.0.1c-0",
            "test::pandas-0.12.0-np16py27_0",
            "test::pip-1.3.1-py27_1",
            "test::python-2.7.5-0",
            "test::pytz-2013b-py27_0",
            "test::readline-6.2-0",
            "test::six-1.3.0-py27_0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
        }

    @pytest.mark.xfail(reason="CONDA_CHANNEL_PRIORITY does not seem to have any effect")
    def test_channel_priority_1(self, monkeypatch, env):
        # XXX: Test is skipped because CONDA_CHANNEL_PRIORITY does not seems to
        #      have any effect. I have also tried conda.common.io.env_var like
        #      the other tests but no luck.
        env.repo_packages = {}
        env.repo_packages["channel-A"] = []
        env.repo_packages["channel-1"] = index_packages(1)

        pandas_0 = self.find_package(
            channel="channel-1",
            name="pandas",
            version="0.10.1",
            build="np17py27_0",
        )
        env.repo_packages["channel-A"].append(pandas_0)

        # channel-1 has pandas np17py27_1, channel-A only has np17py27_0
        # when priority is set, it channel-A should take precedence and
        # np17py27_0 be installed, otherwise np17py27_1 should be installed as
        # it has a higher build version
        monkeypatch.setenv("CONDA_CHANNEL_PRIORITY", "True")
        assert "channel-A::pandas-0.11.0-np16py27_0" in env.install(
            "pandas", "python 2.7*", "numpy 1.6*"
        )
        monkeypatch.setenv("CONDA_CHANNEL_PRIORITY", "False")
        assert "channel-1::pandas-0.11.0-np16py27_1" in env.install(
            "pandas", "python 2.7*", "numpy 1.6*"
        )
        # now lets revert the channels
        env.repo_packages = dict(reversed(env.repo_packages.items()))
        monkeypatch.setenv("CONDA_CHANNEL_PRIORITY", "True")
        assert "channel-1::pandas-0.11.0-np16py27_1" in env.install(
            "pandas", "python 2.7*", "numpy 1.6*"
        )

    @pytest.mark.xfail(reason="CONDA_CHANNEL_PRIORITY does not seem to have any effect")
    def test_unsat_channel_priority(self, monkeypatch, env):
        # XXX: Test is skipped because CONDA_CHANNEL_PRIORITY does not seems to
        #      have any effect. I have also tried conda.common.io.env_var like
        #      the other tests but no luck.
        env.repo_packages = {}
        # higher priority
        env.repo_packages["channel-1"] = [
            helpers.record(
                name="a",
                version="1.0",
                depends=["c"],
            ),
            helpers.record(
                name="b",
                version="1.0",
                depends=["c >=2,<3"],
            ),
            helpers.record(
                name="c",
                version="1.0",
            ),
        ]
        # lower priority, missing c 2.0
        env.repo_packages["channel-2"] = [
            helpers.record(
                name="a",
                version="2.0",
                depends=["c"],
            ),
            helpers.record(
                name="b",
                version="2.0",
                depends=["c >=2,<3"],
            ),
            helpers.record(
                name="c",
                version="1.0",
            ),
            helpers.record(
                name="c",
                version="2.0",
            ),
        ]

        monkeypatch.setenv("CONDA_CHANNEL_PRIORITY", "True")
        records = env.install("a", "b", as_specs=True)
        # channel-1 a and b packages (1.0) installed
        assert any(k.name == "a" and k.version == "1.0" for k in records)
        assert any(k.name == "b" and k.version == "1.0" for k in records)

        monkeypatch.setenv("CONDA_CHANNEL_PRIORITY", "False")
        records = env.install("a", "b", as_specs=True)
        # no channel priority, largest version of a and b (2.0) installed
        assert any(k.name == "a" and k.version == "2.0" for k in records)
        assert any(k.name == "b" and k.version == "2.0" for k in records)

        monkeypatch.setenv("CONDA_CHANNEL_PRIORITY", "True")
        with pytest.raises(UnsatisfiableError) as exc_info:
            env.install("a", "b")
        self.assert_unsatisfiable(exc_info, [("b", "c[version='>=2,<3']")])

    @pytest.mark.xfail(
        reason="There is some weird global state making "
        "this test fail when the whole test suite is run"
    )
    def test_remove(self, env):
        env.repo_packages = index_packages(1)
        records = env.install("pandas", "python 2.7*", as_specs=True)
        assert package_string_set(records) == {
            "test::dateutil-2.1-py27_1",
            "test::distribute-0.6.36-py27_1",
            "test::numpy-1.7.1-py27_0",
            "test::openssl-1.0.1c-0",
            "test::pandas-0.11.0-np17py27_1",
            "test::pip-1.3.1-py27_1",
            "test::python-2.7.5-0",
            "test::pytz-2013b-py27_0",
            "test::readline-6.2-0",
            "test::scipy-0.12.0-np17py27_0",
            "test::six-1.3.0-py27_0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
        }

        env.installed_packages = records
        assert env.remove("pandas") == {
            "test::dateutil-2.1-py27_1",
            "test::distribute-0.6.36-py27_1",
            "test::numpy-1.7.1-py27_0",
            "test::openssl-1.0.1c-0",
            "test::pip-1.3.1-py27_1",
            "test::python-2.7.5-0",
            "test::pytz-2013b-py27_0",
            "test::readline-6.2-0",
            "test::scipy-0.12.0-np17py27_0",
            "test::six-1.3.0-py27_0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
        }
        assert env.remove("numpy") == {
            "test::dateutil-2.1-py27_1",
            "test::distribute-0.6.36-py27_1",
            "test::openssl-1.0.1c-0",
            "test::pip-1.3.1-py27_1",
            "test::python-2.7.5-0",
            "test::pytz-2013b-py27_0",
            "test::readline-6.2-0",
            "test::six-1.3.0-py27_0",
            "test::sqlite-3.7.13-0",
            "test::system-5.8-1",
            "test::tk-8.5.13-0",
            "test::zlib-1.2.7-0",
        }

    def test_surplus_features_1(self, env):
        env.repo_packages += [
            helpers.record(
                name="feature",
                track_features="feature",
            ),
            helpers.record(
                name="package1",
                features="feature",
            ),
            helpers.record(
                name="package2",
                version="1.0",
                features="feature",
                depends=["package1"],
            ),
            helpers.record(
                name="package2",
                version="2.0",
                features="feature",
            ),
        ]
        assert env.install("package2", "feature") == {
            "test::package2-2.0-0",
            "test::feature-1.0-0",
        }

    def test_surplus_features_2(self, env):
        env.repo_packages += [
            helpers.record(
                name="feature",
                track_features="feature",
            ),
            helpers.record(
                name="package1",
                features="feature",
            ),
            helpers.record(
                name="package2",
                version="1.0",
                build_number=0,
                features="feature",
                depends=["package1"],
            ),
            helpers.record(
                name="package2",
                version="1.0",
                build_number=1,
                features="feature",
            ),
        ]
        assert env.install("package2", "feature") == {
            "test::package2-1.0-0",
            "test::feature-1.0-0",
        }

    def test_get_reduced_index_broadening_with_unsatisfiable_early_dep(self, env):
        # Test that spec broadening reduction doesn't kill valid solutions
        #    In other words, the order of packages in the index should not affect the
        #    overall result of the reduced index.
        # see discussion at https://github.com/conda/conda/pull/8117#discussion_r249249815
        env.repo_packages += [
            helpers.record(
                name="a",
                version="1.0",
                # not satisfiable. This record should come first, so that its c==2
                # constraint tries to mess up the inclusion of the c record below,
                # which should be included as part of b's deps, but which is
                # broader than this dep.
                depends=["b", "c==2"],
            ),
            helpers.record(
                name="a",
                version="2.0",
                depends=["b"],
            ),
            helpers.record(
                name="b",
                depends=["c"],
            ),
            helpers.record(
                name="c",
            ),
        ]
        assert env.install("a") == {
            "test::a-2.0-0",
            "test::b-1.0-0",
            "test::c-1.0-0",
        }

    def test_get_reduced_index_broadening_preferred_solution(self, env):
        # test that order of index reduction does not eliminate what should be a preferred solution
        #    https://github.com/conda/conda/pull/8117#discussion_r249216068
        env.repo_packages += [
            helpers.record(
                name="top",
                version="1.0",
                # this is the first processed record, and imposes a broadening constraint on bottom
                #    if things are overly restricted, we'll end up with bottom 1.5 in our solution
                #    instead of the preferred (latest) 2.5
                depends=["middle", "bottom==1.5"],
            ),
            helpers.record(
                name="top",
                version="2.0",
                depends=["middle"],
            ),
            helpers.record(
                name="middle",
                depends=["bottom"],
            ),
            helpers.record(
                name="bottom",
                version="1.5",
            ),
            helpers.record(
                name="bottom",
                version="2.5",
            ),
        ]
        for record in env.install("top", as_specs=True):
            if record.name == "top":
                assert (
                    record.version == "2.0"
                ), f"top version should be 2.0, but is {record.version}"
            elif record.name == "bottom":
                assert (
                    record.version == "2.5"
                ), f"bottom version should be 2.5, but is {record.version}"

    def test_arch_preferred_over_noarch_when_otherwise_equal(self, env):
        env.repo_packages += [
            helpers.record(
                name="package1",
                subdir="noarch",
            ),
            helpers.record(
                name="package1",
            ),
        ]
        records = env.install("package1", as_specs=True)
        assert len(records) == 1
        assert records[0].subdir == context.subdir

    def test_noarch_preferred_over_arch_when_version_greater(self, env):
        env.repo_packages += [
            helpers.record(
                name="package1",
                version="2.0",
                subdir="noarch",
            ),
            helpers.record(
                name="package1",
                version="1.0",
            ),
        ]
        records = env.install("package1", as_specs=True)
        assert len(records) == 1
        assert records[0].subdir == "noarch"

    def test_noarch_preferred_over_arch_when_version_greater_dep(self, env):
        env.repo_packages += [
            helpers.record(
                name="package1",
                version="1.0",
            ),
            helpers.record(
                name="package1",
                version="2.0",
                subdir="noarch",
            ),
            helpers.record(
                name="package2",
                depends=["package1"],
            ),
        ]
        records = env.install("package2", as_specs=True)
        package1 = self.find_package_in_list(records, name="package1")
        assert package1.subdir == "noarch"

    def test_noarch_preferred_over_arch_when_build_greater(self, env):
        env.repo_packages += [
            helpers.record(
                name="package1",
                build_number=0,
            ),
            helpers.record(
                name="package1",
                build_number=1,
                subdir="noarch",
            ),
        ]
        records = env.install("package1", as_specs=True)
        assert len(records) == 1
        assert records[0].subdir == "noarch"

    def test_noarch_preferred_over_arch_when_build_greater_dep(self, env):
        env.repo_packages += [
            helpers.record(
                name="package1",
                build_number=0,
            ),
            helpers.record(
                name="package1",
                build_number=1,
                subdir="noarch",
            ),
            helpers.record(
                name="package2",
                depends=["package1"],
            ),
        ]
        records = env.install("package2", as_specs=True)
        package1 = self.find_package_in_list(records, name="package1")
        assert package1.subdir == "noarch"


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Extends unittest.TestCase to include select pytest fixtures."""

import unittest

import pytest


class BaseTestCase(unittest.TestCase):
    fixture_names = ("tmpdir",)

    @pytest.fixture(autouse=True)
    def auto_injector_fixture(self, request):
        names = self.fixture_names
        for name in names:
            setattr(self, name, request.getfixturevalue(name))


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Collection of helper functions used in conda tests."""

import json
import os
from contextlib import contextmanager
from functools import lru_cache
from os.path import abspath, dirname, join
from pathlib import Path
from tempfile import gettempdir, mkdtemp
from unittest.mock import patch
from uuid import uuid4

import pytest

from ..base.context import conda_tests_ctxt_mgmt_def_pol, context
from ..common.io import captured as common_io_captured
from ..common.io import env_var
from ..core.prefix_data import PrefixData
from ..core.subdir_data import SubdirData, make_feature_record
from ..deprecations import deprecated
from ..gateways.disk.delete import rm_rf
from ..gateways.disk.read import lexists
from ..history import History
from ..models.channel import Channel
from ..models.records import PackageRecord, PrefixRecord
from ..resolve import Resolve

# The default value will only work if we have installed conda in development mode!
TEST_DATA_DIR = os.environ.get(
    "CONDA_TEST_DATA_DIR", abspath(join(dirname(__file__), "..", "..", "tests", "data"))
)
CHANNEL_DIR = CHANNEL_DIR_V1 = abspath(join(TEST_DATA_DIR, "conda_format_repo"))
CHANNEL_DIR_V2 = abspath(join(TEST_DATA_DIR, "base_url_channel"))
EXPORTED_CHANNELS_DIR = mkdtemp(suffix="-test-conda-channels")


expected_error_prefix = "Using Anaconda Cloud api site https://api.anaconda.org"


def strip_expected(stderr):
    if expected_error_prefix and stderr.startswith(expected_error_prefix):
        stderr = stderr[len(expected_error_prefix) :].lstrip()  # noqa
    return stderr


def raises(exception, func, string=None):
    try:
        a = func()
    except exception as e:
        if string:
            assert string in e.args[0]
        print(e)
        return True
    raise Exception(f"did not raise, gave {a}")


@contextmanager
def captured(disallow_stderr=True):
    # same as common.io.captured but raises Exception if unexpected output was written to stderr
    try:
        with common_io_captured() as c:
            yield c
    finally:
        c.stderr = strip_expected(c.stderr)
        if disallow_stderr and c.stderr:
            raise Exception(f"Got stderr output: {c.stderr}")


@deprecated(
    "24.3",
    "24.9",
    addendum="Use `mocker.patch('conda.base.context.Context.active_prefix')` instead.",
)
@contextmanager
def set_active_prefix(prefix: str) -> None:
    old_prefix = os.environ["CONDA_PREFIX"]

    try:
        os.environ["CONDA_PREFIX"] = prefix
        yield
    finally:
        os.environ["CONDA_PREFIX"] = old_prefix


def assert_equals(a, b, output=""):
    output = f"{a.lower()!r} != {b.lower()!r}" + "\n\n" + output
    assert a.lower() == b.lower(), output


def assert_not_in(a, b, output=""):
    assert (
        a.lower() not in b.lower()
    ), f"{output} {a.lower()!r} should not be found in {b.lower()!r}"


def assert_in(a, b, output=""):
    assert (
        a.lower() in b.lower()
    ), f"{output} {a.lower()!r} cannot be found in {b.lower()!r}"


def add_subdir(dist_string):
    channel_str, package_str = dist_string.split("::")
    channel_str = channel_str + "/" + context.subdir
    return "::".join([channel_str, package_str])


def add_subdir_to_iter(iterable):
    if isinstance(iterable, dict):
        return {add_subdir(k): v for k, v in iterable.items()}
    elif isinstance(iterable, list):
        return list(map(add_subdir, iterable))
    elif isinstance(iterable, set):
        return set(map(add_subdir, iterable))
    elif isinstance(iterable, tuple):
        return tuple(map(add_subdir, iterable))
    else:
        raise Exception("Unable to add subdir to object of unknown type.")


@contextmanager
def tempdir():
    tempdirdir = gettempdir()
    dirname = str(uuid4())[:8]
    prefix = join(tempdirdir, dirname)
    try:
        os.makedirs(prefix)
        yield prefix
    finally:
        if lexists(prefix):
            rm_rf(prefix)


def supplement_index_with_repodata(index, repodata, channel, priority):
    repodata_info = repodata["info"]
    arch = repodata_info.get("arch")
    platform = repodata_info.get("platform")
    subdir = repodata_info.get("subdir")
    if not subdir:
        subdir = "{}-{}".format(repodata_info["platform"], repodata_info["arch"])
    auth = channel.auth
    for fn, info in repodata["packages"].items():
        rec = PackageRecord.from_objects(
            info,
            fn=fn,
            arch=arch,
            platform=platform,
            channel=channel,
            subdir=subdir,
            # schannel=schannel,
            priority=priority,
            # url=join_url(channel_url, fn),
            auth=auth,
        )
        index[rec] = rec


def add_feature_records_legacy(index):
    all_features = set()
    for rec in index.values():
        if rec.track_features:
            all_features.update(rec.track_features)

    for feature_name in all_features:
        rec = make_feature_record(feature_name)
        index[rec] = rec


def _export_subdir_data_to_repodata(subdir_data: SubdirData):
    """
    This function is only temporary and meant to patch wrong / undesirable
    testing behaviour. It should end up being replaced with the new class-based,
    backend-agnostic solver tests.
    """
    state = subdir_data._internal_state
    subdir = subdir_data.channel.subdir
    packages = {}
    packages_conda = {}
    for pkg in subdir_data.iter_records():
        if pkg.timestamp:
            # ensure timestamp is dumped as int in milliseconds
            # (pkg.timestamp is a kept as a float in seconds)
            pkg.__fields__["timestamp"]._in_dump = True
        data = pkg.dump()
        if subdir == "noarch" and getattr(pkg, "noarch", None):
            data["subdir"] = "noarch"
            data["platform"] = data["arch"] = None
        if "features" in data:
            # Features are deprecated, so they are not implemented
            # in modern solvers like mamba. Mamba does implement
            # track_features minimization, so we are exposing the
            # features as track_features, which seems to make the
            # tests pass
            data["track_features"] = data["features"]
            del data["features"]
        if pkg.fn.endswith(".conda"):
            packages_conda[pkg.fn] = data
        else:
            packages[pkg.fn] = data
    return {
        "_cache_control": state["_cache_control"],
        "_etag": state["_etag"],
        "_mod": state["_mod"],
        "_url": state["_url"],
        "_add_pip": state["_add_pip"],
        "info": {
            "subdir": subdir,
        },
        "packages": packages,
        "packages.conda": packages_conda,
    }


def _sync_channel_to_disk(subdir_data: SubdirData):
    """
    This function is only temporary and meant to patch wrong / undesirable
    testing behaviour. It should end up being replaced with the new class-based,
    backend-agnostic solver tests.
    """
    base = Path(EXPORTED_CHANNELS_DIR) / subdir_data.channel.name
    subdir_path = base / subdir_data.channel.subdir
    subdir_path.mkdir(parents=True, exist_ok=True)
    with open(subdir_path / "repodata.json", "w") as f:
        json.dump(
            _export_subdir_data_to_repodata(subdir_data), f, indent=2, sort_keys=True
        )
        f.flush()
        os.fsync(f.fileno())


def _alias_canonical_channel_name_cache_to_file_prefixed(name, subdir_data=None):
    """
    This function is only temporary and meant to patch wrong / undesirable
    testing behaviour. It should end up being replaced with the new class-based,
    backend-agnostic solver tests.
    """
    # export repodata state to disk for other solvers to test
    if subdir_data is None:
        cache_key = Channel(name).url(with_credentials=True), "repodata.json"
        subdir_data = SubdirData._cache_.get(cache_key)
    if subdir_data:
        local_proxy_channel = Channel(f"{EXPORTED_CHANNELS_DIR}/{name}")
        SubdirData._cache_[
            (local_proxy_channel.url(with_credentials=True), "repodata.json")
        ] = subdir_data


def _patch_for_local_exports(name, subdir_data):
    """
    This function is only temporary and meant to patch wrong / undesirable
    testing behaviour. It should end up being replaced with the new class-based,
    backend-agnostic solver tests.
    """
    _alias_canonical_channel_name_cache_to_file_prefixed(name, subdir_data)

    # we need to override the modification time here so the
    # cache hits this subdir_data object from the local copy too
    # - without this, the legacy solver will use the local dump too
    # and there's no need for that extra work
    # (check conda.core.subdir_data.SubdirDataType.__call__ for
    # details)
    _sync_channel_to_disk(subdir_data)
    subdir_data._mtime = float("inf")


def _get_index_r_base(
    json_filename_or_packages,
    channel_name,
    subdir=context.subdir,
    add_pip=False,
    merge_noarch=False,
):
    if isinstance(json_filename_or_packages, (str, os.PathLike)):
        with open(join(TEST_DATA_DIR, json_filename_or_packages)) as fi:
            all_packages = json.load(fi)
    elif isinstance(json_filename_or_packages, dict):
        all_packages = json_filename_or_packages
    else:
        raise ValueError("'json_filename_or_data' must be path-like or dict")

    if merge_noarch:
        packages = {subdir: all_packages}
    else:
        packages = {subdir: {}, "noarch": {}}
        for key, pkg in all_packages.items():
            if pkg.get("subdir") == "noarch" or pkg.get("noarch"):
                packages["noarch"][key] = pkg
            else:
                packages[subdir][key] = pkg

    subdir_datas = []
    channels = []
    for subchannel, subchannel_pkgs in packages.items():
        repodata = {
            "info": {
                "subdir": subchannel,
                "arch": context.arch_name,
                "platform": context.platform,
            },
            "packages": subchannel_pkgs,
        }

        channel = Channel(f"https://conda.anaconda.org/{channel_name}/{subchannel}")
        channels.append(channel)
        sd = SubdirData(channel)
        subdir_datas.append(sd)
        with env_var(
            "CONDA_ADD_PIP_AS_PYTHON_DEPENDENCY",
            str(add_pip).lower(),
            stack_callback=conda_tests_ctxt_mgmt_def_pol,
        ):
            sd._process_raw_repodata_str(json.dumps(repodata))
        sd._loaded = True
        SubdirData._cache_[channel.url(with_credentials=True)] = sd
        _patch_for_local_exports(channel_name, sd)

    # this is for the classic solver only, which is fine with a single collapsed index
    index = {}
    for sd in subdir_datas:
        index.update({prec: prec for prec in sd.iter_records()})
    r = Resolve(index, channels=channels)

    return index, r


# this fixture appears to introduce a test-order dependency if cached
def get_index_r_1(subdir=context.subdir, add_pip=True, merge_noarch=False):
    return _get_index_r_base(
        "index.json",
        "channel-1",
        subdir=subdir,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@lru_cache(maxsize=None)
def get_index_r_2(subdir=context.subdir, add_pip=True, merge_noarch=False):
    return _get_index_r_base(
        "index2.json",
        "channel-2",
        subdir=subdir,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@lru_cache(maxsize=None)
def get_index_r_4(subdir=context.subdir, add_pip=True, merge_noarch=False):
    return _get_index_r_base(
        "index4.json",
        "channel-4",
        subdir=subdir,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@lru_cache(maxsize=None)
def get_index_r_5(subdir=context.subdir, add_pip=False, merge_noarch=False):
    return _get_index_r_base(
        "index5.json",
        "channel-5",
        subdir=subdir,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@lru_cache(maxsize=None)
def get_index_must_unfreeze(subdir=context.subdir, add_pip=True, merge_noarch=False):
    repodata = {
        "foobar-1.0-0.tar.bz2": {
            "build": "0",
            "build_number": 0,
            "depends": ["libbar 2.0.*", "libfoo 1.0.*"],
            "md5": "11ec1194bcc56b9a53c127142a272772",
            "name": "foobar",
            "timestamp": 1562861325613,
            "version": "1.0",
        },
        "foobar-2.0-0.tar.bz2": {
            "build": "0",
            "build_number": 0,
            "depends": ["libbar 2.0.*", "libfoo 2.0.*"],
            "md5": "f8eb5a7fa1ff6dead4e360631a6cd048",
            "name": "foobar",
            "version": "2.0",
        },
        "libbar-1.0-0.tar.bz2": {
            "build": "0",
            "build_number": 0,
            "depends": [],
            "md5": "f51f4d48a541b7105b5e343704114f0f",
            "name": "libbar",
            "timestamp": 1562858881022,
            "version": "1.0",
        },
        "libbar-2.0-0.tar.bz2": {
            "build": "0",
            "build_number": 0,
            "depends": [],
            "md5": "27f4e717ed263f909074f64d9cbf935d",
            "name": "libbar",
            "timestamp": 1562858881748,
            "version": "2.0",
        },
        "libfoo-1.0-0.tar.bz2": {
            "build": "0",
            "build_number": 0,
            "depends": [],
            "md5": "ad7c088566ffe2389958daedf8ff312c",
            "name": "libfoo",
            "timestamp": 1562858763881,
            "version": "1.0",
        },
        "libfoo-2.0-0.tar.bz2": {
            "build": "0",
            "build_number": 0,
            "depends": [],
            "md5": "daf7af7086d8f22be49ae11bdc41f332",
            "name": "libfoo",
            "timestamp": 1562858836924,
            "version": "2.0",
        },
        "qux-1.0-0.tar.bz2": {
            "build": "0",
            "build_number": 0,
            "depends": ["libbar 2.0.*", "libfoo 1.0.*"],
            "md5": "18604cbe4f789fe853232eef4babd4f9",
            "name": "qux",
            "timestamp": 1562861393808,
            "version": "1.0",
        },
        "qux-2.0-0.tar.bz2": {
            "build": "0",
            "build_number": 0,
            "depends": ["libbar 1.0.*", "libfoo 2.0.*"],
            "md5": "892aa4b9ec64b67045a46866ef1ea488",
            "name": "qux",
            "timestamp": 1562861394828,
            "version": "2.0",
        },
    }
    _get_index_r_base(
        repodata,
        "channel-freeze",
        subdir=subdir,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


# Do not memoize this get_index to allow different CUDA versions to be detected
def get_index_cuda(subdir=context.subdir, add_pip=True, merge_noarch=False):
    return _get_index_r_base(
        "index.json",
        "channel-1",
        subdir=subdir,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


def record(
    name="a",
    version="1.0",
    depends=None,
    build="0",
    build_number=0,
    timestamp=0,
    channel=None,
    **kwargs,
):
    return PackageRecord(
        name=name,
        version=version,
        depends=depends or [],
        build=build,
        build_number=build_number,
        timestamp=timestamp,
        channel=channel,
        **kwargs,
    )


def _get_solver_base(
    channel_id,
    tmpdir,
    specs_to_add=(),
    specs_to_remove=(),
    prefix_records=(),
    history_specs=(),
    add_pip=False,
    merge_noarch=False,
):
    tmpdir = tmpdir.strpath
    pd = PrefixData(tmpdir)
    pd._PrefixData__prefix_records = {
        rec.name: PrefixRecord.from_objects(rec) for rec in prefix_records
    }
    spec_map = {spec.name: spec for spec in history_specs}
    if channel_id == "channel-1":
        get_index_r_1(context.subdir, add_pip, merge_noarch)
        _alias_canonical_channel_name_cache_to_file_prefixed("channel-1")
        channels = (Channel(f"{EXPORTED_CHANNELS_DIR}/channel-1"),)
    elif channel_id == "channel-2":
        get_index_r_2(context.subdir, add_pip, merge_noarch)
        _alias_canonical_channel_name_cache_to_file_prefixed("channel-2")
        channels = (Channel(f"{EXPORTED_CHANNELS_DIR}/channel-2"),)
    elif channel_id == "channel-4":
        get_index_r_4(context.subdir, add_pip, merge_noarch)
        _alias_canonical_channel_name_cache_to_file_prefixed("channel-4")
        channels = (Channel(f"{EXPORTED_CHANNELS_DIR}/channel-4"),)
    elif channel_id == "channel-5":
        get_index_r_5(context.subdir, add_pip, merge_noarch)
        _alias_canonical_channel_name_cache_to_file_prefixed("channel-5")
        channels = (Channel(f"{EXPORTED_CHANNELS_DIR}/channel-5"),)
    elif channel_id == "aggregate-1":
        get_index_r_2(context.subdir, add_pip, merge_noarch)
        get_index_r_4(context.subdir, add_pip, merge_noarch)
        _alias_canonical_channel_name_cache_to_file_prefixed("channel-2")
        _alias_canonical_channel_name_cache_to_file_prefixed("channel-4")
        channels = (
            Channel(f"{EXPORTED_CHANNELS_DIR}/channel-2"),
            Channel(f"{EXPORTED_CHANNELS_DIR}/channel-4"),
        )
    elif channel_id == "aggregate-2":
        get_index_r_2(context.subdir, add_pip, merge_noarch)
        get_index_r_4(context.subdir, add_pip, merge_noarch)
        _alias_canonical_channel_name_cache_to_file_prefixed("channel-4")
        _alias_canonical_channel_name_cache_to_file_prefixed("channel-2")
        # This is the only difference with aggregate-1: the priority
        channels = (
            Channel(f"{EXPORTED_CHANNELS_DIR}/channel-4"),
            Channel(f"{EXPORTED_CHANNELS_DIR}/channel-2"),
        )
    elif channel_id == "must-unfreeze":
        get_index_must_unfreeze(context.subdir, add_pip, merge_noarch)
        _alias_canonical_channel_name_cache_to_file_prefixed("channel-freeze")
        channels = (Channel(f"{EXPORTED_CHANNELS_DIR}/channel-freeze"),)
    elif channel_id == "cuda":
        get_index_cuda(context.subdir, add_pip, merge_noarch)
        _alias_canonical_channel_name_cache_to_file_prefixed("channel-1")
        channels = (Channel(f"{EXPORTED_CHANNELS_DIR}/channel-1"),)

    subdirs = (context.subdir,) if merge_noarch else (context.subdir, "noarch")

    with patch.object(
        History, "get_requested_specs_map", return_value=spec_map
    ), env_var(
        "CONDA_ADD_PIP_AS_PYTHON_DEPENDENCY",
        str(add_pip).lower(),
        stack_callback=conda_tests_ctxt_mgmt_def_pol,
    ):
        # We need CONDA_ADD_PIP_AS_PYTHON_DEPENDENCY=false here again (it's also in
        # get_index_r_*) to cover solver logics that need to load from disk instead of
        # hitting the SubdirData cache
        yield context.plugin_manager.get_solver_backend()(
            tmpdir,
            channels,
            subdirs,
            specs_to_add=specs_to_add,
            specs_to_remove=specs_to_remove,
        )


@contextmanager
def get_solver(
    tmpdir,
    specs_to_add=(),
    specs_to_remove=(),
    prefix_records=(),
    history_specs=(),
    add_pip=False,
    merge_noarch=False,
):
    yield from _get_solver_base(
        "channel-1",
        tmpdir,
        specs_to_add=specs_to_add,
        specs_to_remove=specs_to_remove,
        prefix_records=prefix_records,
        history_specs=history_specs,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@contextmanager
def get_solver_2(
    tmpdir,
    specs_to_add=(),
    specs_to_remove=(),
    prefix_records=(),
    history_specs=(),
    add_pip=False,
    merge_noarch=False,
):
    yield from _get_solver_base(
        "channel-2",
        tmpdir,
        specs_to_add=specs_to_add,
        specs_to_remove=specs_to_remove,
        prefix_records=prefix_records,
        history_specs=history_specs,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@contextmanager
def get_solver_4(
    tmpdir,
    specs_to_add=(),
    specs_to_remove=(),
    prefix_records=(),
    history_specs=(),
    add_pip=False,
    merge_noarch=False,
):
    yield from _get_solver_base(
        "channel-4",
        tmpdir,
        specs_to_add=specs_to_add,
        specs_to_remove=specs_to_remove,
        prefix_records=prefix_records,
        history_specs=history_specs,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@contextmanager
def get_solver_5(
    tmpdir,
    specs_to_add=(),
    specs_to_remove=(),
    prefix_records=(),
    history_specs=(),
    add_pip=False,
    merge_noarch=False,
):
    yield from _get_solver_base(
        "channel-5",
        tmpdir,
        specs_to_add=specs_to_add,
        specs_to_remove=specs_to_remove,
        prefix_records=prefix_records,
        history_specs=history_specs,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@contextmanager
def get_solver_aggregate_1(
    tmpdir,
    specs_to_add=(),
    specs_to_remove=(),
    prefix_records=(),
    history_specs=(),
    add_pip=False,
    merge_noarch=False,
):
    yield from _get_solver_base(
        "aggregate-1",
        tmpdir,
        specs_to_add=specs_to_add,
        specs_to_remove=specs_to_remove,
        prefix_records=prefix_records,
        history_specs=history_specs,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@contextmanager
def get_solver_aggregate_2(
    tmpdir,
    specs_to_add=(),
    specs_to_remove=(),
    prefix_records=(),
    history_specs=(),
    add_pip=False,
    merge_noarch=False,
):
    yield from _get_solver_base(
        "aggregate-2",
        tmpdir,
        specs_to_add=specs_to_add,
        specs_to_remove=specs_to_remove,
        prefix_records=prefix_records,
        history_specs=history_specs,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@contextmanager
def get_solver_must_unfreeze(
    tmpdir,
    specs_to_add=(),
    specs_to_remove=(),
    prefix_records=(),
    history_specs=(),
    add_pip=False,
    merge_noarch=False,
):
    yield from _get_solver_base(
        "must-unfreeze",
        tmpdir,
        specs_to_add=specs_to_add,
        specs_to_remove=specs_to_remove,
        prefix_records=prefix_records,
        history_specs=history_specs,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


@contextmanager
def get_solver_cuda(
    tmpdir,
    specs_to_add=(),
    specs_to_remove=(),
    prefix_records=(),
    history_specs=(),
    add_pip=False,
    merge_noarch=False,
):
    yield from _get_solver_base(
        "cuda",
        tmpdir,
        specs_to_add=specs_to_add,
        specs_to_remove=specs_to_remove,
        prefix_records=prefix_records,
        history_specs=history_specs,
        add_pip=add_pip,
        merge_noarch=merge_noarch,
    )


def convert_to_dist_str(solution):
    dist_str = []
    for prec in solution:
        # This is needed to remove the local path prefix in the
        # dist_str() calls, otherwise we cannot compare them
        canonical_name = prec.channel._Channel__canonical_name
        prec.channel._Channel__canonical_name = prec.channel.name
        dist_str.append(prec.dist_str())
        prec.channel._Channel__canonical_name = canonical_name
    return tuple(dist_str)


@pytest.fixture()
def solver_class():
    return context.plugin_manager.get_solver_backend()


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Collection of pytest fixtures used in conda tests."""

from __future__ import annotations

import os
import warnings
from typing import TYPE_CHECKING, Literal, TypeVar

import py
import pytest

from ..auxlib.ish import dals
from ..base.context import conda_tests_ctxt_mgmt_def_pol, context, reset_context
from ..common.configuration import YamlRawParameter
from ..common.io import env_vars
from ..common.serialize import yaml_round_trip_load
from ..core.subdir_data import SubdirData
from ..gateways.disk.create import TemporaryDirectory

if TYPE_CHECKING:
    from typing import Iterable

    from pytest import FixtureRequest, MonkeyPatch


@pytest.fixture(autouse=True)
def suppress_resource_warning():
    """
    Suppress `Unclosed Socket Warning`

    It seems urllib3 keeps a socket open to avoid costly recreation costs.

    xref: https://github.com/kennethreitz/requests/issues/1882
    """
    warnings.filterwarnings("ignore", category=ResourceWarning)


@pytest.fixture(scope="function")
def tmpdir(tmpdir, request):
    tmpdir = TemporaryDirectory(dir=str(tmpdir))
    request.addfinalizer(tmpdir.cleanup)
    return py.path.local(tmpdir.name)


@pytest.fixture(autouse=True)
def clear_subdir_cache():
    SubdirData.clear_cached_local_channel_data()


@pytest.fixture(scope="function")
def disable_channel_notices():
    """
    Fixture that will set "context.number_channel_notices" to 0 and then set
    it back to its original value.

    This is also a good example of how to override values in the context object.
    """
    yaml_str = dals(
        """
        number_channel_notices: 0
        """
    )
    reset_context(())
    rd = {
        "testdata": YamlRawParameter.make_raw_parameters(
            "testdata", yaml_round_trip_load(yaml_str)
        )
    }
    context._set_raw_data(rd)

    yield

    reset_context(())


@pytest.fixture(scope="function")
def reset_conda_context():
    """Resets the context object after each test function is run."""
    yield

    reset_context()


@pytest.fixture()
def temp_package_cache(tmp_path_factory):
    """
    Used to isolate package or index cache from other tests.
    """
    pkgs_dir = tmp_path_factory.mktemp("pkgs")
    with env_vars(
        {"CONDA_PKGS_DIRS": str(pkgs_dir)}, stack_callback=conda_tests_ctxt_mgmt_def_pol
    ):
        yield pkgs_dir


@pytest.fixture(
    # allow CI to set the solver backends via the CONDA_TEST_SOLVERS env var
    params=os.environ.get("CONDA_TEST_SOLVERS", "libmamba,classic").split(",")
)
def parametrized_solver_fixture(
    request: FixtureRequest,
    monkeypatch: MonkeyPatch,
) -> Iterable[Literal["libmamba", "classic"]]:
    """
    A parameterized fixture that sets the solver backend to (1) libmamba
    and (2) classic for each test. It's using autouse=True, so only import it in
    modules that actually need it.

    Note that skips and xfails need to be done _inside_ the test body.
    Decorators can't be used because they are evaluated before the
    fixture has done its work!

    So, instead of:

        @pytest.mark.skipif(context.solver == "libmamba", reason="...")
        def test_foo():
            ...

    Do:

        def test_foo():
            if context.solver == "libmamba":
                pytest.skip("...")
            ...
    """
    yield from _solver_helper(request, monkeypatch, request.param)


@pytest.fixture
def solver_classic(
    request: FixtureRequest,
    monkeypatch: MonkeyPatch,
) -> Iterable[Literal["classic"]]:
    yield from _solver_helper(request, monkeypatch, "classic")


@pytest.fixture
def solver_libmamba(
    request: FixtureRequest,
    monkeypatch: MonkeyPatch,
) -> Iterable[Literal["libmamba"]]:
    yield from _solver_helper(request, monkeypatch, "libmamba")


Solver = TypeVar("Solver", Literal["libmamba"], Literal["classic"])


def _solver_helper(
    request: FixtureRequest,
    monkeypatch: MonkeyPatch,
    solver: Solver,
) -> Iterable[Solver]:
    # clear cached solver backends before & after each test
    context.plugin_manager.get_cached_solver_backend.cache_clear()
    request.addfinalizer(context.plugin_manager.get_cached_solver_backend.cache_clear)

    monkeypatch.setenv("CONDA_SOLVER", solver)
    reset_context()
    assert context.solver == solver

    yield solver


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
# Attempt to move any conda entries in PATH to the front of it.
# IDEs have their own ideas about how PATH should be managed and
# they do dumb stuff like add /usr/bin to the front of it
# meaning conda takes a submissive role and the wrong stuff
# runs (when other conda prefixes get activated they replace
# the wrongly placed entries with newer wrongly placed entries).
#
# Note, there's still condabin to worry about here, and also should
# we not remove all traces of conda instead of just this fixup?
# Ideally we'd have two modes, 'removed' and 'fixed'. I have seen
# condabin come from an entirely different installation than
# CONDA_PREFIX too in some instances and that really needs fixing.
from __future__ import annotations

import json
import os
import sys
import uuid
import warnings
from contextlib import contextmanager, nullcontext
from dataclasses import dataclass
from logging import getLogger
from os.path import join
from pathlib import Path
from shutil import copyfile
from typing import TYPE_CHECKING, overload

import pytest

from ..auxlib.entity import EntityEncoder
from ..base.constants import PACKAGE_CACHE_MAGIC_FILE
from ..base.context import context, reset_context
from ..cli.main import main_subshell
from ..common.compat import on_win
from ..common.url import path_to_url
from ..core.package_cache_data import PackageCacheData
from ..deprecations import deprecated
from ..exceptions import CondaExitZero
from ..models.records import PackageRecord

if TYPE_CHECKING:
    from typing import Iterator

    from pytest import CaptureFixture, ExceptionInfo, MonkeyPatch
    from pytest_mock import MockerFixture

log = getLogger(__name__)


@deprecated(
    "24.9",
    "25.3",
    addendum="It don't matter which environment the test suite is run from.",
)
def conda_ensure_sys_python_is_base_env_python():
    # Exit if we try to run tests from a non-base env. The tests end up installing
    # menuinst into the env they are called with and that breaks non-base env activation
    # as it emits a message to stderr:
    # WARNING menuinst_win32:<module>(157): menuinst called from non-root env
    # C:\opt\conda\envs\py27
    # So lets just sys.exit on that.

    if "CONDA_PYTHON_EXE" in os.environ:
        if (
            Path(os.environ["CONDA_PYTHON_EXE"]).resolve()
            != Path(sys.executable).resolve()
        ):
            warnings.warn(
                "ERROR :: Running tests from a non-base Python interpreter. "
                " Tests requires installing menuinst and that causes stderr "
                " output when activated.\n"
                f"- CONDA_PYTHON_EXE={os.environ['CONDA_PYTHON_EXE']}\n"
                f"- sys.executable={sys.executable}"
            )

            # menuinst only really matters on windows
            if on_win:
                sys.exit(-1)


def conda_move_to_front_of_PATH():
    if "CONDA_PREFIX" in os.environ:
        from ..activate import CmdExeActivator, PosixActivator

        if os.name == "nt":
            activator_cls = CmdExeActivator
        else:
            activator_cls = PosixActivator
        activator = activator_cls()
        # But why not just use _replace_prefix_in_path? => because moving
        # the entries to the front of PATH is the goal here, not swapping
        # x for x (which would be pointless anyway).
        p = None
        # It might be nice to have a parameterised fixture with choices of:
        # 'System default PATH',
        # 'IDE default PATH',
        # 'Fully activated conda',
        # 'PATHly activated conda'
        # This will do for now => Note, if you have conda activated multiple
        # times it could mask some test failures but _remove_prefix_from_path
        # cannot be used multiple times; it will only remove *one* conda
        # prefix from the *original* value of PATH, calling it N times will
        # just return the same value every time, even if you update PATH.
        p = activator._remove_prefix_from_path(os.environ["CONDA_PREFIX"])

        # Replace any non sys.prefix condabin with sys.prefix condabin
        new_p = []
        found_condabin = False
        for pe in p:
            if pe.endswith("condabin"):
                if not found_condabin:
                    found_condabin = True
                    if join(sys.prefix, "condabin") != pe:
                        condabin_path = join(sys.prefix, "condabin")
                        print(f"Incorrect condabin, swapping {pe} to {condabin_path}")
                        new_p.append(condabin_path)
                    else:
                        new_p.append(pe)
            else:
                new_p.append(pe)

        os.environ["PATH"] = os.pathsep.join(new_p)
        activator = activator_cls()
        p = activator._add_prefix_to_path(os.environ["CONDA_PREFIX"])
        os.environ["PATH"] = os.pathsep.join(p)


@dataclass
class CondaCLIFixture:
    capsys: CaptureFixture

    @overload
    def __call__(
        self,
        *argv: str | os.PathLike | Path,
        raises: type[Exception] | tuple[type[Exception], ...],
    ) -> tuple[str, str, ExceptionInfo]: ...

    @overload
    def __call__(self, *argv: str | os.PathLike | Path) -> tuple[str, str, int]: ...

    def __call__(
        self,
        *argv: str | os.PathLike | Path,
        raises: type[Exception] | tuple[type[Exception], ...] | None = None,
    ) -> tuple[str, str, int | ExceptionInfo]:
        """Test conda CLI. Mimic what is done in `conda.cli.main.main`.

        `conda ...` == `conda_cli(...)`

        :param argv: Arguments to parse.
        :param raises: Expected exception to intercept. If provided, the raised exception
            will be returned instead of exit code (see pytest.raises and pytest.ExceptionInfo).
        :return: Command results (stdout, stderr, exit code or pytest.ExceptionInfo).
        """
        # clear output
        self.capsys.readouterr()

        # ensure arguments are string
        argv = tuple(map(str, argv))

        # run command
        code = None
        with pytest.raises(raises) if raises else nullcontext() as exception:
            code = main_subshell(*argv)
        # capture output
        out, err = self.capsys.readouterr()

        # restore to prior state
        reset_context()

        return out, err, exception if raises else code


@pytest.fixture
def conda_cli(capsys: CaptureFixture) -> CondaCLIFixture:
    """Fixture returning CondaCLIFixture instance."""
    yield CondaCLIFixture(capsys)


@dataclass
class PathFactoryFixture:
    tmp_path: Path

    def __call__(
        self,
        name: str | None = None,
        prefix: str | None = None,
        suffix: str | None = None,
    ) -> Path:
        """Unique, non-existent path factory.

        Extends pytest's `tmp_path` fixture with a new unique, non-existent path for usage in cases
        where we need a temporary path that doesn't exist yet.

        :param name: Path name to append to `tmp_path`
        :param prefix: Prefix to prepend to unique name generated
        :param suffix: Suffix to append to unique name generated
        :return: A new unique path
        """
        prefix = prefix or ""
        name = name or uuid.uuid4().hex
        suffix = suffix or ""
        return self.tmp_path / (prefix + name + suffix)


@pytest.fixture
def path_factory(tmp_path: Path) -> PathFactoryFixture:
    """Fixture returning PathFactoryFixture instance."""
    yield PathFactoryFixture(tmp_path)


@dataclass
class TmpEnvFixture:
    path_factory: PathFactoryFixture
    conda_cli: CondaCLIFixture

    @contextmanager
    def __call__(
        self,
        *packages: str,
        prefix: str | os.PathLike | None = None,
    ) -> Iterator[Path]:
        """Generate a conda environment with the provided packages.

        :param packages: The packages to install into environment
        :param prefix: The prefix at which to install the conda environment
        :return: The conda environment's prefix
        """
        prefix = Path(prefix or self.path_factory())

        self.conda_cli("create", "--prefix", prefix, *packages, "--yes", "--quiet")
        yield prefix

        # no need to remove prefix since it is in a temporary directory


@pytest.fixture
def tmp_env(
    path_factory: PathFactoryFixture,
    conda_cli: CondaCLIFixture,
) -> TmpEnvFixture:
    """Fixture returning TmpEnvFixture instance."""
    yield TmpEnvFixture(path_factory, conda_cli)


@dataclass
class TmpChannelFixture:
    path_factory: PathFactoryFixture
    conda_cli: CondaCLIFixture

    @contextmanager
    def __call__(self, *packages: str) -> Iterator[tuple[Path, str]]:
        # download packages
        self.conda_cli(
            "create",
            f"--prefix={self.path_factory()}",
            *packages,
            "--yes",
            "--quiet",
            "--download-only",
            raises=CondaExitZero,
        )

        pkgs_dir = Path(PackageCacheData.first_writable().pkgs_dir)
        pkgs_cache = PackageCacheData(pkgs_dir)

        channel = self.path_factory()
        subdir = channel / context.subdir
        subdir.mkdir(parents=True)
        noarch = channel / "noarch"
        noarch.mkdir(parents=True)

        repodata = {"info": {}, "packages": {}}
        for package in packages:
            for pkg_data in pkgs_cache.query(package):
                fname = pkg_data["fn"]

                copyfile(pkgs_dir / fname, subdir / fname)

                repodata["packages"][fname] = PackageRecord(
                    **{
                        field: value
                        for field, value in pkg_data.dump().items()
                        if field not in ("url", "channel", "schannel")
                    }
                )

        (subdir / "repodata.json").write_text(json.dumps(repodata, cls=EntityEncoder))
        (noarch / "repodata.json").write_text(json.dumps({}, cls=EntityEncoder))

        for package in packages:
            assert any(PackageCacheData.query_all(package))

        yield channel, path_to_url(str(channel))


@pytest.fixture
def tmp_channel(
    path_factory: PathFactoryFixture,
    conda_cli: CondaCLIFixture,
) -> TmpChannelFixture:
    """Fixture returning TmpChannelFixture instance."""
    yield TmpChannelFixture(path_factory, conda_cli)


@pytest.fixture(name="monkeypatch")
def context_aware_monkeypatch(monkeypatch: MonkeyPatch) -> MonkeyPatch:
    """A monkeypatch fixture that resets context after each test"""
    yield monkeypatch

    # reset context if any CONDA_ variables were set/unset
    if conda_vars := [
        name
        for obj, name, _ in monkeypatch._setitem
        if obj is os.environ and name.startswith("CONDA_")
    ]:
        log.debug(f"monkeypatch cleanup: undo & reset context: {', '.join(conda_vars)}")
        monkeypatch.undo()
        # reload context without search paths
        reset_context([])


@pytest.fixture
def tmp_pkgs_dir(path_factory: PathFactoryFixture, mocker: MockerFixture) -> Path:
    pkgs_dir = path_factory() / "pkgs"
    pkgs_dir.mkdir(parents=True)
    (pkgs_dir / PACKAGE_CACHE_MAGIC_FILE).touch()

    mocker.patch(
        "conda.base.context.Context.pkgs_dirs",
        new_callable=mocker.PropertyMock,
        return_value=(pkgs_dir_str := str(pkgs_dir),),
    )
    assert context.pkgs_dirs == (pkgs_dir_str,)

    yield pkgs_dir

    PackageCacheData._cache_.pop(pkgs_dir_str, None)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Collection of helper functions used in conda.notices tests."""

from __future__ import annotations

import datetime
import json
import os
import uuid
from itertools import chain
from pathlib import Path
from typing import TYPE_CHECKING

from ...models.channel import get_channel_objs
from ...notices.cache import get_notices_cache_file
from ...notices.core import get_channel_name_and_urls
from ...notices.types import ChannelNoticeResponse

if TYPE_CHECKING:
    from typing import Sequence
    from unittest import mock

    from ...base.context import Context

DEFAULT_NOTICE_MESG = "Here is an example message that will be displayed to users"


def get_test_notices(
    messages: Sequence[str],
    level: str | None = "info",
    created_at: datetime.datetime | None = None,
    expired_at: datetime.datetime | None = None,
) -> dict:
    created_at = created_at or datetime.datetime.now(datetime.timezone.utc)
    expired_at = expired_at or created_at + datetime.timedelta(days=7)

    return {
        "notices": [
            {
                "id": str(uuid.uuid4()),
                "message": message,
                "level": level,
                "created_at": created_at.isoformat(),
                "expired_at": expired_at.isoformat(),
            }
            for message in messages
        ]
    }


def add_resp_to_mock(
    mock_session: mock.MagicMock,
    status_code: int,
    messages_json: dict,
    raise_exc: bool = False,
) -> None:
    """Adds any number of MockResponse to MagicMock object as side_effects"""

    def forever_404():
        while True:
            yield MockResponse(404, {})

    def one_200():
        yield MockResponse(status_code, messages_json, raise_exc=raise_exc)

    chn = chain(one_200(), forever_404())
    mock_session().get.side_effect = tuple(next(chn) for _ in range(100))


def create_notice_cache_files(
    cache_dir: Path,
    cache_files: Sequence[str],
    messages_json_seq: Sequence[dict],
) -> None:
    """Creates the cache files that we use in tests"""
    for message_json, file in zip(messages_json_seq, cache_files):
        with cache_dir.joinpath(file).open("w") as fp:
            json.dump(message_json, fp)


def offset_cache_file_mtime(mtime_offset) -> None:
    """
    Allows for offsetting the mtime of the notices cache file. This is often
    used to mock an older creation time the cache file.
    """
    cache_file = get_notices_cache_file()
    os.utime(
        cache_file,
        times=(cache_file.stat().st_atime, cache_file.stat().st_mtime - mtime_offset),
    )


class DummyArgs:
    """Dummy object that sets all kwargs as object properties."""

    def __init__(self, **kwargs):
        self.no_ansi_colors = True

        for key, val in kwargs.items():
            setattr(self, key, val)


def notices_decorator_assert_message_in_stdout(
    captured,
    messages: Sequence[str],
    dummy_mesg: str | None = None,
    not_in: bool = False,
):
    """
    Tests a run of notices decorator where we expect to see the messages
    print to stdout.
    """
    assert captured.err == ""
    assert dummy_mesg in captured.out

    for mesg in messages:
        if not_in:
            assert mesg not in captured.out
        else:
            assert mesg in captured.out


class MockResponse:
    def __init__(self, status_code, json_data, raise_exc=False):
        self.status_code = status_code
        self.json_data = json_data
        self.raise_exc = raise_exc

    def json(self):
        if self.raise_exc:
            raise ValueError("Error")
        return self.json_data


def get_notice_cache_filenames(ctx: Context) -> tuple[str]:
    """Returns the filenames of the cache files that will be searched for"""
    channel_urls_and_names = get_channel_name_and_urls(get_channel_objs(ctx))

    return tuple(
        ChannelNoticeResponse.get_cache_key(url, Path("")).name
        for url, name in channel_urls_and_names
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Collection of pytest fixtures used in conda.notices tests."""

from pathlib import Path
from unittest import mock

import pytest

from ...base.constants import NOTICES_CACHE_SUBDIR
from ...cli.conda_argparse import generate_parser


@pytest.fixture(scope="function")
def notices_cache_dir(tmpdir):
    """
    Fixture that creates the notices cache dir while also mocking
    out a call to user_cache_dir.
    """
    with mock.patch("conda.notices.cache.user_cache_dir") as user_cache_dir:
        user_cache_dir.return_value = tmpdir
        cache_dir = Path(tmpdir).joinpath(NOTICES_CACHE_SUBDIR)
        cache_dir.mkdir(parents=True, exist_ok=True)

        yield cache_dir


@pytest.fixture(scope="function")
def notices_mock_fetch_get_session():
    with mock.patch("conda.notices.fetch.get_session") as mock_get_session:
        mock_get_session.return_value = mock.MagicMock()
        yield mock_get_session


@pytest.fixture(scope="function")
def conda_notices_args_n_parser():
    parser = generate_parser()
    args = parser.parse_args(["notices"])

    return args, parser


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Collection of pytest fixtures used in conda.gateways tests."""

import json
import os
import socket
from pathlib import Path
from shutil import which

import pytest
from xprocess import ProcessStarter

MINIO_EXE = which("minio")


# rely on tests not requesting this fixture, and pytest not creating this if
# MINIO_EXE was not found
@pytest.fixture()
def minio_s3_server(xprocess, tmp_path):
    """
    Mock a local S3 server using `minio`

    This requires:
    - pytest-xprocess: runs the background process
    - minio: the executable must be in PATH

    Note, the given S3 server will be EMPTY! The test function needs
    to populate it. You can use
    `conda.testing.helpers.populate_s3_server` for that.
    """

    class Minio:
        # The 'name' below will be the name of the S3 bucket containing
        # keys like `noarch/repodata.json`
        # see https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html
        name = "minio-s3-server"
        port = 9000

        def __init__(self):
            (Path(tmp_path) / self.name).mkdir()

        @property
        def server_url(self):
            return f"{self.endpoint}/{self.name}"

        @property
        def endpoint(self):
            return f"http://localhost:{self.port}"

        def populate_bucket(self, endpoint, bucket_name, channel_dir):
            """Prepare the s3 connection for our minio instance"""
            from boto3.session import Session
            from botocore.client import Config

            # Make the minio bucket public first
            # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-bucket-policies.html#set-a-bucket-policy
            session = Session()
            client = session.client(
                "s3",
                endpoint_url=endpoint,
                aws_access_key_id="minioadmin",
                aws_secret_access_key="minioadmin",
                config=Config(signature_version="s3v4"),
                region_name="us-east-1",
            )
            bucket_policy = json.dumps(
                {
                    "Version": "2012-10-17",
                    "Statement": [
                        {
                            "Sid": "AddPerm",
                            "Effect": "Allow",
                            "Principal": "*",
                            "Action": ["s3:GetObject"],
                            "Resource": f"arn:aws:s3:::{bucket_name}/*",
                        }
                    ],
                }
            )
            client.put_bucket_policy(Bucket=bucket_name, Policy=bucket_policy)

            # Minio has to start with an empty directory; once available,
            # we can import all channel files by "uploading" them
            for current, _, files in os.walk(channel_dir):
                for f in files:
                    path = Path(current, f)
                    key = path.relative_to(channel_dir)
                    client.upload_file(
                        str(path),
                        bucket_name,
                        str(key).replace("\\", "/"),  # MinIO expects Unix paths
                        ExtraArgs={"ACL": "public-read"},
                    )

    print("Starting mock_s3_server")
    minio = Minio()

    class Starter(ProcessStarter):
        pattern = "MinIO Object Storage Server"
        terminate_on_interrupt = True
        timeout = 10
        args = [
            MINIO_EXE,
            "server",
            f"--address=:{minio.port}",
            tmp_path,
        ]

        def startup_check(self, port=minio.port):
            s = socket.socket()
            address = "localhost"
            error = False
            try:
                s.connect((address, port))
            except Exception as e:
                print(
                    "something's wrong with %s:%d. Exception is %s" % (address, port, e)
                )
                error = True
            finally:
                s.close()

            return not error

    # ensure process is running and return its logfile
    pid, logfile = xprocess.ensure(minio.name, Starter)
    print(f"Server (PID: {pid}) log file can be found here: {logfile}")
    yield minio
    xprocess.getinfo(minio.name).terminate()


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Implements object describing a symbolic link from the base environment to a private environment.

Since private environments are an unrealized feature of conda and has been deprecated this data
model no longer serves a purpose and has also been deprecated.
"""

from logging import getLogger

from ..auxlib.entity import Entity, EnumField, StringField
from ..deprecations import deprecated
from .enums import LeasedPathType

log = getLogger(__name__)


@deprecated("24.3", "24.9")
class LeasedPathEntry(Entity):
    """
    _path: short path for the leased path, using forward slashes
    target_path: the full path to the executable in the private env
    target_prefix: the full path to the private environment
    leased_path: the full path for the lease in the root prefix
    package_name: the package holding the lease
    leased_path_type: application_entry_point

    """

    _path = StringField()
    target_path = StringField()
    target_prefix = StringField()
    leased_path = StringField()
    package_name = StringField()
    leased_path_type = EnumField(LeasedPathType)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Implements the version spec with parsing and comparison logic.

Object inheritance:

.. autoapi-inheritance-diagram:: BaseSpec VersionSpec BuildNumberMatch
   :top-classes: conda.models.version.BaseSpec
   :parts: 1
"""

from __future__ import annotations

import operator as op
import re
from itertools import zip_longest
from logging import getLogger

from ..exceptions import InvalidVersionSpec

log = getLogger(__name__)


def normalized_version(version: str) -> VersionOrder:
    """Parse a version string and return VersionOrder object."""
    return VersionOrder(version)


def ver_eval(vtest, spec):
    return VersionSpec(spec).match(vtest)


version_check_re = re.compile(r"^[\*\.\+!_0-9a-z]+$")
version_split_re = re.compile("([0-9]+|[*]+|[^0-9*]+)")
version_cache = {}


class SingleStrArgCachingType(type):
    def __call__(cls, arg):
        if isinstance(arg, cls):
            return arg
        elif isinstance(arg, str):
            try:
                return cls._cache_[arg]
            except KeyError:
                val = cls._cache_[arg] = super().__call__(arg)
                return val
        else:
            return super().__call__(arg)


class VersionOrder(metaclass=SingleStrArgCachingType):
    """Implement an order relation between version strings.

    Version strings can contain the usual alphanumeric characters
    (A-Za-z0-9), separated into components by dots and underscores. Empty
    segments (i.e. two consecutive dots, a leading/trailing underscore)
    are not permitted. An optional epoch number - an integer
    followed by '!' - can proceed the actual version string
    (this is useful to indicate a change in the versioning
    scheme itself). Version comparison is case-insensitive.

    Conda supports six types of version strings:
    * Release versions contain only integers, e.g. '1.0', '2.3.5'.
    * Pre-release versions use additional letters such as 'a' or 'rc',
      for example '1.0a1', '1.2.beta3', '2.3.5rc3'.
    * Development versions are indicated by the string 'dev',
      for example '1.0dev42', '2.3.5.dev12'.
    * Post-release versions are indicated by the string 'post',
      for example '1.0post1', '2.3.5.post2'.
    * Tagged versions have a suffix that specifies a particular
      property of interest, e.g. '1.1.parallel'. Tags can be added
      to any of the preceding four types. As far as sorting is concerned,
      tags are treated like strings in pre-release versions.
    * An optional local version string separated by '+' can be appended
      to the main (upstream) version string. It is only considered
      in comparisons when the main versions are equal, but otherwise
      handled in exactly the same manner.

    To obtain a predictable version ordering, it is crucial to keep the
    version number scheme of a given package consistent over time.
    Specifically,
    * version strings should always have the same number of components
      (except for an optional tag suffix or local version string),
    * letters/strings indicating non-release versions should always
      occur at the same position.

    Before comparison, version strings are parsed as follows:
    * They are first split into epoch, version number, and local version
      number at '!' and '+' respectively. If there is no '!', the epoch is
      set to 0. If there is no '+', the local version is empty.
    * The version part is then split into components at '.' and '_'.
    * Each component is split again into runs of numerals and non-numerals
    * Subcomponents containing only numerals are converted to integers.
    * Strings are converted to lower case, with special treatment for 'dev'
      and 'post'.
    * When a component starts with a letter, the fillvalue 0 is inserted
      to keep numbers and strings in phase, resulting in '1.1.a1' == 1.1.0a1'.
    * The same is repeated for the local version part.

    Examples:
        1.2g.beta15.rc  =>  [[0], [1], [2, 'g'], [0, 'beta', 15], [0, 'rc']]
        1!2.15.1_ALPHA  =>  [[1], [2], [15], [1, '_alpha']]

    The resulting lists are compared lexicographically, where the following
    rules are applied to each pair of corresponding subcomponents:
    * integers are compared numerically
    * strings are compared lexicographically, case-insensitive
    * strings are smaller than integers, except
    * 'dev' versions are smaller than all corresponding versions of other types
    * 'post' versions are greater than all corresponding versions of other types
    * if a subcomponent has no correspondent, the missing correspondent is
      treated as integer 0 to ensure '1.1' == '1.1.0'.

    The resulting order is:
           0.4
         < 0.4.0
         < 0.4.1.rc
        == 0.4.1.RC   # case-insensitive comparison
         < 0.4.1
         < 0.5a1
         < 0.5b3
         < 0.5C1      # case-insensitive comparison
         < 0.5
         < 0.9.6
         < 0.960923
         < 1.0
         < 1.1dev1    # special case 'dev'
         < 1.1_       # appended underscore is special case for openssl-like versions
         < 1.1a1
         < 1.1.0dev1  # special case 'dev'
        == 1.1.dev1   # 0 is inserted before string
         < 1.1.a1
         < 1.1.0rc1
         < 1.1.0
        == 1.1
         < 1.1.0post1 # special case 'post'
        == 1.1.post1  # 0 is inserted before string
         < 1.1post1   # special case 'post'
         < 1996.07.12
         < 1!0.4.1    # epoch increased
         < 1!3.1.1.6
         < 2!0.4.1    # epoch increased again

    Some packages (most notably openssl) have incompatible version conventions.
    In particular, openssl interprets letters as version counters rather than
    pre-release identifiers. For openssl, the relation

      1.0.1 < 1.0.1a  =>  False  # should be true for openssl

    holds, whereas conda packages use the opposite ordering. You can work-around
    this problem by appending an underscore to plain version numbers:

      1.0.1_ < 1.0.1a =>  True   # ensure correct ordering for openssl
    """

    _cache_ = {}

    def __init__(self, vstr: str):
        # version comparison is case-insensitive
        version = vstr.strip().rstrip().lower()
        # basic validity checks
        if version == "":
            raise InvalidVersionSpec(vstr, "empty version string")
        invalid = not version_check_re.match(version)
        if invalid and "-" in version and "_" not in version:
            # Allow for dashes as long as there are no underscores
            # as well, by converting the former to the latter.
            version = version.replace("-", "_")
            invalid = not version_check_re.match(version)
        if invalid:
            raise InvalidVersionSpec(vstr, "invalid character(s)")

        # when fillvalue ==  0  =>  1.1 == 1.1.0
        # when fillvalue == -1  =>  1.1  < 1.1.0
        self.norm_version = version
        self.fillvalue = 0

        # find epoch
        version = version.split("!")
        if len(version) == 1:
            # epoch not given => set it to '0'
            epoch = ["0"]
        elif len(version) == 2:
            # epoch given, must be an integer
            if not version[0].isdigit():
                raise InvalidVersionSpec(vstr, "epoch must be an integer")
            epoch = [version[0]]
        else:
            raise InvalidVersionSpec(vstr, "duplicated epoch separator '!'")

        # find local version string
        version = version[-1].split("+")
        if len(version) == 1:
            # no local version
            self.local = []
        # Case 2: We have a local version component in version[1]
        elif len(version) == 2:
            # local version given
            self.local = version[1].replace("_", ".").split(".")
        else:
            raise InvalidVersionSpec(vstr, "duplicated local version separator '+'")

        # Error Case: Version is empty because the version string started with +.
        # e.g. "+", "1.2", "+a", "+1".
        # This is an error because specifying only a local version is invalid.
        # version[0] is empty because vstr.split("+") returns something like ['', '1.2']
        if version[0] == "":
            raise InvalidVersionSpec(
                vstr, "Missing version before local version separator '+'"
            )

        if version[0][-1] == "_":
            # If the last character of version is "-" or "_", don't split that out
            # individually. Implements the instructions for openssl-like versions
            #   > You can work-around this problem by appending a dash to plain version numbers
            split_version = version[0][:-1].replace("_", ".").split(".")
            split_version[-1] += "_"
        else:
            split_version = version[0].replace("_", ".").split(".")
        self.version = epoch + split_version

        # split components into runs of numerals and non-numerals,
        # convert numerals to int, handle special strings
        for v in (self.version, self.local):
            for k in range(len(v)):
                c = version_split_re.findall(v[k])
                if not c:
                    raise InvalidVersionSpec(vstr, "empty version component")
                for j in range(len(c)):
                    if c[j].isdigit():
                        c[j] = int(c[j])
                    elif c[j] == "post":
                        # ensure number < 'post' == infinity
                        c[j] = float("inf")
                    elif c[j] == "dev":
                        # ensure '*' < 'DEV' < '_' < 'a' < number
                        # by upper-casing (all other strings are lower case)
                        c[j] = "DEV"
                if v[k][0].isdigit():
                    v[k] = c
                else:
                    # components shall start with a number to keep numbers and
                    # strings in phase => prepend fillvalue
                    v[k] = [self.fillvalue] + c

    def __str__(self) -> str:
        return self.norm_version

    def __repr__(self) -> str:
        return f'{self.__class__.__name__}("{self}")'

    def _eq(self, t1: list[str], t2: list[str]) -> bool:
        for v1, v2 in zip_longest(t1, t2, fillvalue=[]):
            for c1, c2 in zip_longest(v1, v2, fillvalue=self.fillvalue):
                if c1 != c2:
                    return False
        return True

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, VersionOrder):
            return False
        return self._eq(self.version, other.version) and self._eq(
            self.local, other.local
        )

    def startswith(self, other: object) -> bool:
        if not isinstance(other, VersionOrder):
            return False
        # Tests if the version lists match up to the last element in "other".
        if other.local:
            if not self._eq(self.version, other.version):
                return False
            t1 = self.local
            t2 = other.local
        else:
            t1 = self.version
            t2 = other.version
        nt = len(t2) - 1
        if not self._eq(t1[:nt], t2[:nt]):
            return False
        v1 = [] if len(t1) <= nt else t1[nt]
        v2 = t2[nt]
        nt = len(v2) - 1
        if not self._eq([v1[:nt]], [v2[:nt]]):
            return False
        c1 = self.fillvalue if len(v1) <= nt else v1[nt]
        c2 = v2[nt]
        if isinstance(c2, str):
            return isinstance(c1, str) and c1.startswith(c2)
        return c1 == c2

    def __ne__(self, other: object) -> bool:
        return not (self == other)

    def __lt__(self, other: object) -> bool:
        if not isinstance(other, VersionOrder):
            return False
        for t1, t2 in zip([self.version, self.local], [other.version, other.local]):
            for v1, v2 in zip_longest(t1, t2, fillvalue=[]):
                for c1, c2 in zip_longest(v1, v2, fillvalue=self.fillvalue):
                    if c1 == c2:
                        continue
                    elif isinstance(c1, str):
                        if not isinstance(c2, str):
                            # str < int
                            return True
                    elif isinstance(c2, str):
                        # not (int < str)
                        return False
                    # c1 and c2 have the same type
                    return c1 < c2
        # self == other
        return False

    def __gt__(self, other: object) -> bool:
        return other < self

    def __le__(self, other: object) -> bool:
        return not (other < self)

    def __ge__(self, other: object) -> bool:
        return not (self < other)


# each token slurps up leading whitespace, which we strip out.
VSPEC_TOKENS = (
    r"\s*\^[^$]*[$]|"  # regexes
    r"\s*[()|,]|"  # parentheses, logical and, logical or
    r"[^()|,]+"
)  # everything else


def treeify(spec_str):
    """
    Examples:
        >>> treeify("1.2.3")
        '1.2.3'
        >>> treeify("1.2.3,>4.5.6")
        (',', '1.2.3', '>4.5.6')
        >>> treeify("1.2.3,4.5.6|<=7.8.9")
        ('|', (',', '1.2.3', '4.5.6'), '<=7.8.9')
        >>> treeify("(1.2.3|4.5.6),<=7.8.9")
        (',', ('|', '1.2.3', '4.5.6'), '<=7.8.9')
        >>> treeify("((1.5|((1.6|1.7), 1.8), 1.9 |2.0))|2.1")
        ('|', '1.5', (',', ('|', '1.6', '1.7'), '1.8', '1.9'), '2.0', '2.1')
        >>> treeify("1.5|(1.6|1.7),1.8,1.9|2.0|2.1")
        ('|', '1.5', (',', ('|', '1.6', '1.7'), '1.8', '1.9'), '2.0', '2.1')
    """
    # Converts a VersionSpec expression string into a tuple-based
    # expression tree.
    assert isinstance(spec_str, str)
    tokens = re.findall(VSPEC_TOKENS, f"({spec_str})")
    output = []
    stack = []

    def apply_ops(cstop):
        # cstop: operators with lower precedence
        while stack and stack[-1] not in cstop:
            if len(output) < 2:
                raise InvalidVersionSpec(spec_str, "cannot join single expression")
            c = stack.pop()
            r = output.pop()
            # Fuse expressions with the same operator; e.g.,
            #   ('|', ('|', a, b), ('|', c, d))becomes
            #   ('|', a, b, c d)
            # We're playing a bit of a trick here. Instead of checking
            # if the left or right entries are tuples, we're counting
            # on the fact that if we _do_ see a string instead, its
            # first character cannot possibly be equal to the operator.
            r = r[1:] if r[0] == c else (r,)
            left = output.pop()
            left = left[1:] if left[0] == c else (left,)
            output.append((c,) + left + r)

    for item in tokens:
        item = item.strip()
        if item == "|":
            apply_ops("(")
            stack.append("|")
        elif item == ",":
            apply_ops("|(")
            stack.append(",")
        elif item == "(":
            stack.append("(")
        elif item == ")":
            apply_ops("(")
            if not stack or stack[-1] != "(":
                raise InvalidVersionSpec(spec_str, "expression must start with '('")
            stack.pop()
        else:
            output.append(item)
    if stack:
        raise InvalidVersionSpec(
            spec_str, f"unable to convert to expression tree: {stack}"
        )
    if not output:
        raise InvalidVersionSpec(spec_str, "unable to determine version from spec")
    return output[0]


def untreeify(spec, _inand=False, depth=0):
    """
    Examples:
        >>> untreeify('1.2.3')
        '1.2.3'
        >>> untreeify((',', '1.2.3', '>4.5.6'))
        '1.2.3,>4.5.6'
        >>> untreeify(('|', (',', '1.2.3', '4.5.6'), '<=7.8.9'))
        '(1.2.3,4.5.6)|<=7.8.9'
        >>> untreeify((',', ('|', '1.2.3', '4.5.6'), '<=7.8.9'))
        '(1.2.3|4.5.6),<=7.8.9'
        >>> untreeify(('|', '1.5', (',', ('|', '1.6', '1.7'), '1.8', '1.9'), '2.0', '2.1'))
        '1.5|((1.6|1.7),1.8,1.9)|2.0|2.1'
    """
    if isinstance(spec, tuple):
        if spec[0] == "|":
            res = "|".join(map(lambda x: untreeify(x, depth=depth + 1), spec[1:]))
            if _inand or depth > 0:
                res = f"({res})"
        else:
            res = ",".join(
                map(lambda x: untreeify(x, _inand=True, depth=depth + 1), spec[1:])
            )
            if depth > 0:
                res = f"({res})"
        return res
    return spec


def compatible_release_operator(x, y):
    return op.__ge__(x, y) and x.startswith(
        VersionOrder(".".join(str(y).split(".")[:-1]))
    )


# This RE matches the operators '==', '!=', '<=', '>=', '<', '>'
# followed by a version string. It rejects expressions like
# '<= 1.2' (space after operator), '<>1.2' (unknown operator),
# and '<=!1.2' (nonsensical operator).
version_relation_re = re.compile(r"^(=|==|!=|<=|>=|<|>|~=)(?![=<>!~])(\S+)$")
regex_split_re = re.compile(r".*[()|,^$]")
OPERATOR_MAP = {
    "==": op.__eq__,
    "!=": op.__ne__,
    "<=": op.__le__,
    ">=": op.__ge__,
    "<": op.__lt__,
    ">": op.__gt__,
    "=": lambda x, y: x.startswith(y),
    "!=startswith": lambda x, y: not x.startswith(y),
    "~=": compatible_release_operator,
}
OPERATOR_START = frozenset(("=", "<", ">", "!", "~"))


class BaseSpec:
    def __init__(self, spec_str, matcher, is_exact):
        self.spec_str = spec_str
        self._is_exact = is_exact
        self.match = matcher

    @property
    def spec(self):
        return self.spec_str

    def is_exact(self):
        return self._is_exact

    def __eq__(self, other):
        try:
            other_spec = other.spec
        except AttributeError:
            other_spec = self.__class__(other).spec
        return self.spec == other_spec

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.spec)

    def __str__(self):
        return self.spec

    def __repr__(self):
        return f"{self.__class__.__name__}('{self.spec}')"

    @property
    def raw_value(self):
        return self.spec

    @property
    def exact_value(self):
        return self.is_exact() and self.spec or None

    def merge(self, other):
        raise NotImplementedError()

    def regex_match(self, spec_str):
        return bool(self.regex.match(spec_str))

    def operator_match(self, spec_str):
        return self.operator_func(VersionOrder(str(spec_str)), self.matcher_vo)

    def any_match(self, spec_str):
        return any(s.match(spec_str) for s in self.tup)

    def all_match(self, spec_str):
        return all(s.match(spec_str) for s in self.tup)

    def exact_match(self, spec_str):
        return self.spec == spec_str

    def always_true_match(self, spec_str):
        return True


class VersionSpec(BaseSpec, metaclass=SingleStrArgCachingType):
    _cache_ = {}

    def __init__(self, vspec):
        vspec_str, matcher, is_exact = self.get_matcher(vspec)
        super().__init__(vspec_str, matcher, is_exact)

    def get_matcher(self, vspec):
        if isinstance(vspec, str) and regex_split_re.match(vspec):
            vspec = treeify(vspec)

        if isinstance(vspec, tuple):
            vspec_tree = vspec
            _matcher = self.any_match if vspec_tree[0] == "|" else self.all_match
            tup = tuple(VersionSpec(s) for s in vspec_tree[1:])
            vspec_str = untreeify((vspec_tree[0],) + tuple(t.spec for t in tup))
            self.tup = tup
            matcher = _matcher
            is_exact = False
            return vspec_str, matcher, is_exact

        vspec_str = str(vspec).strip()
        if vspec_str[0] == "^" or vspec_str[-1] == "$":
            if vspec_str[0] != "^" or vspec_str[-1] != "$":
                raise InvalidVersionSpec(
                    vspec_str, "regex specs must start with '^' and end with '$'"
                )
            self.regex = re.compile(vspec_str)
            matcher = self.regex_match
            is_exact = False
        elif vspec_str[0] in OPERATOR_START:
            m = version_relation_re.match(vspec_str)
            if m is None:
                raise InvalidVersionSpec(vspec_str, "invalid operator")
            operator_str, vo_str = m.groups()
            if vo_str[-2:] == ".*":
                if operator_str in ("=", ">="):
                    vo_str = vo_str[:-2]
                elif operator_str == "!=":
                    vo_str = vo_str[:-2]
                    operator_str = "!=startswith"
                elif operator_str == "~=":
                    raise InvalidVersionSpec(vspec_str, "invalid operator with '.*'")
                else:
                    log.warning(
                        "Using .* with relational operator is superfluous and deprecated "
                        "and will be removed in a future version of conda. Your spec was "
                        f"{vo_str}, but conda is ignoring the .* and treating it as {vo_str[:-2]}"
                    )
                    vo_str = vo_str[:-2]
            try:
                self.operator_func = OPERATOR_MAP[operator_str]
            except KeyError:
                raise InvalidVersionSpec(vspec_str, f"invalid operator: {operator_str}")
            self.matcher_vo = VersionOrder(vo_str)
            matcher = self.operator_match
            is_exact = operator_str == "=="
        elif vspec_str == "*":
            matcher = self.always_true_match
            is_exact = False
        elif "*" in vspec_str.rstrip("*"):
            rx = vspec_str.replace(".", r"\.").replace("+", r"\+").replace("*", r".*")
            rx = rf"^(?:{rx})$"

            self.regex = re.compile(rx)
            matcher = self.regex_match
            is_exact = False
        elif vspec_str[-1] == "*":
            if vspec_str[-2:] != ".*":
                vspec_str = vspec_str[:-1] + ".*"

            # if vspec_str[-1] in OPERATOR_START:
            #     m = version_relation_re.match(vspec_str)
            #     if m is None:
            #         raise InvalidVersionSpecError(vspec_str)
            #     operator_str, vo_str = m.groups()
            #
            #
            # else:
            #     pass

            vo_str = vspec_str.rstrip("*").rstrip(".")
            self.operator_func = VersionOrder.startswith
            self.matcher_vo = VersionOrder(vo_str)
            matcher = self.operator_match
            is_exact = False
        elif "@" not in vspec_str:
            self.operator_func = OPERATOR_MAP["=="]
            self.matcher_vo = VersionOrder(vspec_str)
            matcher = self.operator_match
            is_exact = True
        else:
            matcher = self.exact_match
            is_exact = True
        return vspec_str, matcher, is_exact

    def merge(self, other):
        assert isinstance(other, self.__class__)
        return self.__class__(",".join(sorted((self.raw_value, other.raw_value))))

    def union(self, other):
        assert isinstance(other, self.__class__)
        options = {self.raw_value, other.raw_value}
        # important: we only return a string here because the parens get gobbled otherwise
        #    this info is for visual display only, not for feeding into actual matches
        return "|".join(sorted(options))


# TODO: someday switch out these class names for consistency
VersionMatch = VersionSpec


class BuildNumberMatch(BaseSpec, metaclass=SingleStrArgCachingType):
    _cache_ = {}

    def __init__(self, vspec):
        vspec_str, matcher, is_exact = self.get_matcher(vspec)
        super().__init__(vspec_str, matcher, is_exact)

    def get_matcher(self, vspec):
        try:
            vspec = int(vspec)
        except ValueError:
            pass
        else:
            matcher = self.exact_match
            is_exact = True
            return vspec, matcher, is_exact

        vspec_str = str(vspec).strip()
        if vspec_str == "*":
            matcher = self.always_true_match
            is_exact = False
        elif vspec_str.startswith(("=", "<", ">", "!")):
            m = version_relation_re.match(vspec_str)
            if m is None:
                raise InvalidVersionSpec(vspec_str, "invalid operator")
            operator_str, vo_str = m.groups()
            try:
                self.operator_func = OPERATOR_MAP[operator_str]
            except KeyError:
                raise InvalidVersionSpec(vspec_str, f"invalid operator: {operator_str}")
            self.matcher_vo = VersionOrder(vo_str)
            matcher = self.operator_match

            is_exact = operator_str == "=="
        elif vspec_str[0] == "^" or vspec_str[-1] == "$":
            if vspec_str[0] != "^" or vspec_str[-1] != "$":
                raise InvalidVersionSpec(
                    vspec_str, "regex specs must start with '^' and end with '$'"
                )
            self.regex = re.compile(vspec_str)

            matcher = self.regex_match
            is_exact = False
        # if hasattr(spec, 'match'):
        #     self.spec = _spec
        #     self.match = spec.match
        else:
            matcher = self.exact_match
            is_exact = True
        return vspec_str, matcher, is_exact

    def merge(self, other):
        if self.raw_value != other.raw_value:
            raise ValueError(
                f"Incompatible component merge:\n  - {self.raw_value!r}\n  - {other.raw_value!r}"
            )
        return self.raw_value

    def union(self, other):
        options = {self.raw_value, other.raw_value}
        return "|".join(options)

    @property
    def exact_value(self) -> int | None:
        try:
            return int(self.raw_value)
        except ValueError:
            return None

    def __str__(self):
        return str(self.spec)

    def __repr__(self):
        return str(self.spec)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Implements the data model for conda packages.

A PackageRecord is the record of a package present in a channel. A PackageCache is the record of a
downloaded and cached package. A PrefixRecord is the record of a package installed into a conda
environment.

Object inheritance:

.. autoapi-inheritance-diagram:: PackageRecord PackageCacheRecord PrefixRecord
   :top-classes: conda.models.records.PackageRecord
   :parts: 1
"""

from __future__ import annotations

from os.path import basename, join

from boltons.timeutils import dt_to_timestamp, isoparse

from ..auxlib.entity import (
    BooleanField,
    ComposableField,
    DictSafeMixin,
    Entity,
    EnumField,
    IntegerField,
    ListField,
    NumberField,
    StringField,
)
from ..base.context import context
from ..common.compat import isiterable
from ..exceptions import PathNotFoundError
from .channel import Channel
from .enums import FileMode, LinkType, NoarchType, PackageType, PathType, Platform
from .match_spec import MatchSpec


class LinkTypeField(EnumField):
    def box(self, instance, instance_type, val):
        if isinstance(val, str):
            val = val.replace("-", "").replace("_", "").lower()
            if val == "hard":
                val = LinkType.hardlink
            elif val == "soft":
                val = LinkType.softlink
        return super().box(instance, instance_type, val)


class NoarchField(EnumField):
    def box(self, instance, instance_type, val):
        return super().box(instance, instance_type, NoarchType.coerce(val))


class TimestampField(NumberField):
    def __init__(self):
        super().__init__(default=0, required=False, default_in_dump=False)

    @staticmethod
    def _make_seconds(val):
        if val:
            val = val
            if val > 253402300799:  # 9999-12-31
                val /= (
                    1000  # convert milliseconds to seconds; see conda/conda-build#1988
                )
        return val

    @staticmethod
    def _make_milliseconds(val):
        if val:
            if val < 253402300799:  # 9999-12-31
                val *= 1000  # convert seconds to milliseconds
            val = val
        return val

    def box(self, instance, instance_type, val):
        return self._make_seconds(super().box(instance, instance_type, val))

    def dump(self, instance, instance_type, val):
        return int(
            self._make_milliseconds(super().dump(instance, instance_type, val))
        )  # whether in seconds or milliseconds, type must be int (not float) for backward compat

    def __get__(self, instance, instance_type):
        try:
            return super().__get__(instance, instance_type)
        except AttributeError:
            try:
                return int(dt_to_timestamp(isoparse(instance.date)))
            except (AttributeError, ValueError):
                return 0


class Link(DictSafeMixin, Entity):
    source = StringField()
    type = LinkTypeField(LinkType, required=False)


EMPTY_LINK = Link(source="")


class _FeaturesField(ListField):
    def __init__(self, **kwargs):
        super().__init__(str, **kwargs)

    def box(self, instance, instance_type, val):
        if isinstance(val, str):
            val = val.replace(" ", ",").split(",")
        val = tuple(f for f in (ff.strip() for ff in val) if f)
        return super().box(instance, instance_type, val)

    def dump(self, instance, instance_type, val):
        if isiterable(val):
            return " ".join(val)
        else:
            return val or ()  # default value is (), and default_in_dump=False


class ChannelField(ComposableField):
    def __init__(self, aliases=()):
        super().__init__(Channel, required=False, aliases=aliases)

    def dump(self, instance, instance_type, val):
        if val:
            return str(val)
        else:
            val = instance.channel  # call __get__
            return str(val)

    def __get__(self, instance, instance_type):
        try:
            return super().__get__(instance, instance_type)
        except AttributeError:
            url = instance.url
            return self.unbox(instance, instance_type, Channel(url))


class SubdirField(StringField):
    def __init__(self):
        super().__init__(required=False)

    def __get__(self, instance, instance_type):
        try:
            return super().__get__(instance, instance_type)
        except AttributeError:
            try:
                url = instance.url
            except AttributeError:
                url = None
            if url:
                return self.unbox(instance, instance_type, Channel(url).subdir)

            try:
                platform, arch = instance.platform.name, instance.arch
            except AttributeError:
                platform, arch = None, None
            if platform and not arch:
                return self.unbox(instance, instance_type, "noarch")
            elif platform:
                if "x86" in arch:
                    arch = "64" if "64" in arch else "32"
                return self.unbox(instance, instance_type, f"{platform}-{arch}")
            else:
                return self.unbox(instance, instance_type, context.subdir)


class FilenameField(StringField):
    def __init__(self, aliases=()):
        super().__init__(required=False, aliases=aliases)

    def __get__(self, instance, instance_type):
        try:
            return super().__get__(instance, instance_type)
        except AttributeError:
            try:
                url = instance.url
                fn = Channel(url).package_filename
                if not fn:
                    raise AttributeError()
            except AttributeError:
                fn = f"{instance.name}-{instance.version}-{instance.build}"
            assert fn
            return self.unbox(instance, instance_type, fn)


class PackageTypeField(EnumField):
    def __init__(self):
        super().__init__(
            PackageType,
            required=False,
            nullable=True,
            default=None,
            default_in_dump=False,
        )

    def __get__(self, instance, instance_type):
        val = super().__get__(instance, instance_type)
        if val is None:
            # look in noarch field
            noarch_val = instance.noarch
            if noarch_val:
                type_map = {
                    NoarchType.generic: PackageType.NOARCH_GENERIC,
                    NoarchType.python: PackageType.NOARCH_PYTHON,
                }
                val = type_map[NoarchType.coerce(noarch_val)]
                val = self.unbox(instance, instance_type, val)
        return val


class PathData(Entity):
    _path = StringField()
    prefix_placeholder = StringField(
        required=False, nullable=True, default=None, default_in_dump=False
    )
    file_mode = EnumField(FileMode, required=False, nullable=True)
    no_link = BooleanField(
        required=False, nullable=True, default=None, default_in_dump=False
    )
    path_type = EnumField(PathType)

    @property
    def path(self):
        # because I don't have aliases as an option for entity fields yet
        return self._path


class PathDataV1(PathData):
    # TODO: sha256 and size_in_bytes should be required for all PathType.hardlink, but not for softlink and directory  # NOQA
    sha256 = StringField(required=False, nullable=True)
    size_in_bytes = IntegerField(required=False, nullable=True)
    inode_paths = ListField(str, required=False, nullable=True)

    sha256_in_prefix = StringField(required=False, nullable=True)


class PathsData(Entity):
    # from info/paths.json
    paths_version = IntegerField()
    paths = ListField(PathData)


class PackageRecord(DictSafeMixin, Entity):
    name = StringField()
    version = StringField()
    build = StringField(aliases=("build_string",))
    build_number = IntegerField()

    # the canonical code abbreviation for PackageRef is `pref`
    # fields required to uniquely identifying a package

    channel = ChannelField(aliases=("schannel",))
    subdir = SubdirField()
    fn = FilenameField(aliases=("filename",))

    md5 = StringField(
        default=None, required=False, nullable=True, default_in_dump=False
    )
    legacy_bz2_md5 = StringField(
        default=None, required=False, nullable=True, default_in_dump=False
    )
    legacy_bz2_size = IntegerField(required=False, nullable=True, default_in_dump=False)
    url = StringField(
        default=None, required=False, nullable=True, default_in_dump=False
    )
    sha256 = StringField(
        default=None, required=False, nullable=True, default_in_dump=False
    )

    @property
    def schannel(self):
        return self.channel.canonical_name

    @property
    def _pkey(self):
        try:
            return self.__pkey
        except AttributeError:
            __pkey = self.__pkey = [
                self.channel.canonical_name,
                self.subdir,
                self.name,
                self.version,
                self.build_number,
                self.build,
            ]
            # NOTE: fn is included to distinguish between .conda and .tar.bz2 packages
            if context.separate_format_cache:
                __pkey.append(self.fn)
            self.__pkey = tuple(__pkey)
            return self.__pkey

    def __hash__(self):
        try:
            return self._hash
        except AttributeError:
            self._hash = hash(self._pkey)
        return self._hash

    def __eq__(self, other):
        return self._pkey == other._pkey

    def dist_str(self):
        return "{}{}::{}-{}-{}".format(
            self.channel.canonical_name,
            ("/" + self.subdir) if self.subdir else "",
            self.name,
            self.version,
            self.build,
        )

    def dist_fields_dump(self):
        return {
            "base_url": self.channel.base_url,
            "build_number": self.build_number,
            "build_string": self.build,
            "channel": self.channel.name,
            "dist_name": self.dist_str().split(":")[-1],
            "name": self.name,
            "platform": self.subdir,
            "version": self.version,
        }

    arch = StringField(required=False, nullable=True)  # so legacy
    platform = EnumField(Platform, required=False, nullable=True)  # so legacy

    depends = ListField(str, default=())
    constrains = ListField(str, default=())

    track_features = _FeaturesField(required=False, default=(), default_in_dump=False)
    features = _FeaturesField(required=False, default=(), default_in_dump=False)

    noarch = NoarchField(
        NoarchType, required=False, nullable=True, default=None, default_in_dump=False
    )  # TODO: rename to package_type
    preferred_env = StringField(
        required=False, nullable=True, default=None, default_in_dump=False
    )

    license = StringField(
        required=False, nullable=True, default=None, default_in_dump=False
    )
    license_family = StringField(
        required=False, nullable=True, default=None, default_in_dump=False
    )
    package_type = PackageTypeField()

    @property
    def is_unmanageable(self):
        return self.package_type in PackageType.unmanageable_package_types()

    timestamp = TimestampField()

    @property
    def combined_depends(self):
        from .match_spec import MatchSpec

        result = {ms.name: ms for ms in MatchSpec.merge(self.depends)}
        for spec in self.constrains or ():
            ms = MatchSpec(spec)
            result[ms.name] = MatchSpec(ms, optional=(ms.name not in result))
        return tuple(result.values())

    # the canonical code abbreviation for PackageRecord is `prec`, not to be confused with
    # PackageCacheRecord (`pcrec`) or PrefixRecord (`prefix_rec`)
    #
    # important for "choosing" a package (i.e. the solver), listing packages
    # (like search), and for verifying downloads
    #
    # this is the highest level of the record inheritance model that MatchSpec is designed to
    # work with

    date = StringField(required=False)
    size = IntegerField(required=False)

    def __str__(self):
        return f"{self.channel.canonical_name}/{self.subdir}::{self.name}=={self.version}={self.build}"

    def to_match_spec(self):
        return MatchSpec(
            channel=self.channel,
            subdir=self.subdir,
            name=self.name,
            version=self.version,
            build=self.build,
        )

    def to_simple_match_spec(self):
        return MatchSpec(
            name=self.name,
            version=self.version,
        )

    @property
    def namekey(self):
        return "global:" + self.name

    def record_id(self):
        # WARNING: This is right now only used in link.py _change_report_str(). It is not
        #          the official record_id / uid until it gets namespace.  Even then, we might
        #          make the format different.  Probably something like
        #              channel_name/subdir:namespace:name-version-build_number-build_string
        return f"{self.channel.name}/{self.subdir}::{self.name}-{self.version}-{self.build}"

    metadata: set[str]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.metadata = set()


class Md5Field(StringField):
    def __init__(self):
        super().__init__(required=False, nullable=True)

    def __get__(self, instance, instance_type):
        try:
            return super().__get__(instance, instance_type)
        except AttributeError as e:
            try:
                return instance._calculate_md5sum()
            except PathNotFoundError:
                raise e


class PackageCacheRecord(PackageRecord):
    package_tarball_full_path = StringField()
    extracted_package_dir = StringField()

    md5 = Md5Field()

    @property
    def is_fetched(self):
        from ..gateways.disk.read import isfile

        return isfile(self.package_tarball_full_path)

    @property
    def is_extracted(self):
        from ..gateways.disk.read import isdir, isfile

        epd = self.extracted_package_dir
        return isdir(epd) and isfile(join(epd, "info", "index.json"))

    @property
    def tarball_basename(self):
        return basename(self.package_tarball_full_path)

    def _calculate_md5sum(self):
        memoized_md5 = getattr(self, "_memoized_md5", None)
        if memoized_md5:
            return memoized_md5

        from os.path import isfile

        if isfile(self.package_tarball_full_path):
            from ..gateways.disk.read import compute_sum

            md5sum = compute_sum(self.package_tarball_full_path, "md5")
            setattr(self, "_memoized_md5", md5sum)
            return md5sum


class PrefixRecord(PackageRecord):
    package_tarball_full_path = StringField(required=False)
    extracted_package_dir = StringField(required=False)

    files = ListField(str, default=(), required=False)
    paths_data = ComposableField(
        PathsData, required=False, nullable=True, default_in_dump=False
    )
    link = ComposableField(Link, required=False)
    # app = ComposableField(App, required=False)

    requested_spec = StringField(required=False)

    # There have been requests in the past to save remote server auth
    # information with the package.  Open to rethinking that though.
    auth = StringField(required=False, nullable=True)

    # @classmethod
    # def load(cls, conda_meta_json_path):
    #     return cls()


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""(Legacy) Low-level implementation of a PackageRecord."""

from logging import getLogger

from ..auxlib.entity import (
    ComposableField,
    Entity,
    EnumField,
    ImmutableEntity,
    IntegerField,
    ListField,
    StringField,
)
from .channel import Channel
from .enums import NoarchType
from .records import PackageRecord, PathsData

log = getLogger(__name__)


class NoarchField(EnumField):
    def box(self, instance, instance_type, val):
        return super().box(instance, instance_type, NoarchType.coerce(val))


class Noarch(Entity):
    type = NoarchField(NoarchType)
    entry_points = ListField(
        str, required=False, nullable=True, default=None, default_in_dump=False
    )


class PreferredEnv(Entity):
    name = StringField()
    executable_paths = ListField(str, required=False, nullable=True)
    softlink_paths = ListField(str, required=False, nullable=True)


class PackageMetadata(Entity):
    # from info/package_metadata.json
    package_metadata_version = IntegerField()
    noarch = ComposableField(Noarch, required=False, nullable=True)
    preferred_env = ComposableField(
        PreferredEnv, required=False, nullable=True, default=None, default_in_dump=False
    )


class PackageInfo(ImmutableEntity):
    # attributes external to the package tarball
    extracted_package_dir = StringField()
    package_tarball_full_path = StringField()
    channel = ComposableField(Channel)
    repodata_record = ComposableField(PackageRecord)
    url = StringField()

    # attributes within the package tarball
    icondata = StringField(required=False, nullable=True)
    package_metadata = ComposableField(PackageMetadata, required=False, nullable=True)
    paths_data = ComposableField(PathsData)

    def dist_str(self):
        return f"{self.channel.canonical_name}::{self.name}-{self.version}-{self.build}"

    @property
    def name(self):
        return self.repodata_record.name

    @property
    def version(self):
        return self.repodata_record.version

    @property
    def build(self):
        return self.repodata_record.build

    @property
    def build_number(self):
        return self.repodata_record.build_number


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""(Legacy) Low-level implementation of a Channel."""

import re
from logging import getLogger
from typing import NamedTuple

from .. import CondaError
from ..auxlib.entity import Entity, EntityType, IntegerField, StringField
from ..base.constants import (
    CONDA_PACKAGE_EXTENSIONS,
    DEFAULTS_CHANNEL_NAME,
    UNKNOWN_CHANNEL,
)
from ..base.context import context
from ..common.compat import ensure_text_type
from ..common.constants import NULL
from ..common.url import has_platform, is_url, join_url
from ..deprecations import deprecated
from .channel import Channel
from .package_info import PackageInfo
from .records import PackageRecord

log = getLogger(__name__)


class DistDetails(NamedTuple):
    name: str
    version: str
    build_string: str
    build_number: str
    dist_name: str
    fmt: str


deprecated.constant(
    "24.3",
    "24.9",
    "IndexRecord",
    PackageRecord,
    addendum="Use `conda.models.records.PackageRecord` instead.",
)


class DistType(EntityType):
    def __call__(cls, *args, **kwargs):
        if len(args) == 1 and not kwargs:
            value = args[0]
            if value in Dist._cache_:
                return Dist._cache_[value]
            elif isinstance(value, Dist):
                dist = value
            elif isinstance(value, PackageRecord):
                dist = Dist.from_string(
                    value.fn, channel_override=value.channel.canonical_name
                )
            elif hasattr(value, "dist") and isinstance(value.dist, Dist):
                dist = value.dist
            elif isinstance(value, PackageInfo):
                dist = Dist.from_string(
                    value.repodata_record.fn,
                    channel_override=value.channel.canonical_name,
                )
            elif isinstance(value, Channel):
                dist = Dist.from_url(value.url())
            else:
                dist = Dist.from_string(value)
            Dist._cache_[value] = dist
            return dist
        else:
            return super().__call__(*args, **kwargs)


def strip_extension(original_dist):
    for ext in CONDA_PACKAGE_EXTENSIONS:
        if original_dist.endswith(ext):
            original_dist = original_dist[: -len(ext)]
    return original_dist


def split_extension(original_dist):
    stripped = strip_extension(original_dist)
    return stripped, original_dist[len(stripped) :]


class Dist(Entity, metaclass=DistType):
    _cache_ = {}
    _lazy_validate = True

    channel = StringField(required=False, nullable=True, immutable=True)

    dist_name = StringField(immutable=True)
    name = StringField(immutable=True)
    fmt = StringField(immutable=True)
    version = StringField(immutable=True)
    build_string = StringField(immutable=True)
    build_number = IntegerField(immutable=True)

    base_url = StringField(required=False, nullable=True, immutable=True)
    platform = StringField(required=False, nullable=True, immutable=True)

    def __init__(
        self,
        channel,
        dist_name=None,
        name=None,
        version=None,
        build_string=None,
        build_number=None,
        base_url=None,
        platform=None,
        fmt=".tar.bz2",
    ):
        super().__init__(
            channel=channel,
            dist_name=dist_name,
            name=name,
            version=version,
            build_string=build_string,
            build_number=build_number,
            base_url=base_url,
            platform=platform,
            fmt=fmt,
        )

    def to_package_ref(self):
        return PackageRecord(
            channel=self.channel,
            subdir=self.platform,
            name=self.name,
            version=self.version,
            build=self.build_string,
            build_number=self.build_number,
        )

    @property
    def full_name(self):
        return self.__str__()

    @property
    def build(self):
        return self.build_string

    @property
    def subdir(self):
        return self.platform

    @property
    def pair(self):
        return self.channel or DEFAULTS_CHANNEL_NAME, self.dist_name

    @property
    def quad(self):
        # returns: name, version, build_string, channel
        parts = self.dist_name.rsplit("-", 2) + ["", ""]
        return parts[0], parts[1], parts[2], self.channel or DEFAULTS_CHANNEL_NAME

    def __str__(self):
        return f"{self.channel}::{self.dist_name}" if self.channel else self.dist_name

    @property
    def is_feature_package(self):
        return self.dist_name.endswith("@")

    @property
    def is_channel(self):
        return bool(self.base_url and self.platform)

    def to_filename(self, extension=None):
        if self.is_feature_package:
            return self.dist_name
        else:
            return self.dist_name + self.fmt

    def to_matchspec(self):
        return " ".join(self.quad[:3])

    def to_match_spec(self):
        from .match_spec import MatchSpec

        base = "=".join(self.quad[:3])
        return MatchSpec(f"{self.channel}::{base}" if self.channel else base)

    @classmethod
    def from_string(cls, string, channel_override=NULL):
        string = str(string)

        if is_url(string) and channel_override == NULL:
            return cls.from_url(string)

        if string.endswith("@"):
            return cls(
                channel="@",
                name=string,
                version="",
                build_string="",
                build_number=0,
                dist_name=string,
            )

        REGEX_STR = (
            r"(?:([^\s\[\]]+)::)?"  # optional channel
            r"([^\s\[\]]+)"  # 3.x dist
            r"(?:\[([a-zA-Z0-9_-]+)\])?"  # with_features_depends
        )
        channel, original_dist, w_f_d = re.search(REGEX_STR, string).groups()

        original_dist, fmt = split_extension(original_dist)

        if channel_override != NULL:
            channel = channel_override
        if not channel:
            channel = UNKNOWN_CHANNEL

        # enforce dist format
        dist_details = cls.parse_dist_name(original_dist)
        return cls(
            channel=channel,
            name=dist_details.name,
            version=dist_details.version,
            build_string=dist_details.build_string,
            build_number=dist_details.build_number,
            dist_name=original_dist,
            fmt=fmt,
        )

    @staticmethod
    def parse_dist_name(string):
        original_string = string
        try:
            string = ensure_text_type(string)
            no_fmt_string, fmt = split_extension(string)

            # remove any directory or channel information
            if "::" in no_fmt_string:
                dist_name = no_fmt_string.rsplit("::", 1)[-1]
            else:
                dist_name = no_fmt_string.rsplit("/", 1)[-1]

            parts = dist_name.rsplit("-", 2)

            name = parts[0]
            version = parts[1]
            build_string = parts[2] if len(parts) >= 3 else ""
            build_number_as_string = "".join(
                filter(
                    lambda x: x.isdigit(),
                    (build_string.rsplit("_")[-1] if build_string else "0"),
                )
            )
            build_number = int(build_number_as_string) if build_number_as_string else 0

            return DistDetails(
                name, version, build_string, build_number, dist_name, fmt
            )

        except:
            raise CondaError(
                f"dist_name is not a valid conda package: {original_string}"
            )

    @classmethod
    def from_url(cls, url):
        assert is_url(url), url
        if (
            not any(url.endswith(ext) for ext in CONDA_PACKAGE_EXTENSIONS)
            and "::" not in url
        ):
            raise CondaError(f"url '{url}' is not a conda package")

        dist_details = cls.parse_dist_name(url)
        if "::" in url:
            url_no_tarball = url.rsplit("::", 1)[0]
            platform = context.subdir
            base_url = url_no_tarball.split("::")[0]
            channel = str(Channel(base_url))
        else:
            url_no_tarball = url.rsplit("/", 1)[0]
            platform = has_platform(url_no_tarball, context.known_subdirs)
            base_url = url_no_tarball.rsplit("/", 1)[0] if platform else url_no_tarball
            channel = Channel(base_url).canonical_name if platform else UNKNOWN_CHANNEL

        return cls(
            channel=channel,
            name=dist_details.name,
            version=dist_details.version,
            build_string=dist_details.build_string,
            build_number=dist_details.build_number,
            dist_name=dist_details.dist_name,
            base_url=base_url,
            platform=platform,
            fmt=dist_details.fmt,
        )

    def to_url(self):
        if not self.base_url:
            return None
        filename = self.dist_name + self.fmt
        return (
            join_url(self.base_url, self.platform, filename)
            if self.platform
            else join_url(self.base_url, filename)
        )

    def __key__(self):
        return self.channel, self.dist_name

    def __lt__(self, other):
        assert isinstance(other, self.__class__)
        return self.__key__() < other.__key__()

    def __gt__(self, other):
        assert isinstance(other, self.__class__)
        return self.__key__() > other.__key__()

    def __le__(self, other):
        assert isinstance(other, self.__class__)
        return self.__key__() <= other.__key__()

    def __ge__(self, other):
        assert isinstance(other, self.__class__)
        return self.__key__() >= other.__key__()

    def __hash__(self):
        # dists compare equal regardless of fmt, but fmt is taken into account for
        #    object identity
        return hash((self.__key__(), self.fmt))

    def __eq__(self, other):
        return isinstance(other, self.__class__) and self.__key__() == other.__key__()

    def __ne__(self, other):
        return not self.__eq__(other)

    # ############ conda-build compatibility ################

    def split(self, sep=None, maxsplit=-1):
        assert sep == "::"
        return [self.channel, self.dist_name] if self.channel else [self.dist_name]

    def rsplit(self, sep=None, maxsplit=-1):
        assert sep == "-"
        assert maxsplit == 2
        name = f"{self.channel}::{self.quad[0]}" if self.channel else self.quad[0]
        return name, self.quad[1], self.quad[2]

    def startswith(self, match):
        return self.dist_name.startswith(match)

    def __contains__(self, item):
        item = strip_extension(ensure_text_type(item))
        return item in self.__str__()

    @property
    def fn(self):
        return self.to_filename()


def dist_str_to_quad(dist_str):
    dist_str = strip_extension(dist_str)
    if "::" in dist_str:
        channel_str, dist_str = dist_str.split("::", 1)
    else:
        channel_str = UNKNOWN_CHANNEL
    name, version, build = dist_str.rsplit("-", 2)
    return name, version, build, channel_str


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Defines Channel and MultiChannel objects and other channel-related functions.

Object inheritance:

.. autoapi-inheritance-diagram:: Channel MultiChannel
   :top-classes: conda.models.channel.Channel
   :parts: 1
"""

from copy import copy
from itertools import chain
from logging import getLogger

from boltons.setutils import IndexedSet

from ..base.constants import (
    DEFAULTS_CHANNEL_NAME,
    MAX_CHANNEL_PRIORITY,
    UNKNOWN_CHANNEL,
)
from ..base.context import Context, context
from ..common.compat import ensure_text_type, isiterable
from ..common.path import is_package_file, is_path, win_path_backout
from ..common.url import (
    Url,
    has_scheme,
    is_url,
    join_url,
    path_to_url,
    split_conda_url_easy_parts,
    split_platform,
    split_scheme_auth_token,
    urlparse,
)

log = getLogger(__name__)


class ChannelType(type):
    """
    This metaclass does basic caching and enables static constructor method usage with a
    single arg.
    """

    def __call__(cls, *args, **kwargs):
        if len(args) == 1 and not kwargs:
            value = args[0]
            if isinstance(value, Channel):
                return value
            elif value in Channel._cache_:
                return Channel._cache_[value]
            else:
                c = Channel._cache_[value] = Channel.from_value(value)
                return c
        elif "channels" in kwargs:
            # presence of 'channels' kwarg indicates MultiChannel
            channels = tuple(cls(**_kwargs) for _kwargs in kwargs["channels"])
            return MultiChannel(kwargs["name"], channels)
        else:
            return super().__call__(*args, **kwargs)


class Channel(metaclass=ChannelType):
    """
    Channel:
    scheme <> auth <> location <> token <> channel <> subchannel <> platform <> package_filename

    Package Spec:
    channel <> subchannel <> namespace <> package_name

    """

    _cache_ = {}

    @staticmethod
    def _reset_state():
        Channel._cache_ = {}

    def __init__(
        self,
        scheme=None,
        auth=None,
        location=None,
        token=None,
        name=None,
        platform=None,
        package_filename=None,
    ):
        self.scheme = scheme
        self.auth = auth
        self.location = location
        self.token = token
        self.name = name or ""
        self.platform = platform
        self.package_filename = package_filename

    @property
    def channel_location(self):
        return self.location

    @property
    def channel_name(self):
        return self.name

    @property
    def subdir(self):
        return self.platform

    @staticmethod
    def from_url(url):
        return parse_conda_channel_url(url)

    @staticmethod
    def from_channel_name(channel_name):
        return _get_channel_for_name(channel_name)

    @staticmethod
    def from_value(value):
        if value in (None, "<unknown>", "None:///<unknown>", "None"):
            return Channel(name=UNKNOWN_CHANNEL)
        value = ensure_text_type(value)
        if has_scheme(value):
            if value.startswith("file:"):
                value = win_path_backout(value)
            return Channel.from_url(value)
        elif is_path(value):
            return Channel.from_url(path_to_url(value))
        elif is_package_file(value):
            if value.startswith("file:"):
                value = win_path_backout(value)
            return Channel.from_url(value)
        else:
            # at this point assume we don't have a bare (non-scheme) url
            #   e.g. this would be bad:  repo.anaconda.com/pkgs/free
            _stripped, platform = split_platform(context.known_subdirs, value)
            if _stripped in context.custom_multichannels:
                return MultiChannel(
                    _stripped, context.custom_multichannels[_stripped], platform
                )
            else:
                return Channel.from_channel_name(value)

    @staticmethod
    def make_simple_channel(channel_alias, channel_url, name=None):
        ca = channel_alias
        test_url, scheme, auth, token = split_scheme_auth_token(channel_url)
        if name and scheme:
            return Channel(
                scheme=scheme,
                auth=auth,
                location=test_url,
                token=token,
                name=name.strip("/"),
            )
        if scheme:
            if ca.location and test_url.startswith(ca.location):
                location, name = ca.location, test_url.replace(ca.location, "", 1)
            else:
                url_parts = urlparse(test_url)
                location = str(Url(hostname=url_parts.hostname, port=url_parts.port))
                name = url_parts.path or ""
            return Channel(
                scheme=scheme,
                auth=auth,
                location=location,
                token=token,
                name=name.strip("/"),
            )
        else:
            return Channel(
                scheme=ca.scheme,
                auth=ca.auth,
                location=ca.location,
                token=ca.token,
                name=name and name.strip("/") or channel_url.strip("/"),
            )

    @property
    def canonical_name(self):
        try:
            return self.__canonical_name
        except AttributeError:
            pass

        for multiname, channels in context.custom_multichannels.items():
            for channel in channels:
                if self.name == channel.name:
                    cn = self.__canonical_name = multiname
                    return cn

        for that_name in context.custom_channels:
            if self.name and tokenized_startswith(
                self.name.split("/"), that_name.split("/")
            ):
                cn = self.__canonical_name = self.name
                return cn

        if any(
            alias.location == self.location
            for alias in (
                context.channel_alias,
                *context.migrated_channel_aliases,
            )
        ):
            cn = self.__canonical_name = self.name
            return cn

        # fall back to the equivalent of self.base_url
        # re-defining here because base_url for MultiChannel is None
        if self.scheme:
            cn = self.__canonical_name = (
                f"{self.scheme}://{join_url(self.location, self.name)}"
            )
            return cn
        else:
            cn = self.__canonical_name = join_url(self.location, self.name).lstrip("/")
            return cn

    def urls(self, with_credentials=False, subdirs=None):
        if subdirs is None:
            subdirs = context.subdirs

        assert isiterable(subdirs), subdirs  # subdirs must be a non-string iterable

        if self.canonical_name == UNKNOWN_CHANNEL:
            return Channel(DEFAULTS_CHANNEL_NAME).urls(with_credentials, subdirs)

        base = [self.location]
        if with_credentials and self.token:
            base.extend(["t", self.token])
        base.append(self.name)
        base = join_url(*base)

        def _platforms():
            if self.platform:
                yield self.platform
                if self.platform != "noarch":
                    yield "noarch"
            else:
                yield from subdirs

        bases = (join_url(base, p) for p in _platforms())
        if with_credentials and self.auth:
            return [f"{self.scheme}://{self.auth}@{b}" for b in bases]
        else:
            return [f"{self.scheme}://{b}" for b in bases]

    def url(self, with_credentials=False):
        if self.canonical_name == UNKNOWN_CHANNEL:
            return None

        base = [self.location]
        if with_credentials and self.token:
            base.extend(["t", self.token])
        base.append(self.name)
        if self.platform:
            base.append(self.platform)
            if self.package_filename:
                base.append(self.package_filename)
        else:
            first_non_noarch = next(
                (s for s in context.subdirs if s != "noarch"), "noarch"
            )
            base.append(first_non_noarch)

        base = join_url(*base)

        if with_credentials and self.auth:
            return f"{self.scheme}://{self.auth}@{base}"
        else:
            return f"{self.scheme}://{base}"

    @property
    def base_url(self):
        if self.canonical_name == UNKNOWN_CHANNEL:
            return None
        return f"{self.scheme}://{join_url(self.location, self.name)}"

    @property
    def base_urls(self):
        return (self.base_url,)

    @property
    def subdir_url(self):
        url = self.url(True)
        if self.package_filename and url:
            url = url.rsplit("/", 1)[0]
        return url

    def __str__(self):
        base = self.base_url or self.name
        if self.subdir:
            return join_url(base, self.subdir)
        else:
            return base

    def __repr__(self):
        return 'Channel("%s")' % (
            join_url(self.name, self.subdir) if self.subdir else self.name
        )

    def __eq__(self, other):
        if isinstance(other, Channel):
            return self.location == other.location and self.name == other.name
        else:
            try:
                _other = Channel(other)
                return self.location == _other.location and self.name == _other.name
            except Exception as e:
                log.debug("%r", e)
                return False

    def __hash__(self):
        return hash((self.location, self.name))

    def __nonzero__(self):
        return any((self.location, self.name))

    def __bool__(self):
        return self.__nonzero__()

    def __json__(self):
        return self.__dict__

    @property
    def url_channel_wtf(self):
        return self.base_url, self.canonical_name

    def dump(self):
        return {
            "scheme": self.scheme,
            "auth": self.auth,
            "location": self.location,
            "token": self.token,
            "name": self.name,
            "platform": self.platform,
            "package_filename": self.package_filename,
        }


class MultiChannel(Channel):
    def __init__(self, name, channels, platform=None):
        self.name = name
        self.location = None

        if platform:
            self._channels = tuple(
                Channel(**{**channel.dump(), "platform": platform})
                for channel in channels
            )
        else:
            self._channels = channels

        self.scheme = None
        self.auth = None
        self.token = None
        self.platform = platform
        self.package_filename = None

    @property
    def channel_location(self):
        return self.location

    @property
    def canonical_name(self):
        return self.name

    def urls(self, with_credentials=False, subdirs=None):
        _channels = self._channels
        return list(
            chain.from_iterable(c.urls(with_credentials, subdirs) for c in _channels)
        )

    @property
    def base_url(self):
        return None

    @property
    def base_urls(self):
        return tuple(c.base_url for c in self._channels)

    def url(self, with_credentials=False):
        return None

    def dump(self):
        return {"name": self.name, "channels": tuple(c.dump() for c in self._channels)}


def tokenized_startswith(test_iterable, startswith_iterable):
    return all(t == sw for t, sw in zip(test_iterable, startswith_iterable))


def tokenized_conda_url_startswith(test_url, startswith_url):
    test_url, startswith_url = urlparse(test_url), urlparse(startswith_url)
    if (
        test_url.hostname != startswith_url.hostname
        or test_url.port != startswith_url.port
    ):
        return False
    norm_url_path = lambda url: url.path.strip("/") or "/"
    return tokenized_startswith(
        norm_url_path(test_url).split("/"), norm_url_path(startswith_url).split("/")
    )


def _get_channel_for_name(channel_name):
    def _get_channel_for_name_helper(name):
        if name in context.custom_channels:
            return context.custom_channels[name]
        else:
            test_name = name.rsplit("/", 1)[0]  # progressively strip off path segments
            if test_name == name:
                return None
            return _get_channel_for_name_helper(test_name)

    _stripped, platform = split_platform(context.known_subdirs, channel_name)
    channel = _get_channel_for_name_helper(_stripped)

    if channel is not None:
        # stripping off path threw information away from channel_name (i.e. any potential subname)
        # channel.name *should still be* channel_name
        channel = copy(channel)
        channel.name = _stripped
        if platform:
            channel.platform = platform
        return channel
    else:
        ca = context.channel_alias
        return Channel(
            scheme=ca.scheme,
            auth=ca.auth,
            location=ca.location,
            token=ca.token,
            name=_stripped,
            platform=platform,
        )


def _read_channel_configuration(scheme, host, port, path):
    # return location, name, scheme, auth, token

    path = path and path.rstrip("/")
    test_url = str(Url(hostname=host, port=port, path=path))

    # Step 1. No path given; channel name is None
    if not path:
        return (
            str(Url(hostname=host, port=port)).rstrip("/"),
            None,
            scheme or None,
            None,
            None,
        )

    # Step 2. migrated_custom_channels matches
    for name, location in sorted(
        context.migrated_custom_channels.items(), reverse=True, key=lambda x: len(x[0])
    ):
        location, _scheme, _auth, _token = split_scheme_auth_token(location)
        if tokenized_conda_url_startswith(test_url, join_url(location, name)):
            # translate location to new location, with new credentials
            subname = test_url.replace(join_url(location, name), "", 1).strip("/")
            channel_name = join_url(name, subname)
            channel = _get_channel_for_name(channel_name)
            return (
                channel.location,
                channel_name,
                channel.scheme,
                channel.auth,
                channel.token,
            )

    # Step 3. migrated_channel_aliases matches
    for migrated_alias in context.migrated_channel_aliases:
        if test_url.startswith(migrated_alias.location):
            name = test_url.replace(migrated_alias.location, "", 1).strip("/")
            ca = context.channel_alias
            return ca.location, name, ca.scheme, ca.auth, ca.token

    # Step 4. custom_channels matches
    for name, channel in sorted(
        context.custom_channels.items(), reverse=True, key=lambda x: len(x[0])
    ):
        that_test_url = join_url(channel.location, channel.name)
        if tokenized_startswith(test_url.split("/"), that_test_url.split("/")):
            subname = test_url.replace(that_test_url, "", 1).strip("/")
            return (
                channel.location,
                join_url(channel.name, subname),
                scheme,
                channel.auth,
                channel.token,
            )

    # Step 5. channel_alias match
    ca = context.channel_alias
    if ca.location and tokenized_startswith(
        test_url.split("/"), ca.location.split("/")
    ):
        name = test_url.replace(ca.location, "", 1).strip("/") or None
        return ca.location, name, scheme, ca.auth, ca.token

    # Step 6. not-otherwise-specified file://-type urls
    if host is None:
        # this should probably only happen with a file:// type url
        assert port is None
        location, name = test_url.rsplit("/", 1)
        if not location:
            location = "/"
        _scheme, _auth, _token = "file", None, None
        return location, name, _scheme, _auth, _token

    # Step 7. fall through to host:port as channel_location and path as channel_name
    #  but bump the first token of paths starting with /conda for compatibility with
    #  Anaconda Enterprise Repository software.
    bump = None
    path_parts = path.strip("/").split("/")
    if path_parts and path_parts[0] == "conda":
        bump, path = "conda", "/".join(path_parts[1:])
    return (
        str(Url(hostname=host, port=port, path=bump)).rstrip("/"),
        path.strip("/") or None,
        scheme or None,
        None,
        None,
    )


def parse_conda_channel_url(url):
    (
        scheme,
        auth,
        token,
        platform,
        package_filename,
        host,
        port,
        path,
        query,
    ) = split_conda_url_easy_parts(context.known_subdirs, url)

    # recombine host, port, path to get a channel_name and channel_location
    (
        channel_location,
        channel_name,
        configured_scheme,
        configured_auth,
        configured_token,
    ) = _read_channel_configuration(scheme, host, port, path)

    # if we came out with no channel_location or channel_name, we need to figure it out
    # from host, port, path
    assert channel_location is not None or channel_name is not None

    return Channel(
        configured_scheme or "https",
        auth or configured_auth,
        channel_location,
        token or configured_token,
        channel_name,
        platform,
        package_filename,
    )


# backward compatibility for conda-build
def get_conda_build_local_url():
    return (context.local_build_root,)


def prioritize_channels(channels, with_credentials=True, subdirs=None):
    # prioritize_channels returns a dict with platform-specific channel
    #   urls as the key, and a tuple of canonical channel name and channel priority
    #   number as the value
    # ('https://conda.anaconda.org/conda-forge/osx-64/', ('conda-forge', 1))
    channels = chain.from_iterable(
        (Channel(cc) for cc in c._channels) if isinstance(c, MultiChannel) else (c,)
        for c in (Channel(c) for c in channels)
    )
    result = {}
    for priority_counter, chn in enumerate(channels):
        channel = Channel(chn)
        for url in channel.urls(with_credentials, subdirs):
            if url in result:
                continue
            result[url] = (
                channel.canonical_name,
                min(priority_counter, MAX_CHANNEL_PRIORITY - 1),
            )
    return result


def all_channel_urls(channels, subdirs=None, with_credentials=True):
    result = IndexedSet()
    for chn in channels:
        channel = Channel(chn)
        result.update(channel.urls(with_credentials, subdirs))
    return result


def offline_keep(url):
    return not context.offline or not is_url(url) or url.startswith("file:/")


def get_channel_objs(ctx: Context):
    """Return current channels as Channel objects"""
    return tuple(Channel(chn) for chn in ctx.channels)


context.register_reset_callaback(Channel._reset_state)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Implements directed graphs to sort and manipulate packages within a prefix.

Object inheritance:

.. autoapi-inheritance-diagram:: PrefixGraph GeneralGraph
   :top-classes: conda.models.prefix_graph.PrefixGraph
   :parts: 1
"""

from collections import defaultdict
from logging import getLogger

from boltons.setutils import IndexedSet

from ..base.context import context
from ..common.compat import on_win
from ..exceptions import CyclicalDependencyError
from .enums import NoarchType
from .match_spec import MatchSpec

log = getLogger(__name__)


class PrefixGraph:
    """
    A directed graph structure used for sorting packages (prefix_records) in prefixes and
    manipulating packages within prefixes (e.g. removing and pruning).

    The terminology used for edge direction is "parents" and "children" rather than "successors"
    and "predecessors". The parent nodes of a record are those records in the graph that
    match the record's "depends" field.  E.g. NodeA depends on NodeB, then NodeA is a child
    of NodeB, and NodeB is a parent of NodeA.  Nodes can have zero parents, or more than two
    parents.

    Most public methods mutate the graph.
    """

    def __init__(self, records, specs=()):
        records = tuple(records)
        specs = set(specs)
        self.graph = graph = {}  # dict[PrefixRecord, set[PrefixRecord]]
        self.spec_matches = spec_matches = {}  # dict[PrefixRecord, set[MatchSpec]]
        for node in records:
            parent_match_specs = tuple(MatchSpec(d) for d in node.depends)
            parent_nodes = {
                rec for rec in records if any(m.match(rec) for m in parent_match_specs)
            }
            graph[node] = parent_nodes
            matching_specs = IndexedSet(s for s in specs if s.match(node))
            if matching_specs:
                spec_matches[node] = matching_specs

        self._toposort()

    def remove_spec(self, spec):
        """
        Remove all matching nodes, and any associated child nodes.

        Args:
            spec (MatchSpec):

        Returns:
            tuple[PrefixRecord]: The removed nodes.

        """
        node_matches = {node for node in self.graph if spec.match(node)}

        # If the spec was a track_features spec, then we need to also remove every
        # package with a feature that matches the track_feature.
        for feature_name in spec.get_raw_value("track_features") or ():
            feature_spec = MatchSpec(features=feature_name)
            node_matches.update(node for node in self.graph if feature_spec.match(node))

        remove_these = set()
        for node in node_matches:
            remove_these.add(node)
            remove_these.update(self.all_descendants(node))
        remove_these = tuple(filter(lambda node: node in remove_these, self.graph))
        for node in remove_these:
            self._remove_node(node)
        self._toposort()
        return tuple(remove_these)

    def remove_youngest_descendant_nodes_with_specs(self):
        """
        A specialized method used to determine only dependencies of requested specs.

        Returns:
            tuple[PrefixRecord]: The removed nodes.

        """
        graph = self.graph
        spec_matches = self.spec_matches
        inverted_graph = {
            node: {key for key in graph if node in graph[key]} for node in graph
        }
        youngest_nodes_with_specs = tuple(
            node
            for node, children in inverted_graph.items()
            if not children and node in spec_matches
        )
        removed_nodes = tuple(
            filter(lambda node: node in youngest_nodes_with_specs, self.graph)
        )
        for node in removed_nodes:
            self._remove_node(node)
        self._toposort()
        return removed_nodes

    @property
    def records(self):
        return iter(self.graph)

    def prune(self):
        """Prune back all packages until all child nodes are anchored by a spec.

        Returns:
            tuple[PrefixRecord]: The pruned nodes.

        """
        graph = self.graph
        spec_matches = self.spec_matches
        original_order = tuple(self.graph)

        removed_nodes = set()
        while True:
            inverted_graph = {
                node: {key for key in graph if node in graph[key]} for node in graph
            }
            prunable_nodes = tuple(
                node
                for node, children in inverted_graph.items()
                if not children and node not in spec_matches
            )
            if not prunable_nodes:
                break
            for node in prunable_nodes:
                removed_nodes.add(node)
                self._remove_node(node)

        removed_nodes = tuple(
            filter(lambda node: node in removed_nodes, original_order)
        )
        self._toposort()
        return removed_nodes

    def get_node_by_name(self, name):
        return next(rec for rec in self.graph if rec.name == name)

    def all_descendants(self, node):
        graph = self.graph
        inverted_graph = {
            node: {key for key in graph if node in graph[key]} for node in graph
        }

        nodes = [node]
        nodes_seen = set()
        q = 0
        while q < len(nodes):
            for child_node in inverted_graph[nodes[q]]:
                if child_node not in nodes_seen:
                    nodes_seen.add(child_node)
                    nodes.append(child_node)
            q += 1
        return tuple(filter(lambda node: node in nodes_seen, graph))

    def all_ancestors(self, node):
        graph = self.graph
        nodes = [node]
        nodes_seen = set()
        q = 0
        while q < len(nodes):
            for parent_node in graph[nodes[q]]:
                if parent_node not in nodes_seen:
                    nodes_seen.add(parent_node)
                    nodes.append(parent_node)
            q += 1
        return tuple(filter(lambda node: node in nodes_seen, graph))

    def _remove_node(self, node):
        """Removes this node and all edges referencing it."""
        graph = self.graph
        if node not in graph:
            raise KeyError(f"node {node} does not exist")
        graph.pop(node)
        self.spec_matches.pop(node, None)

        for node, edges in graph.items():
            if node in edges:
                edges.remove(node)

    def _toposort(self):
        graph_copy = {node: IndexedSet(parents) for node, parents in self.graph.items()}
        self._toposort_prepare_graph(graph_copy)
        if context.allow_cycles:
            sorted_nodes = tuple(self._topo_sort_handle_cycles(graph_copy))
        else:
            sorted_nodes = tuple(self._toposort_raise_on_cycles(graph_copy))
        original_graph = self.graph
        self.graph = {node: original_graph[node] for node in sorted_nodes}
        return sorted_nodes

    @classmethod
    def _toposort_raise_on_cycles(cls, graph):
        if not graph:
            return

        while True:
            no_parent_nodes = IndexedSet(
                sorted(
                    (node for node, parents in graph.items() if len(parents) == 0),
                    key=lambda x: x.name,
                )
            )
            if not no_parent_nodes:
                break

            for node in no_parent_nodes:
                yield node
                graph.pop(node, None)

            for parents in graph.values():
                parents -= no_parent_nodes

        if len(graph) != 0:
            raise CyclicalDependencyError(tuple(graph))

    @classmethod
    def _topo_sort_handle_cycles(cls, graph):
        # remove edges that point directly back to the node
        for k, v in graph.items():
            v.discard(k)

        # disconnected nodes go first
        nodes_that_are_parents = {
            node for parents in graph.values() for node in parents
        }
        nodes_without_parents = (node for node in graph if not graph[node])
        disconnected_nodes = sorted(
            (
                node
                for node in nodes_without_parents
                if node not in nodes_that_are_parents
            ),
            key=lambda x: x.name,
        )
        yield from disconnected_nodes

        t = cls._toposort_raise_on_cycles(graph)

        while True:
            try:
                value = next(t)
                yield value
            except CyclicalDependencyError as e:
                # TODO: Turn this into a warning, but without being too annoying with
                #       multiple messages.  See https://github.com/conda/conda/issues/4067
                log.debug("%r", e)

                yield cls._toposort_pop_key(graph)

                t = cls._toposort_raise_on_cycles(graph)
                continue

            except StopIteration:
                return

    @staticmethod
    def _toposort_pop_key(graph):
        """
        Pop an item from the graph that has the fewest parents.
        In the case of a tie, use the node with the alphabetically-first package name.
        """
        node_with_fewest_parents = sorted(
            (len(parents), node.dist_str(), node) for node, parents in graph.items()
        )[0][2]
        graph.pop(node_with_fewest_parents)

        for parents in graph.values():
            parents.discard(node_with_fewest_parents)

        return node_with_fewest_parents

    @staticmethod
    def _toposort_prepare_graph(graph):
        # There are currently at least three special cases to be aware of.

        # 1. Remove any circular dependency between python and pip. This typically comes about
        #    because of the add_pip_as_python_dependency configuration parameter.
        for node in graph:
            if node.name == "python":
                parents = graph[node]
                for parent in tuple(parents):
                    if parent.name == "pip":
                        parents.remove(parent)

        # 2. Special case code for menuinst.
        #    Always link/unlink menuinst first/last in case a subsequent
        #    package tries to import it to create/remove a shortcut.
        menuinst_node = next((node for node in graph if node.name == "menuinst"), None)
        python_node = next((node for node in graph if node.name == "python"), None)
        if menuinst_node:
            # add menuinst as a parent if python is a parent and the node
            # isn't a parent of menuinst
            assert python_node is not None
            menuinst_parents = graph[menuinst_node]
            for node, parents in graph.items():
                if python_node in parents and node not in menuinst_parents:
                    parents.add(menuinst_node)

        if on_win:
            # 3. On windows, python noarch packages need an implicit dependency on conda added, if
            #    conda is in the list of packages for the environment.  Python noarch packages
            #    that have entry points use conda's own conda.exe python entry point binary. If
            #    conda is going to be updated during an operation, the unlink / link order matters.
            #    See issue #6057.
            conda_node = next((node for node in graph if node.name == "conda"), None)
            if conda_node:
                # add conda as a parent if python is a parent and node isn't a parent of conda
                conda_parents = graph[conda_node]
                for node, parents in graph.items():
                    if (
                        hasattr(node, "noarch")
                        and node.noarch == NoarchType.python
                        and node not in conda_parents
                    ):
                        parents.add(conda_node)


#     def dot_repr(self, title=None):  # pragma: no cover
#         # graphviz DOT graph description language
#
#         builder = ['digraph g {']
#         if title:
#             builder.append('  labelloc="t";')
#             builder.append('  label="%s";' % title)
#         builder.append('  size="10.5,8";')
#         builder.append('  rankdir=BT;')
#         for node in self.get_nodes_ordered_from_roots():
#             label = "%s %s" % (node.record.name, node.record.version)
#             if node.specs:
#                 # TODO: combine?
#                 spec = next(iter(node.specs))
#                 label += "\\n%s" % ("?%s" if spec.optional else "%s") % spec
#             if node.is_orphan:
#                 shape = "box"
#             elif node.is_root:
#                 shape = "invhouse"
#             elif node.is_leaf:
#                 shape = "house"
#             else:
#                 shape = "ellipse"
#             builder.append('  "%s" [label="%s", shape=%s];' % (node.record.name, label, shape))
#             for child in node.required_children:
#                 builder.append('    "%s" -> "%s";' % (child.record.name, node.record.name))
#             for child in node.optional_children:
#                 builder.append('    "%s -> "%s" [color=lightgray];' % (child.record.name,
#                                                                        node.record.name))
#         builder.append('}')
#         return '\n'.join(builder)
#
#     def format_url(self):  # pragma: no cover
#         return "https://condaviz.glitch.me/%s" % url_quote(self.dot_repr())
#
#     def request_svg(self):  # pragma: no cover
#         from tempfile import NamedTemporaryFile
#         import requests
#         from ..common.compat import ensure_binary
#         response = requests.post("https://condaviz.glitch.me/post",
#                                  data={"digraph": self.dot_repr()})
#         response.raise_for_status()
#         with NamedTemporaryFile(suffix='.svg', delete=False) as fh:
#             fh.write(ensure_binary(response.text))
#         print("saved to: %s" % fh.name, file=sys.stderr)
#         return fh.name
#
#     def open_url(self):  # pragma: no cover
#         import webbrowser
#         from ..common.url import path_to_url
#         location = self.request_svg()
#         try:
#             browser = webbrowser.get("safari")
#         except webbrowser.Error:
#             browser = webbrowser.get()
#         browser.open_new_tab(path_to_url(location))


class GeneralGraph(PrefixGraph):
    """
    Compared with PrefixGraph, this class takes in more than one record of a given name,
    and operates on that graph from the higher view across any matching dependencies.  It is
    not a Prefix thing, but more like a "graph of all possible candidates" thing, and is used
    for unsatisfiability analysis
    """

    def __init__(self, records, specs=()):
        records = tuple(records)
        super().__init__(records, specs)
        self.specs_by_name = defaultdict(dict)
        for node in records:
            parent_dict = self.specs_by_name.get(node.name, {})
            for dep in tuple(MatchSpec(d) for d in node.depends):
                deps = parent_dict.get(dep.name, set())
                deps.add(dep)
                parent_dict[dep.name] = deps
            self.specs_by_name[node.name] = parent_dict

        consolidated_graph = {}
        # graph is toposorted, so looping over it is in dependency order
        for node, parent_nodes in reversed(list(self.graph.items())):
            cg = consolidated_graph.get(node.name, set())
            cg.update(_.name for _ in parent_nodes)
            consolidated_graph[node.name] = cg
        self.graph_by_name = consolidated_graph

    def breadth_first_search_by_name(self, root_spec, target_spec):
        """Return shorted path from root_spec to spec_name"""
        queue = []
        queue.append([root_spec])
        visited = []
        while queue:
            path = queue.pop(0)
            node = path[-1]
            if node in visited:
                continue
            visited.append(node)
            if node == target_spec:
                return path
            children = []
            specs = self.specs_by_name.get(node.name)
            if specs is None:
                continue
            for _, deps in specs.items():
                children.extend(list(deps))
            for adj in children:
                if adj.name == target_spec.name and adj.version != target_spec.version:
                    pass
                else:
                    new_path = list(path)
                    new_path.append(adj)
                    queue.append(new_path)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Collection of enums used throughout conda."""

import sys
from enum import Enum
from platform import machine

from ..auxlib.decorators import classproperty
from ..auxlib.ish import dals
from ..auxlib.type_coercion import TypeCoercionError, boolify
from ..deprecations import deprecated
from ..exceptions import CondaUpgradeError


class Arch(Enum):
    x86 = "x86"
    x86_64 = "x86_64"
    # arm64 is for macOS and Windows
    arm64 = "arm64"
    armv6l = "armv6l"
    armv7l = "armv7l"
    # aarch64 is for Linux only
    aarch64 = "aarch64"
    ppc64 = "ppc64"
    ppc64le = "ppc64le"
    riscv64 = "riscv64"
    s390x = "s390x"
    wasm32 = "wasm32"
    z = "z"

    @classmethod
    def from_sys(cls):
        if sys.platform == "zos":
            return cls["z"]
        return cls[machine()]

    def __json__(self):
        return self.value


class Platform(Enum):
    freebsd = "freebsd"
    linux = "linux"
    win = "win32"
    openbsd = "openbsd5"
    osx = "darwin"
    zos = "zos"
    emscripten = "emscripten"
    wasi = "wasi"

    @classmethod
    def from_sys(cls):
        return cls(sys.platform)

    def __json__(self):
        return self.value


class FileMode(Enum):
    text = "text"
    binary = "binary"

    def __str__(self):
        return f"{self.value}"


class LinkType(Enum):
    # directory is not a link type, and copy is not a path type
    # LinkType is still probably the best name here
    hardlink = 1
    softlink = 2
    copy = 3
    directory = 4

    def __int__(self):
        return self.value

    def __str__(self):
        return self.name

    def __json__(self):
        return self.name


class PathType(Enum):
    """
    Refers to if the file in question is hard linked or soft linked. Originally designed to be used
    in paths.json
    """

    hardlink = "hardlink"
    softlink = "softlink"
    directory = "directory"

    # these additional types should not be included by conda-build in packages
    linked_package_record = (
        "linked_package_record"  # a package's .json file in conda-meta
    )
    pyc_file = "pyc_file"
    unix_python_entry_point = "unix_python_entry_point"
    windows_python_entry_point_script = "windows_python_entry_point_script"
    windows_python_entry_point_exe = "windows_python_entry_point_exe"

    @classproperty
    def basic_types(self):
        return (PathType.hardlink, PathType.softlink, PathType.directory)

    def __str__(self):
        return self.name

    def __json__(self):
        return self.name


class LeasedPathType(Enum):
    application_entry_point = "application_entry_point"
    application_entry_point_windows_exe = "application_entry_point_windows_exe"
    application_softlink = "application_softlink"

    def __str__(self):
        return self.name

    def __json__(self):
        return self.name


deprecated.constant("24.3", "24.9", "LeasedPathType", LeasedPathType)
del LeasedPathType


class PackageType(Enum):
    NOARCH_GENERIC = "noarch_generic"
    NOARCH_PYTHON = "noarch_python"
    VIRTUAL_PRIVATE_ENV = "virtual_private_env"
    VIRTUAL_PYTHON_WHEEL = "virtual_python_wheel"  # manageable
    VIRTUAL_PYTHON_EGG_MANAGEABLE = "virtual_python_egg_manageable"
    VIRTUAL_PYTHON_EGG_UNMANAGEABLE = "virtual_python_egg_unmanageable"
    VIRTUAL_PYTHON_EGG_LINK = "virtual_python_egg_link"  # unmanageable
    VIRTUAL_SYSTEM = "virtual_system"  # virtual packages representing system attributes

    @staticmethod
    def conda_package_types():
        return {
            None,
            PackageType.NOARCH_GENERIC,
            PackageType.NOARCH_PYTHON,
        }

    @staticmethod
    def unmanageable_package_types():
        return {
            PackageType.VIRTUAL_PYTHON_EGG_UNMANAGEABLE,
            PackageType.VIRTUAL_PYTHON_EGG_LINK,
            PackageType.VIRTUAL_SYSTEM,
        }


class NoarchType(Enum):
    generic = "generic"
    python = "python"

    @staticmethod
    def coerce(val):
        # what a mess
        if isinstance(val, NoarchType):
            return val
        valtype = getattr(val, "type", None)
        if isinstance(valtype, NoarchType):  # see issue #8311
            return valtype
        if isinstance(val, bool):
            val = NoarchType.generic if val else None
        if isinstance(val, str):
            val = val.lower()
            if val == "python":
                val = NoarchType.python
            elif val == "generic":
                val = NoarchType.generic
            else:
                try:
                    val = NoarchType.generic if boolify(val) else None
                except TypeCoercionError:
                    raise CondaUpgradeError(
                        dals(
                            f"""
                    The noarch type for this package is set to '{val}'.
                    The current version of conda is too old to install this package.
                    Please update conda.
                    """
                        )
                    )
        return val


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Models are data transfer objects or "light-weight" domain objects with no appreciable logic
other than their own validation. Models are used to pass data between layers of the stack. In
many ways they are similar to ORM objects.  Unlike ORM objects, they are NOT themselves allowed
to load data from a remote resource.  Thought of another way, they cannot import from
``conda.gateways``, but rather ``conda.gateways`` imports from ``conda.models`` as appropriate
to create model objects from remote resources.

Conda modules importable from ``conda.models`` are

- ``conda._vendor``
- ``conda.common``
- ``conda.models``

"""


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Implements the query language for conda packages (a.k.a, MatchSpec).

The MatchSpec is the conda package specification (e.g. `conda==23.3`, `python<3.7`,
`cryptography * *_0`) and is used to communicate the desired packages to install.
"""

import re
import warnings
from abc import ABCMeta, abstractmethod, abstractproperty
from collections.abc import Mapping
from functools import reduce
from itertools import chain
from logging import getLogger
from operator import attrgetter
from os.path import basename

from ..auxlib.decorators import memoizedproperty
from ..base.constants import CONDA_PACKAGE_EXTENSION_V1, CONDA_PACKAGE_EXTENSION_V2
from ..base.context import context
from ..common.compat import isiterable
from ..common.io import dashlist
from ..common.iterators import groupby_to_dict as groupby
from ..common.path import expand, is_package_file, strip_pkg_extension, url_to_path
from ..common.url import is_url, path_to_url, unquote
from ..exceptions import InvalidMatchSpec, InvalidSpec
from .channel import Channel
from .version import BuildNumberMatch, VersionSpec

try:
    from frozendict import frozendict
except ImportError:
    from ..auxlib.collection import frozendict

log = getLogger(__name__)


class MatchSpecType(type):
    def __call__(cls, spec_arg=None, **kwargs):
        try:
            if spec_arg:
                if isinstance(spec_arg, MatchSpec) and not kwargs:
                    return spec_arg
                elif isinstance(spec_arg, MatchSpec):
                    new_kwargs = dict(spec_arg._match_components)
                    new_kwargs.setdefault("optional", spec_arg.optional)
                    new_kwargs.setdefault("target", spec_arg.target)
                    new_kwargs["_original_spec_str"] = spec_arg.original_spec_str
                    new_kwargs.update(**kwargs)
                    return super().__call__(**new_kwargs)
                elif isinstance(spec_arg, str):
                    parsed = _parse_spec_str(spec_arg)
                    if kwargs:
                        parsed = dict(parsed, **kwargs)
                        if set(kwargs) - {"optional", "target"}:
                            # if kwargs has anything but optional and target,
                            # strip out _original_spec_str from parsed
                            parsed.pop("_original_spec_str", None)
                    return super().__call__(**parsed)
                elif isinstance(spec_arg, Mapping):
                    parsed = dict(spec_arg, **kwargs)
                    return super().__call__(**parsed)
                elif hasattr(spec_arg, "to_match_spec"):
                    spec = spec_arg.to_match_spec()
                    if kwargs:
                        return MatchSpec(spec, **kwargs)
                    else:
                        return spec
                else:
                    raise InvalidSpec(
                        f"Invalid MatchSpec:\n  spec_arg={spec_arg}\n  kwargs={kwargs}"
                    )
            else:
                return super().__call__(**kwargs)
        except InvalidSpec as e:
            msg = ""
            if spec_arg:
                msg += f"{spec_arg}"
            if kwargs:
                msg += " " + ", ".join(f"{k}={v}" for k, v in kwargs.items())
            raise InvalidMatchSpec(msg, details=e) from e


class MatchSpec(metaclass=MatchSpecType):
    """The query language for conda packages.

    Any of the fields that comprise a :class:`PackageRecord` can be used to compose a
    :class:`MatchSpec`.

    :class:`MatchSpec` can be composed with keyword arguments, where keys are any of the
    attributes of :class:`PackageRecord`.  Values for keyword arguments are the exact values the
    attribute should match against.  Many fields can also be matched against non-exact values--by
    including wildcard `*` and `>`/`<` ranges--where supported.  Any non-specified field is
    the equivalent of a full wildcard match.

    :class:`MatchSpec` can also be composed using a single positional argument, with optional
    keyword arguments.  Keyword arguments also override any conflicting information provided in
    the positional argument.  The positional argument can be either an existing :class:`MatchSpec`
    instance or a string.  Conda has historically had several string representations for equivalent
    :class:`MatchSpec`s.  This :class:`MatchSpec` should accept any existing valid spec string, and
    correctly compose a :class:`MatchSpec` instance.

    A series of rules are now followed for creating the canonical string representation of a
    :class:`MatchSpec` instance.  The canonical string representation can generically be
    represented by

        (channel(/subdir):(namespace):)name(version(build))[key1=value1,key2=value2]

    where `()` indicate optional fields.  The rules for constructing a canonical string
    representation are:

    1. `name` (i.e. "package name") is required, but its value can be '*'.  Its position is always
       outside the key-value brackets.
    2. If `version` is an exact version, it goes outside the key-value brackets and is prepended
       by `==`. If `version` is a "fuzzy" value (e.g. `1.11.*`), it goes outside the key-value
       brackets with the `.*` left off and is prepended by `=`.  Otherwise `version` is included
       inside key-value brackets.
    3. If `version` is an exact version, and `build` is an exact value, `build` goes outside
       key-value brackets prepended by a `=`.  Otherwise, `build` goes inside key-value brackets.
       `build_string` is an alias for `build`.
    4. The `namespace` position is being held for a future conda feature.
    5. If `channel` is included and is an exact value, a `::` separator is ued between `channel`
       and `name`.  `channel` can either be a canonical channel name or a channel url.  In the
       canonical string representation, the canonical channel name will always be used.
    6. If `channel` is an exact value and `subdir` is an exact value, `subdir` is appended to
       `channel` with a `/` separator.  Otherwise, `subdir` is included in the key-value brackets.
    7. Key-value brackets can be delimited by comma, space, or comma+space.  Value can optionally
       be wrapped in single or double quotes, but must be wrapped if `value` contains a comma,
       space, or equal sign.  The canonical format uses comma delimiters and single quotes.
    8. When constructing a :class:`MatchSpec` instance from a string, any key-value pair given
       inside the key-value brackets overrides any matching parameter given outside the brackets.

    When :class:`MatchSpec` attribute values are simple strings, the are interpreted using the
    following conventions:

      - If the string begins with `^` and ends with `$`, it is converted to a regex.
      - If the string contains an asterisk (`*`), it is transformed from a glob to a regex.
      - Otherwise, an exact match to the string is sought.


    Examples:
        >>> str(MatchSpec(name='foo', build='py2*', channel='conda-forge'))
        'conda-forge::foo[build=py2*]'
        >>> str(MatchSpec('foo 1.0 py27_0'))
        'foo==1.0=py27_0'
        >>> str(MatchSpec('foo=1.0=py27_0'))
        'foo==1.0=py27_0'
        >>> str(MatchSpec('conda-forge::foo[version=1.0.*]'))
        'conda-forge::foo=1.0'
        >>> str(MatchSpec('conda-forge/linux-64::foo>=1.0'))
        "conda-forge/linux-64::foo[version='>=1.0']"
        >>> str(MatchSpec('*/linux-64::foo>=1.0'))
        "foo[subdir=linux-64,version='>=1.0']"

    To fully-specify a package with a full, exact spec, the fields
      - channel
      - subdir
      - name
      - version
      - build
    must be given as exact values.  In the future, the namespace field will be added to this list.
    Alternatively, an exact spec is given by '*[md5=12345678901234567890123456789012]'
    or '*[sha256=f453db4ffe2271ec492a2913af4e61d4a6c118201f07de757df0eff769b65d2e]'.
    """

    FIELD_NAMES = (
        "channel",
        "subdir",
        "name",
        "version",
        "build",
        "build_number",
        "track_features",
        "features",
        "url",
        "md5",
        "sha256",
        "license",
        "license_family",
        "fn",
    )
    FIELD_NAMES_SET = frozenset(FIELD_NAMES)
    _MATCHER_CACHE = {}

    def __init__(self, optional=False, target=None, **kwargs):
        self._optional = optional
        self._target = target
        self._original_spec_str = kwargs.pop("_original_spec_str", None)
        self._match_components = self._build_components(**kwargs)

    @classmethod
    def from_dist_str(cls, dist_str):
        parts = {}
        if dist_str[-len(CONDA_PACKAGE_EXTENSION_V2) :] == CONDA_PACKAGE_EXTENSION_V2:
            dist_str = dist_str[: -len(CONDA_PACKAGE_EXTENSION_V2)]
        elif dist_str[-len(CONDA_PACKAGE_EXTENSION_V1) :] == CONDA_PACKAGE_EXTENSION_V1:
            dist_str = dist_str[: -len(CONDA_PACKAGE_EXTENSION_V1)]
        if "::" in dist_str:
            channel_subdir_str, dist_str = dist_str.split("::", 1)
            if "/" in channel_subdir_str:
                channel_str, subdir = channel_subdir_str.rsplit("/", 1)
                if subdir not in context.known_subdirs:
                    channel_str = channel_subdir_str
                    subdir = None
                parts["channel"] = channel_str
                if subdir:
                    parts["subdir"] = subdir
            else:
                parts["channel"] = channel_subdir_str

        name, version, build = dist_str.rsplit("-", 2)
        parts.update(
            {
                "name": name,
                "version": version,
                "build": build,
            }
        )
        return cls(**parts)

    def get_exact_value(self, field_name):
        v = self._match_components.get(field_name)
        return v and v.exact_value

    def get_raw_value(self, field_name):
        v = self._match_components.get(field_name)
        return v and v.raw_value

    def get(self, field_name, default=None):
        v = self.get_raw_value(field_name)
        return default if v is None else v

    @property
    def is_name_only_spec(self):
        return (
            len(self._match_components) == 1
            and "name" in self._match_components
            and self.name != "*"
        )

    def dist_str(self):
        return self.__str__()

    @property
    def optional(self):
        return self._optional

    @property
    def target(self):
        return self._target

    @property
    def original_spec_str(self):
        return self._original_spec_str

    def match(self, rec):
        """
        Accepts a `PackageRecord` or a dict, and matches can pull from any field
        in that record.  Returns True for a match, and False for no match.
        """
        if isinstance(rec, dict):
            # TODO: consider AttrDict instead of PackageRecord
            from .records import PackageRecord

            rec = PackageRecord.from_objects(rec)
        for field_name, v in self._match_components.items():
            if not self._match_individual(rec, field_name, v):
                return False
        return True

    def _match_individual(self, record, field_name, match_component):
        val = getattr(record, field_name)
        try:
            return match_component.match(val)
        except AttributeError:
            return match_component == val

    def _is_simple(self):
        return (
            len(self._match_components) == 1
            and self.get_exact_value("name") is not None
        )

    def _is_single(self):
        return len(self._match_components) == 1

    def _to_filename_do_not_use(self):
        # WARNING: this is potentially unreliable and use should probably be limited
        #   returns None if a filename can't be constructed
        fn_field = self.get_exact_value("fn")
        if fn_field:
            return fn_field
        vals = tuple(self.get_exact_value(x) for x in ("name", "version", "build"))
        if not any(x is None for x in vals):
            return ("{}-{}-{}".format(*vals)) + CONDA_PACKAGE_EXTENSION_V1
        else:
            return None

    def __repr__(self):
        builder = [f'{self.__class__.__name__}("{self}"']
        if self.target:
            builder.append(f', target="{self.target}"')
        if self.optional:
            builder.append(", optional=True")
        builder.append(")")
        return "".join(builder)

    def __str__(self):
        builder = []
        brackets = []

        channel_matcher = self._match_components.get("channel")
        if channel_matcher and channel_matcher.exact_value:
            builder.append(str(channel_matcher))
        elif channel_matcher and not channel_matcher.matches_all:
            brackets.append(f"channel={str(channel_matcher)}")

        subdir_matcher = self._match_components.get("subdir")
        if subdir_matcher:
            if channel_matcher and channel_matcher.exact_value:
                builder.append(f"/{subdir_matcher}")
            else:
                brackets.append(f"subdir={subdir_matcher}")

        name_matcher = self._match_components.get("name", "*")
        builder.append(("::%s" if builder else "%s") % name_matcher)

        version = self._match_components.get("version")
        build = self._match_components.get("build")
        version_exact = False
        if version:
            version = str(version)
            if any(s in version for s in "><$^|,"):
                brackets.append(f"version='{version}'")
            elif version[:2] in ("!=", "~="):
                if build:
                    brackets.append(f"version='{version}'")
                else:
                    builder.append(version)
            elif version[-2:] == ".*":
                builder.append("=" + version[:-2])
            elif version[-1] == "*":
                builder.append("=" + version[:-1])
            elif version.startswith("=="):
                builder.append(version)
                version_exact = True
            else:
                builder.append("==" + version)
                version_exact = True

        if build:
            build = str(build)
            if any(s in build for s in "><$^|,"):
                brackets.append(f"build='{build}'")
            elif "*" in build:
                brackets.append(f"build={build}")
            elif version_exact:
                builder.append("=" + build)
            else:
                brackets.append(f"build={build}")

        _skip = {"channel", "subdir", "name", "version", "build"}
        if "url" in self._match_components and "fn" in self._match_components:
            _skip.add("fn")
        for key in self.FIELD_NAMES:
            if key not in _skip and key in self._match_components:
                if key == "url" and channel_matcher:
                    # skip url in canonical str if channel already included
                    continue
                value = str(self._match_components[key])
                if any(s in value for s in ", ="):
                    brackets.append(f"{key}='{value}'")
                else:
                    brackets.append(f"{key}={value}")

        if brackets:
            builder.append("[{}]".format(",".join(brackets)))

        return "".join(builder)

    def __json__(self):
        return self.__str__()

    def conda_build_form(self):
        builder = []
        name = self.get_exact_value("name")
        assert name
        builder.append(name)

        build = self.get_raw_value("build")
        version = self.get_raw_value("version")

        if build:
            assert version
            builder += [version, build]
        elif version:
            builder.append(version)

        return " ".join(builder)

    def __eq__(self, other):
        if isinstance(other, MatchSpec):
            return self._hash_key == other._hash_key
        else:
            return False

    def __hash__(self):
        return hash(self._hash_key)

    @memoizedproperty
    def _hash_key(self):
        return self._match_components, self.optional, self.target

    def __contains__(self, field):
        return field in self._match_components

    def _build_components(self, **kwargs):
        not_fields = set(kwargs) - MatchSpec.FIELD_NAMES_SET
        if not_fields:
            raise InvalidMatchSpec(
                self._original_spec_str, f"Cannot match on field(s): {not_fields}"
            )
        _make_component = MatchSpec._make_component
        return frozendict(_make_component(key, value) for key, value in kwargs.items())

    @staticmethod
    def _make_component(field_name, value):
        if hasattr(value, "match"):
            matcher = value
            return field_name, matcher

        _MATCHER_CACHE = MatchSpec._MATCHER_CACHE
        cache_key = (field_name, value)
        cached_matcher = _MATCHER_CACHE.get(cache_key)
        if cached_matcher:
            return field_name, cached_matcher
        if field_name in _implementors:
            matcher = _implementors[field_name](value)
        else:
            matcher = ExactStrMatch(str(value))
        _MATCHER_CACHE[(field_name, value)] = matcher
        return field_name, matcher

    @property
    def name(self):
        return self.get_exact_value("name") or "*"

    #
    # Remaining methods are for back compatibility with conda-build. Do not remove
    # without coordination with the conda-build team.
    #
    @property
    def strictness(self):
        # With the old MatchSpec, strictness==3 if name, version, and
        # build were all specified.
        s = sum(f in self._match_components for f in ("name", "version", "build"))
        if s < len(self._match_components):
            return 3
        elif not self.get_exact_value("name") or "build" in self._match_components:
            return 3
        elif "version" in self._match_components:
            return 2
        else:
            return 1

    @property
    def spec(self):
        return self.conda_build_form()

    @property
    def version(self):
        # in the old MatchSpec object, version was a VersionSpec, not a str
        # so we'll keep that API here
        return self._match_components.get("version")

    @property
    def fn(self):
        val = self.get_raw_value("fn") or self.get_raw_value("url")
        if val:
            val = basename(val)
        assert val
        return val

    @classmethod
    def merge(cls, match_specs, union=False):
        match_specs = sorted(tuple(cls(s) for s in match_specs if s), key=str)
        name_groups = groupby(attrgetter("name"), match_specs)
        unmergeable = name_groups.pop("*", []) + name_groups.pop(None, [])

        merged_specs = []
        mergeable_groups = tuple(
            chain.from_iterable(
                groupby(lambda s: s.optional, group).values()
                for group in name_groups.values()
            )
        )
        for group in mergeable_groups:
            target_groups = groupby(attrgetter("target"), group)
            target_groups.pop(None, None)
            if len(target_groups) > 1:
                raise ValueError(f"Incompatible MatchSpec merge:{dashlist(group)}")
            merged_specs.append(
                reduce(lambda x, y: x._merge(y, union), group)
                if len(group) > 1
                else group[0]
            )
        return (*merged_specs, *unmergeable)

    @classmethod
    def union(cls, match_specs):
        return cls.merge(match_specs, union=True)

    def _merge(self, other, union=False):
        if self.optional != other.optional or self.target != other.target:
            raise ValueError(f"Incompatible MatchSpec merge:  - {self}\n  - {other}")

        final_components = {}
        component_names = set(self._match_components) | set(other._match_components)
        for component_name in component_names:
            this_component = self._match_components.get(component_name)
            that_component = other._match_components.get(component_name)
            if this_component is None and that_component is None:
                continue
            elif this_component is None:
                final_components[component_name] = that_component
            elif that_component is None:
                final_components[component_name] = this_component
            else:
                if union:
                    try:
                        final = this_component.union(that_component)
                    except (AttributeError, ValueError, TypeError):
                        final = f"{this_component}|{that_component}"
                else:
                    final = this_component.merge(that_component)
                final_components[component_name] = final
        return self.__class__(
            optional=self.optional, target=self.target, **final_components
        )


def _parse_version_plus_build(v_plus_b):
    """This should reliably pull the build string out of a version + build string combo.
    Examples:
        >>> _parse_version_plus_build("=1.2.3 0")
        ('=1.2.3', '0')
        >>> _parse_version_plus_build("1.2.3=0")
        ('1.2.3', '0')
        >>> _parse_version_plus_build(">=1.0 , < 2.0 py34_0")
        ('>=1.0,<2.0', 'py34_0')
        >>> _parse_version_plus_build(">=1.0 , < 2.0 =py34_0")
        ('>=1.0,<2.0', 'py34_0')
        >>> _parse_version_plus_build("=1.2.3 ")
        ('=1.2.3', None)
        >>> _parse_version_plus_build(">1.8,<2|==1.7")
        ('>1.8,<2|==1.7', None)
        >>> _parse_version_plus_build("* openblas_0")
        ('*', 'openblas_0')
        >>> _parse_version_plus_build("* *")
        ('*', '*')
    """
    parts = re.search(
        r"((?:.+?)[^><!,|]?)(?:(?<![=!|,<>~])(?:[ =])([^-=,|<>~]+?))?$", v_plus_b
    )
    if parts:
        version, build = parts.groups()
        build = build and build.strip()
    else:
        version, build = v_plus_b, None
    return version and version.replace(" ", ""), build


def _parse_legacy_dist(dist_str):
    """
    Examples:
        >>> _parse_legacy_dist("_license-1.1-py27_1.tar.bz2")
        ('_license', '1.1', 'py27_1')
        >>> _parse_legacy_dist("_license-1.1-py27_1")
        ('_license', '1.1', 'py27_1')
    """
    dist_str, _ = strip_pkg_extension(dist_str)
    name, version, build = dist_str.rsplit("-", 2)
    return name, version, build


def _parse_channel(channel_val):
    if not channel_val:
        return None, None
    chn = Channel(channel_val)
    channel_name = chn.name or chn.base_url
    return channel_name, chn.subdir


_PARSE_CACHE = {}


def _parse_spec_str(spec_str):
    cached_result = _PARSE_CACHE.get(spec_str)
    if cached_result:
        return cached_result

    original_spec_str = spec_str

    # pre-step for ugly backward compat
    if spec_str.endswith("@"):
        feature_name = spec_str[:-1]
        return {
            "name": "*",
            "track_features": (feature_name,),
        }

    # Step 1. strip '#' comment
    if "#" in spec_str:
        ndx = spec_str.index("#")
        spec_str, _ = spec_str[:ndx], spec_str[ndx:]
        spec_str.strip()

    # Step 1.b strip ' if ' anticipating future compatibility issues
    spec_split = spec_str.split(" if ", 1)
    if len(spec_split) > 1:
        log.debug("Ignoring conditional in spec %s", spec_str)
    spec_str = spec_split[0]

    # Step 2. done if spec_str is a tarball
    if is_package_file(spec_str):
        # treat as a normal url
        if not is_url(spec_str):
            spec_str = unquote(path_to_url(expand(spec_str)))

        channel = Channel(spec_str)
        if channel.subdir:
            name, version, build = _parse_legacy_dist(channel.package_filename)
            result = {
                "channel": channel.canonical_name,
                "subdir": channel.subdir,
                "name": name,
                "version": version,
                "build": build,
                "fn": channel.package_filename,
                "url": spec_str,
            }
        else:
            # url is not a channel
            if spec_str.startswith("file://"):
                # We must undo percent-encoding when generating fn.
                path_or_url = url_to_path(spec_str)
            else:
                path_or_url = spec_str

            return {
                "name": "*",
                "fn": basename(path_or_url),
                "url": spec_str,
            }
        return result

    # Step 3. strip off brackets portion
    brackets = {}
    m3 = re.match(r".*(?:(\[.*\]))", spec_str)
    if m3:
        brackets_str = m3.groups()[0]
        spec_str = spec_str.replace(brackets_str, "")
        brackets_str = brackets_str[1:-1]
        m3b = re.finditer(
            r'([a-zA-Z0-9_-]+?)=(["\']?)([^\'"]*?)(\2)(?:[, ]|$)', brackets_str
        )
        for match in m3b:
            key, _, value, _ = match.groups()
            if not key or not value:
                raise InvalidMatchSpec(
                    original_spec_str, "key-value mismatch in brackets"
                )
            brackets[key] = value

    # Step 4. strip off parens portion
    m4 = re.match(r".*(?:(\(.*\)))", spec_str)
    parens = {}
    if m4:
        parens_str = m4.groups()[0]
        spec_str = spec_str.replace(parens_str, "")
        parens_str = parens_str[1:-1]
        m4b = re.finditer(
            r'([a-zA-Z0-9_-]+?)=(["\']?)([^\'"]*?)(\2)(?:[, ]|$)', parens_str
        )
        for match in m4b:
            key, _, value, _ = match.groups()
            parens[key] = value
        if "optional" in parens_str:
            parens["optional"] = True

    # Step 5. strip off '::' channel and namespace
    m5 = spec_str.rsplit(":", 2)
    m5_len = len(m5)
    if m5_len == 3:
        channel_str, namespace, spec_str = m5
    elif m5_len == 2:
        namespace, spec_str = m5
        channel_str = None
    elif m5_len:
        spec_str = m5[0]
        channel_str, namespace = None, None
    else:
        raise NotImplementedError()
    channel, subdir = _parse_channel(channel_str)
    if "channel" in brackets:
        b_channel, b_subdir = _parse_channel(brackets.pop("channel"))
        if b_channel:
            channel = b_channel
        if b_subdir:
            subdir = b_subdir
    if "subdir" in brackets:
        subdir = brackets.pop("subdir")

    # Step 6. strip off package name from remaining version + build
    m3 = re.match(r"([^ =<>!~]+)?([><!=~ ].+)?", spec_str)
    if m3:
        name, spec_str = m3.groups()
        if name is None:
            raise InvalidMatchSpec(
                original_spec_str, f"no package name found in '{spec_str}'"
            )
    else:
        raise InvalidMatchSpec(original_spec_str, "no package name found")

    # Step 7. otherwise sort out version + build
    spec_str = spec_str and spec_str.strip()
    # This was an attempt to make MatchSpec('numpy-1.11.0-py27_0') work like we'd want. It's
    # not possible though because plenty of packages have names with more than one '-'.
    # if spec_str is None and name.count('-') >= 2:
    #     name, version, build = _parse_legacy_dist(name)
    if spec_str:
        if "[" in spec_str:
            raise InvalidMatchSpec(
                original_spec_str, "multiple brackets sections not allowed"
            )

        version, build = _parse_version_plus_build(spec_str)

        # Catch cases where version ends up as "==" and pass it through so existing error
        # handling code can treat it like cases where version ends up being "<=" or ">=".
        # This is necessary because the "Translation" code below mangles "==" into a empty
        # string, which results in an empty version field on "components." The set of fields
        # on components drives future logic which breaks on an empty string but will deal with
        # missing versions like "==", "<=", and ">=" "correctly."
        #
        # All of these "missing version" cases result from match specs like "numpy==",
        # "numpy<=", "numpy>=", "numpy= " (with trailing space). Existing code indicates
        # these should be treated as an error and an exception raised.
        # IMPORTANT: "numpy=" (no trailing space) is treated as valid.
        if version == "==" or version == "=":
            pass
        # Otherwise,
        # translate version '=1.2.3' to '1.2.3*'
        # is it a simple version starting with '='? i.e. '=1.2.3'
        elif version[0] == "=":
            test_str = version[1:]
            if version[:2] == "==" and build is None:
                version = version[2:]
            elif not any(c in test_str for c in "=,|"):
                if build is None and test_str[-1] != "*":
                    version = test_str + "*"
                else:
                    version = test_str
    else:
        version, build = None, None

    # Step 8. now compile components together
    components = {}
    components["name"] = name or "*"

    if channel is not None:
        components["channel"] = channel
    if subdir is not None:
        components["subdir"] = subdir
    if namespace is not None:
        # components['namespace'] = namespace
        pass
    if version is not None:
        components["version"] = version
    if build is not None:
        components["build"] = build

    # anything in brackets will now strictly override key as set in other area of spec str
    # EXCEPT FOR: name
    # If we let name in brackets override a name outside of brackets it is possible to write
    # MatchSpecs that appear to install one package but actually install a completely different one
    # e.g. tensorflow[name=* version=* md5=<hash of pytorch package> ] will APPEAR to install
    # tensorflow but actually install pytorch.
    if "name" in components and "name" in brackets:
        msg = (
            f"'name' specified both inside ({brackets['name']}) and outside "
            f"({components['name']}) of brackets. The value outside of brackets "
            f"({components['name']}) will be used."
        )
        warnings.warn(msg, UserWarning)
        del brackets["name"]
    components.update(brackets)
    components["_original_spec_str"] = original_spec_str
    _PARSE_CACHE[original_spec_str] = components
    return components


class MatchInterface(metaclass=ABCMeta):
    def __init__(self, value):
        self._raw_value = value

    @abstractmethod
    def match(self, other):
        raise NotImplementedError()

    def matches(self, value):
        return self.match(value)

    @property
    def raw_value(self):
        return self._raw_value

    @abstractproperty
    def exact_value(self):
        """If the match value is an exact specification, returns the value.
        Otherwise returns None.
        """
        raise NotImplementedError()

    def merge(self, other):
        if self.raw_value != other.raw_value:
            raise ValueError(
                f"Incompatible component merge:\n  - {self.raw_value!r}\n  - {other.raw_value!r}"
            )
        return self.raw_value

    def union(self, other):
        options = {self.raw_value, other.raw_value}
        return "|".join(options)


class _StrMatchMixin:
    def __str__(self):
        return self._raw_value

    def __repr__(self):
        return f"{self.__class__.__name__}('{self._raw_value}')"

    def __eq__(self, other):
        return isinstance(other, self.__class__) and self._raw_value == other._raw_value

    def __hash__(self):
        return hash(self._raw_value)

    @property
    def exact_value(self):
        return self._raw_value


class ExactStrMatch(_StrMatchMixin, MatchInterface):
    __slots__ = ("_raw_value",)

    def __init__(self, value):
        super().__init__(value)

    def match(self, other):
        try:
            _other_val = other._raw_value
        except AttributeError:
            _other_val = str(other)
        return self._raw_value == _other_val


class ExactLowerStrMatch(ExactStrMatch):
    def __init__(self, value):
        super().__init__(value.lower())

    def match(self, other):
        try:
            _other_val = other._raw_value
        except AttributeError:
            _other_val = str(other)
        return self._raw_value == _other_val.lower()


class GlobStrMatch(_StrMatchMixin, MatchInterface):
    __slots__ = "_raw_value", "_re_match"

    def __init__(self, value):
        super().__init__(value)
        self._re_match = None

        try:
            if value.startswith("^") and value.endswith("$"):
                self._re_match = re.compile(value).match
            elif "*" in value:
                value = re.escape(value).replace("\\*", r".*")
                self._re_match = re.compile(rf"^(?:{value})$").match
        except re.error as e:
            raise InvalidMatchSpec(
                value, f"Contains an invalid regular expression. '{e}'"
            )

    def match(self, other):
        try:
            _other_val = other._raw_value
        except AttributeError:
            _other_val = str(other)

        if self._re_match:
            return self._re_match(_other_val)
        else:
            return self._raw_value == _other_val

    @property
    def exact_value(self):
        return self._raw_value if self._re_match is None else None

    @property
    def matches_all(self):
        return self._raw_value == "*"


class GlobLowerStrMatch(GlobStrMatch):
    def __init__(self, value):
        super().__init__(value.lower())


class SplitStrMatch(MatchInterface):
    __slots__ = ("_raw_value",)

    def __init__(self, value):
        super().__init__(self._convert(value))

    def _convert(self, value):
        try:
            return frozenset(value.replace(" ", ",").split(","))
        except AttributeError:
            if isiterable(value):
                return frozenset(value)
            raise

    def match(self, other):
        try:
            return other and self._raw_value & other._raw_value
        except AttributeError:
            return self._raw_value & self._convert(other)

    def __repr__(self):
        if self._raw_value:
            return "{{{}}}".format(", ".join(f"'{s}'" for s in sorted(self._raw_value)))
        else:
            return "set()"

    def __str__(self):
        # this space delimiting makes me nauseous
        return " ".join(sorted(self._raw_value))

    def __eq__(self, other):
        return isinstance(other, self.__class__) and self._raw_value == other._raw_value

    def __hash__(self):
        return hash(self._raw_value)

    @property
    def exact_value(self):
        return self._raw_value


class FeatureMatch(MatchInterface):
    __slots__ = ("_raw_value",)

    def __init__(self, value):
        super().__init__(self._convert(value))

    def _convert(self, value):
        if not value:
            return frozenset()
        elif isinstance(value, str):
            return frozenset(
                f
                for f in (ff.strip() for ff in value.replace(" ", ",").split(","))
                if f
            )
        else:
            return frozenset(f for f in (ff.strip() for ff in value) if f)

    def match(self, other):
        other = self._convert(other)
        return self._raw_value == other

    def __repr__(self):
        return "[{}]".format(", ".join(f"'{k}'" for k in sorted(self._raw_value)))

    def __str__(self):
        return " ".join(sorted(self._raw_value))

    def __eq__(self, other):
        return isinstance(other, self.__class__) and self._raw_value == other._raw_value

    def __hash__(self):
        return hash(self._raw_value)

    @property
    def exact_value(self):
        return self._raw_value


class ChannelMatch(GlobStrMatch):
    def __init__(self, value):
        self._re_match = None

        try:
            if isinstance(value, str):
                if value.startswith("^") and value.endswith("$"):
                    self._re_match = re.compile(value).match
                elif "*" in value:
                    self._re_match = re.compile(
                        r"^(?:{})$".format(value.replace("*", r".*"))
                    ).match
                else:
                    value = Channel(value)
        except re.error as e:
            raise InvalidMatchSpec(
                value, f"Contains an invalid regular expression. '{e}'"
            )

        super(GlobStrMatch, self).__init__(value)

    def match(self, other):
        try:
            _other_val = Channel(other._raw_value)
        except AttributeError:
            _other_val = Channel(other)

        if self._re_match:
            return self._re_match(_other_val.canonical_name)
        else:
            # assert ChannelMatch('pkgs/free').match('defaults') is False
            # assert ChannelMatch('defaults').match('pkgs/free') is True
            return self._raw_value.name in (_other_val.name, _other_val.canonical_name)

    def __str__(self):
        try:
            return f"{self._raw_value.name}"
        except AttributeError:
            return f"{self._raw_value}"

    def __repr__(self):
        return f"'{self.__str__()}'"


class CaseInsensitiveStrMatch(GlobLowerStrMatch):
    def match(self, other):
        try:
            _other_val = other._raw_value
        except AttributeError:
            _other_val = str(other)

        _other_val = _other_val.lower()
        if self._re_match:
            return self._re_match(_other_val)
        else:
            return self._raw_value == _other_val


_implementors = {
    "channel": ChannelMatch,
    "name": GlobLowerStrMatch,
    "version": VersionSpec,
    "build": GlobStrMatch,
    "build_number": BuildNumberMatch,
    "track_features": FeatureMatch,
    "features": FeatureMatch,
    "license": CaseInsensitiveStrMatch,
    "license_family": CaseInsensitiveStrMatch,
}


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Environment object describing the conda environment.yaml file."""

import json
import os
import re
from itertools import chain
from os.path import abspath, expanduser, expandvars

from ..base.context import context
from ..cli import common, install
from ..common.iterators import groupby_to_dict as groupby
from ..common.iterators import unique
from ..common.serialize import yaml_safe_dump, yaml_safe_load
from ..core.prefix_data import PrefixData
from ..exceptions import EnvironmentFileEmpty, EnvironmentFileNotFound
from ..gateways.connection.download import download_text
from ..gateways.connection.session import CONDA_SESSION_SCHEMES
from ..history import History
from ..models.enums import PackageType
from ..models.match_spec import MatchSpec
from ..models.prefix_graph import PrefixGraph

VALID_KEYS = ("name", "dependencies", "prefix", "channels", "variables")


def validate_keys(data, kwargs):
    """Check for unknown keys, remove them and print a warning"""
    invalid_keys = []
    new_data = data.copy() if data else {}
    for key in data.keys():
        if key not in VALID_KEYS:
            invalid_keys.append(key)
            new_data.pop(key)

    if invalid_keys:
        filename = kwargs.get("filename")
        verb = "are" if len(invalid_keys) != 1 else "is"
        plural = "s" if len(invalid_keys) != 1 else ""
        print(
            f"\nEnvironmentSectionNotValid: The following section{plural} on "
            f"'{filename}' {verb} invalid and will be ignored:"
        )
        for key in invalid_keys:
            print(f" - {key}")
        print()

    deps = data.get("dependencies", [])
    depsplit = re.compile(r"[<>~\s=]")
    is_pip = lambda dep: "pip" in depsplit.split(dep)[0].split("::")
    lists_pip = any(is_pip(dep) for dep in deps if not isinstance(dep, dict))
    for dep in deps:
        if isinstance(dep, dict) and "pip" in dep and not lists_pip:
            print(
                "Warning: you have pip-installed dependencies in your environment file, "
                "but you do not list pip itself as one of your conda dependencies.  Conda "
                "may not use the correct pip to install your packages, and they may end up "
                "in the wrong place.  Please add an explicit pip dependency.  I'm adding one"
                " for you, but still nagging you."
            )
            new_data["dependencies"].insert(0, "pip")
            break
    return new_data


def from_environment(
    name, prefix, no_builds=False, ignore_channels=False, from_history=False
):
    """
        Get ``Environment`` object from prefix
    Args:
        name: The name of environment
        prefix: The path of prefix
        no_builds: Whether has build requirement
        ignore_channels: whether ignore_channels
        from_history: Whether environment file should be based on explicit specs in history

    Returns:     Environment object
    """
    pd = PrefixData(prefix, pip_interop_enabled=True)
    variables = pd.get_environment_env_vars()

    if from_history:
        history = History(prefix).get_requested_specs_map()
        deps = [str(package) for package in history.values()]
        return Environment(
            name=name,
            dependencies=deps,
            channels=list(context.channels),
            prefix=prefix,
            variables=variables,
        )

    precs = tuple(PrefixGraph(pd.iter_records()).graph)
    grouped_precs = groupby(lambda x: x.package_type, precs)
    conda_precs = sorted(
        (
            *grouped_precs.get(None, ()),
            *grouped_precs.get(PackageType.NOARCH_GENERIC, ()),
            *grouped_precs.get(PackageType.NOARCH_PYTHON, ()),
        ),
        key=lambda x: x.name,
    )

    pip_precs = sorted(
        (
            *grouped_precs.get(PackageType.VIRTUAL_PYTHON_WHEEL, ()),
            *grouped_precs.get(PackageType.VIRTUAL_PYTHON_EGG_MANAGEABLE, ()),
            *grouped_precs.get(PackageType.VIRTUAL_PYTHON_EGG_UNMANAGEABLE, ()),
        ),
        key=lambda x: x.name,
    )

    if no_builds:
        dependencies = ["=".join((a.name, a.version)) for a in conda_precs]
    else:
        dependencies = ["=".join((a.name, a.version, a.build)) for a in conda_precs]
    if pip_precs:
        dependencies.append({"pip": [f"{a.name}=={a.version}" for a in pip_precs]})

    channels = list(context.channels)
    if not ignore_channels:
        for prec in conda_precs:
            canonical_name = prec.channel.canonical_name
            if canonical_name not in channels:
                channels.insert(0, canonical_name)
    return Environment(
        name=name,
        dependencies=dependencies,
        channels=channels,
        prefix=prefix,
        variables=variables,
    )


def from_yaml(yamlstr, **kwargs):
    """Load and return a ``Environment`` from a given ``yaml`` string"""
    data = yaml_safe_load(yamlstr)
    filename = kwargs.get("filename")
    if data is None:
        raise EnvironmentFileEmpty(filename)
    data = validate_keys(data, kwargs)

    if kwargs is not None:
        for key, value in kwargs.items():
            data[key] = value
    _expand_channels(data)
    return Environment(**data)


def _expand_channels(data):
    """Expands ``Environment`` variables for the channels found in the ``yaml`` data"""
    data["channels"] = [
        os.path.expandvars(channel) for channel in data.get("channels", [])
    ]


def from_file(filename):
    """Load and return an ``Environment`` from a given file"""
    url_scheme = filename.split("://", 1)[0]
    if url_scheme in CONDA_SESSION_SCHEMES:
        yamlstr = download_text(filename)
    elif not os.path.exists(filename):
        raise EnvironmentFileNotFound(filename)
    else:
        with open(filename, "rb") as fp:
            yamlb = fp.read()
            try:
                yamlstr = yamlb.decode("utf-8")
            except UnicodeDecodeError:
                yamlstr = yamlb.decode("utf-16")
    return from_yaml(yamlstr, filename=filename)


class Dependencies(dict):
    """A ``dict`` subclass that parses the raw dependencies into a conda and pip list"""

    def __init__(self, raw, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.raw = raw
        self.parse()

    def parse(self):
        """Parse the raw dependencies into a conda and pip list"""
        if not self.raw:
            return

        self.update({"conda": []})

        for line in self.raw:
            if isinstance(line, dict):
                self.update(line)
            else:
                self["conda"].append(common.arg2spec(line))

        if "pip" in self:
            if not self["pip"]:
                del self["pip"]
            if not any(MatchSpec(s).name == "pip" for s in self["conda"]):
                self["conda"].append("pip")

    # TODO only append when it's not already present
    def add(self, package_name):
        """Add a package to the ``Environment``"""
        self.raw.append(package_name)
        self.parse()


class Environment:
    """A class representing an ``environment.yaml`` file"""

    def __init__(
        self,
        name=None,
        filename=None,
        channels=None,
        dependencies=None,
        prefix=None,
        variables=None,
    ):
        self.name = name
        self.filename = filename
        self.prefix = prefix
        self.dependencies = Dependencies(dependencies)
        self.variables = variables

        if channels is None:
            channels = []
        self.channels = channels

    def add_channels(self, channels):
        """Add channels to the ``Environment``"""
        self.channels = list(unique(chain.from_iterable((channels, self.channels))))

    def remove_channels(self):
        """Remove all channels from the ``Environment``"""
        self.channels = []

    def to_dict(self, stream=None):
        """Convert information related to the ``Environment`` into a dictionary"""
        d = {"name": self.name}
        if self.channels:
            d["channels"] = self.channels
        if self.dependencies:
            d["dependencies"] = self.dependencies.raw
        if self.variables:
            d["variables"] = self.variables
        if self.prefix:
            d["prefix"] = self.prefix
        if stream is None:
            return d
        stream.write(json.dumps(d))

    def to_yaml(self, stream=None):
        """Convert information related to the ``Environment`` into a ``yaml`` string"""
        d = self.to_dict()
        out = yaml_safe_dump(d, stream)
        if stream is None:
            return out

    def save(self):
        """Save the ``Environment`` data to a ``yaml`` file"""
        with open(self.filename, "wb") as fp:
            self.to_yaml(stream=fp)


def get_filename(filename):
    """Expand filename if local path or return the ``url``"""
    url_scheme = filename.split("://", 1)[0]
    if url_scheme in CONDA_SESSION_SCHEMES:
        return filename
    else:
        return abspath(expanduser(expandvars(filename)))


def print_result(args, prefix, result):
    """Print the result of an install operation"""
    if context.json:
        if result["conda"] is None and result["pip"] is None:
            common.stdout_json_success(
                message="All requested packages already installed."
            )
        else:
            if result["conda"] is not None:
                actions = result["conda"]
            else:
                actions = {}
            if result["pip"] is not None:
                actions["PIP"] = result["pip"]
            common.stdout_json_success(prefix=prefix, actions=actions)
    else:
        install.print_activate(args.name or prefix)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Functions related to core conda functionality that relates to pip

NOTE: This modules used to in conda, as conda/pip.py
"""

import os
import re
import sys
from logging import getLogger

from ..base.context import context
from ..common.compat import on_win
from ..exceptions import CondaEnvException
from ..gateways.subprocess import any_subprocess

log = getLogger(__name__)


def pip_subprocess(args, prefix, cwd):
    """Run pip in a subprocess"""
    if on_win:
        python_path = os.path.join(prefix, "python.exe")
    else:
        python_path = os.path.join(prefix, "bin", "python")
    run_args = [python_path, "-m", "pip"] + args
    stdout, stderr, rc = any_subprocess(run_args, prefix, cwd=cwd)
    if not context.quiet and not context.json:
        print("Ran pip subprocess with arguments:")
        print(run_args)
        print("Pip subprocess output:")
        print(stdout)
    if rc != 0:
        print("Pip subprocess error:", file=sys.stderr)
        print(stderr, file=sys.stderr)
        raise CondaEnvException("Pip failed")

    return stdout, stderr


def get_pip_installed_packages(stdout):
    """Return the list of pip packages installed based on the command output"""
    m = re.search(r"Successfully installed\ (.*)", stdout)
    if m:
        return m.group(1).strip().split()
    else:
        return None


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Define binstar spec."""

from __future__ import annotations

import re
from functools import cached_property
from typing import TYPE_CHECKING

from ...env.env import from_yaml
from ...exceptions import EnvironmentFileNotDownloaded
from ...models.version import normalized_version

if TYPE_CHECKING:
    from types import ModuleType

    from ...env.env import Environment

ENVIRONMENT_TYPE = "env"


class BinstarSpec:
    """
    spec = BinstarSpec('darth/deathstar')
    spec.can_handle() # => True / False
    spec.environment # => YAML string
    spec.msg # => Error messages
    :raises: EnvironmentFileNotDownloaded
    """

    msg = None

    def __init__(self, name=None):
        self.name = name

    def can_handle(self) -> bool:
        """
        Validates loader can process environment definition.
        :return: True or False
        """
        # TODO: log information about trying to find the package in binstar.org
        if self.valid_name():
            if not self.binstar:
                self.msg = (
                    "Anaconda Client is required to interact with anaconda.org or an "
                    "Anaconda API. Please run `conda install anaconda-client -n base`."
                )
                return False

            return self.package is not None and self.valid_package()
        return False

    def valid_name(self) -> bool:
        """
        Validates name
        :return: True or False
        """
        if re.match("^(.+)/(.+)$", str(self.name)) is not None:
            return True
        elif self.name is None:
            self.msg = "Can't process without a name"
        else:
            self.msg = f"Invalid name {self.name!r}, try the format: user/package"
        return False

    def valid_package(self) -> bool:
        """
        Returns True if package has an environment file
        :return: True or False
        """
        return len(self.file_data) > 0

    @cached_property
    def binstar(self) -> ModuleType:
        try:
            from binstar_client.utils import get_server_api

            return get_server_api()
        except ImportError:
            pass

    @cached_property
    def file_data(self) -> list[dict[str, str]]:
        return [
            data for data in self.package["files"] if data["type"] == ENVIRONMENT_TYPE
        ]

    @cached_property
    def environment(self) -> Environment:
        versions = [
            {"normalized": normalized_version(d["version"]), "original": d["version"]}
            for d in self.file_data
        ]
        latest_version = max(versions, key=lambda x: x["normalized"])["original"]
        file_data = [
            data for data in self.package["files"] if data["version"] == latest_version
        ]
        req = self.binstar.download(
            self.username, self.packagename, latest_version, file_data[0]["basename"]
        )
        if req is None:
            raise EnvironmentFileNotDownloaded(self.username, self.packagename)
        return from_yaml(req.text)

    @cached_property
    def package(self):
        try:
            return self.binstar.package(self.username, self.packagename)
        except (IndexError, AttributeError):
            self.msg = (
                f"{self.name} was not found on anaconda.org.\n"
                "You may need to be logged in. Try running:\n"
                "    anaconda login"
            )

    @cached_property
    def username(self) -> str:
        return self.name.split("/", 1)[0]

    @cached_property
    def packagename(self) -> str:
        return self.name.split("/", 1)[1]


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Define YAML spec."""

from ...exceptions import EnvironmentFileEmpty, EnvironmentFileNotFound
from .. import env


class YamlFileSpec:
    _environment = None
    extensions = {".yaml", ".yml"}

    def __init__(self, filename=None, **kwargs):
        self.filename = filename
        self.msg = None

    def can_handle(self):
        try:
            self._environment = env.from_file(self.filename)
            return True
        except EnvironmentFileNotFound as e:
            self.msg = str(e)
            return False
        except EnvironmentFileEmpty as e:
            self.msg = e.message
            return False
        except TypeError:
            self.msg = f"{self.filename} is not a valid yaml file."
            return False

    @property
    def environment(self):
        if not self._environment:
            self.can_handle()
        return self._environment


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Define requirements.txt spec."""

import os

from ..env import Environment


class RequirementsSpec:
    """
    Reads dependencies from a requirements.txt file
    and returns an Environment object from it.
    """

    msg = None
    extensions = {".txt"}

    def __init__(self, filename=None, name=None, **kwargs):
        self.filename = filename
        self.name = name
        self.msg = None

    def _valid_file(self):
        if os.path.exists(self.filename):
            return True
        else:
            self.msg = "There is no requirements.txt"
            return False

    def _valid_name(self):
        if self.name is None:
            self.msg = "Environment with requirements.txt file needs a name"
            return False
        else:
            return True

    def can_handle(self):
        return self._valid_file() and self._valid_name()

    @property
    def environment(self):
        dependencies = []
        with open(self.filename) as reqfile:
            for line in reqfile:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                dependencies.append(line)
        return Environment(name=self.name, dependencies=dependencies)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
from __future__ import annotations

import os
from typing import Type, Union

from ...exceptions import (
    EnvironmentFileExtensionNotValid,
    EnvironmentFileNotFound,
    SpecNotFound,
)
from ...gateways.connection.session import CONDA_SESSION_SCHEMES
from .binstar import BinstarSpec
from .requirements import RequirementsSpec
from .yaml_file import YamlFileSpec

FileSpecTypes = Union[Type[YamlFileSpec], Type[RequirementsSpec]]


def get_spec_class_from_file(filename: str) -> FileSpecTypes:
    """
    Determine spec class to use from the provided ``filename``

    :raises EnvironmentFileExtensionNotValid | EnvironmentFileNotFound:
    """
    # Check extensions
    all_valid_exts = YamlFileSpec.extensions.union(RequirementsSpec.extensions)
    _, ext = os.path.splitext(filename)

    # First check if file exists and test the known valid extension for specs
    file_exists = (
        os.path.isfile(filename) or filename.split("://", 1)[0] in CONDA_SESSION_SCHEMES
    )
    if file_exists:
        if ext == "" or ext not in all_valid_exts:
            raise EnvironmentFileExtensionNotValid(filename)
        elif ext in YamlFileSpec.extensions:
            return YamlFileSpec
        elif ext in RequirementsSpec.extensions:
            return RequirementsSpec
    else:
        raise EnvironmentFileNotFound(filename=filename)


SpecTypes = Union[BinstarSpec, YamlFileSpec, RequirementsSpec]


def detect(
    name: str = None,
    filename: str = None,
    directory: str = None,
    remote_definition: str = None,
) -> SpecTypes:
    """
    Return the appropriate spec type to use.

    :raises SpecNotFound: Raised if no suitable spec class could be found given the input
    :raises EnvironmentFileExtensionNotValid | EnvironmentFileNotFound:
    """
    if remote_definition is not None:
        spec = BinstarSpec(name=remote_definition)
        if spec.can_handle():
            return spec
        else:
            raise SpecNotFound(spec.msg)

    if filename is not None:
        spec_class = get_spec_class_from_file(filename)
        spec = spec_class(name=name, filename=filename, directory=directory)
        if spec.can_handle():
            return spec

    raise SpecNotFound(spec.msg)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Pip-flavored installer."""

import os
import os.path as op
from logging import getLogger

from ...auxlib.compat import Utf8NamedTemporaryFile
from ...base.context import context
from ...common.io import Spinner
from ...env.pip_util import get_pip_installed_packages, pip_subprocess
from ...gateways.connection.session import CONDA_SESSION_SCHEMES

log = getLogger(__name__)


def _pip_install_via_requirements(prefix, specs, args, *_, **kwargs):
    """
    Installs the pip dependencies in specs using a temporary pip requirements file.

    Args
    ----
    prefix: string
      The path to the python and pip executables.

    specs: iterable of strings
      Each element should be a valid pip dependency.
      See: https://pip.pypa.io/en/stable/user_guide/#requirements-files
           https://pip.pypa.io/en/stable/reference/pip_install/#requirements-file-format
    """
    url_scheme = args.file.split("://", 1)[0]
    if url_scheme in CONDA_SESSION_SCHEMES:
        pip_workdir = None
    else:
        try:
            pip_workdir = op.dirname(op.abspath(args.file))
            if not os.access(pip_workdir, os.W_OK):
                pip_workdir = None
        except AttributeError:
            pip_workdir = None
    requirements = None
    try:
        # Generate the temporary requirements file
        requirements = Utf8NamedTemporaryFile(
            mode="w",
            prefix="condaenv.",
            suffix=".requirements.txt",
            dir=pip_workdir,
            delete=False,
        )
        requirements.write("\n".join(specs))
        requirements.close()
        # pip command line...
        # see https://pip.pypa.io/en/stable/cli/pip/#exists-action-option
        pip_cmd = ["install", "-U", "-r", requirements.name, "--exists-action=b"]
        stdout, stderr = pip_subprocess(pip_cmd, prefix, cwd=pip_workdir)
    finally:
        # Win/Appveyor does not like it if we use context manager + delete=True.
        # So we delete the temporary file in a finally block.
        if requirements is not None and op.isfile(requirements.name):
            if "CONDA_TEST_SAVE_TEMPS" not in os.environ:
                os.remove(requirements.name)
            else:
                log.warning(
                    f"CONDA_TEST_SAVE_TEMPS :: retaining pip requirements.txt {requirements.name}"
                )
    return get_pip_installed_packages(stdout)


def install(*args, **kwargs):
    with Spinner(
        "Installing pip dependencies",
        not context.verbose and not context.quiet,
        context.json,
    ):
        return _pip_install_via_requirements(*args, **kwargs)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Conda-flavored installer."""

import tempfile
from os.path import basename

from boltons.setutils import IndexedSet

from ...base.constants import UpdateModifier
from ...base.context import context
from ...common.constants import NULL
from ...env.env import Environment
from ...exceptions import UnsatisfiableError
from ...models.channel import Channel, prioritize_channels


def _solve(prefix, specs, args, env, *_, **kwargs):
    """Solve the environment"""
    # TODO: support all various ways this happens
    # Including 'nodefaults' in the channels list disables the defaults
    channel_urls = [chan for chan in env.channels if chan != "nodefaults"]

    if "nodefaults" not in env.channels:
        channel_urls.extend(context.channels)
    _channel_priority_map = prioritize_channels(channel_urls)

    channels = IndexedSet(Channel(url) for url in _channel_priority_map)
    subdirs = IndexedSet(basename(url) for url in _channel_priority_map)

    solver_backend = context.plugin_manager.get_cached_solver_backend()
    solver = solver_backend(prefix, channels, subdirs, specs_to_add=specs)
    return solver


def dry_run(specs, args, env, *_, **kwargs):
    """Do a dry run of the environment solve"""
    solver = _solve(tempfile.mkdtemp(), specs, args, env, *_, **kwargs)
    pkgs = solver.solve_final_state()
    solved_env = Environment(
        name=env.name, dependencies=[str(p) for p in pkgs], channels=env.channels
    )
    return solved_env


def install(prefix, specs, args, env, *_, **kwargs):
    """Install packages into an environment"""
    solver = _solve(prefix, specs, args, env, *_, **kwargs)

    try:
        unlink_link_transaction = solver.solve_for_transaction(
            prune=getattr(args, "prune", False),
            update_modifier=UpdateModifier.FREEZE_INSTALLED,
        )
    except (UnsatisfiableError, SystemExit):
        unlink_link_transaction = solver.solve_for_transaction(
            prune=getattr(args, "prune", False), update_modifier=NULL
        )

    if unlink_link_transaction.nothing_to_do:
        return None
    unlink_link_transaction.download_and_extract()
    unlink_link_transaction.execute()
    return unlink_link_transaction._make_legacy_action_groups()[0]


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Dynamic installer loading."""

import importlib

from ...exceptions import InvalidInstaller


def get_installer(name):
    """
        Gets the installer for the given environment.

    Raises: InvalidInstaller if unable to load installer
    """
    try:
        return importlib.import_module(f"conda.env.installers.{name}")
    except ImportError:
        raise InvalidInstaller(name)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Definition of specific return types for use when defining a conda plugin hook.

Each type corresponds to the plugin hook for which it is used.

"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import TYPE_CHECKING, NamedTuple

from requests.auth import AuthBase

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace
    from typing import Callable

    from ..common.configuration import Parameter
    from ..core.solve import Solver
    from ..models.match_spec import MatchSpec
    from ..models.records import PackageRecord


@dataclass
class CondaSubcommand:
    """
    Return type to use when defining a conda subcommand plugin hook.

    For details on how this is used, see
    :meth:`~conda.plugins.hookspec.CondaSpecs.conda_subcommands`.

    :param name: Subcommand name (e.g., ``conda my-subcommand-name``).
    :param summary: Subcommand summary, will be shown in ``conda --help``.
    :param action: Callable that will be run when the subcommand is invoked.
    :param configure_parser: Callable that will be run when the subcommand parser is initialized.
    """

    name: str
    summary: str
    action: Callable[
        [Namespace | tuple[str]],  # arguments
        int | None,  # return code
    ]
    configure_parser: Callable[[ArgumentParser], None] | None = field(default=None)


class CondaVirtualPackage(NamedTuple):
    """
    Return type to use when defining a conda virtual package plugin hook.

    For details on how this is used, see
    :meth:`~conda.plugins.hookspec.CondaSpecs.conda_virtual_packages`.

    :param name: Virtual package name (e.g., ``my_custom_os``).
    :param version: Virtual package version (e.g., ``1.2.3``).
    :param build: Virtual package build string (e.g., ``x86_64``).
    """

    name: str
    version: str | None
    build: str | None


class CondaSolver(NamedTuple):
    """
    Return type to use when defining a conda solver plugin hook.

    For details on how this is used, see
    :meth:`~conda.plugins.hookspec.CondaSpecs.conda_solvers`.

    :param name: Solver name (e.g., ``custom-solver``).
    :param backend: Type that will be instantiated as the solver backend.
    """

    name: str
    backend: type[Solver]


class CondaPreCommand(NamedTuple):
    """
    Return type to use when defining a conda pre-command plugin hook.

    For details on how this is used, see
    :meth:`~conda.plugins.hookspec.CondaSpecs.conda_pre_commands`.

    :param name: Pre-command name (e.g., ``custom_plugin_pre_commands``).
    :param action: Callable which contains the code to be run.
    :param run_for: Represents the command(s) this will be run on (e.g. ``install`` or ``create``).
    """

    name: str
    action: Callable[[str], None]
    run_for: set[str]


class CondaPostCommand(NamedTuple):
    """
    Return type to use when defining a conda post-command plugin hook.

    For details on how this is used, see
    :meth:`~conda.plugins.hookspec.CondaSpecs.conda_post_commands`.

    :param name: Post-command name (e.g., ``custom_plugin_post_commands``).
    :param action: Callable which contains the code to be run.
    :param run_for: Represents the command(s) this will be run on (e.g. ``install`` or ``create``).
    """

    name: str
    action: Callable[[str], None]
    run_for: set[str]


class ChannelNameMixin:
    """
    Class mixin to make all plugin implementations compatible, e.g. when they
    use an existing (e.g. 3rd party) requests authentication handler.

    Please use the concrete :class:`~conda.plugins.types.ChannelAuthBase`
    in case you're creating an own implementation.
    """

    def __init__(self, channel_name: str, *args, **kwargs):
        self.channel_name = channel_name
        super().__init__(*args, **kwargs)


class ChannelAuthBase(ChannelNameMixin, AuthBase):
    """
    Base class that we require all plugin implementations to use to be compatible.

    Authentication is tightly coupled with individual channels. Therefore, an additional
    ``channel_name`` property must be set on the ``requests.auth.AuthBase`` based class.
    """


class CondaAuthHandler(NamedTuple):
    """
    Return type to use when the defining the conda auth handlers hook.

    :param name: Name (e.g., ``basic-auth``). This name should be unique
                 and only one may be registered at a time.
    :param handler: Type that will be used as the authentication handler
                    during network requests.
    """

    name: str
    handler: type[ChannelAuthBase]


class CondaHealthCheck(NamedTuple):
    """
    Return type to use when defining conda health checks plugin hook.
    """

    name: str
    action: Callable[[str, bool], None]


@dataclass
class CondaPreSolve:
    """
    Return type to use when defining a conda pre-solve plugin hook.

    For details on how this is used, see
    :meth:`~conda.plugins.hookspec.CondaSpecs.conda_pre_solves`.

    :param name: Pre-solve name (e.g., ``custom_plugin_pre_solve``).
    :param action: Callable which contains the code to be run.
    """

    name: str
    action: Callable[[frozenset[MatchSpec], frozenset[MatchSpec]], None]


@dataclass
class CondaPostSolve:
    """
    Return type to use when defining a conda post-solve plugin hook.

    For details on how this is used, see
    :meth:`~conda.plugins.hookspec.CondaSpecs.conda_post_solves`.

    :param name: Post-solve name (e.g., ``custom_plugin_post_solve``).
    :param action: Callable which contains the code to be run.
    """

    name: str
    action: Callable[[str, tuple[PackageRecord, ...], tuple[PackageRecord, ...]], None]


@dataclass
class CondaSetting:
    """
    Return type to use when defining a conda setting plugin hook.

    For details on how this is used, see
    :meth:`~conda.plugins.hookspec.CondaSpecs.conda_settings`.

    :param name: name of the setting (e.g., ``config_param``)
    :param description: description of the setting that should be targeted
                        towards users of the plugin
    :param parameter: Parameter instance containing the setting definition
    :param aliases: alternative names of the setting
    """

    name: str
    description: str
    parameter: Parameter
    aliases: tuple[str, ...] = tuple()


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Register the classic conda solver."""

from ..base.constants import CLASSIC_SOLVER
from . import CondaSolver, hookimpl


@hookimpl(tryfirst=True)  # make sure the classic solver can't be overwritten
def conda_solvers():
    """The classic solver as shipped by default in conda."""
    from ..core.solve import Solver

    yield CondaSolver(
        name=CLASSIC_SOLVER,
        backend=Solver,
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Pluggy hook specifications ("hookspecs") to register conda plugins.

Each hookspec defined in :class:`~conda.plugins.hookspec.CondaSpecs` contains
an example of how to use it.

"""

from __future__ import annotations

from typing import TYPE_CHECKING

import pluggy

if TYPE_CHECKING:
    from collections.abc import Iterable

    from .types import (
        CondaAuthHandler,
        CondaHealthCheck,
        CondaPostCommand,
        CondaPostSolve,
        CondaPreCommand,
        CondaPreSolve,
        CondaSetting,
        CondaSolver,
        CondaSubcommand,
        CondaVirtualPackage,
    )

spec_name = "conda"
"""Name used for organizing conda hook specifications"""

_hookspec = pluggy.HookspecMarker(spec_name)
"""
The conda plugin hook specifications, to be used by developers
"""

hookimpl = pluggy.HookimplMarker(spec_name)
"""
Decorator used to mark plugin hook implementations
"""


class CondaSpecs:
    """The conda plugin hookspecs, to be used by developers."""

    @_hookspec
    def conda_solvers(self) -> Iterable[CondaSolver]:
        """
        Register solvers in conda.

        **Example:**

        .. code-block:: python

            import logging

            from conda import plugins
            from conda.core import solve

            log = logging.getLogger(__name__)


            class VerboseSolver(solve.Solver):
                def solve_final_state(self, *args, **kwargs):
                    log.info("My verbose solver!")
                    return super().solve_final_state(*args, **kwargs)


            @plugins.hookimpl
            def conda_solvers():
                yield plugins.CondaSolver(
                    name="verbose-classic",
                    backend=VerboseSolver,
                )

        :return: An iterable of solver entries.
        """

    @_hookspec
    def conda_subcommands(self) -> Iterable[CondaSubcommand]:
        """
        Register external subcommands in conda.

        **Example:**

        .. code-block:: python

            from conda import plugins


            def example_command(args):
                print("This is an example command!")


            @plugins.hookimpl
            def conda_subcommands():
                yield plugins.CondaSubcommand(
                    name="example",
                    summary="example command",
                    action=example_command,
                )

        :return: An iterable of subcommand entries.
        """

    @_hookspec
    def conda_virtual_packages(self) -> Iterable[CondaVirtualPackage]:
        """
        Register virtual packages in Conda.

        **Example:**

        .. code-block:: python

            from conda import plugins


            @plugins.hookimpl
            def conda_virtual_packages():
                yield plugins.CondaVirtualPackage(
                    name="my_custom_os",
                    version="1.2.3",
                    build="x86_64",
                )

        :return: An iterable of virtual package entries.
        """

    @_hookspec
    def conda_pre_commands(self) -> Iterable[CondaPreCommand]:
        """
        Register pre-command functions in conda.

        **Example:**

        .. code-block:: python

           from conda import plugins


           def example_pre_command(command):
               print("pre-command action")


           @plugins.hookimpl
           def conda_pre_commands():
               yield plugins.CondaPreCommand(
                   name="example-pre-command",
                   action=example_pre_command,
                   run_for={"install", "create"},
               )
        """

    @_hookspec
    def conda_post_commands(self) -> Iterable[CondaPostCommand]:
        """
        Register post-command functions in conda.

        **Example:**

        .. code-block:: python

           from conda import plugins


           def example_post_command(command):
               print("post-command action")


           @plugins.hookimpl
           def conda_post_commands():
               yield plugins.CondaPostCommand(
                   name="example-post-command",
                   action=example_post_command,
                   run_for={"install", "create"},
               )
        """

    @_hookspec
    def conda_auth_handlers(self) -> Iterable[CondaAuthHandler]:
        """
        Register a conda auth handler derived from the requests API.

        This plugin hook allows attaching requests auth handler subclasses,
        e.g. when authenticating requests against individual channels hosted
        at HTTP/HTTPS services.

        **Example:**

        .. code-block:: python

            import os
            from conda import plugins
            from requests.auth import AuthBase


            class EnvironmentHeaderAuth(AuthBase):
                def __init__(self, *args, **kwargs):
                    self.username = os.environ["EXAMPLE_CONDA_AUTH_USERNAME"]
                    self.password = os.environ["EXAMPLE_CONDA_AUTH_PASSWORD"]

                def __call__(self, request):
                    request.headers["X-Username"] = self.username
                    request.headers["X-Password"] = self.password
                    return request


            @plugins.hookimpl
            def conda_auth_handlers():
                yield plugins.CondaAuthHandler(
                    name="environment-header-auth",
                    auth_handler=EnvironmentHeaderAuth,
                )
        """

    @_hookspec
    def conda_health_checks(self) -> Iterable[CondaHealthCheck]:
        """
        Register health checks for conda doctor.

        This plugin hook allows you to add more "health checks" to conda doctor
        that you can write to diagnose problems in your conda environment.
        Check out the health checks already shipped with conda for inspiration.

        **Example:**

        .. code-block:: python

            from conda import plugins


            def example_health_check(prefix: str, verbose: bool):
                print("This is an example health check!")


            @plugins.hookimpl
            def conda_health_checks():
                yield plugins.CondaHealthCheck(
                    name="example-health-check",
                    action=example_health_check,
                )
        """

    @_hookspec
    def conda_pre_solves(self) -> Iterable[CondaPreSolve]:
        """
        Register pre-solve functions in conda that are used in the
        general solver API, before the solver processes the package specs in
        search of a solution.

        **Example:**

        .. code-block:: python

           from conda import plugins
           from conda.models.match_spec import MatchSpec


           def example_pre_solve(
               specs_to_add: frozenset[MatchSpec],
               specs_to_remove: frozenset[MatchSpec],
           ):
               print(f"Adding {len(specs_to_add)} packages")
               print(f"Removing {len(specs_to_remove)} packages")


           @plugins.hookimpl
           def conda_pre_solves():
               yield plugins.CondaPreSolve(
                   name="example-pre-solve",
                   action=example_pre_solve,
               )
        """

    @_hookspec
    def conda_post_solves(self) -> Iterable[CondaPostSolve]:
        """
        Register post-solve functions in conda that are used in the
        general solver API, after the solver has provided the package
        records to add or remove from the conda environment.

        **Example:**

        .. code-block:: python

           from conda import plugins
           from conda.models.records import PackageRecord


           def example_post_solve(
               repodata_fn: str,
               unlink_precs: tuple[PackageRecord, ...],
               link_precs: tuple[PackageRecord, ...],
           ):
               print(f"Uninstalling {len(unlink_precs)} packages")
               print(f"Installing {len(link_precs)} packages")


           @plugins.hookimpl
           def conda_post_solves():
               yield plugins.CondaPostSolve(
                   name="example-post-solve",
                   action=example_post_solve,
               )
        """

    @_hookspec
    def conda_settings(self) -> Iterable[CondaSetting]:
        """
        Register new setting

        The example below defines a simple string type parameter

        **Example:**

        .. code-block:: python

           from conda import plugins
           from conda.common.configuration import PrimitiveParameter, SequenceParameter


           @plugins.hookimpl
           def conda_settings():
               yield plugins.CondaSetting(
                   name="example_option",
                   description="This is an example option",
                   parameter=PrimitiveParameter("default_value", element_type=str),
                   aliases=("example_option_alias",),
               )
        """


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
In this module, you will find everything relevant to conda's plugin system.
It contains all of the code that plugin authors will use to write plugins,
as well as conda's internal implementations of plugins.

**Modules relevant for plugin authors**

- :mod:`conda.plugins.hookspec`: all available hook specifications are listed here, including
  examples of how to use them
- :mod:`conda.plugins.types`: important types to use when defining plugin hooks

**Modules relevant for internal development**

- :mod:`conda.plugins.manager`: includes our custom subclass of pluggy's
  `PluginManager <https://pluggy.readthedocs.io/en/stable/api_reference.html#pluggy.PluginManager>`_ class

**Modules with internal plugin implementations**

- :mod:`conda.plugins.solvers`: implementation of the "classic" solver
- :mod:`conda.plugins.subcommands.doctor`: ``conda doctor`` subcommand
- :mod:`conda.plugins.virtual_packages`: registers virtual packages in conda

"""  # noqa: E501

from .hookspec import hookimpl  # noqa: F401
from .types import (  # noqa: F401
    CondaAuthHandler,
    CondaHealthCheck,
    CondaPostCommand,
    CondaPostSolve,
    CondaPreCommand,
    CondaPreSolve,
    CondaSetting,
    CondaSolver,
    CondaSubcommand,
    CondaVirtualPackage,
)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
This module contains a subclass implementation of pluggy's
`PluginManager <https://pluggy.readthedocs.io/en/stable/api_reference.html#pluggy.PluginManager>`_.

Additionally, it contains a function we use to construct the ``PluginManager`` object and
register all plugins during conda's startup process.
"""

from __future__ import annotations

import functools
import logging
from importlib.metadata import distributions
from inspect import getmodule, isclass
from typing import TYPE_CHECKING, overload

import pluggy

from ..auxlib.ish import dals
from ..base.context import add_plugin_setting, context
from ..exceptions import CondaValueError, PluginError
from . import post_solves, solvers, subcommands, virtual_packages
from .hookspec import CondaSpecs, spec_name
from .subcommands.doctor import health_checks

if TYPE_CHECKING:
    from typing import Literal

    from requests.auth import AuthBase

    from ..common.configuration import ParameterLoader
    from ..core.solve import Solver
    from ..models.match_spec import MatchSpec
    from ..models.records import PackageRecord
    from .types import (
        CondaAuthHandler,
        CondaHealthCheck,
        CondaPostCommand,
        CondaPostSolve,
        CondaPreCommand,
        CondaPreSolve,
        CondaSetting,
        CondaSolver,
        CondaSubcommand,
        CondaVirtualPackage,
    )

log = logging.getLogger(__name__)


class CondaPluginManager(pluggy.PluginManager):
    """
    The conda plugin manager to implement behavior additional to pluggy's default plugin manager.
    """

    #: Cached version of the :meth:`~conda.plugins.manager.CondaPluginManager.get_solver_backend`
    #: method.
    get_cached_solver_backend = None

    def __init__(self, project_name: str | None = None, *args, **kwargs) -> None:
        # Setting the default project name to the spec name for ease of use
        if project_name is None:
            project_name = spec_name
        super().__init__(project_name, *args, **kwargs)
        # Make the cache containers local to the instances so that the
        # reference from cache to the instance gets garbage collected with the instance
        self.get_cached_solver_backend = functools.lru_cache(maxsize=None)(
            self.get_solver_backend
        )

    def get_canonical_name(self, plugin: object) -> str:
        # detect the fully qualified module name
        prefix = "<unknown_module>"
        if (module := getmodule(plugin)) and module.__spec__:
            prefix = module.__spec__.name

        # return the fully qualified name for modules
        if module is plugin:
            return prefix

        # return the fully qualified name for classes
        elif isclass(plugin):
            return f"{prefix}.{plugin.__qualname__}"

        # return the fully qualified name for instances
        else:
            return f"{prefix}.{plugin.__class__.__qualname__}[{id(plugin)}]"

    def register(self, plugin, name: str | None = None) -> str | None:
        """
        Call :meth:`pluggy.PluginManager.register` and return the result or
        ignore errors raised, except ``ValueError``, which means the plugin
        had already been registered.
        """
        try:
            # register plugin but ignore ValueError since that means
            # the plugin has already been registered
            return super().register(plugin, name=name)
        except ValueError:
            return None
        except Exception as err:
            raise PluginError(
                f"Error while loading conda plugin: "
                f"{name or self.get_canonical_name(plugin)} ({err})"
            ) from err

    def load_plugins(self, *plugins) -> int:
        """
        Load the provided list of plugins and fail gracefully on error.
        The provided list of plugins can either be classes or modules with
        :attr:`~conda.plugins.hookimpl`.
        """
        count = 0
        for plugin in plugins:
            if self.register(plugin):
                count += 1
        return count

    def load_entrypoints(self, group: str, name: str | None = None) -> int:
        """Load modules from querying the specified setuptools ``group``.

        :param str group: Entry point group to load plugins.
        :param str name: If given, loads only plugins with the given ``name``.
        :rtype: int
        :return: The number of plugins loaded by this call.
        """
        count = 0
        for dist in distributions():
            for entry_point in dist.entry_points:
                # skip entry points that don't match the group/name
                if entry_point.group != group or (
                    name is not None and entry_point.name != name
                ):
                    continue

                # attempt to load plugin from entry point
                try:
                    plugin = entry_point.load()
                except Exception as err:
                    # not using exc_info=True here since the CLI loggers are
                    # set up after CLI initialization and argument parsing,
                    # meaning that it comes too late to properly render
                    # a traceback; instead we pass exc_info conditionally on
                    # context.verbosity
                    log.warning(
                        f"Error while loading conda entry point: {entry_point.name} ({err})",
                        exc_info=err if context.info else None,
                    )
                    continue

                if self.register(plugin):
                    count += 1
        return count

    @overload
    def get_hook_results(
        self, name: Literal["subcommands"]
    ) -> list[CondaSubcommand]: ...

    @overload
    def get_hook_results(
        self, name: Literal["virtual_packages"]
    ) -> list[CondaVirtualPackage]: ...

    @overload
    def get_hook_results(self, name: Literal["solvers"]) -> list[CondaSolver]: ...

    @overload
    def get_hook_results(
        self, name: Literal["pre_commands"]
    ) -> list[CondaPreCommand]: ...

    @overload
    def get_hook_results(
        self, name: Literal["post_commands"]
    ) -> list[CondaPostCommand]: ...

    @overload
    def get_hook_results(
        self, name: Literal["auth_handlers"]
    ) -> list[CondaAuthHandler]: ...

    @overload
    def get_hook_results(
        self, name: Literal["health_checks"]
    ) -> list[CondaHealthCheck]: ...

    @overload
    def get_hook_results(self, name: Literal["pre_solves"]) -> list[CondaPreSolve]: ...

    @overload
    def get_hook_results(
        self, name: Literal["post_solves"]
    ) -> list[CondaPostSolve]: ...

    @overload
    def get_hook_results(self, name: Literal["settings"]) -> list[CondaSetting]: ...

    def get_hook_results(self, name):
        """
        Return results of the plugin hooks with the given name and
        raise an error if there is a conflict.
        """
        specname = f"{self.project_name}_{name}"  # e.g. conda_solvers
        hook = getattr(self.hook, specname, None)
        if hook is None:
            raise PluginError(f"Could not find requested `{name}` plugins")

        plugins = [item for items in hook() for item in items]

        # Check for invalid names
        invalid = [plugin for plugin in plugins if not isinstance(plugin.name, str)]
        if invalid:
            raise PluginError(
                dals(
                    f"""
                    Invalid plugin names found:

                    {', '.join([str(plugin) for plugin in invalid])}

                    Please report this issue to the plugin author(s).
                    """
                )
            )
        plugins = sorted(plugins, key=lambda plugin: plugin.name)

        # Check for conflicts
        seen = set()
        conflicts = [
            plugin for plugin in plugins if plugin.name in seen or seen.add(plugin.name)
        ]
        if conflicts:
            raise PluginError(
                dals(
                    f"""
                    Conflicting `{name}` plugins found:

                    {', '.join([str(conflict) for conflict in conflicts])}

                    Multiple conda plugins are registered via the `{specname}` hook.
                    Please make sure that you don't have any incompatible plugins installed.
                    """
                )
            )
        return plugins

    def get_solvers(self) -> dict[str, CondaSolver]:
        """Return a mapping from solver name to solver class."""
        return {
            solver_plugin.name.lower(): solver_plugin
            for solver_plugin in self.get_hook_results("solvers")
        }

    def get_solver_backend(self, name: str | None = None) -> type[Solver]:
        """
        Get the solver backend with the given name (or fall back to the
        name provided in the context).

        See ``context.solver`` for more details.

        Please use the cached version of this method called
        :meth:`get_cached_solver_backend` for high-throughput code paths
        which is set up as a instance-specific LRU cache.
        """
        # Some light data validation in case name isn't given.
        if name is None:
            name = context.solver
        name = name.lower()

        solvers_mapping = self.get_solvers()

        # Look up the solver mapping and fail loudly if it can't
        # find the requested solver.
        solver_plugin = solvers_mapping.get(name, None)
        if solver_plugin is None:
            raise CondaValueError(
                f"You have chosen a non-default solver backend ({name}) "
                f"but it was not recognized. Choose one of: "
                f"{', '.join(solvers_mapping)}"
            )

        return solver_plugin.backend

    def get_auth_handler(self, name: str) -> type[AuthBase] | None:
        """
        Get the auth handler with the given name or None
        """
        auth_handlers = self.get_hook_results("auth_handlers")
        matches = tuple(
            item for item in auth_handlers if item.name.lower() == name.lower().strip()
        )

        if len(matches) > 0:
            return matches[0].handler
        return None

    def get_settings(self) -> dict[str, ParameterLoader]:
        """
        Return a mapping of plugin setting name to ParameterLoader class

        This method intentionally overwrites any duplicates that may be present
        """
        return {
            config_param.name.lower(): (config_param.parameter, config_param.aliases)
            for config_param in self.get_hook_results("settings")
        }

    def invoke_pre_commands(self, command: str) -> None:
        """
        Invokes ``CondaPreCommand.action`` functions registered with ``conda_pre_commands``.

        :param command: name of the command that is currently being invoked
        """
        for hook in self.get_hook_results("pre_commands"):
            if command in hook.run_for:
                hook.action(command)

    def invoke_post_commands(self, command: str) -> None:
        """
        Invokes ``CondaPostCommand.action`` functions registered with ``conda_post_commands``.

        :param command: name of the command that is currently being invoked
        """
        for hook in self.get_hook_results("post_commands"):
            if command in hook.run_for:
                hook.action(command)

    def disable_external_plugins(self) -> None:
        """
        Disables all currently registered plugins except built-in conda plugins
        """
        for name, plugin in self.list_name_plugin():
            if not name.startswith("conda.plugins.") and not self.is_blocked(name):
                self.set_blocked(name)

    def get_subcommands(self) -> dict[str, CondaSubcommand]:
        return {
            subcommand.name.lower(): subcommand
            for subcommand in self.get_hook_results("subcommands")
        }

    def get_virtual_packages(self) -> tuple[CondaVirtualPackage, ...]:
        return tuple(self.get_hook_results("virtual_packages"))

    def invoke_health_checks(self, prefix: str, verbose: bool) -> None:
        for hook in self.get_hook_results("health_checks"):
            try:
                hook.action(prefix, verbose)
            except Exception as err:
                log.warning(f"Error running health check: {hook.name} ({err})")
                continue

    def invoke_pre_solves(
        self,
        specs_to_add: frozenset[MatchSpec],
        specs_to_remove: frozenset[MatchSpec],
    ) -> None:
        """
        Invokes ``CondaPreSolve.action`` functions registered with ``conda_pre_solves``.

        :param specs_to_add:
        :param specs_to_remove:
        """
        for hook in self.get_hook_results("pre_solves"):
            hook.action(specs_to_add, specs_to_remove)

    def invoke_post_solves(
        self,
        repodata_fn: str,
        unlink_precs: tuple[PackageRecord, ...],
        link_precs: tuple[PackageRecord, ...],
    ) -> None:
        """
        Invokes ``CondaPostSolve.action`` functions registered with ``conda_post_solves``.

        :param repodata_fn:
        :param unlink_precs:
        :param link_precs:
        """
        for hook in self.get_hook_results("post_solves"):
            hook.action(repodata_fn, unlink_precs, link_precs)

    def load_settings(self) -> None:
        """
        Iterates through all registered settings and adds them to the
        :class:`conda.common.configuration.PluginConfig` class.
        """
        for name, (parameter, aliases) in self.get_settings().items():
            add_plugin_setting(name, parameter, aliases)


@functools.lru_cache(maxsize=None)  # FUTURE: Python 3.9+, replace w/ functools.cache
def get_plugin_manager() -> CondaPluginManager:
    """
    Get a cached version of the :class:`~conda.plugins.manager.CondaPluginManager` instance,
    with the built-in and entrypoints provided by the plugins loaded.
    """
    plugin_manager = CondaPluginManager()
    plugin_manager.add_hookspecs(CondaSpecs)
    plugin_manager.load_plugins(
        solvers,
        *virtual_packages.plugins,
        *subcommands.plugins,
        health_checks,
        *post_solves.plugins,
    )
    plugin_manager.load_entrypoints(spec_name)
    return plugin_manager


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
from . import doctor

plugins = [doctor]


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Implementation for `conda doctor` subcommand.
Adds various environment and package checks to detect issues or possible environment
corruption.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

from ....base.context import context
from ....cli.helpers import (
    add_parser_help,
    add_parser_prefix,
    add_parser_verbose,
)
from ....deprecations import deprecated
from ... import CondaSubcommand, hookimpl

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace


@deprecated(
    "24.3", "24.9", addendum="Use `conda.base.context.context.target_prefix` instead."
)
def get_prefix(args: Namespace) -> str:
    context.__init__(argparse_args=args)
    return context.target_prefix


def configure_parser(parser: ArgumentParser):
    add_parser_verbose(parser)
    add_parser_help(parser)
    add_parser_prefix(parser)


def execute(args: Namespace) -> None:
    """Run registered health_check plugins."""
    print(f"Environment Health Report for: {context.target_prefix}\n")
    context.plugin_manager.invoke_health_checks(context.target_prefix, context.verbose)


@hookimpl
def conda_subcommands():
    yield CondaSubcommand(
        name="doctor",
        summary="Display a health report for your environment.",
        action=execute,
        configure_parser=configure_parser,
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Backend logic implementation for `conda doctor`."""

from __future__ import annotations

import json
from logging import getLogger
from pathlib import Path
from typing import TYPE_CHECKING

from ....base.context import context
from ....core.envs_manager import get_user_environments_txt_file
from ....deprecations import deprecated
from ....exceptions import CondaError
from ....gateways.disk.read import compute_sum
from ... import CondaHealthCheck, hookimpl

if TYPE_CHECKING:
    import os

logger = getLogger(__name__)

OK_MARK = ""
X_MARK = ""


@deprecated("24.3", "24.9")
def display_report_heading(prefix: str) -> None:
    """Displays our report heading."""
    print(f"Environment Health Report for: {Path(prefix)}\n")


def check_envs_txt_file(prefix: str | os.PathLike | Path) -> bool:
    """Checks whether the environment is listed in the environments.txt file"""
    prefix = Path(prefix)
    envs_txt_file = Path(get_user_environments_txt_file())

    def samefile(path1: Path, path2: Path) -> bool:
        try:
            return path1.samefile(path2)
        except FileNotFoundError:
            # FileNotFoundError: path doesn't exist
            return path1 == path2

    try:
        for line in envs_txt_file.read_text().splitlines():
            stripped_line = line.strip()
            if stripped_line and samefile(prefix, Path(stripped_line)):
                return True
    except (IsADirectoryError, FileNotFoundError, PermissionError) as err:
        logger.error(
            f"{envs_txt_file} could not be "
            f"accessed because of the following error: {err}"
        )
    return False


def excluded_files_check(filename: str) -> bool:
    excluded_extensions = (".pyc", ".pyo")
    return filename.endswith(excluded_extensions)


def find_packages_with_missing_files(prefix: str | Path) -> dict[str, list[str]]:
    """Finds packages listed in conda-meta which have missing files."""
    packages_with_missing_files = {}
    prefix = Path(prefix)
    for file in (prefix / "conda-meta").glob("*.json"):
        for file_name in json.loads(file.read_text()).get("files", []):
            # Add warnings if json file has missing "files"
            if (
                not excluded_files_check(file_name)
                and not (prefix / file_name).exists()
            ):
                packages_with_missing_files.setdefault(file.stem, []).append(file_name)
    return packages_with_missing_files


def find_altered_packages(prefix: str | Path) -> dict[str, list[str]]:
    """Finds altered packages"""
    altered_packages = {}

    prefix = Path(prefix)
    for file in (prefix / "conda-meta").glob("*.json"):
        try:
            metadata = json.loads(file.read_text())
        except Exception as exc:
            logger.error(
                f"Could not load the json file {file} because of the following error: {exc}."
            )
            continue

        try:
            paths_data = metadata["paths_data"]
            paths = paths_data["paths"]
        except KeyError:
            continue

        if paths_data.get("paths_version") != 1:
            continue

        for path in paths:
            _path = path.get("_path")
            old_sha256 = path.get("sha256_in_prefix")
            if _path is None or old_sha256 is None:
                continue

            file_location = prefix / _path
            if not file_location.is_file():
                continue

            try:
                new_sha256 = compute_sum(file_location, "sha256")
            except OSError as err:
                raise CondaError(
                    f"Could not generate checksum for file {file_location} "
                    f"because of the following error: {err}."
                )

            if old_sha256 != new_sha256:
                altered_packages.setdefault(file.stem, []).append(_path)

    return altered_packages


@deprecated("24.3", "24.9")
def display_health_checks(prefix: str, verbose: bool = False) -> None:
    """Prints health report."""
    print(f"Environment Health Report for: {prefix}\n")
    context.plugin_manager.invoke_health_checks(prefix, verbose)


def missing_files(prefix: str, verbose: bool) -> None:
    print("Missing Files:\n")
    missing_files = find_packages_with_missing_files(prefix)
    if missing_files:
        for package_name, missing_files in missing_files.items():
            if verbose:
                delimiter = "\n  "
                print(f"{package_name}:{delimiter}{delimiter.join(missing_files)}")
            else:
                print(f"{package_name}: {len(missing_files)}\n")
    else:
        print(f"{OK_MARK} There are no packages with missing files.\n")


def altered_files(prefix: str, verbose: bool) -> None:
    print("Altered Files:\n")
    altered_packages = find_altered_packages(prefix)
    if altered_packages:
        for package_name, altered_files in altered_packages.items():
            if verbose:
                delimiter = "\n  "
                print(f"{package_name}:{delimiter}{delimiter.join(altered_files)}\n")
            else:
                print(f"{package_name}: {len(altered_files)}\n")
    else:
        print(f"{OK_MARK} There are no packages with altered files.\n")


def env_txt_check(prefix: str, verbose: bool) -> None:
    present = OK_MARK if check_envs_txt_file(prefix) else X_MARK
    print(f"Environment listed in environments.txt file: {present}\n")


@hookimpl
def conda_health_checks():
    yield CondaHealthCheck(name="Missing Files", action=missing_files)
    yield CondaHealthCheck(name="Altered Files", action=altered_files)
    yield CondaHealthCheck(name="Environment.txt File Check", action=env_txt_check)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Detect whether this is macOS."""

import os

from ...base.context import context
from .. import CondaVirtualPackage, hookimpl


@hookimpl
def conda_virtual_packages():
    if not context.subdir.startswith("osx-"):
        return

    yield CondaVirtualPackage("unix", None, None)

    _, dist_version = context.os_distribution_name_version
    dist_version = os.environ.get("CONDA_OVERRIDE_OSX", dist_version)
    if dist_version:
        yield CondaVirtualPackage("osx", dist_version, None)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Detect whether this is Linux."""

import os
import re

from ...base.context import context
from ...common._os.linux import linux_get_libc_version
from .. import CondaVirtualPackage, hookimpl


@hookimpl
def conda_virtual_packages():
    if not context.subdir.startswith("linux-"):
        return

    yield CondaVirtualPackage("unix", None, None)

    # By convention, the kernel release string should be three or four
    # numeric components, separated by dots, followed by vendor-specific
    # bits.  For the purposes of versioning the `__linux` virtual package,
    # discard everything after the last digit of the third or fourth
    # numeric component; note that this breaks version ordering for
    # development (`-rcN`) kernels, but that can be a TODO for later.
    _, dist_version = context.platform_system_release
    dist_version = os.environ.get("CONDA_OVERRIDE_LINUX", dist_version)
    m = re.match(r"\d+\.\d+(\.\d+)?(\.\d+)?", dist_version)
    yield CondaVirtualPackage("linux", m.group() if m else "0", None)

    libc_family, libc_version = linux_get_libc_version()
    if not (libc_family and libc_version):
        # Default to glibc when using CONDA_SUBDIR var
        libc_family = "glibc"
    libc_version = os.getenv(f"CONDA_OVERRIDE_{libc_family.upper()}", libc_version)
    if libc_version:
        yield CondaVirtualPackage(libc_family, libc_version, None)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Detect whether this is FeeBSD."""

from ...base.context import context
from .. import CondaVirtualPackage, hookimpl


@hookimpl
def conda_virtual_packages():
    if not context.subdir.startswith("freebsd-"):
        return

    yield CondaVirtualPackage("unix", None, None)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Detect whether this is Windows."""

from ...base.context import context
from .. import CondaVirtualPackage, hookimpl


@hookimpl
def conda_virtual_packages():
    if not context.subdir.startswith("win-"):
        return

    yield CondaVirtualPackage("win", None, None)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Detect CUDA version."""

import ctypes
import functools
import itertools
import multiprocessing
import os
import platform
from contextlib import suppress

from .. import CondaVirtualPackage, hookimpl


def cuda_version():
    """
    Attempt to detect the version of CUDA present in the operating system.

    On Windows and Linux, the CUDA library is installed by the NVIDIA
    driver package, and is typically found in the standard library path,
    rather than with the CUDA SDK (which is optional for running CUDA apps).

    On macOS, the CUDA library is only installed with the CUDA SDK, and
    might not be in the library path.

    Returns: version string (e.g., '9.2') or None if CUDA is not found.
    """
    if "CONDA_OVERRIDE_CUDA" in os.environ:
        return os.environ["CONDA_OVERRIDE_CUDA"].strip() or None

    # Do not inherit file descriptors and handles from the parent process.
    # The `fork` start method should be considered unsafe as it can lead to
    # crashes of the subprocess. The `spawn` start method is preferred.
    context = multiprocessing.get_context("spawn")
    queue = context.SimpleQueue()
    try:
        # Spawn a subprocess to detect the CUDA version
        detector = context.Process(
            target=_cuda_driver_version_detector_target,
            args=(queue,),
            name="CUDA driver version detector",
            daemon=True,
        )
        detector.start()
        detector.join(timeout=60.0)
    finally:
        # Always cleanup the subprocess
        detector.kill()  # requires Python 3.7+

    if queue.empty():
        return None

    result = queue.get()
    return result


@functools.lru_cache(maxsize=None)
def cached_cuda_version():
    """A cached version of the cuda detection system."""
    return cuda_version()


@hookimpl
def conda_virtual_packages():
    cuda_version = cached_cuda_version()
    if cuda_version is not None:
        yield CondaVirtualPackage("cuda", cuda_version, None)


def _cuda_driver_version_detector_target(queue):
    """
    Attempt to detect the version of CUDA present in the operating system in a
    subprocess.

    On Windows and Linux, the CUDA library is installed by the NVIDIA
    driver package, and is typically found in the standard library path,
    rather than with the CUDA SDK (which is optional for running CUDA apps).

    On macOS, the CUDA library is only installed with the CUDA SDK, and
    might not be in the library path.

    Returns: version string (e.g., '9.2') or None if CUDA is not found.
             The result is put in the queue rather than a return value.
    """
    # Platform-specific libcuda location
    system = platform.system()
    if system == "Darwin":
        lib_filenames = [
            "libcuda.1.dylib",  # check library path first
            "libcuda.dylib",
            "/usr/local/cuda/lib/libcuda.1.dylib",
            "/usr/local/cuda/lib/libcuda.dylib",
        ]
    elif system == "Linux":
        lib_filenames = [
            "libcuda.so",  # check library path first
            "/usr/lib64/nvidia/libcuda.so",  # RHEL/Centos/Fedora
            "/usr/lib/x86_64-linux-gnu/libcuda.so",  # Ubuntu
            "/usr/lib/wsl/lib/libcuda.so",  # WSL
        ]
        # Also add libraries with version suffix `.1`
        lib_filenames = list(
            itertools.chain.from_iterable((f"{lib}.1", lib) for lib in lib_filenames)
        )
    elif system == "Windows":
        bits = platform.architecture()[0].replace("bit", "")  # e.g. "64" or "32"
        lib_filenames = [f"nvcuda{bits}.dll", "nvcuda.dll"]
    else:
        queue.put(None)  # CUDA not available for other operating systems
        return

    # Open library
    if system == "Windows":
        dll = ctypes.windll
    else:
        dll = ctypes.cdll
    for lib_filename in lib_filenames:
        with suppress(Exception):
            libcuda = dll.LoadLibrary(lib_filename)
            break
    else:
        queue.put(None)
        return

    # Empty `CUDA_VISIBLE_DEVICES` can cause `cuInit()` returns `CUDA_ERROR_NO_DEVICE`
    # Invalid `CUDA_VISIBLE_DEVICES` can cause `cuInit()` returns `CUDA_ERROR_INVALID_DEVICE`
    # Unset this environment variable to avoid these errors
    os.environ.pop("CUDA_VISIBLE_DEVICES", None)

    # Get CUDA version
    try:
        cuInit = libcuda.cuInit
        flags = ctypes.c_uint(0)
        ret = cuInit(flags)
        if ret != 0:
            queue.put(None)
            return

        cuDriverGetVersion = libcuda.cuDriverGetVersion
        version_int = ctypes.c_int(0)
        ret = cuDriverGetVersion(ctypes.byref(version_int))
        if ret != 0:
            queue.put(None)
            return

        # Convert version integer to version string
        value = version_int.value
        queue.put(f"{value // 1000}.{(value % 1000) // 10}")
        return
    except Exception:
        queue.put(None)
        return


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Detect archspec name."""

import os

from .. import CondaVirtualPackage, hookimpl


@hookimpl
def conda_virtual_packages():
    from ...core.index import get_archspec_name

    archspec_name = get_archspec_name()
    archspec_name = os.getenv("CONDA_OVERRIDE_ARCHSPEC", archspec_name)
    if archspec_name:
        yield CondaVirtualPackage("archspec", "1", archspec_name)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Expose conda version."""

from .. import CondaVirtualPackage, hookimpl


@hookimpl
def conda_virtual_packages():
    from ... import __version__

    yield CondaVirtualPackage("conda", __version__, None)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
from __future__ import annotations

from . import archspec, conda, cuda, freebsd, linux, osx, windows

#: The list of virtual package plugins for easier registration with pluggy
plugins = [archspec, conda, cuda, freebsd, linux, osx, windows]


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Register signature verification as a post-solve plugin."""

from .. import CondaPostSolve, hookimpl


@hookimpl
def conda_post_solves():
    from ...trust.signature_verification import signature_verification

    yield CondaPostSolve(
        name="signature-verification",
        action=signature_verification,
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Register the built-in post_solves hook implementations."""

from . import signature_verification

#: The list of post-solve plugins for easier registration with pluggy
plugins = [signature_verification]


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Implements all conda.notices types."""

from __future__ import annotations

import hashlib
from datetime import datetime
from typing import TYPE_CHECKING, NamedTuple

from ..base.constants import NoticeLevel

if TYPE_CHECKING:
    from pathlib import Path
    from typing import Sequence

#: Value to use for message ID when it is not provided
UNDEFINED_MESSAGE_ID = "undefined"


class ChannelNotice(NamedTuple):
    """Represents an individual channel notice."""

    id: str
    channel_name: str | None
    message: str | None
    level: NoticeLevel
    created_at: datetime | None
    expired_at: datetime | None
    interval: int | None

    def to_dict(self):
        return {
            "id": self.id,
            "channel_name": self.channel_name,
            "message": self.message,
            "level": self.level.name.lower(),
            "created_at": self.created_at.isoformat(),
            "expired_at": self.expired_at.isoformat(),
            "interval": self.interval,
        }


class ChannelNoticeResultSet(NamedTuple):
    """
    Represents a list of a channel notices, plus some accompanying
    metadata such as `viewed_channel_notices`.
    """

    #: Channel notices that are included in this particular set
    channel_notices: Sequence[ChannelNotice]

    #: Total number of channel notices; not just the ones that will be displayed
    total_number_channel_notices: int

    #: The number of channel notices that have already been viewed
    viewed_channel_notices: int


class ChannelNoticeResponse(NamedTuple):
    url: str
    name: str
    json_data: dict | None

    @property
    def notices(self) -> Sequence[ChannelNotice]:
        if self.json_data:
            notices = self.json_data.get("notices", ())

            return tuple(
                ChannelNotice(
                    id=str(notice.get("id", UNDEFINED_MESSAGE_ID)),
                    channel_name=self.name,
                    message=notice.get("message"),
                    level=self._parse_notice_level(notice.get("level")),
                    created_at=self._parse_iso_timestamp(notice.get("created_at")),
                    expired_at=self._parse_iso_timestamp(notice.get("expired_at")),
                    interval=notice.get("interval"),
                )
                for notice in notices
            )

        # Default value
        return ()

    @staticmethod
    def _parse_notice_level(level: str | None) -> NoticeLevel:
        """
        We use this to validate notice levels and provide reasonable defaults
        if any are invalid.
        """
        try:
            return NoticeLevel(level)
        except ValueError:
            # If we get an invalid value, rather than fail, we simply use a reasonable default
            return NoticeLevel(NoticeLevel.INFO)

    @staticmethod
    def _parse_iso_timestamp(iso_timestamp: str | None) -> datetime | None:
        """Parse ISO timestamp and fail over to a default value of none."""
        if iso_timestamp is None:
            return None
        try:
            return datetime.fromisoformat(iso_timestamp)
        except ValueError:
            return None

    @classmethod
    def get_cache_key(cls, url: str, cache_dir: Path) -> Path:
        """Returns where this channel response will be cached by hashing the URL."""
        bytes_filename = url.encode()
        sha256_hash = hashlib.sha256(bytes_filename)
        cache_filename = f"{sha256_hash.hexdigest()}.json"

        return cache_dir.joinpath(cache_filename)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Handles all display/view logic."""

import json
from typing import Sequence

from ..base.context import context
from .types import ChannelNotice


def print_notices(channel_notices: Sequence[ChannelNotice]):
    """
    Accepts a list of channel notice responses and prints a display.

    Args:
        channel_notices: A sequence of ChannelNotice objects.
    """
    current_channel = None

    if context.json:
        json_output = json.dumps(
            [channel_notice.to_dict() for channel_notice in channel_notices]
        )
        print(json_output)

    else:
        for channel_notice in channel_notices:
            if current_channel != channel_notice.channel_name:
                print()
                channel_header = "Channel"
                channel_header += (
                    f' "{channel_notice.channel_name}" has the following notices:'
                )
                print(channel_header)
                current_channel = channel_notice.channel_name
            print_notice_message(channel_notice)
            print()


def print_notice_message(notice: ChannelNotice, indent: str = "  ") -> None:
    """Prints a single channel notice."""
    timestamp = f"{notice.created_at:%c}" if notice.created_at else ""

    level = f"[{notice.level}] -- {timestamp}"

    print(f"{indent}{level}\n{indent}{notice.message}")


def print_more_notices_message(
    total_notices: int, displayed_notices: int, viewed_notices: int
) -> None:
    """Conditionally shows a message informing users how many more message there are."""
    notices_not_shown = total_notices - viewed_notices - displayed_notices

    if notices_not_shown > 0:
        if notices_not_shown > 1:
            msg = f"There are {notices_not_shown} more messages. To retrieve them run:\n\n"
        else:
            msg = f"There is {notices_not_shown} more message. To retrieve it run:\n\n"
        print(f"{msg}conda notices\n")


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Core conda notices logic."""

from __future__ import annotations

import logging
import time
from functools import wraps
from typing import TYPE_CHECKING

from ..base.constants import NOTICES_DECORATOR_DISPLAY_INTERVAL, NOTICES_FN
from ..base.context import context
from ..models.channel import get_channel_objs
from . import cache, fetch, views
from .types import ChannelNoticeResultSet

if TYPE_CHECKING:
    from typing import Sequence

    from ..base.context import Context
    from ..models.channel import Channel, MultiChannel
    from .types import ChannelNotice, ChannelNoticeResponse

# Used below in type hints
ChannelName = str
ChannelUrl = str

logger = logging.getLogger(__name__)


def retrieve_notices(
    limit: int | None = None,
    always_show_viewed: bool = True,
    silent: bool = False,
) -> ChannelNoticeResultSet:
    """
    Function used for retrieving notices. This is called by the "notices" decorator as well
    as the sub-command "notices"

    Args:
        limit: Limit the number of notices to show (defaults to None).
        always_show_viewed: Whether all notices should be shown, not only the unread ones
                            (defaults to True).
        silent: Whether to use a spinner when fetching and caching notices.
    """
    channel_name_urls = get_channel_name_and_urls(get_channel_objs(context))
    channel_notice_responses = fetch.get_notice_responses(
        channel_name_urls, silent=silent
    )
    channel_notices = flatten_notice_responses(channel_notice_responses)
    total_number_channel_notices = len(channel_notices)

    cache_file = cache.get_notices_cache_file()

    # We always want to modify the mtime attribute of the file if we are trying to retrieve notices
    # This is used later in "is_channel_notices_cache_expired"
    cache_file.touch()

    viewed_notices = None
    viewed_channel_notices = 0
    if not always_show_viewed:
        viewed_notices = cache.get_viewed_channel_notice_ids(
            cache_file, channel_notices
        )
        viewed_channel_notices = len(viewed_notices)

    channel_notices = filter_notices(
        channel_notices, limit=limit, exclude=viewed_notices
    )

    return ChannelNoticeResultSet(
        channel_notices=channel_notices,
        viewed_channel_notices=viewed_channel_notices,
        total_number_channel_notices=total_number_channel_notices,
    )


def display_notices(channel_notice_set: ChannelNoticeResultSet) -> None:
    """Prints the channel notices to std out."""
    views.print_notices(channel_notice_set.channel_notices)

    # Updates cache database, marking displayed notices as "viewed"
    cache_file = cache.get_notices_cache_file()
    cache.mark_channel_notices_as_viewed(cache_file, channel_notice_set.channel_notices)

    views.print_more_notices_message(
        channel_notice_set.total_number_channel_notices,
        len(channel_notice_set.channel_notices),
        channel_notice_set.viewed_channel_notices,
    )


def notices(func):
    """
    Wrapper for "execute" entry points for subcommands.

    If channel notices need to be fetched, we do that first and then
    run the command normally. We then display these notices at the very
    end of the command output so that the user is more likely to see them.

    This ordering was specifically done to address the following bug report:
        - https://github.com/conda/conda/issues/11847

    Args:
        func: Function to be decorated
    """

    @wraps(func)
    def wrapper(*args, **kwargs):
        if is_channel_notices_enabled(context):
            channel_notice_set = None

            try:
                if is_channel_notices_cache_expired():
                    channel_notice_set = retrieve_notices(
                        limit=context.number_channel_notices,
                        always_show_viewed=False,
                        silent=True,
                    )
            except OSError as exc:
                # If we encounter any OSError related error, we simply abandon
                # fetching notices
                logger.error(f"Unable to open cache file: {str(exc)}")

            if channel_notice_set is not None:
                return_value = func(*args, **kwargs)
                display_notices(channel_notice_set)

                return return_value

        return func(*args, **kwargs)

    return wrapper


def get_channel_name_and_urls(
    channels: Sequence[Channel | MultiChannel],
) -> list[tuple[ChannelUrl, ChannelName]]:
    """
    Return a sequence of Channel URL and name tuples.

    This function handles both Channel and MultiChannel object types.
    """
    channel_name_and_urls = []

    for channel in channels:
        name = channel.name or channel.location

        for url in channel.base_urls:
            full_url = url.rstrip("/")
            channel_name_and_urls.append((f"{full_url}/{NOTICES_FN}", name))

    return channel_name_and_urls


def flatten_notice_responses(
    channel_notice_responses: Sequence[ChannelNoticeResponse],
) -> Sequence[ChannelNotice]:
    return tuple(
        notice
        for channel in channel_notice_responses
        if channel.notices
        for notice in channel.notices
    )


def filter_notices(
    channel_notices: Sequence[ChannelNotice],
    limit: int | None = None,
    exclude: set[str] | None = None,
) -> Sequence[ChannelNotice]:
    """Perform filtering actions for the provided sequence of ChannelNotice objects."""
    if exclude:
        channel_notices = tuple(
            channel_notice
            for channel_notice in channel_notices
            if channel_notice.id not in exclude
        )

    if limit is not None:
        channel_notices = channel_notices[:limit]

    return channel_notices


def is_channel_notices_enabled(ctx: Context) -> bool:
    """
    Determines whether channel notices are enabled and therefore displayed when
    invoking the `notices` command decorator.

    This only happens when:
     - offline is False
     - number_channel_notices is greater than 0

    Args:
        ctx: The conda context object
    """
    return ctx.number_channel_notices > 0 and not ctx.offline and not ctx.json


def is_channel_notices_cache_expired() -> bool:
    """
    Checks to see if the notices cache file we use to keep track of
    displayed notices is expired. This involves checking the mtime
    attribute of the file. Anything older than what is specified as
    the NOTICES_DECORATOR_DISPLAY_INTERVAL is considered expired.
    """
    cache_file = cache.get_notices_cache_file()

    cache_file_stat = cache_file.stat()
    now = time.time()
    seconds_since_checked = now - cache_file_stat.st_mtime

    return seconds_since_checked >= NOTICES_DECORATOR_DISPLAY_INTERVAL


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Handles all caching logic including:
  - Retrieving from cache
  - Saving to cache
  - Determining whether not certain items have expired and need to be refreshed
"""

from __future__ import annotations

import json
import logging
import os
from datetime import datetime, timezone
from functools import wraps
from pathlib import Path
from typing import TYPE_CHECKING

try:
    from platformdirs import user_cache_dir
except ImportError:  # pragma: no cover
    from .._vendor.appdirs import user_cache_dir

from ..base.constants import APP_NAME, NOTICES_CACHE_FN, NOTICES_CACHE_SUBDIR
from ..utils import ensure_dir_exists
from .types import ChannelNoticeResponse

if TYPE_CHECKING:
    from typing import Sequence

    from .types import ChannelNotice

logger = logging.getLogger(__name__)


def cached_response(func):
    @wraps(func)
    def wrapper(url: str, name: str):
        cache_dir = get_notices_cache_dir()
        cache_val = get_notice_response_from_cache(url, name, cache_dir)

        if cache_val:
            return cache_val

        return_value = func(url, name)
        if return_value is not None:
            write_notice_response_to_cache(return_value, cache_dir)

        return return_value

    return wrapper


def is_notice_response_cache_expired(
    channel_notice_response: ChannelNoticeResponse,
) -> bool:
    """
    This checks the contents of the cache response to see if it is expired.

    If for whatever reason we encounter an exception while parsing the individual
    messages, we assume an invalid cache and return true.
    """
    now = datetime.now(timezone.utc)

    def is_channel_notice_expired(expired_at: datetime | None) -> bool:
        """If there is no "expired_at" field present assume it is expired."""
        if expired_at is None:
            return True
        return expired_at < now

    return any(
        is_channel_notice_expired(chn.expired_at)
        for chn in channel_notice_response.notices
    )


@ensure_dir_exists
def get_notices_cache_dir() -> Path:
    """Returns the location of the notices cache directory as a Path object"""
    cache_dir = user_cache_dir(APP_NAME, appauthor=APP_NAME)

    return Path(cache_dir).joinpath(NOTICES_CACHE_SUBDIR)


def get_notices_cache_file() -> Path:
    """Returns the location of the notices cache file as a Path object"""
    cache_dir = get_notices_cache_dir()
    cache_file = cache_dir.joinpath(NOTICES_CACHE_FN)

    if not cache_file.is_file():
        with open(cache_file, "w") as fp:
            fp.write("")

    return cache_file


def get_notice_response_from_cache(
    url: str, name: str, cache_dir: Path
) -> ChannelNoticeResponse | None:
    """Retrieves a notice response object from cache if it exists."""
    cache_key = ChannelNoticeResponse.get_cache_key(url, cache_dir)

    if os.path.isfile(cache_key):
        with open(cache_key) as fp:
            data = json.load(fp)
        chn_ntc_resp = ChannelNoticeResponse(url, name, data)

        if not is_notice_response_cache_expired(chn_ntc_resp):
            return chn_ntc_resp


def write_notice_response_to_cache(
    channel_notice_response: ChannelNoticeResponse, cache_dir: Path
) -> None:
    """Writes our notice data to our local cache location."""
    cache_key = ChannelNoticeResponse.get_cache_key(
        channel_notice_response.url, cache_dir
    )

    with open(cache_key, "w") as fp:
        json.dump(channel_notice_response.json_data, fp)


def mark_channel_notices_as_viewed(
    cache_file: Path, channel_notices: Sequence[ChannelNotice]
) -> None:
    """Insert channel notice into our database marking it as read."""
    notice_ids = {chn.id for chn in channel_notices}

    with open(cache_file) as fp:
        contents: str = fp.read()

    contents_unique = set(filter(None, set(contents.splitlines())))
    contents_new = contents_unique.union(notice_ids)

    # Save new version of cache file
    with open(cache_file, "w") as fp:
        fp.write("\n".join(contents_new))


def get_viewed_channel_notice_ids(
    cache_file: Path, channel_notices: Sequence[ChannelNotice]
) -> set[str]:
    """Return the ids of the channel notices which have already been seen."""
    notice_ids = {chn.id for chn in channel_notices}

    with open(cache_file) as fp:
        contents: str = fp.read()

    contents_unique = set(filter(None, set(contents.splitlines())))

    return notice_ids.intersection(contents_unique)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Notices network fetch logic."""

from __future__ import annotations

import logging
from concurrent.futures import ThreadPoolExecutor
from typing import TYPE_CHECKING

import requests

from ..base.context import context
from ..common.io import Spinner
from ..gateways.connection.session import get_session
from .cache import cached_response
from .types import ChannelNoticeResponse

if TYPE_CHECKING:
    from typing import Sequence

logger = logging.getLogger(__name__)


def get_notice_responses(
    url_and_names: Sequence[tuple[str, str]],
    silent: bool = False,
    max_workers: int = 10,
) -> Sequence[ChannelNoticeResponse]:
    """
    Provided a list of channel notification url/name tuples, return a sequence of
    ChannelNoticeResponse objects.

    Args:
        url_and_names: channel url and the channel name
        silent: turn off "loading animation" (defaults to False)
        max_workers: increase worker number in thread executor (defaults to 10)
    Returns:
        Sequence[ChannelNoticeResponse]
    """
    executor = ThreadPoolExecutor(max_workers=max_workers)

    with Spinner("Retrieving notices", enabled=not silent, json=context.json):
        return tuple(
            filter(
                None,
                (
                    chn_info
                    for chn_info in executor.map(
                        lambda args: get_channel_notice_response(*args), url_and_names
                    )
                ),
            )
        )


@cached_response
def get_channel_notice_response(url: str, name: str) -> ChannelNoticeResponse | None:
    """
    Return a channel response object. We use this to wrap the response with
    additional channel information to use. If the response was invalid we suppress/log
    and error message.
    """
    session = get_session(url)
    try:
        resp = session.get(
            url, allow_redirects=False, timeout=5
        )  # timeout: connect, read
    except requests.exceptions.Timeout:
        logger.info(f"Request timed out for channel: {name} url: {url}")
        return
    except requests.exceptions.RequestException as exc:
        logger.error(f"Request error <{exc}> for channel: {name} url: {url}")
        return

    try:
        if resp.status_code < 300:
            return ChannelNoticeResponse(url, name, json_data=resp.json())
        else:
            logger.info(f"Received {resp.status_code} when trying to GET {url}")
    except ValueError:
        logger.info(f"Unable to parse JSON data for {url}")
        return ChannelNoticeResponse(url, name, json_data=None)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
from .core import notices  # noqa: F401


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda list`.

Lists all packages installed into an environment.
"""

import logging
import re
from argparse import ArgumentParser, Namespace, _SubParsersAction
from os.path import isdir, isfile

log = logging.getLogger(__name__)


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .helpers import (
        add_parser_json,
        add_parser_prefix,
        add_parser_show_channel_urls,
    )

    summary = "List installed packages in a conda environment."
    description = summary
    epilog = dals(
        """
        Examples:

        List all packages in the current environment::

            conda list

        List all packages in reverse order::

            conda list --reverse

        List all packages installed into the environment 'myenv'::

            conda list -n myenv

        List all packages that begin with the letters "py", using regex::

            conda list ^py

        Save packages for future use::

            conda list --export > package-list.txt

        Reinstall packages from an export file::

            conda create -n myenv --file package-list.txt

        """
    )

    p = sub_parsers.add_parser(
        "list",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    add_parser_prefix(p)
    add_parser_json(p)
    add_parser_show_channel_urls(p)
    p.add_argument(
        "--reverse",
        action="store_true",
        default=False,
        help="List installed packages in reverse order.",
    )
    p.add_argument(
        "-c",
        "--canonical",
        action="store_true",
        help="Output canonical names of packages only.",
    )
    p.add_argument(
        "-f",
        "--full-name",
        action="store_true",
        help="Only search for full names, i.e., ^<regex>$. "
        "--full-name NAME is identical to regex '^NAME$'.",
    )
    p.add_argument(
        "--explicit",
        action="store_true",
        help="List explicitly all installed conda packages with URL "
        "(output may be used by conda create --file).",
    )
    p.add_argument(
        "--md5",
        action="store_true",
        help="Add MD5 hashsum when using --explicit.",
    )
    p.add_argument(
        "-e",
        "--export",
        action="store_true",
        help="Output explicit, machine-readable requirement strings instead of "
        "human-readable lists of packages. This output may be used by "
        "conda create --file.",
    )
    p.add_argument(
        "-r",
        "--revisions",
        action="store_true",
        help="List the revision history.",
    )
    p.add_argument(
        "--no-pip",
        action="store_false",
        default=True,
        dest="pip",
        help="Do not include pip-only installed packages.",
    )
    p.add_argument(
        "--auth",
        action="store_false",
        default=True,
        dest="remove_auth",
        help="In explicit mode, leave authentication details in package URLs. "
        "They are removed by default otherwise.",
    )
    p.add_argument(
        "regex",
        action="store",
        nargs="?",
        help="List only packages matching this regular expression.",
    )
    p.set_defaults(func="conda.cli.main_list.execute")

    return p


def print_export_header(subdir):
    print("# This file may be used to create an environment using:")
    print("# $ conda create --name <env> --file <this file>")
    print(f"# platform: {subdir}")


def get_packages(installed, regex):
    pat = re.compile(regex, re.I) if regex else None
    for prefix_rec in sorted(installed, key=lambda x: x.name.lower()):
        if pat and pat.search(prefix_rec.name) is None:
            continue
        yield prefix_rec


def list_packages(
    prefix,
    regex=None,
    format="human",
    reverse=False,
    show_channel_urls=None,
):
    from ..base.constants import DEFAULTS_CHANNEL_NAME
    from ..base.context import context
    from ..core.prefix_data import PrefixData
    from .common import disp_features

    res = 0

    installed = sorted(
        PrefixData(prefix, pip_interop_enabled=True).iter_records(),
        key=lambda x: x.name,
    )

    packages = []
    for prec in get_packages(installed, regex) if regex else installed:
        if format == "canonical":
            packages.append(
                prec.dist_fields_dump() if context.json else prec.dist_str()
            )
            continue
        if format == "export":
            packages.append("=".join((prec.name, prec.version, prec.build)))
            continue

        features = set(prec.get("features") or ())
        disp = "%(name)-25s %(version)-15s %(build)15s" % prec
        disp += f"  {disp_features(features)}"
        schannel = prec.get("schannel")
        show_channel_urls = show_channel_urls or context.show_channel_urls
        if (
            show_channel_urls
            or show_channel_urls is None
            and schannel != DEFAULTS_CHANNEL_NAME
        ):
            disp += f"  {schannel}"

        packages.append(disp)

    if reverse:
        packages = reversed(packages)

    result = []
    if format == "human":
        result = [
            f"# packages in environment at {prefix}:",
            "#",
            "# %-23s %-15s %15s  Channel" % ("Name", "Version", "Build"),
        ]
    result.extend(packages)

    return res, result


def print_packages(
    prefix,
    regex=None,
    format="human",
    reverse=False,
    piplist=False,
    json=False,
    show_channel_urls=None,
):
    from ..base.context import context
    from .common import stdout_json

    if not isdir(prefix):
        from ..exceptions import EnvironmentLocationNotFound

        raise EnvironmentLocationNotFound(prefix)

    if not json:
        if format == "export":
            print_export_header(context.subdir)

    exitcode, output = list_packages(
        prefix,
        regex,
        format=format,
        reverse=reverse,
        show_channel_urls=show_channel_urls,
    )
    if context.json:
        stdout_json(output)

    else:
        print("\n".join(map(str, output)))

    return exitcode


def print_explicit(prefix, add_md5=False, remove_auth=True):
    from ..base.constants import UNKNOWN_CHANNEL
    from ..base.context import context
    from ..common import url as common_url
    from ..core.prefix_data import PrefixData

    if not isdir(prefix):
        from ..exceptions import EnvironmentLocationNotFound

        raise EnvironmentLocationNotFound(prefix)
    print_export_header(context.subdir)
    print("@EXPLICIT")
    for prefix_record in PrefixData(prefix).iter_records_sorted():
        url = prefix_record.get("url")
        if not url or url.startswith(UNKNOWN_CHANNEL):
            print("# no URL for: {}".format(prefix_record["fn"]))
            continue
        if remove_auth:
            url = common_url.remove_auth(common_url.split_anaconda_token(url)[0])
        md5 = prefix_record.get("md5")
        print(url + (f"#{md5}" if add_md5 and md5 else ""))


def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.context import context
    from ..gateways.disk.test import is_conda_environment
    from ..history import History
    from .common import stdout_json

    prefix = context.target_prefix
    if not is_conda_environment(prefix):
        from ..exceptions import EnvironmentLocationNotFound

        raise EnvironmentLocationNotFound(prefix)

    regex = args.regex
    if args.full_name:
        regex = rf"^{regex}$"

    if args.revisions:
        h = History(prefix)
        if isfile(h.path):
            if not context.json:
                h.print_log()
            else:
                stdout_json(h.object_log())
        else:
            from ..exceptions import PathNotFoundError

            raise PathNotFoundError(h.path)
        return 0

    if args.explicit:
        print_explicit(prefix, args.md5, args.remove_auth)
        return 0

    if args.canonical:
        format = "canonical"
    elif args.export:
        format = "export"
    else:
        format = "human"

    if context.json:
        format = "canonical"

    return print_packages(
        prefix,
        regex,
        format,
        reverse=args.reverse,
        piplist=args.pip,
        json=context.json,
        show_channel_urls=context.show_channel_urls,
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda-env create`.

Creates new conda environments with the specified packages.
"""

import json
import os
from argparse import (
    ArgumentParser,
    Namespace,
    _SubParsersAction,
)

from .. import CondaError
from ..notices import notices


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .helpers import (
        add_output_and_prompt_options,
        add_parser_default_packages,
        add_parser_networking,
        add_parser_platform,
        add_parser_prefix,
        add_parser_solver,
    )

    summary = "Create an environment based on an environment definition file."
    description = dals(
        f"""
        {summary}

        If using an environment.yml file (the default), you can name the
        environment in the first line of the file with 'name: envname' or
        you can specify the environment name in the CLI command using the
        -n/--name argument. The name specified in the CLI will override
        the name specified in the environment.yml file.

        Unless you are in the directory containing the environment definition
        file, use -f to specify the file path of the environment definition
        file you want to use.

        """
    )
    epilog = dals(
        """
        Examples::

            conda env create
            conda env create -n envname
            conda env create folder/envname
            conda env create -f /path/to/environment.yml
            conda env create -f /path/to/requirements.txt -n envname
            conda env create -f /path/to/requirements.txt -p /home/user/envname

        """
    )

    p = sub_parsers.add_parser(
        "create",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    p.add_argument(
        "-f",
        "--file",
        action="store",
        help="Environment definition file (default: environment.yml)",
        default="environment.yml",
    )

    # Add name and prefix args
    add_parser_prefix(p)

    # Add networking args
    add_parser_networking(p)

    p.add_argument(
        "remote_definition",
        help="Remote environment definition / IPython notebook",
        action="store",
        default=None,
        nargs="?",
    )
    add_parser_default_packages(p)
    add_output_and_prompt_options(p)
    add_parser_solver(p)
    add_parser_platform(p)

    p.set_defaults(func="conda.cli.main_env_create.execute")

    return p


@notices
def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..auxlib.ish import dals
    from ..base.context import context, determine_target_prefix
    from ..core.prefix_data import PrefixData
    from ..env import specs
    from ..env.env import get_filename, print_result
    from ..env.installers.base import get_installer
    from ..exceptions import InvalidInstaller
    from ..gateways.disk.delete import rm_rf
    from ..misc import touch_nonadmin
    from . import install as cli_install

    spec = specs.detect(
        name=args.name,
        filename=get_filename(args.file),
        directory=os.getcwd(),
        remote_definition=args.remote_definition,
    )
    env = spec.environment

    # FIXME conda code currently requires args to have a name or prefix
    # don't overwrite name if it's given. gh-254
    if args.prefix is None and args.name is None:
        args.name = env.name

    prefix = determine_target_prefix(context, args)

    if args.yes and prefix != context.root_prefix and os.path.exists(prefix):
        rm_rf(prefix)
    cli_install.check_prefix(prefix, json=args.json)

    # TODO, add capability
    # common.ensure_override_channels_requires_channel(args)
    # channel_urls = args.channel or ()

    result = {"conda": None, "pip": None}

    args_packages = (
        context.create_default_packages if not args.no_default_packages else []
    )

    if args.dry_run:
        installer_type = "conda"
        installer = get_installer(installer_type)

        pkg_specs = env.dependencies.get(installer_type, [])
        pkg_specs.extend(args_packages)

        solved_env = installer.dry_run(pkg_specs, args, env)
        if args.json:
            print(json.dumps(solved_env.to_dict(), indent=2))
        else:
            print(solved_env.to_yaml(), end="")

    else:
        if args_packages:
            installer_type = "conda"
            installer = get_installer(installer_type)
            result[installer_type] = installer.install(prefix, args_packages, args, env)

        if len(env.dependencies.items()) == 0:
            installer_type = "conda"
            pkg_specs = []
            installer = get_installer(installer_type)
            result[installer_type] = installer.install(prefix, pkg_specs, args, env)
        else:
            for installer_type, pkg_specs in env.dependencies.items():
                try:
                    installer = get_installer(installer_type)
                    result[installer_type] = installer.install(
                        prefix, pkg_specs, args, env
                    )
                except InvalidInstaller:
                    raise CondaError(
                        dals(
                            f"""
                            Unable to install package for {installer_type}.

                            Please double check and ensure your dependencies file has
                            the correct spelling. You might also try installing the
                            conda-env-{installer_type} package to see if provides
                            the required installer.
                            """
                        )
                    )

        if env.variables:
            pd = PrefixData(prefix)
            pd.set_environment_env_vars(env.variables)

        touch_nonadmin(prefix)
        print_result(args, prefix, result)

    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda-env config vars`.

Allows for configuring conda-env's vars.
"""

from argparse import (
    ArgumentParser,
    Namespace,
    _SubParsersAction,
)
from os.path import lexists

from ..base.context import context, determine_target_prefix
from ..core.prefix_data import PrefixData
from ..exceptions import EnvironmentLocationNotFound


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .helpers import add_parser_json, add_parser_prefix

    var_summary = (
        "Interact with environment variables associated with Conda environments."
    )
    var_description = var_summary
    var_epilog = dals(
        """
        Examples::

            conda env config vars list -n my_env
            conda env config vars set MY_VAR=something OTHER_THING=ohhhhya
            conda env config vars unset MY_VAR

        """
    )

    var_parser = sub_parsers.add_parser(
        "vars",
        help=var_summary,
        description=var_description,
        epilog=var_epilog,
        **kwargs,
    )
    var_subparser = var_parser.add_subparsers()

    list_summary = "List environment variables for a conda environment."
    list_description = list_summary
    list_epilog = dals(
        """
        Example::

            conda env config vars list -n my_env

        """
    )

    list_parser = var_subparser.add_parser(
        "list",
        help=list_summary,
        description=list_description,
        epilog=list_epilog,
    )
    add_parser_prefix(list_parser)
    add_parser_json(list_parser)
    list_parser.set_defaults(func="conda.cli.main_env_vars.execute_list")

    set_summary = "Set environment variables for a conda environment."
    set_description = set_summary
    set_epilog = dals(
        """
        Example::

            conda env config vars set MY_VAR=weee

        """
    )

    set_parser = var_subparser.add_parser(
        "set",
        help=set_summary,
        description=set_description,
        epilog=set_epilog,
    )

    set_parser.add_argument(
        "vars",
        action="store",
        nargs="*",
        help="Environment variables to set in the form <KEY>=<VALUE> separated by spaces",
    )
    add_parser_prefix(set_parser)
    set_parser.set_defaults(func="conda.cli.main_env_vars.execute_set")

    unset_summary = "Unset environment variables for a conda environment."
    unset_description = unset_summary
    unset_epilog = dals(
        """
        Example::

            conda env config vars unset MY_VAR

        """
    )
    unset_parser = var_subparser.add_parser(
        "unset",
        help=unset_summary,
        description=unset_description,
        epilog=unset_epilog,
    )
    unset_parser.add_argument(
        "vars",
        action="store",
        nargs="*",
        help="Environment variables to unset in the form <KEY> separated by spaces",
    )
    add_parser_prefix(unset_parser)
    unset_parser.set_defaults(func="conda.cli.main_env_vars.execute_unset")


def execute_list(args: Namespace, parser: ArgumentParser) -> int:
    from . import common

    prefix = determine_target_prefix(context, args)
    if not lexists(prefix):
        raise EnvironmentLocationNotFound(prefix)

    pd = PrefixData(prefix)

    env_vars = pd.get_environment_env_vars()
    if args.json:
        common.stdout_json(env_vars)
    else:
        for k, v in env_vars.items():
            print(f"{k} = {v}")

    return 0


def execute_set(args: Namespace, parser: ArgumentParser) -> int:
    prefix = determine_target_prefix(context, args)
    pd = PrefixData(prefix)
    if not lexists(prefix):
        raise EnvironmentLocationNotFound(prefix)

    env_vars_to_add = {}
    for var in args.vars:
        var_def = var.split("=")
        env_vars_to_add[var_def[0].strip()] = "=".join(var_def[1:]).strip()
    pd.set_environment_env_vars(env_vars_to_add)
    if prefix == context.active_prefix:
        print("To make your changes take effect please reactivate your environment")

    return 0


def execute_unset(args: Namespace, parser: ArgumentParser) -> int:
    prefix = determine_target_prefix(context, args)
    pd = PrefixData(prefix)
    if not lexists(prefix):
        raise EnvironmentLocationNotFound(prefix)

    vars_to_unset = [var.strip() for var in args.vars]
    pd.unset_environment_env_vars(vars_to_unset)
    if prefix == context.active_prefix:
        print("To make your changes take effect please reactivate your environment")

    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda info`.

Display information about current conda installation.
"""

from __future__ import annotations

import json
import os
import re
import sys
from argparse import SUPPRESS
from logging import getLogger
from os.path import exists, expanduser, isfile, join
from textwrap import wrap
from typing import TYPE_CHECKING

from ..deprecations import deprecated

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace, _SubParsersAction
    from typing import Any, Iterable

    from ..models.records import PackageRecord

log = getLogger(__name__)


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..common.constants import NULL
    from .helpers import add_parser_json

    summary = "Display information about current conda install."
    description = summary
    epilog = ""

    p = sub_parsers.add_parser(
        "info",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    add_parser_json(p)
    p.add_argument(
        "--offline",
        action="store_true",
        default=NULL,
        help=SUPPRESS,
    )
    p.add_argument(
        "-a",
        "--all",
        action="store_true",
        help="Show all information.",
    )
    p.add_argument(
        "--base",
        action="store_true",
        help="Display base environment path.",
    )
    p.add_argument(
        "-e",
        "--envs",
        action="store_true",
        help="List all known conda environments.",
    )
    p.add_argument(
        "-l",
        "--license",
        action="store_true",
        help=SUPPRESS,
    )
    p.add_argument(
        "-s",
        "--system",
        action="store_true",
        help="List environment variables.",
    )
    p.add_argument(
        "--root",
        action="store_true",
        help=SUPPRESS,
        dest="base",
    )
    p.add_argument(
        "--unsafe-channels",
        action="store_true",
        help="Display list of channels with tokens exposed.",
    )

    p.set_defaults(func="conda.cli.main_info.execute")

    return p


def get_user_site() -> list[str]:  # pragma: no cover
    """
    Method used to populate ``site_dirs`` in ``conda info``.

    :returns: List of directories.
    """

    from ..common.compat import on_win

    site_dirs = []
    try:
        if not on_win:
            if exists(expanduser("~/.local/lib")):
                python_re = re.compile(r"python\d\.\d")
                for path in os.listdir(expanduser("~/.local/lib/")):
                    if python_re.match(path):
                        site_dirs.append(f"~/.local/lib/{path}")
        else:
            if "APPDATA" not in os.environ:
                return site_dirs
            APPDATA = os.environ["APPDATA"]
            if exists(join(APPDATA, "Python")):
                site_dirs = [
                    join(APPDATA, "Python", i)
                    for i in os.listdir(join(APPDATA, "PYTHON"))
                ]
    except OSError as e:
        log.debug("Error accessing user site directory.\n%r", e)
    return site_dirs


IGNORE_FIELDS: set[str] = {"files", "auth", "preferred_env", "priority"}

SKIP_FIELDS: set[str] = {
    *IGNORE_FIELDS,
    "name",
    "version",
    "build",
    "build_number",
    "channel",
    "schannel",
    "size",
    "fn",
    "depends",
}


def dump_record(prec: PackageRecord) -> dict[str, Any]:
    """
    Returns a dictionary of key/value pairs from ``prec``.  Keys included in ``IGNORE_FIELDS`` are not returned.

    :param prec: A ``PackageRecord`` object.
    :returns: A dictionary of elements dumped from ``prec``
    """
    return {k: v for k, v in prec.dump().items() if k not in IGNORE_FIELDS}


def pretty_package(prec: PackageRecord) -> None:
    """
    Pretty prints contents of a ``PackageRecord``

    :param prec: A ``PackageRecord``
    """

    from ..utils import human_bytes

    pkg = dump_record(prec)
    d = {
        "file name": prec.fn,
        "name": pkg["name"],
        "version": pkg["version"],
        "build string": pkg["build"],
        "build number": pkg["build_number"],
        "channel": str(prec.channel),
        "size": human_bytes(pkg["size"]),
    }
    for key in sorted(set(pkg.keys()) - SKIP_FIELDS):
        d[key] = pkg[key]

    print()
    header = "{} {} {}".format(d["name"], d["version"], d["build string"])
    print(header)
    print("-" * len(header))
    for key in d:
        print("%-12s: %s" % (key, d[key]))
    print("dependencies:")
    for dep in pkg["depends"]:
        print(f"    {dep}")


@deprecated.argument("24.9", "25.3", "system")
def get_info_dict() -> dict[str, Any]:
    """
    Returns a dictionary of contextual information.

    :returns:  Dictionary of conda information to be sent to stdout.
    """

    from .. import CONDA_PACKAGE_ROOT
    from .. import __version__ as conda_version
    from ..base.context import (
        DEFAULT_SOLVER,
        context,
        env_name,
        sys_rc_path,
        user_rc_path,
    )
    from ..common.compat import on_win
    from ..common.url import mask_anaconda_token
    from ..core.index import _supplement_index_with_system
    from ..models.channel import all_channel_urls, offline_keep

    try:
        from conda_build import __version__ as conda_build_version
    except ImportError as err:
        # ImportError: conda-build is not installed
        log.debug("Unable to import conda-build: %s", err)
        conda_build_version = "not installed"
    except Exception as err:
        log.error("Error importing conda-build: %s", err)
        conda_build_version = "error"

    virtual_pkg_index = {}
    _supplement_index_with_system(virtual_pkg_index)
    virtual_pkgs = [[p.name, p.version, p.build] for p in virtual_pkg_index.values()]

    channels = list(all_channel_urls(context.channels))
    if not context.json:
        channels = [c + ("" if offline_keep(c) else "  (offline)") for c in channels]
    channels = [mask_anaconda_token(c) for c in channels]

    netrc_file = os.environ.get("NETRC")
    if not netrc_file:
        user_netrc = expanduser("~/.netrc")
        if isfile(user_netrc):
            netrc_file = user_netrc

    active_prefix_name = env_name(context.active_prefix)

    solver = {
        "name": context.solver,
        "user_agent": context.solver_user_agent(),
        "default": context.solver == DEFAULT_SOLVER,
    }

    info_dict = dict(
        platform=context.subdir,
        conda_version=conda_version,
        conda_env_version=conda_version,
        conda_build_version=conda_build_version,
        root_prefix=context.root_prefix,
        conda_prefix=context.conda_prefix,
        av_data_dir=context.av_data_dir,
        av_metadata_url_base=context.signing_metadata_url_base,
        root_writable=context.root_writable,
        pkgs_dirs=context.pkgs_dirs,
        envs_dirs=context.envs_dirs,
        default_prefix=context.default_prefix,
        active_prefix=context.active_prefix,
        active_prefix_name=active_prefix_name,
        conda_shlvl=context.shlvl,
        channels=channels,
        user_rc_path=user_rc_path,
        rc_path=user_rc_path,
        sys_rc_path=sys_rc_path,
        # is_foreign=bool(foreign),
        offline=context.offline,
        envs=[],
        python_version=".".join(map(str, sys.version_info)),
        requests_version=context.requests_version,
        user_agent=context.user_agent,
        conda_location=CONDA_PACKAGE_ROOT,
        config_files=context.config_files,
        netrc_file=netrc_file,
        virtual_pkgs=virtual_pkgs,
        solver=solver,
    )
    if on_win:
        from ..common._os.windows import is_admin_on_windows

        info_dict["is_windows_admin"] = is_admin_on_windows()
    else:
        info_dict["UID"] = os.geteuid()
        info_dict["GID"] = os.getegid()

    env_var_keys = {
        "CIO_TEST",
        "CURL_CA_BUNDLE",
        "REQUESTS_CA_BUNDLE",
        "SSL_CERT_FILE",
        "LD_PRELOAD",
    }

    # add all relevant env vars, e.g. startswith('CONDA') or endswith('PATH')
    env_var_keys.update(v for v in os.environ if v.upper().startswith("CONDA"))
    env_var_keys.update(v for v in os.environ if v.upper().startswith("PYTHON"))
    env_var_keys.update(v for v in os.environ if v.upper().endswith("PATH"))
    env_var_keys.update(v for v in os.environ if v.upper().startswith("SUDO"))

    env_vars = {
        ev: os.getenv(ev, os.getenv(ev.lower(), "<not set>")) for ev in env_var_keys
    }

    proxy_keys = (v for v in os.environ if v.upper().endswith("PROXY"))
    env_vars.update({ev: "<set>" for ev in proxy_keys})

    info_dict.update(
        {
            "sys.version": sys.version,
            "sys.prefix": sys.prefix,
            "sys.executable": sys.executable,
            "site_dirs": get_user_site(),
            "env_vars": env_vars,
        }
    )

    return info_dict


def get_env_vars_str(info_dict: dict[str, Any]) -> str:
    """
    Returns a printable string representing environment variables from the dictionary returned by ``get_info_dict``.

    :param info_dict:  The returned dictionary from ``get_info_dict()``.
    :returns:  String to print.
    """

    builder = []
    builder.append("%23s:" % "environment variables")
    env_vars = info_dict.get("env_vars", {})
    for key in sorted(env_vars):
        value = wrap(env_vars[key])
        first_line = value[0] if len(value) else ""
        other_lines = value[1:] if len(value) > 1 else ()
        builder.append("%25s=%s" % (key, first_line))
        for val in other_lines:
            builder.append(" " * 26 + val)
    return "\n".join(builder)


def get_main_info_str(info_dict: dict[str, Any]) -> str:
    """
    Returns a printable string of the contents of ``info_dict``.

    :param info_dict:  The output of ``get_info_dict()``.
    :returns:  String to print.
    """

    from ..common.compat import on_win

    def flatten(lines: Iterable[str]) -> str:
        return ("\n" + 26 * " ").join(map(str, lines))

    def builder():
        if info_dict["active_prefix_name"]:
            yield ("active environment", info_dict["active_prefix_name"])
            yield ("active env location", info_dict["active_prefix"])
        else:
            yield ("active environment", info_dict["active_prefix"])

        if info_dict["conda_shlvl"] >= 0:
            yield ("shell level", info_dict["conda_shlvl"])

        yield ("user config file", info_dict["user_rc_path"])
        yield ("populated config files", flatten(info_dict["config_files"]))
        yield ("conda version", info_dict["conda_version"])
        yield ("conda-build version", info_dict["conda_build_version"])
        yield ("python version", info_dict["python_version"])
        yield (
            "solver",
            f"{info_dict['solver']['name']}{' (default)' if info_dict['solver']['default'] else ''}",
        )
        yield (
            "virtual packages",
            flatten("=".join(pkg) for pkg in info_dict["virtual_pkgs"]),
        )
        writable = "writable" if info_dict["root_writable"] else "read only"
        yield ("base environment", f"{info_dict['root_prefix']}  ({writable})")
        yield ("conda av data dir", info_dict["av_data_dir"])
        yield ("conda av metadata url", info_dict["av_metadata_url_base"])
        yield ("channel URLs", flatten(info_dict["channels"]))
        yield ("package cache", flatten(info_dict["pkgs_dirs"]))
        yield ("envs directories", flatten(info_dict["envs_dirs"]))
        yield ("platform", info_dict["platform"])
        yield ("user-agent", info_dict["user_agent"])

        if on_win:
            yield ("administrator", info_dict["is_windows_admin"])
        else:
            yield ("UID:GID", f"{info_dict['UID']}:{info_dict['GID']}")

        yield ("netrc file", info_dict["netrc_file"])
        yield ("offline mode", info_dict["offline"])

    return "\n".join(("", *(f"{key:>23} : {value}" for key, value in builder()), ""))


def execute(args: Namespace, parser: ArgumentParser) -> int:
    """
    Implements ``conda info`` commands.

     * ``conda info``
     * ``conda info --base``
     * ``conda info <package_spec> ...``
     * ``conda info --unsafe-channels``
     * ``conda info --envs``
     * ``conda info --system``
    """

    from ..base.context import context
    from .common import print_envs_list, stdout_json

    if args.base:
        if context.json:
            stdout_json({"root_prefix": context.root_prefix})
        else:
            print(f"{context.root_prefix}")
        return 0

    if args.unsafe_channels:
        if not context.json:
            print("\n".join(context.channels))
        else:
            print(json.dumps({"channels": context.channels}))
        return 0

    options = "envs", "system"

    if args.all or context.json:
        for option in options:
            setattr(args, option, True)
    info_dict = get_info_dict()

    if (
        args.all or all(not getattr(args, opt) for opt in options)
    ) and not context.json:
        print(get_main_info_str(info_dict) + "\n")

    if args.envs:
        from ..core.envs_manager import list_all_known_prefixes

        info_dict["envs"] = list_all_known_prefixes()
        print_envs_list(info_dict["envs"], not context.json)

    if args.system:
        if not context.json:
            from .find_commands import find_commands, find_executable

            print(f"sys.version: {sys.version[:40]}...")
            print(f"sys.prefix: {sys.prefix}")
            print(f"sys.executable: {sys.executable}")
            print("conda location: {}".format(info_dict["conda_location"]))
            for cmd in sorted(set(find_commands() + ("build",))):
                print("conda-{}: {}".format(cmd, find_executable("conda-" + cmd)))
            print("user site dirs: ", end="")
            site_dirs = info_dict["site_dirs"]
            if site_dirs:
                print(site_dirs[0])
            else:
                print()
            for site_dir in site_dirs[1:]:
                print(f"                {site_dir}")
            print()

            for name, value in sorted(info_dict["env_vars"].items()):
                print(f"{name}: {value}")
            print()

    if context.json:
        stdout_json(info_dict)
    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda compare`.

Compare the packages in an environment with the packages listed in an environment file.
"""

from __future__ import annotations

import logging
import os
from os.path import abspath, expanduser, expandvars
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace, _SubParsersAction

log = logging.getLogger(__name__)


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .helpers import add_parser_json, add_parser_prefix

    summary = "Compare packages between conda environments."
    description = summary
    epilog = dals(
        """
        Examples:

        Compare packages in the current environment with respect
        to 'environment.yml' located in the current working directory::

            conda compare environment.yml

        Compare packages installed into the environment 'myenv' with respect
        to 'environment.yml' in a different directory::

            conda compare -n myenv path/to/file/environment.yml

        """
    )

    p = sub_parsers.add_parser(
        "compare",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    add_parser_json(p)
    add_parser_prefix(p)
    p.add_argument(
        "file",
        action="store",
        help="Path to the environment file that is to be compared against.",
    )
    p.set_defaults(func="conda.cli.main_compare.execute")

    return p


def get_packages(prefix):
    from ..core.prefix_data import PrefixData
    from ..exceptions import EnvironmentLocationNotFound

    if not os.path.isdir(prefix):
        raise EnvironmentLocationNotFound(prefix)

    return sorted(
        PrefixData(prefix, pip_interop_enabled=True).iter_records(),
        key=lambda x: x.name,
    )


def compare_packages(active_pkgs, specification_pkgs) -> tuple[int, list[str]]:
    from ..models.match_spec import MatchSpec

    output = []
    miss = False
    for pkg in specification_pkgs:
        pkg_spec = MatchSpec(pkg)
        if (name := pkg_spec.name) in active_pkgs:
            if not pkg_spec.match(active_pkg := active_pkgs[name]):
                miss = True
                output.append(
                    f"{name} found but mismatch. Specification pkg: {pkg}, "
                    f"Running pkg: {active_pkg.name}=={active_pkg.version}={active_pkg.build}"
                )
        else:
            miss = True
            output.append(f"{name} not found")
    if not miss:
        output.append(
            "Success. All the packages in the "
            "specification file are present in the environment "
            "with matching version and build string."
        )
    return int(miss), output


def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.context import context
    from ..env import specs
    from ..exceptions import EnvironmentLocationNotFound, SpecNotFound
    from ..gateways.connection.session import CONDA_SESSION_SCHEMES
    from ..gateways.disk.test import is_conda_environment
    from .common import stdout_json

    prefix = context.target_prefix
    if not is_conda_environment(prefix):
        raise EnvironmentLocationNotFound(prefix)

    try:
        url_scheme = args.file.split("://", 1)[0]
        if url_scheme in CONDA_SESSION_SCHEMES:
            filename = args.file
        else:
            filename = abspath(expanduser(expandvars(args.file)))

        spec = specs.detect(name=args.name, filename=filename, directory=os.getcwd())
        env = spec.environment

        if args.prefix is None and args.name is None:
            args.name = env.name
    except SpecNotFound:
        raise

    active_pkgs = {pkg.name: pkg for pkg in get_packages(prefix)}
    specification_pkgs = []
    if "conda" in env.dependencies:
        specification_pkgs = specification_pkgs + env.dependencies["conda"]
    if "pip" in env.dependencies:
        specification_pkgs = specification_pkgs + env.dependencies["pip"]

    exitcode, output = compare_packages(active_pkgs, specification_pkgs)

    if context.json:
        stdout_json(output)
    else:
        print("\n".join(map(str, output)))

    return exitcode


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda init`.

Prepares the user's profile for running conda, and sets up the conda shell interface.
"""

from __future__ import annotations

from argparse import SUPPRESS
from logging import getLogger
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace, _SubParsersAction

log = getLogger(__name__)


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from ..base.constants import COMPATIBLE_SHELLS
    from ..common.compat import on_win
    from ..common.constants import NULL
    from .helpers import add_parser_json

    summary = "Initialize conda for shell interaction."
    description = summary
    epilog = dals(
        """
        Key parts of conda's functionality require that it interact directly with the shell
        within which conda is being invoked. The `conda activate` and `conda deactivate` commands
        specifically are shell-level commands. That is, they affect the state (e.g. environment
        variables) of the shell context being interacted with. Other core commands, like
        `conda create` and `conda install`, also necessarily interact with the shell environment.
        They're therefore implemented in ways specific to each shell. Each shell must be configured
        to make use of them.

        This command makes changes to your system that are specific and customized for each shell.
        To see the specific files and locations on your system that will be affected before, use
        the '--dry-run' flag.  To see the exact changes that are being or will be made to each
        location, use the '--verbose' flag.

        IMPORTANT: After running `conda init`, most shells will need to be closed and restarted for
        changes to take effect.

        """
    )

    p = sub_parsers.add_parser(
        "init",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )

    p.add_argument(
        "--dev",
        action="store_true",
        help=SUPPRESS,
        default=NULL,
    )

    p.add_argument(
        "--all",
        action="store_true",
        help="Initialize all currently available shells.",
        default=NULL,
    )

    setup_type_group = p.add_argument_group("setup type")
    setup_type_group.add_argument(
        "--install",
        action="store_true",
        help=SUPPRESS,
        default=NULL,
    )
    setup_type_group.add_argument(
        "--user",
        action="store_true",
        dest="user",
        help="Initialize conda for the current user (default).",
        default=True,
    )
    setup_type_group.add_argument(
        "--no-user",
        action="store_false",
        dest="user",
        help="Don't initialize conda for the current user.",
    )
    setup_type_group.add_argument(
        "--system",
        action="store_true",
        help="Initialize conda for all users on the system.",
        default=NULL,
    )
    setup_type_group.add_argument(
        "--reverse",
        action="store_true",
        help="Undo effects of last conda init.",
        default=NULL,
    )

    p.add_argument(
        "shells",
        nargs="*",
        choices=COMPATIBLE_SHELLS,
        metavar="SHELLS",
        help=(
            "One or more shells to be initialized. If not given, the default value is 'bash' on "
            "unix and 'cmd.exe' & 'powershell' on Windows. Use the '--all' flag to initialize all "
            f"shells. Available shells: {sorted(COMPATIBLE_SHELLS)}"
        ),
        default=["cmd.exe", "powershell"] if on_win else ["bash"],
    )

    if on_win:
        p.add_argument(
            "--anaconda-prompt",
            action="store_true",
            help="Add an 'Anaconda Prompt' icon to your desktop.",
            default=NULL,
        )

    add_parser_json(p)
    p.add_argument(
        "-d",
        "--dry-run",
        action="store_true",
        help="Only display what would have been done.",
    )
    p.set_defaults(func="conda.cli.main_init.execute")

    return p


def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.constants import COMPATIBLE_SHELLS
    from ..base.context import context
    from ..common.compat import on_win
    from ..core.initialize import initialize, initialize_dev, install
    from ..exceptions import ArgumentError

    if args.install:
        return install(context.conda_prefix)

    selected_shells: tuple[str, ...]
    if args.all:
        selected_shells = COMPATIBLE_SHELLS
    else:
        selected_shells = tuple(args.shells)

    if args.dev:
        if len(selected_shells) != 1:
            raise ArgumentError("--dev can only handle one shell at a time right now")
        return initialize_dev(selected_shells[0])

    else:
        for_user = args.user and not args.system
        anaconda_prompt = on_win and args.anaconda_prompt
        return initialize(
            context.conda_prefix,
            selected_shells,
            for_user,
            args.system,
            anaconda_prompt,
            args.reverse,
        )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Mock CLI implementation for `conda activate`.

A mock implementation of the activate shell command for better UX.
"""

from argparse import SUPPRESS

from .. import CondaError


def configure_parser(sub_parsers):
    p = sub_parsers.add_parser(
        "activate",
        help="Activate a conda environment.",
    )
    p.set_defaults(func="conda.cli.main_mock_activate.execute")
    p.add_argument("args", action="store", nargs="*", help=SUPPRESS)


def execute(args, parser):
    raise CondaError("Run 'conda init' before 'conda activate'")


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Wrapper for running conda CLI commands as a Python API."""

from logging import getLogger

from ..base.constants import SEARCH_PATH
from ..base.context import context
from ..common.io import CaptureTarget, argv, captured
from ..deprecations import deprecated
from ..exceptions import conda_exception_handler
from ..gateways.logging import initialize_std_loggers
from .conda_argparse import do_call, generate_parser

deprecated.module("24.3", "24.9", addendum="Use `conda.testing.conda_cli` instead.")

log = getLogger(__name__)


class Commands:
    CLEAN = "clean"
    CONFIG = "config"
    CREATE = "create"
    INFO = "info"
    INSTALL = "install"
    LIST = "list"
    REMOVE = "remove"
    SEARCH = "search"
    UPDATE = "update"
    RUN = "run"
    NOTICES = "notices"


STRING = CaptureTarget.STRING
STDOUT = CaptureTarget.STDOUT


# Note, a deviated copy of this code appears in tests/test_create.py
def run_command(command, *arguments, **kwargs):
    """Runs a conda command in-process with a given set of command-line interface arguments.

    Differences from the command-line interface:
        Always uses --yes flag, thus does not ask for confirmation.

    Args:
        command: one of the Commands.
        *arguments: instructions you would normally pass to the conda command on the command line
                    see below for examples. Be very careful to delimit arguments exactly as you
                    want them to be delivered. No 'combine then split at spaces' or other
                    information destroying processing gets performed on the arguments.
        **kwargs: special instructions for programmatic overrides

    Keyword Args:
        use_exception_handler: defaults to False. False will let the code calling
          `run_command` handle all exceptions.  True won't raise when an exception
          has occurred, and instead give a non-zero return code
        search_path: an optional non-standard search path for configuration information
          that overrides the default SEARCH_PATH
        stdout: Define capture behavior for stream sys.stdout. Defaults to STRING.
          STRING captures as a string.  None leaves stream untouched.
          Otherwise redirect to file-like object stdout.
        stderr: Define capture behavior for stream sys.stderr. Defaults to STRING.
          STRING captures as a string.  None leaves stream untouched.
          STDOUT redirects to stdout target and returns None as stderr value.
          Otherwise redirect to file-like object stderr.

    Returns:
        a tuple of stdout, stderr, and return_code.
        stdout, stderr are either strings, None or the corresponding file-like function argument.

    Examples:
        >>> run_command(Commands.CREATE, "-n", "newenv", "python=3", "flask", \
                        use_exception_handler=True)
        >>> run_command(Commands.CREATE, "-n", "newenv", "python=3", "flask")
        >>> run_command(Commands.CREATE, ["-n", "newenv", "python=3", "flask"], search_path=())
    """
    initialize_std_loggers()
    use_exception_handler = kwargs.pop("use_exception_handler", False)
    configuration_search_path = kwargs.pop("search_path", SEARCH_PATH)
    stdout = kwargs.pop("stdout", STRING)
    stderr = kwargs.pop("stderr", STRING)
    p = generate_parser()

    if arguments and isinstance(arguments[0], list):
        arguments = arguments[0]

    arguments = list(arguments)
    arguments.insert(0, command)

    args = p.parse_args(arguments)
    args.yes = True  # always skip user confirmation, force setting context.always_yes
    context.__init__(
        search_path=configuration_search_path,
        argparse_args=args,
    )

    from subprocess import list2cmdline

    log.debug("executing command >>>  conda %s", list2cmdline(arguments))

    is_run = arguments[0] == "run"
    if is_run:
        cap_args = (None, None)
    else:
        cap_args = (stdout, stderr)
    try:
        with argv(["python_api", *arguments]), captured(*cap_args) as c:
            if use_exception_handler:
                result = conda_exception_handler(do_call, args, p)
            else:
                result = do_call(args, p)
        if is_run:
            stdout = result.stdout
            stderr = result.stderr
            result = result.rc
        else:
            stdout = c.stdout
            stderr = c.stderr
    except Exception as e:
        log.debug("\n  stdout: %s\n  stderr: %s", stdout, stderr)
        e.stdout, e.stderr = stdout, stderr
        raise e
    return_code = result or 0
    log.debug(
        "\n  stdout: %s\n  stderr: %s\n  return_code: %s", stdout, stderr, return_code
    )
    return stdout, stderr, return_code


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda package`.

Provides some low-level tools for creating conda packages.
"""

import hashlib
import json
import os
import re
import tarfile
import tempfile
from argparse import ArgumentParser, Namespace, _SubParsersAction
from os.path import abspath, basename, dirname, isdir, isfile, islink, join


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from .helpers import add_parser_prefix

    summary = "Create low-level conda packages. (EXPERIMENTAL)"
    description = summary
    epilog = ""

    p = sub_parsers.add_parser(
        "package",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    add_parser_prefix(p)
    p.add_argument(
        "-w",
        "--which",
        metavar="PATH",
        nargs="+",
        action="store",
        help="Given some file's PATH, print which conda package the file came from.",
    )
    p.add_argument(
        "-r",
        "--reset",
        action="store_true",
        help="Remove all untracked files and exit.",
    )
    p.add_argument(
        "-u",
        "--untracked",
        action="store_true",
        help="Display all untracked files and exit.",
    )
    p.add_argument(
        "--pkg-name",
        action="store",
        default="unknown",
        help="Designate package name of the package being created.",
    )
    p.add_argument(
        "--pkg-version",
        action="store",
        default="0.0",
        help="Designate package version of the package being created.",
    )
    p.add_argument(
        "--pkg-build",
        action="store",
        default=0,
        help="Designate package build number of the package being created.",
    )
    p.set_defaults(func="conda.cli.main_package.execute")

    return p


def remove(prefix, files):
    """Remove files for a given prefix."""
    dst_dirs = set()
    for f in files:
        dst = join(prefix, f)
        dst_dirs.add(dirname(dst))
        os.unlink(dst)

    for path in sorted(dst_dirs, key=len, reverse=True):
        try:
            os.rmdir(path)
        except OSError:  # directory might not be empty
            pass


def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.context import context
    from ..misc import untracked

    prefix = context.target_prefix

    if args.which:
        for path in args.which:
            for prec in which_package(path):
                print("%-50s  %s" % (path, prec.dist_str()))
        return 0

    print("# prefix:", prefix)

    if args.reset:
        remove(prefix, untracked(prefix))
        return 0

    if args.untracked:
        files = sorted(untracked(prefix))
        print("# untracked files: %d" % len(files))
        for fn in files:
            print(fn)
        return 0

    make_tarbz2(
        prefix,
        name=args.pkg_name.lower(),
        version=args.pkg_version,
        build_number=int(args.pkg_build),
    )
    return 0


def get_installed_version(prefix, name):
    from ..core.prefix_data import PrefixData

    for info in PrefixData(prefix).iter_records():
        if info["name"] == name:
            return str(info["version"])
    return None


def create_info(name, version, build_number, requires_py):
    from ..base.context import context

    d = dict(
        name=name,
        version=version,
        platform=context.platform,
        arch=context.arch_name,
        build_number=int(build_number),
        build=str(build_number),
        depends=[],
    )
    if requires_py:
        d["build"] = ("py%d%d_" % requires_py) + d["build"]
        d["depends"].append("python %d.%d*" % requires_py)
    return d


shebang_pat = re.compile(r"^#!.+$", re.M)


def fix_shebang(tmp_dir, path):
    from ..base.constants import PREFIX_PLACEHOLDER

    if open(path, "rb").read(2) != "#!":
        return False

    with open(path) as fi:
        data = fi.read()
    m = shebang_pat.match(data)
    if not (m and "python" in m.group()):
        return False

    data = shebang_pat.sub(f"#!{PREFIX_PLACEHOLDER}/bin/python", data, count=1)
    tmp_path = join(tmp_dir, basename(path))
    with open(tmp_path, "w") as fo:
        fo.write(data)
    os.chmod(tmp_path, int("755", 8))
    return True


def _add_info_dir(t, tmp_dir, files, has_prefix, info):
    from ..auxlib.entity import EntityEncoder

    info_dir = join(tmp_dir, "info")
    os.mkdir(info_dir)
    with open(join(info_dir, "files"), "w") as fo:
        for f in files:
            fo.write(f + "\n")

    with open(join(info_dir, "index.json"), "w") as fo:
        json.dump(info, fo, indent=2, sort_keys=True, cls=EntityEncoder)

    if has_prefix:
        with open(join(info_dir, "has_prefix"), "w") as fo:
            for f in has_prefix:
                fo.write(f + "\n")

    for fn in os.listdir(info_dir):
        t.add(join(info_dir, fn), "info/" + fn)


def create_conda_pkg(prefix, files, info, tar_path, update_info=None):
    """Create a conda package and return a list of warnings."""
    from ..gateways.disk.delete import rmtree

    files = sorted(files)
    warnings = []
    has_prefix = []
    tmp_dir = tempfile.mkdtemp()
    t = tarfile.open(tar_path, "w:bz2")
    h = hashlib.new("sha1")
    for f in files:
        assert not (f.startswith("/") or f.endswith("/") or "\\" in f or f == ""), f
        path = join(prefix, f)
        if f.startswith("bin/") and fix_shebang(tmp_dir, path):
            path = join(tmp_dir, basename(path))
            has_prefix.append(f)
        t.add(path, f)
        h.update(f.encode("utf-8"))
        h.update(b"\x00")
        if islink(path):
            link = os.readlink(path)
            if isinstance(link, str):
                h.update(bytes(link, "utf-8"))
            else:
                h.update(link)
            if link.startswith("/"):
                warnings.append(f"found symlink to absolute path: {f} -> {link}")
        elif isfile(path):
            h.update(open(path, "rb").read())
            if path.endswith(".egg-link"):
                warnings.append(f"found egg link: {f}")

    info["file_hash"] = h.hexdigest()
    if update_info:
        update_info(info)
    _add_info_dir(t, tmp_dir, files, has_prefix, info)
    t.close()
    rmtree(tmp_dir)
    return warnings


def make_tarbz2(prefix, name="unknown", version="0.0", build_number=0, files=None):
    from ..base.constants import CONDA_PACKAGE_EXTENSION_V1
    from ..misc import untracked

    if files is None:
        files = untracked(prefix)
    print("# files: %d" % len(files))
    if len(files) == 0:
        print("# failed: nothing to do")
        return None

    if any("/site-packages/" in f for f in files):
        python_version = get_installed_version(prefix, "python")
        assert python_version is not None
        requires_py = tuple(int(x) for x in python_version[:3].split("."))
    else:
        requires_py = False

    info = create_info(name, version, build_number, requires_py)
    tarbz2_fn = ("{name}-{version}-{build}".format(**info)) + CONDA_PACKAGE_EXTENSION_V1
    create_conda_pkg(prefix, files, info, tarbz2_fn)
    print("# success")
    print(tarbz2_fn)
    return tarbz2_fn


def which_package(path):
    """Return the package containing the path.

    Provided the path of a (presumably) conda installed file, iterate over
    the conda packages the file came from. Usually the iteration yields
    only one package.
    """
    from ..common.path import paths_equal
    from ..core.prefix_data import PrefixData

    path = abspath(path)
    prefix = which_prefix(path)
    if prefix is None:
        from ..exceptions import CondaVerificationError

        raise CondaVerificationError(f"could not determine conda prefix from: {path}")

    for prec in PrefixData(prefix).iter_records():
        if any(paths_equal(join(prefix, f), path) for f in prec["files"] or ()):
            yield prec


def which_prefix(path):
    """Return the prefix for the provided path.

    Provided the path of a (presumably) conda installed file, return the
    environment prefix in which the file in located.
    """
    prefix = abspath(path)
    while True:
        if isdir(join(prefix, "conda-meta")):
            # we found the it, so let's return it
            return prefix
        if prefix == dirname(prefix):
            # we cannot chop off any more directories, so we didn't find it
            return None
        prefix = dirname(prefix)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda clean`.

Removes cached package tarballs, index files, package metadata, temporary files, and log files.
"""

from __future__ import annotations

import os
import sys
from logging import getLogger
from os.path import isdir, join
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace, _SubParsersAction
    from typing import Any, Iterable

log = getLogger(__name__)


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .actions import ExtendConstAction
    from .helpers import add_output_and_prompt_options

    summary = "Remove unused packages and caches."
    description = summary
    epilog = dals(
        """
        Examples::

            conda clean --tarballs
        """
    )

    p = sub_parsers.add_parser(
        "clean",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )

    removal_target_options = p.add_argument_group("Removal Targets")
    removal_target_options.add_argument(
        "-a",
        "--all",
        action="store_true",
        help="Remove index cache, lock files, unused cache packages, tarballs, and logfiles.",
    )
    removal_target_options.add_argument(
        "-i",
        "--index-cache",
        action="store_true",
        help="Remove index cache.",
    )
    removal_target_options.add_argument(
        "-p",
        "--packages",
        action="store_true",
        help="Remove unused packages from writable package caches. "
        "WARNING: This does not check for packages installed using "
        "symlinks back to the package cache.",
    )
    removal_target_options.add_argument(
        "-t",
        "--tarballs",
        action="store_true",
        help="Remove cached package tarballs.",
    )
    removal_target_options.add_argument(
        "-f",
        "--force-pkgs-dirs",
        action="store_true",
        help="Remove *all* writable package caches. This option is not included with the --all "
        "flag. WARNING: This will break environments with packages installed using symlinks "
        "back to the package cache.",
    )
    removal_target_options.add_argument(
        "-c",  # for tempfile extension (.c~)
        "--tempfiles",
        const=sys.prefix,
        action=ExtendConstAction,
        help=(
            "Remove temporary files that could not be deleted earlier due to being in-use.  "
            "The argument for the --tempfiles flag is a path (or list of paths) to the "
            "environment(s) where the tempfiles should be found and removed."
        ),
    )
    removal_target_options.add_argument(
        "-l",
        "--logfiles",
        action="store_true",
        help="Remove log files.",
    )

    add_output_and_prompt_options(p)

    p.set_defaults(func="conda.cli.main_clean.execute")

    return p


def _get_size(*parts: str, warnings: list[str] | None) -> int:
    path = join(*parts)
    try:
        stat = os.lstat(path)
    except OSError as e:
        if warnings is None:
            raise
        warnings.append(f"WARNING: {path}: {e}")

        # let the user deal with the issue
        raise NotImplementedError
    else:
        # TODO: This doesn't handle packages that have hard links to files within
        # themselves, like bin/python3.3 and bin/python3.3m in the Python package
        if stat.st_nlink > 1:
            raise NotImplementedError

        return stat.st_size


def _get_pkgs_dirs(pkg_sizes: dict[str, dict[str, int]]) -> dict[str, tuple[str, ...]]:
    return {pkgs_dir: tuple(pkgs) for pkgs_dir, pkgs in pkg_sizes.items()}


def _get_total_size(pkg_sizes: dict[str, dict[str, int]]) -> int:
    return sum(sum(pkgs.values()) for pkgs in pkg_sizes.values())


def _rm_rf(*parts: str, quiet: bool, verbose: bool) -> None:
    from ..gateways.disk.delete import rm_rf

    path = join(*parts)
    try:
        if rm_rf(path):
            if not quiet and verbose:
                print(f"Removed {path}")
        elif not quiet:
            print(f"WARNING: cannot remove, file permissions: {path}")
    except OSError as e:
        if not quiet:
            print(f"WARNING: cannot remove, file permissions: {path}\n{e!r}")
        else:
            log.info("%r", e)


def find_tarballs() -> dict[str, Any]:
    from ..base.constants import CONDA_PACKAGE_EXTENSIONS, CONDA_PACKAGE_PARTS

    warnings: list[str] = []
    pkg_sizes: dict[str, dict[str, int]] = {}
    for pkgs_dir in find_pkgs_dirs():
        # tarballs are files in pkgs_dir
        _, _, tars = next(os.walk(pkgs_dir))
        for tar in tars:
            # tarballs also end in .tar.bz2, .conda, .tar.bz2.part, or .conda.part
            if not tar.endswith((*CONDA_PACKAGE_EXTENSIONS, *CONDA_PACKAGE_PARTS)):
                continue

            # get size
            try:
                size = _get_size(pkgs_dir, tar, warnings=warnings)
            except NotImplementedError:
                pass
            else:
                pkg_sizes.setdefault(pkgs_dir, {})[tar] = size

    return {
        "warnings": warnings,
        "pkg_sizes": pkg_sizes,
        "pkgs_dirs": _get_pkgs_dirs(pkg_sizes),
        "total_size": _get_total_size(pkg_sizes),
    }


def find_pkgs() -> dict[str, Any]:
    warnings: list[str] = []
    pkg_sizes: dict[str, dict[str, int]] = {}
    for pkgs_dir in find_pkgs_dirs():
        # pkgs are directories in pkgs_dir
        _, pkgs, _ = next(os.walk(pkgs_dir))
        for pkg in pkgs:
            # pkgs also have an info directory
            if not isdir(join(pkgs_dir, pkg, "info")):
                continue

            # get size
            try:
                size = sum(
                    _get_size(root, file, warnings=warnings)
                    for root, _, files in os.walk(join(pkgs_dir, pkg))
                    for file in files
                )
            except NotImplementedError:
                pass
            else:
                pkg_sizes.setdefault(pkgs_dir, {})[pkg] = size

    return {
        "warnings": warnings,
        "pkg_sizes": pkg_sizes,
        "pkgs_dirs": _get_pkgs_dirs(pkg_sizes),
        "total_size": _get_total_size(pkg_sizes),
    }


def rm_pkgs(
    pkgs_dirs: dict[str, tuple[str]],
    warnings: list[str],
    total_size: int,
    pkg_sizes: dict[str, dict[str, int]],
    *,
    quiet: bool,
    verbose: bool,
    dry_run: bool,
    name: str,
) -> None:
    from ..base.context import context
    from ..utils import human_bytes
    from .common import confirm_yn

    if not quiet and warnings:
        for warning in warnings:
            print(warning)

    if not any(pkgs for pkgs in pkg_sizes.values()):
        if not quiet:
            print(f"There are no unused {name} to remove.")
        return

    if not quiet:
        if verbose:
            print(f"Will remove the following {name}:")
            for pkgs_dir, pkgs in pkg_sizes.items():
                print(f"  {pkgs_dir}")
                print(f"  {'-' * len(pkgs_dir)}")
                for pkg, size in pkgs.items():
                    print(f"  - {pkg:<40} {human_bytes(size):>10}")
                print()
            print("-" * 17)
            print(f"Total: {human_bytes(total_size):>10}")
            print()
        else:
            count = sum(len(pkgs) for pkgs in pkg_sizes.values())
            print(f"Will remove {count} ({human_bytes(total_size)}) {name}.")

    if dry_run:
        return
    if not context.json or not context.always_yes:
        confirm_yn()

    for pkgs_dir, pkgs in pkg_sizes.items():
        for pkg in pkgs:
            _rm_rf(pkgs_dir, pkg, quiet=quiet, verbose=verbose)


def find_index_cache() -> list[str]:
    files = []
    for pkgs_dir in find_pkgs_dirs():
        # caches are directories in pkgs_dir
        path = join(pkgs_dir, "cache")
        if isdir(path):
            files.append(path)
    return files


def find_pkgs_dirs() -> list[str]:
    from ..core.package_cache_data import PackageCacheData

    return [
        pc.pkgs_dir for pc in PackageCacheData.writable_caches() if isdir(pc.pkgs_dir)
    ]


def find_tempfiles(paths: Iterable[str]) -> list[str]:
    from ..base.constants import CONDA_TEMP_EXTENSIONS

    tempfiles = []
    for path in sorted(set(paths or [sys.prefix])):
        # tempfiles are files in path
        for root, _, files in os.walk(path):
            for file in files:
                # tempfiles also end in .c~ or .trash
                if not file.endswith(CONDA_TEMP_EXTENSIONS):
                    continue

                tempfiles.append(join(root, file))

    return tempfiles


def find_logfiles() -> list[str]:
    from ..base.constants import CONDA_LOGS_DIR

    files = []
    for pkgs_dir in find_pkgs_dirs():
        # .logs are directories in pkgs_dir
        path = join(pkgs_dir, CONDA_LOGS_DIR)
        if not isdir(path):
            continue

        try:
            # logfiles are files in .logs
            _, _, logs = next(os.walk(path))
            files.extend([join(path, log) for log in logs])
        except StopIteration:
            # StopIteration: .logs is empty
            pass

    return files


def rm_items(
    items: list[str],
    *,
    quiet: bool,
    verbose: bool,
    dry_run: bool,
    name: str,
) -> None:
    from ..base.context import context
    from .common import confirm_yn

    if not items:
        if not quiet:
            print(f"There are no {name} to remove.")
        return

    if not quiet:
        if verbose:
            print(f"Will remove the following {name}:")
            for item in items:
                print(f"  - {item}")
            print()
        else:
            print(f"Will remove {len(items)} {name}.")

    if dry_run:
        return
    if not context.json or not context.always_yes:
        confirm_yn()

    for item in items:
        _rm_rf(item, quiet=quiet, verbose=verbose)


def _execute(args, parser):
    from ..base.context import context

    json_result = {"success": True}
    kwargs = {
        "quiet": context.json or context.quiet,
        "verbose": context.verbose,
        "dry_run": context.dry_run,
    }

    if args.force_pkgs_dirs:
        json_result["pkgs_dirs"] = pkgs_dirs = find_pkgs_dirs()
        rm_items(pkgs_dirs, **kwargs, name="package cache(s)")

        # we return here because all other clean operations target individual parts of
        # package caches
        return json_result

    if not (
        args.all
        or args.tarballs
        or args.index_cache
        or args.packages
        or args.tempfiles
        or args.logfiles
    ):
        from ..exceptions import ArgumentError

        raise ArgumentError(
            "At least one removal target must be given. See 'conda clean --help'."
        )

    if args.tarballs or args.all:
        json_result["tarballs"] = tars = find_tarballs()
        rm_pkgs(**tars, **kwargs, name="tarball(s)")

    if args.index_cache or args.all:
        cache = find_index_cache()
        json_result["index_cache"] = {"files": cache}
        rm_items(cache, **kwargs, name="index cache(s)")

    if args.packages or args.all:
        json_result["packages"] = pkgs = find_pkgs()
        rm_pkgs(**pkgs, **kwargs, name="package(s)")

    if args.tempfiles or args.all:
        json_result["tempfiles"] = tmps = find_tempfiles(args.tempfiles)
        rm_items(tmps, **kwargs, name="tempfile(s)")

    if args.logfiles or args.all:
        json_result["logfiles"] = logs = find_logfiles()
        rm_items(logs, **kwargs, name="logfile(s)")

    return json_result


def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.context import context
    from .common import stdout_json

    json_result = _execute(args, parser)
    if context.json:
        stdout_json(json_result)
    if args.dry_run:
        from ..exceptions import DryRunExit

        raise DryRunExit
    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Conda command line interface parsers."""

from __future__ import annotations

import argparse
import os
import sys
from argparse import (
    SUPPRESS,
    RawDescriptionHelpFormatter,
)
from argparse import ArgumentParser as ArgumentParserBase
from importlib import import_module
from logging import getLogger
from subprocess import Popen

from .. import __version__
from ..auxlib.compat import isiterable
from ..auxlib.ish import dals
from ..base.context import context, sys_rc_path, user_rc_path
from ..common.compat import on_win
from ..common.constants import NULL
from ..deprecations import deprecated
from .actions import ExtendConstAction, NullCountAction  # noqa: F401
from .find_commands import find_commands, find_executable
from .helpers import (  # noqa: F401
    add_output_and_prompt_options,
    add_parser_channels,
    add_parser_create_install_update,
    add_parser_default_packages,
    add_parser_help,
    add_parser_json,
    add_parser_known,
    add_parser_networking,
    add_parser_package_install_options,
    add_parser_platform,
    add_parser_prefix,
    add_parser_prune,
    add_parser_pscheck,
    add_parser_show_channel_urls,
    add_parser_solver,
    add_parser_solver_mode,
    add_parser_update_modifiers,
    add_parser_verbose,
)
from .main_clean import configure_parser as configure_parser_clean
from .main_compare import configure_parser as configure_parser_compare
from .main_config import configure_parser as configure_parser_config
from .main_create import configure_parser as configure_parser_create
from .main_env import configure_parser as configure_parser_env
from .main_export import configure_parser as configure_parser_export
from .main_info import configure_parser as configure_parser_info
from .main_init import configure_parser as configure_parser_init
from .main_install import configure_parser as configure_parser_install
from .main_list import configure_parser as configure_parser_list
from .main_mock_activate import configure_parser as configure_parser_mock_activate
from .main_mock_deactivate import configure_parser as configure_parser_mock_deactivate
from .main_notices import configure_parser as configure_parser_notices
from .main_package import configure_parser as configure_parser_package
from .main_remove import configure_parser as configure_parser_remove
from .main_rename import configure_parser as configure_parser_rename
from .main_run import configure_parser as configure_parser_run
from .main_search import configure_parser as configure_parser_search
from .main_update import configure_parser as configure_parser_update

log = getLogger(__name__)

escaped_user_rc_path = user_rc_path.replace("%", "%%")
escaped_sys_rc_path = sys_rc_path.replace("%", "%%")

#: List of built-in commands; these cannot be overridden by plugin subcommands
BUILTIN_COMMANDS = {
    "activate",  # Mock entry for shell command
    "clean",
    "compare",
    "config",
    "create",
    "deactivate",  # Mock entry for shell command
    "export",
    "info",
    "init",
    "install",
    "list",
    "package",
    "remove",
    "rename",
    "run",
    "search",
    "update",
    "upgrade",
    "notices",
}


def generate_pre_parser(**kwargs) -> ArgumentParser:
    pre_parser = ArgumentParser(
        description="conda is a tool for managing and deploying applications,"
        " environments and packages.",
        **kwargs,
    )

    add_parser_verbose(pre_parser)
    pre_parser.add_argument(
        "--json",
        action="store_true",
        default=NULL,
        help=SUPPRESS,
    )
    pre_parser.add_argument(
        "--no-plugins",
        action="store_true",
        default=NULL,
        help="Disable all plugins that are not built into conda.",
    )

    return pre_parser


def generate_parser(**kwargs) -> ArgumentParser:
    parser = generate_pre_parser(**kwargs)

    parser.add_argument(
        "-V",
        "--version",
        action="version",
        version=f"conda {__version__}",
        help="Show the conda version number and exit.",
    )

    sub_parsers = parser.add_subparsers(
        metavar="COMMAND",
        title="commands",
        description="The following built-in and plugins subcommands are available.",
        dest="cmd",
        action=_GreedySubParsersAction,
        required=True,
    )

    configure_parser_mock_activate(sub_parsers)
    configure_parser_mock_deactivate(sub_parsers)
    configure_parser_clean(sub_parsers)
    configure_parser_compare(sub_parsers)
    configure_parser_config(sub_parsers)
    configure_parser_create(sub_parsers)
    configure_parser_env(sub_parsers)
    configure_parser_export(sub_parsers)
    configure_parser_info(sub_parsers)
    configure_parser_init(sub_parsers)
    configure_parser_install(sub_parsers)
    configure_parser_list(sub_parsers)
    configure_parser_notices(sub_parsers)
    configure_parser_package(sub_parsers)
    configure_parser_remove(sub_parsers, aliases=["uninstall"])
    configure_parser_rename(sub_parsers)
    configure_parser_run(sub_parsers)
    configure_parser_search(sub_parsers)
    configure_parser_update(sub_parsers, aliases=["upgrade"])
    configure_parser_plugins(sub_parsers)

    return parser


def do_call(args: argparse.Namespace, parser: ArgumentParser):
    """
    Serves as the primary entry point for commands referred to in this file and for
    all registered plugin subcommands.
    """
    # let's see if during the parsing phase it was discovered that the
    # called command was in fact a plugin subcommand
    if plugin_subcommand := getattr(args, "_plugin_subcommand", None):
        # pass on the rest of the plugin specific args or fall back to
        # the whole discovered arguments
        context.plugin_manager.invoke_pre_commands(plugin_subcommand.name)
        result = plugin_subcommand.action(getattr(args, "_args", args))
        context.plugin_manager.invoke_post_commands(plugin_subcommand.name)
    elif name := getattr(args, "_executable", None):
        # run the subcommand from executables; legacy path
        deprecated.topic(
            "23.3",
            "25.3",
            topic="Loading conda subcommands via executables",
            addendum="Use the plugin system instead.",
        )
        executable = find_executable(f"conda-{name}")
        if not executable:
            from ..exceptions import CommandNotFoundError

            raise CommandNotFoundError(name)
        return _exec([executable, *args._args], os.environ)
    else:
        # let's call the subcommand the old-fashioned way via the assigned func..
        module_name, func_name = args.func.rsplit(".", 1)
        # func_name should always be 'execute'
        module = import_module(module_name)
        command = module_name.split(".")[-1].replace("main_", "")

        context.plugin_manager.invoke_pre_commands(command)
        result = getattr(module, func_name)(args, parser)
        context.plugin_manager.invoke_post_commands(command)
    return result


def find_builtin_commands(parser):
    # ArgumentParser doesn't have an API for getting back what subparsers
    # exist, so we need to use internal properties to do so.
    return tuple(parser._subparsers._group_actions[0].choices.keys())


class ArgumentParser(ArgumentParserBase):
    def __init__(self, *args, add_help=True, **kwargs):
        kwargs.setdefault("formatter_class", RawDescriptionHelpFormatter)
        super().__init__(*args, add_help=False, **kwargs)

        if add_help:
            add_parser_help(self)

    def _check_value(self, action, value):
        # extend to properly handle when we accept multiple choices and the default is a list
        if action.choices is not None and isiterable(value):
            for element in value:
                super()._check_value(action, element)
        else:
            super()._check_value(action, value)

    def parse_args(self, *args, override_args=None, **kwargs):
        parsed_args = super().parse_args(*args, **kwargs)
        for name, value in (override_args or {}).items():
            if value is not NULL and getattr(parsed_args, name, NULL) is NULL:
                setattr(parsed_args, name, value)
        return parsed_args


class _GreedySubParsersAction(argparse._SubParsersAction):
    """A custom subparser action to conditionally act as a greedy consumer.

    This is a workaround since argparse.REMAINDER does not work as expected,
    see https://github.com/python/cpython/issues/61252.
    """

    def __call__(self, parser, namespace, values, option_string=None):
        super().__call__(parser, namespace, values, option_string)

        parser = self._name_parser_map[values[0]]

        # if the parser has a greedy=True attribute we want to consume all arguments
        # i.e. all unknown args should be passed to the subcommand as is
        if getattr(parser, "greedy", False):
            try:
                unknown = getattr(namespace, argparse._UNRECOGNIZED_ARGS_ATTR)
                delattr(namespace, argparse._UNRECOGNIZED_ARGS_ATTR)
            except AttributeError:
                unknown = ()

            # underscore prefixed indicating this is not a normal argparse argument
            namespace._args = tuple(unknown)

    def _get_subactions(self):
        """Sort actions for subcommands to appear alphabetically in help blurb."""
        return sorted(self._choices_actions, key=lambda action: action.dest)


def _exec(executable_args, env_vars):
    return (_exec_win if on_win else _exec_unix)(executable_args, env_vars)


def _exec_win(executable_args, env_vars):
    p = Popen(executable_args, env=env_vars)
    try:
        p.communicate()
    except KeyboardInterrupt:
        p.wait()
    finally:
        sys.exit(p.returncode)


def _exec_unix(executable_args, env_vars):
    os.execvpe(executable_args[0], executable_args, env_vars)


def configure_parser_plugins(sub_parsers) -> None:
    """
    For each of the provided plugin-based subcommands, we'll create
    a new subparser for an improved help printout and calling the
    :meth:`~conda.plugins.types.CondaSubcommand.configure_parser`
    with the newly created subcommand specific argument parser.
    """
    plugin_subcommands = context.plugin_manager.get_subcommands()
    for name, plugin_subcommand in plugin_subcommands.items():
        # if the name of the plugin-based subcommand overlaps a built-in
        # subcommand, we print an error
        if name in BUILTIN_COMMANDS:
            log.error(
                dals(
                    f"""
                    The plugin '{name}' is trying to override the built-in command
                    with the same name, which is not allowed.

                    Please uninstall the plugin to stop seeing this error message.
                    """
                )
            )
            continue

        parser = sub_parsers.add_parser(
            name,
            description=plugin_subcommand.summary,
            help=plugin_subcommand.summary,
            add_help=False,  # defer to subcommand's help processing
        )

        # case 1: plugin extends the parser
        if plugin_subcommand.configure_parser:
            plugin_subcommand.configure_parser(parser)

            # attempt to add standard help processing, will fail if plugin defines their own
            try:
                add_parser_help(parser)
            except argparse.ArgumentError:
                pass

        # case 2: plugin has their own parser, see _GreedySubParsersAction
        else:
            parser.greedy = True

        # underscore prefixed indicating this is not a normal argparse argument
        parser.set_defaults(_plugin_subcommand=plugin_subcommand)

    if context.no_plugins:
        return

    # Ignore the legacy `conda-env` entrypoints since we already register `env`
    # as a subcommand in `generate_parser` above
    legacy = set(find_commands()).difference(plugin_subcommands) - {"env"}

    for name in legacy:
        # if the name of the plugin-based subcommand overlaps a built-in
        # subcommand, we print an error
        if name in BUILTIN_COMMANDS:
            log.error(
                dals(
                    f"""
                    The (legacy) plugin '{name}' is trying to override the built-in command
                    with the same name, which is not allowed.

                    Please uninstall the plugin to stop seeing this error message.
                    """
                )
            )
            continue

        parser = sub_parsers.add_parser(
            name,
            description=f"See `conda {name} --help`.",
            help=f"See `conda {name} --help`.",
            add_help=False,  # defer to subcommand's help processing
        )

        # case 3: legacy plugins are always greedy
        parser.greedy = True

        parser.set_defaults(_executable=name)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda update`.

Updates the specified packages in an existing environment.
"""

from __future__ import annotations

import sys
from typing import TYPE_CHECKING

from ..notices import notices

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace, _SubParsersAction


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from ..common.constants import NULL
    from .helpers import (
        add_parser_create_install_update,
        add_parser_prune,
        add_parser_solver,
        add_parser_update_modifiers,
    )

    summary = "Update conda packages to the latest compatible version."
    description = dals(
        f"""
        {summary}

        This command accepts a list of package names and updates them to the latest
        versions that are compatible with all other packages in the environment.

        Conda attempts to install the newest versions of the requested packages. To
        accomplish this, it may update some packages that are already installed, or
        install additional packages. To prevent existing packages from updating,
        use the --no-update-deps option. This may force conda to install older
        versions of the requested packages, and it does not prevent additional
        dependency packages from being installed.
        """
    )
    epilog = dals(
        """
        Examples:

            conda update -n myenv scipy

        """
    )

    p = sub_parsers.add_parser(
        "update",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    solver_mode_options, package_install_options, _ = add_parser_create_install_update(
        p
    )

    add_parser_prune(solver_mode_options)
    add_parser_solver(solver_mode_options)
    solver_mode_options.add_argument(
        "--force-reinstall",
        action="store_true",
        default=NULL,
        help="Ensure that any user-requested package for the current operation is uninstalled and "
        "reinstalled, even if that package already exists in the environment.",
    )
    add_parser_update_modifiers(solver_mode_options)

    package_install_options.add_argument(
        "--clobber",
        action="store_true",
        default=NULL,
        help="Allow clobbering of overlapping file paths within packages, "
        "and suppress related warnings.",
    )
    p.set_defaults(func="conda.cli.main_update.execute")

    return p


@notices
def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.context import context
    from .install import install

    if context.force:
        print(
            "\n\n"
            "WARNING: The --force flag will be removed in a future conda release.\n"
            "         See 'conda update --help' for details about the --force-reinstall\n"
            "         and --clobber flags.\n"
            "\n",
            file=sys.stderr,
        )

    install(args, parser, "update")
    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda remove`.

Removes the specified packages from an existing environment.
"""

import logging
from argparse import ArgumentParser, Namespace, _SubParsersAction
from os.path import isfile, join

from .common import confirm_yn

log = logging.getLogger(__name__)


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from ..common.constants import NULL
    from .actions import NullCountAction
    from .helpers import (
        add_output_and_prompt_options,
        add_parser_channels,
        add_parser_networking,
        add_parser_prefix,
        add_parser_prune,
        add_parser_pscheck,
        add_parser_solver,
    )

    summary = "Remove a list of packages from a specified conda environment. "
    description = dals(
        f"""
        {summary}

        Use `--all` flag to remove all packages and the environment itself.

        This command will also remove any package that depends on any of the
        specified packages as well---unless a replacement can be found without
        that dependency. If you wish to skip this dependency checking and remove
        just the requested packages, add the '--force' option. Note however that
        this may result in a broken environment, so use this with caution.
        """
    )
    epilog = dals(
        """
        Examples:

        Remove the package 'scipy' from the currently-active environment::

            conda remove scipy

        Remove a list of packages from an environment 'myenv'::

            conda remove -n myenv scipy curl wheel

        Remove all packages from environment `myenv` and the environment itself::

            conda remove -n myenv --all

        Remove all packages from the environment `myenv` but retain the environment::

            conda remove -n myenv --all --keep-env

        """
    )

    p = sub_parsers.add_parser(
        "remove",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    add_parser_pscheck(p)

    add_parser_prefix(p)
    add_parser_channels(p)

    solver_mode_options = p.add_argument_group("Solver Mode Modifiers")
    solver_mode_options.add_argument(
        "--features",
        action="store_true",
        help="Remove features (instead of packages).",
    )
    solver_mode_options.add_argument(
        "--force-remove",
        "--force",
        action="store_true",
        help="Forces removal of a package without removing packages that depend on it. "
        "Using this option will usually leave your environment in a broken and "
        "inconsistent state.",
        dest="force_remove",
    )
    solver_mode_options.add_argument(
        "--no-pin",
        action="store_true",
        dest="ignore_pinned",
        default=NULL,
        help="Ignore pinned package(s) that apply to the current operation. "
        "These pinned packages might come from a .condarc file or a file in "
        "<TARGET_ENVIRONMENT>/conda-meta/pinned.",
    )
    add_parser_prune(solver_mode_options)
    add_parser_solver(solver_mode_options)

    add_parser_networking(p)
    add_output_and_prompt_options(p)

    p.add_argument(
        "--all",
        action="store_true",
        help="Remove all packages, i.e., the entire environment.",
    )
    p.add_argument(
        "--keep-env",
        action="store_true",
        help="Used with `--all`, delete all packages but keep the environment.",
    )
    p.add_argument(
        "package_names",
        metavar="package_name",
        action="store",
        nargs="*",
        help="Package names to remove from the environment.",
    )
    p.add_argument(
        "--dev",
        action=NullCountAction,
        help="Use `sys.executable -m conda` in wrapper scripts instead of CONDA_EXE. "
        "This is mainly for use during tests where we test new conda sources "
        "against old Python versions.",
        dest="dev",
        default=NULL,
    )

    p.set_defaults(func="conda.cli.main_remove.execute")

    return p


def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.context import context
    from ..core.envs_manager import unregister_env
    from ..core.link import PrefixSetup, UnlinkLinkTransaction
    from ..core.prefix_data import PrefixData
    from ..exceptions import (
        CondaEnvironmentError,
        CondaValueError,
        DirectoryNotACondaEnvironmentError,
        PackagesNotFoundError,
    )
    from ..gateways.disk.delete import path_is_clean, rm_rf
    from ..models.match_spec import MatchSpec
    from .common import check_non_admin, specs_from_args
    from .install import handle_txn

    if not (args.all or args.package_names):
        raise CondaValueError(
            "no package names supplied,\n"
            '       try "conda remove -h" for more details'
        )

    prefix = context.target_prefix
    check_non_admin()

    if args.all and prefix == context.default_prefix:
        msg = "cannot remove current environment. deactivate and run conda remove again"
        raise CondaEnvironmentError(msg)

    if args.all and path_is_clean(prefix):
        # full environment removal was requested, but environment doesn't exist anyway

        # .. but you know what? If you call `conda remove --all` you'd expect the dir
        # not to exist afterwards, would you not? If not (fine, I can see the argument
        # about deleting people's work in envs being a very bad thing indeed), but if
        # being careful is the goal it would still be nice if after `conda remove --all`
        # to be able to do `conda create` on the same environment name.
        #
        # try:
        #     rm_rf(prefix, clean_empty_parents=True)
        # except:
        #     log.warning("Failed rm_rf() of partially existent env {}".format(prefix))

        return 0

    if args.all:
        if prefix == context.root_prefix:
            raise CondaEnvironmentError(
                "cannot remove root environment, add -n NAME or -p PREFIX option"
            )
        if not isfile(join(prefix, "conda-meta", "history")):
            raise DirectoryNotACondaEnvironmentError(prefix)
        if not args.json:
            print(f"\nRemove all packages in environment {prefix}:\n")

        if "package_names" in args:
            stp = PrefixSetup(
                target_prefix=prefix,
                unlink_precs=tuple(PrefixData(prefix).iter_records()),
                link_precs=(),
                remove_specs=(),
                update_specs=(),
                neutered_specs={},
            )
            txn = UnlinkLinkTransaction(stp)
            try:
                handle_txn(txn, prefix, args, False, True)
            except PackagesNotFoundError:
                if not args.json:
                    print(
                        f"No packages found in {prefix}. Continuing environment removal"
                    )
        if not context.dry_run:
            if not args.keep_env:
                if not args.json:
                    confirm_yn(
                        f"Everything found within the environment ({prefix}), including any conda environment configurations and any non-conda files, will be deleted. Do you wish to continue?\n",
                        default="no",
                        dry_run=False,
                    )
                rm_rf(prefix, clean_empty_parents=True)
                unregister_env(prefix)

        return 0

    else:
        if args.features:
            specs = tuple(MatchSpec(track_features=f) for f in set(args.package_names))
        else:
            specs = specs_from_args(args.package_names)
        channel_urls = ()
        subdirs = ()
        solver_backend = context.plugin_manager.get_cached_solver_backend()
        solver = solver_backend(prefix, channel_urls, subdirs, specs_to_remove=specs)
        txn = solver.solve_for_transaction()
        handle_txn(txn, prefix, args, False, True)
        return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Utilities for finding executables and `conda-*` commands."""

import os
import re
import sys
import sysconfig
from functools import lru_cache
from os.path import basename, expanduser, isfile, join

from ..common.compat import on_win


def find_executable(executable, include_others=True):
    # backwards compatibility
    global dir_paths

    if include_others:
        from ..utils import sys_prefix_unfollowed

        prefixes = [sys_prefix_unfollowed()]
        if sys.prefix != prefixes[0]:
            prefixes.append(sys.prefix)
        dir_paths = [join(p, basename(sysconfig.get_path("scripts"))) for p in prefixes]
        # Is this still needed?
        if on_win:
            dir_paths.append("C:\\cygwin\\bin")
    else:
        dir_paths = []

    dir_paths.extend(os.environ.get("PATH", "").split(os.pathsep))

    for dir_path in dir_paths:
        if on_win:
            for ext in (".exe", ".bat", ""):
                path = join(dir_path, executable + ext)
                if isfile(path):
                    return path
        else:
            path = join(dir_path, executable)
            if isfile(expanduser(path)):
                return expanduser(path)
    return None


@lru_cache(maxsize=None)
def find_commands(include_others=True):
    if include_others:
        from ..utils import sys_prefix_unfollowed

        prefixes = [sys_prefix_unfollowed()]
        if sys.prefix != prefixes[0]:
            prefixes.append(sys.prefix)
        dir_paths = [join(p, basename(sysconfig.get_path("scripts"))) for p in prefixes]
        # Is this still needed?
        if on_win:
            dir_paths.append("C:\\cygwin\\bin")
    else:
        dir_paths = []

    dir_paths.extend(os.environ.get("PATH", "").split(os.pathsep))

    if on_win:
        pat = re.compile(r"conda-([\w\-]+)(\.(exe|bat))?$")
    else:
        pat = re.compile(r"conda-([\w\-]+)$")

    res = set()
    for dir_path in dir_paths:
        try:
            for entry in os.scandir(dir_path):
                m = pat.match(entry.name)
                if m and entry.is_file():
                    res.add(m.group(1))
        except (FileNotFoundError, NotADirectoryError, PermissionError, OSError):
            # FileNotFoundError: path doesn't exist
            # NotADirectoryError: path is not a directory
            # PermissionError: user doesn't have read access
            # OSError: [WinError 123] The filename, directory name, or volume
            # label syntax is incorrect
            continue
    return tuple(sorted(res))


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda create`.

Creates new conda environments with the specified packages.
"""

from __future__ import annotations

from argparse import _StoreTrueAction
from logging import getLogger
from os.path import isdir
from typing import TYPE_CHECKING

from ..deprecations import deprecated
from ..notices import notices

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace, _SubParsersAction

log = getLogger(__name__)


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from ..common.constants import NULL
    from .actions import NullCountAction
    from .helpers import (
        add_parser_create_install_update,
        add_parser_default_packages,
        add_parser_platform,
        add_parser_solver,
    )

    summary = "Create a new conda environment from a list of specified packages. "
    description = dals(
        f"""
        {summary}

        To use the newly-created environment, use 'conda activate envname'.
        This command requires either the -n NAME or -p PREFIX option.
        """
    )
    epilog = dals(
        """
        Examples:

        Create an environment containing the package 'sqlite'::

            conda create -n myenv sqlite

        Create an environment (env2) as a clone of an existing environment (env1)::

            conda create -n env2 --clone path/to/file/env1

        """
    )
    p = sub_parsers.add_parser(
        "create",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    p.add_argument(
        "--clone",
        action="store",
        help="Create a new environment as a copy of an existing local environment.",
        metavar="ENV",
    )
    solver_mode_options, _, channel_options = add_parser_create_install_update(p)
    add_parser_default_packages(solver_mode_options)
    add_parser_platform(channel_options)
    add_parser_solver(solver_mode_options)
    p.add_argument(
        "-m",
        "--mkdir",
        action=deprecated.action(
            "24.9",
            "25.3",
            _StoreTrueAction,
            addendum="Redundant argument.",
        ),
    )
    p.add_argument(
        "--dev",
        action=NullCountAction,
        help="Use `sys.executable -m conda` in wrapper scripts instead of CONDA_EXE. "
        "This is mainly for use during tests where we test new conda sources "
        "against old Python versions.",
        dest="dev",
        default=NULL,
    )
    p.set_defaults(func="conda.cli.main_create.execute")

    return p


@notices
def execute(args: Namespace, parser: ArgumentParser) -> int:
    import os
    from tempfile import mktemp

    from ..base.constants import UNUSED_ENV_NAME
    from ..base.context import context
    from ..common.path import paths_equal
    from ..exceptions import ArgumentError, CondaValueError
    from ..gateways.disk.delete import rm_rf
    from ..gateways.disk.test import is_conda_environment
    from .common import confirm_yn
    from .install import install

    if not args.name and not args.prefix:
        if context.dry_run:
            args.prefix = os.path.join(mktemp(), UNUSED_ENV_NAME)
            context.__init__(argparse_args=args)
        else:
            raise ArgumentError(
                "one of the arguments -n/--name -p/--prefix is required"
            )

    if is_conda_environment(context.target_prefix):
        if paths_equal(context.target_prefix, context.root_prefix):
            raise CondaValueError("The target prefix is the base prefix. Aborting.")
        if context.dry_run:
            # Taking the "easy" way out, rather than trying to fake removing
            # the existing environment before creating a new one.
            raise CondaValueError(
                "Cannot `create --dry-run` with an existing conda environment"
            )
        confirm_yn(
            f"WARNING: A conda environment already exists at '{context.target_prefix}'\n"
            "Remove existing environment",
            default="no",
            dry_run=False,
        )
        log.info("Removing existing environment %s", context.target_prefix)
        rm_rf(context.target_prefix)
    elif isdir(context.target_prefix):
        confirm_yn(
            f"WARNING: A directory already exists at the target location '{context.target_prefix}'\n"
            "but it is not a conda environment.\n"
            "Continue creating environment",
            default="no",
            dry_run=False,
        )

    return install(args, parser, "create")


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""PEP 621 compatible entry point used when `conda init` has not updated the user shell profile."""

import os
import sys
from logging import getLogger

log = getLogger(__name__)


def pip_installed_post_parse_hook(args, p):
    from .. import CondaError

    if args.cmd not in ("init", "info"):
        raise CondaError(
            "Conda has not been initialized.\n"
            "\n"
            "To enable full conda functionality, please run 'conda init'.\n"
            "For additional information, see 'conda init --help'.\n"
        )


def main(*args, **kwargs):
    from .main import main

    os.environ["CONDA_PIP_UNINITIALIZED"] = "true"
    kwargs["post_parse_hook"] = pip_installed_post_parse_hook
    return main(*args, **kwargs)


if __name__ == "__main__":
    sys.exit(main())


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda config`.

Allows for programmatically interacting with conda's configuration files (e.g., `~/.condarc`).
"""

from __future__ import annotations

import json
import os
import sys
from argparse import SUPPRESS
from collections.abc import Mapping, Sequence
from itertools import chain
from logging import getLogger
from os.path import isfile, join
from pathlib import Path
from textwrap import wrap
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace, _SubParsersAction
    from typing import Any


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from ..base.constants import CONDA_HOMEPAGE_URL
    from ..base.context import context, sys_rc_path, user_rc_path
    from ..common.constants import NULL
    from .helpers import add_parser_json

    escaped_user_rc_path = user_rc_path.replace("%", "%%")
    escaped_sys_rc_path = sys_rc_path.replace("%", "%%")

    summary = "Modify configuration values in .condarc."
    description = dals(
        f"""
        {summary}

        This is modeled after the git config command.  Writes to the user .condarc
        file ({escaped_user_rc_path}) by default. Use the
        --show-sources flag to display all identified configuration locations on
        your computer.

        """
    )
    epilog = dals(
        f"""
        See `conda config --describe` or {CONDA_HOMEPAGE_URL}/docs/config.html
        for details on all the options that can go in .condarc.

        Examples:

        Display all configuration values as calculated and compiled::

            conda config --show

        Display all identified configuration sources::

            conda config --show-sources

        Print the descriptions of all available configuration
        options to your command line::

            conda config --describe

        Print the description for the "channel_priority" configuration
        option to your command line::

            conda config --describe channel_priority

        Add the conda-canary channel::

            conda config --add channels conda-canary

        Set the output verbosity to level 3 (highest) for
        the current activate environment::

            conda config --set verbosity 3 --env

        Add the 'conda-forge' channel as a backup to 'defaults'::

            conda config --append channels conda-forge

        """
    )

    p = sub_parsers.add_parser(
        "config",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    add_parser_json(p)

    # TODO: use argparse.FileType
    config_file_location_group = p.add_argument_group(
        "Config File Location Selection",
        f"Without one of these flags, the user config file at '{escaped_user_rc_path}' is used.",
    )
    location = config_file_location_group.add_mutually_exclusive_group()
    location.add_argument(
        "--system",
        action="store_true",
        help=f"Write to the system .condarc file at '{escaped_sys_rc_path}'.",
    )
    location.add_argument(
        "--env",
        action="store_true",
        help="Write to the active conda environment .condarc file ({}). "
        "If no environment is active, write to the user config file ({})."
        "".format(
            context.active_prefix or "<no active environment>",
            escaped_user_rc_path,
        ),
    )
    location.add_argument("--file", action="store", help="Write to the given file.")

    # XXX: Does this really have to be mutually exclusive. I think the below
    # code will work even if it is a regular group (although combination of
    # --add and --remove with the same keys will not be well-defined).
    _config_subcommands = p.add_argument_group("Config Subcommands")
    config_subcommands = _config_subcommands.add_mutually_exclusive_group()
    config_subcommands.add_argument(
        "--show",
        nargs="*",
        default=None,
        help="Display configuration values as calculated and compiled. "
        "If no arguments given, show information for all configuration values.",
    )
    config_subcommands.add_argument(
        "--show-sources",
        action="store_true",
        help="Display all identified configuration sources.",
    )
    config_subcommands.add_argument(
        "--validate",
        action="store_true",
        help="Validate all configuration sources. Iterates over all .condarc files "
        "and checks for parsing errors.",
    )
    config_subcommands.add_argument(
        "--describe",
        nargs="*",
        default=None,
        help="Describe given configuration parameters. If no arguments given, show "
        "information for all configuration parameters.",
    )
    config_subcommands.add_argument(
        "--write-default",
        action="store_true",
        help="Write the default configuration to a file. "
        "Equivalent to `conda config --describe > ~/.condarc`.",
    )

    _config_modifiers = p.add_argument_group("Config Modifiers")
    config_modifiers = _config_modifiers.add_mutually_exclusive_group()
    config_modifiers.add_argument(
        "--get",
        nargs="*",
        action="store",
        help="Get a configuration value.",
        default=None,
        metavar="KEY",
    )
    config_modifiers.add_argument(
        "--append",
        nargs=2,
        action="append",
        help="""Add one configuration value to the end of a list key.""",
        default=[],
        metavar=("KEY", "VALUE"),
    )
    config_modifiers.add_argument(
        "--prepend",
        "--add",
        nargs=2,
        action="append",
        help="""Add one configuration value to the beginning of a list key.""",
        default=[],
        metavar=("KEY", "VALUE"),
    )
    config_modifiers.add_argument(
        "--set",
        nargs=2,
        action="append",
        help="""Set a boolean or string key.""",
        default=[],
        metavar=("KEY", "VALUE"),
    )
    config_modifiers.add_argument(
        "--remove",
        nargs=2,
        action="append",
        help="""Remove a configuration value from a list key.
                This removes all instances of the value.""",
        default=[],
        metavar=("KEY", "VALUE"),
    )
    config_modifiers.add_argument(
        "--remove-key",
        action="append",
        help="""Remove a configuration key (and all its values).""",
        default=[],
        metavar="KEY",
    )
    config_modifiers.add_argument(
        "--stdin",
        action="store_true",
        help="Apply configuration information given in yaml format piped through stdin.",
    )

    p.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=NULL,
        help=SUPPRESS,  # TODO: No longer used.  Remove in a future release.
    )

    p.set_defaults(func="conda.cli.main_config.execute")

    return p


def execute(args: Namespace, parser: ArgumentParser) -> int:
    from .. import CondaError
    from ..exceptions import CouldntParseError

    try:
        return execute_config(args, parser)
    except (CouldntParseError, NotImplementedError) as e:
        raise CondaError(e)


def format_dict(d):
    from ..common.compat import isiterable
    from ..common.configuration import pretty_list, pretty_map

    lines = []
    for k, v in d.items():
        if isinstance(v, Mapping):
            if v:
                lines.append(f"{k}:")
                lines.append(pretty_map(v))
            else:
                lines.append(f"{k}: {{}}")
        elif isiterable(v):
            if v:
                lines.append(f"{k}:")
                lines.append(pretty_list(v))
            else:
                lines.append(f"{k}: []")
        else:
            lines.append("{}: {}".format(k, v if v is not None else "None"))
    return lines


def parameter_description_builder(name):
    from ..auxlib.entity import EntityEncoder
    from ..base.context import context
    from ..common.serialize import yaml_round_trip_dump

    builder = []
    details = context.describe_parameter(name)
    aliases = details["aliases"]
    string_delimiter = details.get("string_delimiter")
    element_types = details["element_types"]
    default_value_str = json.dumps(details["default_value"], cls=EntityEncoder)

    if details["parameter_type"] == "primitive":
        builder.append(
            "{} ({})".format(name, ", ".join(sorted({et for et in element_types})))
        )
    else:
        builder.append(
            "{} ({}: {})".format(
                name,
                details["parameter_type"],
                ", ".join(sorted({et for et in element_types})),
            )
        )

    if aliases:
        builder.append("  aliases: {}".format(", ".join(aliases)))
    if string_delimiter:
        builder.append(f"  env var string delimiter: '{string_delimiter}'")

    builder.extend("  " + line for line in wrap(details["description"], 70))

    builder.append("")
    builder = ["# " + line for line in builder]

    builder.extend(
        yaml_round_trip_dump({name: json.loads(default_value_str)}).strip().split("\n")
    )

    builder = ["# " + line for line in builder]
    builder.append("")
    return builder


def describe_all_parameters():
    from ..base.context import context

    builder = []
    skip_categories = ("CLI-only", "Hidden and Undocumented")
    for category, parameter_names in context.category_map.items():
        if category in skip_categories:
            continue
        builder.append("# ######################################################")
        builder.append(f"# ## {category:^48} ##")
        builder.append("# ######################################################")
        builder.append("")
        builder.extend(
            chain.from_iterable(
                parameter_description_builder(name) for name in parameter_names
            )
        )
        builder.append("")
    return "\n".join(builder)


def print_config_item(key, value):
    stdout_write = getLogger("conda.stdout").info
    if isinstance(value, (dict,)):
        for k, v in value.items():
            print_config_item(key + "." + k, v)
    elif isinstance(value, (bool, int, str)):
        stdout_write(" ".join(("--set", key, str(value))))
    elif isinstance(value, (list, tuple)):
        # Note, since `conda config --add` prepends, print `--add` commands in
        # reverse order (using repr), so that entering them in this order will
        # recreate the same file.
        numitems = len(value)
        for q, item in enumerate(reversed(value)):
            if key == "channels" and q in (0, numitems - 1):
                stdout_write(
                    " ".join(
                        (
                            "--add",
                            key,
                            repr(item),
                            "  # lowest priority" if q == 0 else "  # highest priority",
                        )
                    )
                )
            else:
                stdout_write(" ".join(("--add", key, repr(item))))


def _get_key(
    key: str,
    config: dict,
    *,
    json: dict[str, Any] = {},
    warnings: list[str] = [],
) -> None:
    from ..base.context import context

    key_parts = key.split(".")

    if key_parts[0] not in context.list_parameters():
        if context.json:
            warnings.append(f"Unknown key: {key_parts[0]!r}")
        else:
            print(f"Unknown key: {key_parts[0]!r}", file=sys.stderr)
        return

    sub_config = config
    try:
        for part in key_parts:
            sub_config = sub_config[part]
    except KeyError:
        # KeyError: part not found, nothing to get
        pass
    else:
        if context.json:
            json[key] = sub_config
        else:
            print_config_item(key, sub_config)


def _set_key(key: str, item: Any, config: dict) -> None:
    from ..base.context import context

    key_parts = key.split(".")
    try:
        parameter_type = context.describe_parameter(key_parts[0])["parameter_type"]
    except KeyError:
        # KeyError: key_parts[0] is an unknown parameter
        from ..exceptions import CondaKeyError

        raise CondaKeyError(key, "unknown parameter")

    if parameter_type == "primitive" and len(key_parts) == 1:
        (key,) = key_parts
        config[key] = context.typify_parameter(key, item, "--set parameter")
    elif parameter_type == "map" and len(key_parts) == 2:
        key, subkey = key_parts
        config.setdefault(key, {})[subkey] = item
    else:
        from ..exceptions import CondaKeyError

        raise CondaKeyError(key, "invalid parameter")


def _remove_item(key: str, item: Any, config: dict) -> None:
    from ..base.context import context

    key_parts = key.split(".")
    try:
        parameter_type = context.describe_parameter(key_parts[0])["parameter_type"]
    except KeyError:
        # KeyError: key_parts[0] is an unknown parameter
        from ..exceptions import CondaKeyError

        raise CondaKeyError(key, "unknown parameter")

    if parameter_type == "sequence" and len(key_parts) == 1:
        (key,) = key_parts
        if key not in config:
            if key != "channels":
                from ..exceptions import CondaKeyError

                raise CondaKeyError(key, "undefined in config")
            config[key] = ["defaults"]

        if item not in config[key]:
            from ..exceptions import CondaKeyError

            raise CondaKeyError(key, f"value {item!r} not present in config")
        config[key] = [i for i in config[key] if i != item]
    else:
        from ..exceptions import CondaKeyError

        raise CondaKeyError(key, "invalid parameter")


def _remove_key(key: str, config: dict) -> None:
    key_parts = key.split(".")

    sub_config = config
    try:
        for part in key_parts[:-1]:
            sub_config = sub_config[part]
        del sub_config[key_parts[-1]]
    except KeyError:
        # KeyError: part not found, nothing to remove
        from ..exceptions import CondaKeyError

        raise CondaKeyError(key, "undefined in config")


def _read_rc(path: str | os.PathLike | Path) -> dict:
    from ..common.serialize import yaml_round_trip_load

    try:
        return yaml_round_trip_load(Path(path).read_text()) or {}
    except FileNotFoundError:
        # FileNotFoundError: path does not exist
        return {}


def _write_rc(path: str | os.PathLike | Path, config: dict) -> None:
    from .. import CondaError
    from ..base.constants import (
        ChannelPriority,
        DepsModifier,
        PathConflict,
        SafetyChecks,
        SatSolverChoice,
        UpdateModifier,
    )
    from ..common.serialize import yaml, yaml_round_trip_dump

    # Add representers for enums.
    # Because a representer cannot be added for the base Enum class (it must be added for
    # each specific Enum subclass - and because of import rules), I don't know of a better
    # location to do this.
    def enum_representer(dumper, data):
        return dumper.represent_str(str(data))

    yaml.representer.RoundTripRepresenter.add_representer(
        SafetyChecks, enum_representer
    )
    yaml.representer.RoundTripRepresenter.add_representer(
        PathConflict, enum_representer
    )
    yaml.representer.RoundTripRepresenter.add_representer(
        DepsModifier, enum_representer
    )
    yaml.representer.RoundTripRepresenter.add_representer(
        UpdateModifier, enum_representer
    )
    yaml.representer.RoundTripRepresenter.add_representer(
        ChannelPriority, enum_representer
    )
    yaml.representer.RoundTripRepresenter.add_representer(
        SatSolverChoice, enum_representer
    )

    try:
        Path(path).write_text(yaml_round_trip_dump(config))
    except OSError as e:
        raise CondaError(f"Cannot write to condarc file at {path}\nCaused by {e!r}")


def set_keys(*args: tuple[str, Any], path: str | os.PathLike | Path) -> None:
    config = _read_rc(path)
    for key, value in args:
        _set_key(key, value, config)
    _write_rc(path, config)


def execute_config(args, parser):
    from .. import CondaError
    from ..auxlib.entity import EntityEncoder
    from ..base.context import context, sys_rc_path, user_rc_path
    from ..common.io import timeout
    from ..common.iterators import groupby_to_dict as groupby
    from ..common.serialize import yaml_round_trip_load

    stdout_write = getLogger("conda.stdout").info
    stderr_write = getLogger("conda.stderr").info
    json_warnings = []
    json_get = {}

    if args.show_sources:
        if context.json:
            stdout_write(
                json.dumps(
                    {
                        str(source): values
                        for source, values in context.collect_all().items()
                    },
                    sort_keys=True,
                    indent=2,
                    separators=(",", ": "),
                    cls=EntityEncoder,
                )
            )
        else:
            lines = []
            for source, reprs in context.collect_all().items():
                lines.append(f"==> {source} <==")
                lines.extend(format_dict(reprs))
                lines.append("")
            stdout_write("\n".join(lines))
        return

    if args.show is not None:
        if args.show:
            paramater_names = args.show
            all_names = context.list_parameters()
            not_params = set(paramater_names) - set(all_names)
            if not_params:
                from ..common.io import dashlist
                from ..exceptions import ArgumentError

                raise ArgumentError(
                    f"Invalid configuration parameters: {dashlist(not_params)}"
                )
        else:
            paramater_names = context.list_parameters()

        d = {key: getattr(context, key) for key in paramater_names}
        if context.json:
            stdout_write(
                json.dumps(
                    d,
                    sort_keys=True,
                    indent=2,
                    separators=(",", ": "),
                    cls=EntityEncoder,
                )
            )
        else:
            # Add in custom formatting
            if "custom_channels" in d:
                d["custom_channels"] = {
                    channel.name: f"{channel.scheme}://{channel.location}"
                    for channel in d["custom_channels"].values()
                }
            if "custom_multichannels" in d:
                from ..common.io import dashlist

                d["custom_multichannels"] = {
                    multichannel_name: dashlist(channels, indent=4)
                    for multichannel_name, channels in d["custom_multichannels"].items()
                }
            if "channel_settings" in d:
                ident = " " * 4
                d["channel_settings"] = tuple(
                    f"\n{ident}".join(format_dict(mapping))
                    for mapping in d["channel_settings"]
                )

            stdout_write("\n".join(format_dict(d)))
        context.validate_configuration()
        return

    if args.describe is not None:
        if args.describe:
            paramater_names = args.describe
            all_names = context.list_parameters()
            not_params = set(paramater_names) - set(all_names)
            if not_params:
                from ..common.io import dashlist
                from ..exceptions import ArgumentError

                raise ArgumentError(
                    f"Invalid configuration parameters: {dashlist(not_params)}"
                )
            if context.json:
                stdout_write(
                    json.dumps(
                        [context.describe_parameter(name) for name in paramater_names],
                        sort_keys=True,
                        indent=2,
                        separators=(",", ": "),
                        cls=EntityEncoder,
                    )
                )
            else:
                builder = []
                builder.extend(
                    chain.from_iterable(
                        parameter_description_builder(name) for name in paramater_names
                    )
                )
                stdout_write("\n".join(builder))
        else:
            if context.json:
                skip_categories = ("CLI-only", "Hidden and Undocumented")
                paramater_names = sorted(
                    chain.from_iterable(
                        parameter_names
                        for category, parameter_names in context.category_map.items()
                        if category not in skip_categories
                    )
                )
                stdout_write(
                    json.dumps(
                        [context.describe_parameter(name) for name in paramater_names],
                        sort_keys=True,
                        indent=2,
                        separators=(",", ": "),
                        cls=EntityEncoder,
                    )
                )
            else:
                stdout_write(describe_all_parameters())
        return

    if args.validate:
        context.validate_all()
        return

    if args.system:
        rc_path = sys_rc_path
    elif args.env:
        if context.active_prefix:
            rc_path = join(context.active_prefix, ".condarc")
        else:
            rc_path = user_rc_path
    elif args.file:
        rc_path = args.file
    else:
        rc_path = user_rc_path

    if args.write_default:
        if isfile(rc_path):
            with open(rc_path) as fh:
                data = fh.read().strip()
            if data:
                raise CondaError(
                    f"The file '{rc_path}' "
                    "already contains configuration information.\n"
                    "Remove the file to proceed.\n"
                    "Use `conda config --describe` to display default configuration."
                )

        with open(rc_path, "w") as fh:
            fh.write(describe_all_parameters())
        return

    # read existing condarc
    if os.path.exists(rc_path):
        with open(rc_path) as fh:
            # round trip load required because... we need to round trip
            rc_config = yaml_round_trip_load(fh) or {}
    elif os.path.exists(sys_rc_path):
        # In case the considered rc file doesn't exist, fall back to the system rc
        with open(sys_rc_path) as fh:
            rc_config = yaml_round_trip_load(fh) or {}
    else:
        rc_config = {}

    grouped_paramaters = groupby(
        lambda p: context.describe_parameter(p)["parameter_type"],
        context.list_parameters(),
    )
    sequence_parameters = grouped_paramaters["sequence"]
    map_parameters = grouped_paramaters["map"]

    # Get
    if args.get is not None:
        context.validate_all()

        for key in args.get or sorted(rc_config.keys()):
            _get_key(key, rc_config, json=json_get, warnings=json_warnings)

    if args.stdin:
        content = timeout(5, sys.stdin.read)
        if not content:
            return
        try:
            # round trip load required because... we need to round trip
            parsed = yaml_round_trip_load(content)
            rc_config.update(parsed)
        except Exception:  # pragma: no cover
            from ..exceptions import ParseError

            raise ParseError(f"invalid yaml content:\n{content}")

    # prepend, append, add
    for arg, prepend in zip((args.prepend, args.append), (True, False)):
        for key, item in arg:
            key, subkey = key.split(".", 1) if "." in key else (key, None)
            if key == "channels" and key not in rc_config:
                rc_config[key] = ["defaults"]
            if key in sequence_parameters:
                arglist = rc_config.setdefault(key, [])
            elif key in map_parameters:
                arglist = rc_config.setdefault(key, {}).setdefault(subkey, [])
            else:
                from ..exceptions import CondaValueError

                raise CondaValueError(f"Key '{key}' is not a known sequence parameter.")
            if not (isinstance(arglist, Sequence) and not isinstance(arglist, str)):
                from ..exceptions import CouldntParseError

                bad = rc_config[key].__class__.__name__
                raise CouldntParseError(f"key {key!r} should be a list, not {bad}.")
            if item in arglist:
                message_key = key + "." + subkey if subkey is not None else key
                # Right now, all list keys should not contain duplicates
                message = "Warning: '{}' already in '{}' list, moving to the {}".format(
                    item, message_key, "top" if prepend else "bottom"
                )
                if subkey is None:
                    arglist = rc_config[key] = [p for p in arglist if p != item]
                else:
                    arglist = rc_config[key][subkey] = [p for p in arglist if p != item]
                if not context.json:
                    stderr_write(message)
                else:
                    json_warnings.append(message)
            arglist.insert(0 if prepend else len(arglist), item)

    # Set
    for key, item in args.set:
        _set_key(key, item, rc_config)

    # Remove
    for key, item in args.remove:
        _remove_item(key, item, rc_config)

    # Remove Key
    for key in args.remove_key:
        _remove_key(key, rc_config)

    # config.rc_keys
    if not args.get:
        _write_rc(rc_path, rc_config)

    if context.json:
        from .common import stdout_json_success

        stdout_json_success(rc_path=rc_path, warnings=json_warnings, get=json_get)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Entry point for all conda subcommands."""

import sys

from ..deprecations import deprecated


@deprecated.argument(
    "24.3",
    "24.9",
    "context",
    addendum="The context is a global state, no need to pass it around.",
)
def init_loggers():
    import logging

    from ..base.context import context
    from ..gateways.logging import initialize_logging, set_log_level

    initialize_logging()

    # silence logging info to avoid interfering with JSON output
    if context.json:
        for logger in ("conda.stdout.verbose", "conda.stdoutlog", "conda.stderrlog"):
            logging.getLogger(logger).setLevel(logging.CRITICAL + 10)

    # set log_level
    set_log_level(context.log_level)


@deprecated(
    "24.3",
    "24.9",
    addendum="Use `conda.cli.conda_argparse.generate_parser` instead.",
)
def generate_parser(*args, **kwargs):
    """
    Some code paths import this function directly from this module instead
    of from conda_argparse. We add the forwarder for backwards compatibility.
    """
    from .conda_argparse import generate_parser

    return generate_parser(*args, **kwargs)


def main_subshell(*args, post_parse_hook=None, **kwargs):
    """Entrypoint for the "subshell" invocation of CLI interface. E.g. `conda create`."""
    # defer import here so it doesn't hit the 'conda shell.*' subcommands paths
    from ..base.context import context
    from .conda_argparse import do_call, generate_parser, generate_pre_parser

    args = args or ["--help"]

    pre_parser = generate_pre_parser(add_help=False)
    pre_args, _ = pre_parser.parse_known_args(args)

    # the arguments that we want to pass to the main parser later on
    override_args = {
        "json": pre_args.json,
        "debug": pre_args.debug,
        "trace": pre_args.trace,
        "verbosity": pre_args.verbosity,
    }

    context.__init__(argparse_args=pre_args)
    if context.no_plugins:
        context.plugin_manager.disable_external_plugins()

    # reinitialize in case any of the entrypoints modified the context
    context.__init__(argparse_args=pre_args)

    parser = generate_parser(add_help=True)
    args = parser.parse_args(args, override_args=override_args, namespace=pre_args)

    context.__init__(argparse_args=args)
    init_loggers()

    # used with main_pip.py
    if post_parse_hook:
        post_parse_hook(args, parser)

    exit_code = do_call(args, parser)
    if isinstance(exit_code, int):
        return exit_code
    elif hasattr(exit_code, "rc"):
        return exit_code.rc


def main_sourced(shell, *args, **kwargs):
    """Entrypoint for the "sourced" invocation of CLI interface. E.g. `conda activate`."""
    shell = shell.replace("shell.", "", 1)

    # This is called any way later in conda.activate, so no point in removing it
    from ..base.context import context

    context.__init__()

    from ..activate import _build_activator_cls

    try:
        activator_cls = _build_activator_cls(shell)
    except KeyError:
        from ..exceptions import CondaError

        raise CondaError(f"{shell} is not a supported shell.")

    activator = activator_cls(args)
    print(activator.execute(), end="")
    return 0


def main(*args, **kwargs):
    # conda.common.compat contains only stdlib imports
    from ..common.compat import ensure_text_type
    from ..exception_handler import conda_exception_handler

    # cleanup argv
    args = args or sys.argv[1:]  # drop executable/script
    args = tuple(ensure_text_type(s) for s in args)

    if args and args[0].strip().startswith("shell."):
        main = main_sourced
    else:
        main = main_subshell

    return conda_exception_handler(main, *args, **kwargs)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Collection of helper functions to standardize reused CLI arguments.
"""

from __future__ import annotations

from argparse import SUPPRESS, _HelpAction
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from argparse import ArgumentParser, _ArgumentGroup, _MutuallyExclusiveGroup

try:
    from argparse import BooleanOptionalAction
except ImportError:
    # Python < 3.9
    from argparse import Action

    class BooleanOptionalAction(Action):
        # from Python 3.9+ argparse.py
        def __init__(
            self,
            option_strings,
            dest,
            default=None,
            type=None,
            choices=None,
            required=False,
            help=None,
            metavar=None,
        ):
            _option_strings = []
            for option_string in option_strings:
                _option_strings.append(option_string)

                if option_string.startswith("--"):
                    option_string = "--no-" + option_string[2:]
                    _option_strings.append(option_string)

            super().__init__(
                option_strings=_option_strings,
                dest=dest,
                nargs=0,
                default=default,
                type=type,
                choices=choices,
                required=required,
                help=help,
                metavar=metavar,
            )

        def __call__(self, parser, namespace, values, option_string=None):
            if option_string in self.option_strings:
                setattr(namespace, self.dest, not option_string.startswith("--no-"))

        def format_usage(self):
            return " | ".join(self.option_strings)


def add_parser_create_install_update(p, prefix_required=False):
    from ..common.constants import NULL

    add_parser_prefix(p, prefix_required)
    channel_options = add_parser_channels(p)
    solver_mode_options = add_parser_solver_mode(p)
    package_install_options = add_parser_package_install_options(p)
    add_parser_networking(p)

    output_and_prompt_options = add_output_and_prompt_options(p)
    output_and_prompt_options.add_argument(
        "--download-only",
        action="store_true",
        default=NULL,
        help="Solve an environment and ensure package caches are populated, but exit "
        "prior to unlinking and linking packages into the prefix.",
    )
    add_parser_show_channel_urls(output_and_prompt_options)

    add_parser_pscheck(p)
    add_parser_known(p)

    # Add the file kwarg. We don't use {action="store", nargs='*'} as we don't
    # want to gobble up all arguments after --file.
    p.add_argument(
        "--file",
        default=[],
        action="append",
        help="Read package versions from the given file. Repeated file "
        "specifications can be passed (e.g. --file=file1 --file=file2).",
    )
    p.add_argument(
        "packages",
        metavar="package_spec",
        action="store",
        nargs="*",
        help="List of packages to install or update in the conda environment.",
    )

    return solver_mode_options, package_install_options, channel_options


def add_parser_pscheck(p: ArgumentParser) -> None:
    p.add_argument("--force-pscheck", action="store_true", help=SUPPRESS)


def add_parser_show_channel_urls(p: ArgumentParser | _ArgumentGroup) -> None:
    from ..common.constants import NULL

    p.add_argument(
        "--show-channel-urls",
        action="store_true",
        dest="show_channel_urls",
        default=NULL,
        help="Show channel urls. "
        "Overrides the value given by `conda config --show show_channel_urls`.",
    )
    p.add_argument(
        "--no-show-channel-urls",
        action="store_false",
        dest="show_channel_urls",
        help=SUPPRESS,
    )


def add_parser_help(p: ArgumentParser) -> None:
    """
    So we can use consistent capitalization and periods in the help. You must
    use the add_help=False argument to ArgumentParser or add_parser to use
    this. Add this first to be consistent with the default argparse output.

    """
    p.add_argument(
        "-h",
        "--help",
        action=_HelpAction,
        help="Show this help message and exit.",
    )


def add_parser_prefix(
    p: ArgumentParser,
    prefix_required: bool = False,
) -> _MutuallyExclusiveGroup:
    target_environment_group = p.add_argument_group("Target Environment Specification")
    npgroup = target_environment_group.add_mutually_exclusive_group(
        required=prefix_required
    )
    npgroup.add_argument(
        "-n",
        "--name",
        action="store",
        help="Name of environment.",
        metavar="ENVIRONMENT",
    )
    npgroup.add_argument(
        "-p",
        "--prefix",
        action="store",
        help="Full path to environment location (i.e. prefix).",
        metavar="PATH",
    )
    return npgroup


def add_parser_json(p: ArgumentParser) -> _ArgumentGroup:
    from ..common.constants import NULL

    output_and_prompt_options = p.add_argument_group(
        "Output, Prompt, and Flow Control Options"
    )
    output_and_prompt_options.add_argument(
        "--json",
        action="store_true",
        default=NULL,
        help="Report all output as json. Suitable for using conda programmatically.",
    )
    add_parser_verbose(output_and_prompt_options)
    output_and_prompt_options.add_argument(
        "-q",
        "--quiet",
        action="store_true",
        default=NULL,
        help="Do not display progress bar.",
    )
    return output_and_prompt_options


def add_output_and_prompt_options(p: ArgumentParser) -> _ArgumentGroup:
    from ..common.constants import NULL

    output_and_prompt_options = add_parser_json(p)
    output_and_prompt_options.add_argument(
        "-d",
        "--dry-run",
        action="store_true",
        help="Only display what would have been done.",
    )
    output_and_prompt_options.add_argument(
        "-y",
        "--yes",
        action="store_true",
        default=NULL,
        help="Sets any confirmation values to 'yes' automatically. "
        "Users will not be asked to confirm any adding, deleting, backups, etc.",
    )
    return output_and_prompt_options


def add_parser_channels(p: ArgumentParser) -> _ArgumentGroup:
    from ..common.constants import NULL

    channel_customization_options = p.add_argument_group("Channel Customization")
    channel_customization_options.add_argument(
        "-c",
        "--channel",
        # beware conda-build uses this (currently or in the past?)
        # if ever renaming to "channels" consider removing context.channels alias to channel
        dest="channel",
        action="append",
        help=(
            "Additional channel to search for packages. These are URLs searched in the order "
            "they are given (including local directories using the 'file://' syntax or "
            "simply a path like '/home/conda/mychan' or '../mychan'). Then, the defaults "
            "or channels from .condarc are searched (unless --override-channels is given). "
            "You can use 'defaults' to get the default packages for conda. You can also "
            "use any name and the .condarc channel_alias value will be prepended. The "
            "default channel_alias is https://conda.anaconda.org/."
        ),
    )
    channel_customization_options.add_argument(
        "--use-local",
        action="store_true",
        default=NULL,
        help="Use locally built packages. Identical to '-c local'.",
    )
    channel_customization_options.add_argument(
        "--override-channels",
        action="store_true",
        help="""Do not search default or .condarc channels.  Requires --channel.""",
    )
    channel_customization_options.add_argument(
        "--repodata-fn",
        action="append",
        dest="repodata_fns",
        help=(
            "Specify file name of repodata on the remote server where your channels "
            "are configured or within local backups. Conda will try whatever you "
            "specify, but will ultimately fall back to repodata.json if your specs are "
            "not satisfiable with what you specify here. This is used to employ repodata "
            "that is smaller and reduced in time scope. You may pass this flag more than "
            "once. Leftmost entries are tried first, and the fallback to repodata.json "
            "is added for you automatically. For more information, see "
            "conda config --describe repodata_fns."
        ),
    )
    channel_customization_options.add_argument(
        "--experimental",
        action="append",
        choices=["jlap", "lock"],
        help="jlap: Download incremental package index data from repodata.jlap; implies 'lock'. "
        "lock: use locking when reading, updating index (repodata.json) cache. Now enabled.",
    )
    channel_customization_options.add_argument(
        "--no-lock",
        action="store_true",
        help="Disable locking when reading, updating index (repodata.json) cache. ",
    )

    channel_customization_options.add_argument(
        "--repodata-use-zst",
        action=BooleanOptionalAction,
        dest="repodata_use_zst",
        default=NULL,
        help="Check for/do not check for repodata.json.zst. Enabled by default.",
    )
    return channel_customization_options


def add_parser_solver_mode(p: ArgumentParser) -> _ArgumentGroup:
    from ..base.constants import DepsModifier
    from ..common.constants import NULL

    solver_mode_options = p.add_argument_group("Solver Mode Modifiers")
    deps_modifiers = solver_mode_options.add_mutually_exclusive_group()
    solver_mode_options.add_argument(
        "--strict-channel-priority",
        action="store_const",
        dest="channel_priority",
        default=NULL,
        const="strict",
        help="Packages in lower priority channels are not considered if a package "
        "with the same name appears in a higher priority channel.",
    )
    solver_mode_options.add_argument(
        "--channel-priority",
        action="store_true",
        dest="channel_priority",
        default=NULL,
        help=SUPPRESS,
    )
    solver_mode_options.add_argument(
        "--no-channel-priority",
        action="store_const",
        dest="channel_priority",
        default=NULL,
        const="disabled",
        help="Package version takes precedence over channel priority. "
        "Overrides the value given by `conda config --show channel_priority`.",
    )
    deps_modifiers.add_argument(
        "--no-deps",
        action="store_const",
        const=DepsModifier.NO_DEPS,
        dest="deps_modifier",
        help="Do not install, update, remove, or change dependencies. This WILL lead "
        "to broken environments and inconsistent behavior. Use at your own risk.",
        default=NULL,
    )
    deps_modifiers.add_argument(
        "--only-deps",
        action="store_const",
        const=DepsModifier.ONLY_DEPS,
        dest="deps_modifier",
        help="Only install dependencies.",
        default=NULL,
    )
    solver_mode_options.add_argument(
        "--no-pin",
        action="store_true",
        dest="ignore_pinned",
        default=NULL,
        help="Ignore pinned file.",
    )
    return solver_mode_options


def add_parser_update_modifiers(solver_mode_options: ArgumentParser):
    from ..base.constants import UpdateModifier
    from ..common.constants import NULL

    update_modifiers = solver_mode_options.add_mutually_exclusive_group()
    update_modifiers.add_argument(
        "--freeze-installed",
        "--no-update-deps",
        action="store_const",
        const=UpdateModifier.FREEZE_INSTALLED,
        dest="update_modifier",
        default=NULL,
        help="Do not update or change already-installed dependencies.",
    )
    update_modifiers.add_argument(
        "--update-deps",
        action="store_const",
        const=UpdateModifier.UPDATE_DEPS,
        dest="update_modifier",
        default=NULL,
        help="Update dependencies that have available updates.",
    )
    update_modifiers.add_argument(
        "-S",
        "--satisfied-skip-solve",
        action="store_const",
        const=UpdateModifier.SPECS_SATISFIED_SKIP_SOLVE,
        dest="update_modifier",
        default=NULL,
        help="Exit early and do not run the solver if the requested specs are satisfied. "
        "Also skips aggressive updates as configured by the "
        "'aggressive_update_packages' config setting. Use "
        "'conda config --describe aggressive_update_packages' to view your setting. "
        "--satisfied-skip-solve is similar to the default behavior of 'pip install'.",
    )
    update_modifiers.add_argument(
        "--update-all",
        "--all",
        action="store_const",
        const=UpdateModifier.UPDATE_ALL,
        dest="update_modifier",
        help="Update all installed packages in the environment.",
        default=NULL,
    )
    update_modifiers.add_argument(
        "--update-specs",
        action="store_const",
        const=UpdateModifier.UPDATE_SPECS,
        dest="update_modifier",
        help="Update based on provided specifications.",
        default=NULL,
    )


def add_parser_prune(p: ArgumentParser) -> None:
    from ..common.constants import NULL

    p.add_argument(
        "--prune",
        action="store_true",
        default=NULL,
        help=SUPPRESS,
    )


def add_parser_solver(p: ArgumentParser) -> None:
    """
    Add a command-line flag for alternative solver backends.

    See ``context.solver`` for more info.
    """
    from ..base.context import context
    from ..common.constants import NULL

    group = p.add_mutually_exclusive_group()
    group.add_argument(
        "--solver",
        dest="solver",
        choices=context.plugin_manager.get_solvers(),
        help="Choose which solver backend to use.",
        default=NULL,
    )


def add_parser_networking(p: ArgumentParser) -> _ArgumentGroup:
    from ..common.constants import NULL

    networking_options = p.add_argument_group("Networking Options")
    networking_options.add_argument(
        "-C",
        "--use-index-cache",
        action="store_true",
        default=False,
        help="Use cache of channel index files, even if it has expired. This is useful "
        "if you don't want conda to check whether a new version of the repodata "
        "file exists, which will save bandwidth.",
    )
    networking_options.add_argument(
        "-k",
        "--insecure",
        action="store_false",
        dest="ssl_verify",
        default=NULL,
        help='Allow conda to perform "insecure" SSL connections and transfers. '
        "Equivalent to setting 'ssl_verify' to 'false'.",
    )
    networking_options.add_argument(
        "--offline",
        action="store_true",
        default=NULL,
        help="Offline mode. Don't connect to the Internet.",
    )
    return networking_options


def add_parser_package_install_options(p: ArgumentParser) -> _ArgumentGroup:
    from ..common.constants import NULL

    package_install_options = p.add_argument_group(
        "Package Linking and Install-time Options"
    )
    package_install_options.add_argument(
        "-f",
        "--force",
        action="store_true",
        default=NULL,
        help=SUPPRESS,
    )
    package_install_options.add_argument(
        "--copy",
        action="store_true",
        default=NULL,
        help="Install all packages using copies instead of hard- or soft-linking.",
    )
    package_install_options.add_argument(
        "--shortcuts",
        action="store_true",
        help=SUPPRESS,
        dest="shortcuts",
        default=NULL,
    )
    package_install_options.add_argument(
        "--no-shortcuts",
        action="store_false",
        help="Don't install start menu shortcuts",
        dest="shortcuts",
        default=NULL,
    )
    package_install_options.add_argument(
        "--shortcuts-only",
        action="append",
        help="Install shortcuts only for this package name. Can be used several times.",
        dest="shortcuts_only",
    )
    return package_install_options


def add_parser_known(p: ArgumentParser) -> None:
    p.add_argument(
        "--unknown",
        action="store_true",
        default=False,
        dest="unknown",
        help=SUPPRESS,
    )


def add_parser_default_packages(p: ArgumentParser) -> None:
    p.add_argument(
        "--no-default-packages",
        action="store_true",
        help="Ignore create_default_packages in the .condarc file.",
    )


def add_parser_platform(parser):
    from ..base.constants import KNOWN_SUBDIRS
    from ..common.constants import NULL

    parser.add_argument(
        "--subdir",
        "--platform",
        default=NULL,
        dest="subdir",
        choices=[s for s in KNOWN_SUBDIRS if s != "noarch"],
        metavar="SUBDIR",
        help="Use packages built for this platform. "
        "The new environment will be configured to remember this choice. "
        "Should be formatted like 'osx-64', 'linux-32', 'win-64', and so on. "
        "Defaults to the current (native) platform.",
    )


def add_parser_verbose(parser: ArgumentParser | _ArgumentGroup) -> None:
    from ..common.constants import NULL
    from .actions import NullCountAction

    parser.add_argument(
        "-v",
        "--verbose",
        action=NullCountAction,
        help=(
            "Can be used multiple times. Once for detailed output, twice for INFO logging, "
            "thrice for DEBUG logging, four times for TRACE logging."
        ),
        dest="verbosity",
        default=NULL,
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help=SUPPRESS,
        default=NULL,
    )
    parser.add_argument(
        "--trace",
        action="store_true",
        help=SUPPRESS,
        default=NULL,
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda-env config`.

Allows for programmatically interacting with conda-env's configuration files (e.g., `~/.condarc`).
"""

from argparse import ArgumentParser, Namespace, _SubParsersAction


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .main_env_vars import configure_parser as configure_vars_parser

    summary = "Configure a conda environment."
    description = summary
    epilog = dals(
        """
        Examples::

            conda env config vars list
            conda env config --append channels conda-forge

        """
    )

    p = sub_parsers.add_parser(
        "config",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    p.set_defaults(func="conda.cli.main_env_config.execute")
    config_subparser = p.add_subparsers()
    configure_vars_parser(config_subparser)

    return p


def execute(args: Namespace, parser: ArgumentParser) -> int:
    parser.parse_args(["env", "config", "--help"])

    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda export`.

Dumps specified environment package specifications to the screen.
"""

from argparse import (
    ArgumentParser,
    Namespace,
    _SubParsersAction,
)

from ..common.configuration import YAML_EXTENSIONS
from ..exceptions import CondaValueError


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .helpers import add_parser_json, add_parser_prefix

    summary = "Export a given environment"
    description = summary
    epilog = dals(
        """
        Examples::

            conda export
            conda export --file FILE_NAME

        """
    )

    p = sub_parsers.add_parser(
        "export",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )

    p.add_argument(
        "-c",
        "--channel",
        action="append",
        help="Additional channel to include in the export",
    )

    p.add_argument(
        "--override-channels",
        action="store_true",
        help="Do not include .condarc channels",
    )
    add_parser_prefix(p)

    p.add_argument(
        "-f",
        "--file",
        default=None,
        required=False,
        help=(
            "File name or path for the exported environment. "
            "Note: This will silently overwrite any existing file "
            "of the same name in the current directory."
        ),
    )

    p.add_argument(
        "--no-builds",
        default=False,
        action="store_true",
        required=False,
        help="Remove build specification from dependencies",
    )

    p.add_argument(
        "--ignore-channels",
        default=False,
        action="store_true",
        required=False,
        help="Do not include channel names with package names.",
    )
    add_parser_json(p)

    p.add_argument(
        "--from-history",
        default=False,
        action="store_true",
        required=False,
        help="Build environment spec from explicit specs in history",
    )
    p.set_defaults(func="conda.cli.main_export.execute")

    return p


# TODO Make this aware of channels that were used to install packages
def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.context import context, determine_target_prefix, env_name
    from ..env.env import from_environment
    from .common import stdout_json

    prefix = determine_target_prefix(context, args)
    env = from_environment(
        env_name(prefix),
        prefix,
        no_builds=args.no_builds,
        ignore_channels=args.ignore_channels,
        from_history=args.from_history,
    )

    if args.override_channels:
        env.remove_channels()

    if args.channel is not None:
        env.add_channels(args.channel)

    if args.file is None:
        stdout_json(env.to_dict()) if args.json else print(env.to_yaml(), end="")
    else:
        filename = args.file
        # check for the proper file extension; otherwise when the export file is used later,
        # the user will get a file parsing error
        if not filename.endswith(YAML_EXTENSIONS):
            raise CondaValueError(
                f"Export files must have a valid extension {YAML_EXTENSIONS}: {filename}"
            )
        fp = open(args.file, "wb")
        env.to_dict(stream=fp) if args.json else env.to_yaml(stream=fp)
        fp.close()

    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda install`.

Installs the specified packages into an existing environment.
"""

from __future__ import annotations

import sys
from argparse import _StoreTrueAction
from typing import TYPE_CHECKING

from ..deprecations import deprecated
from ..notices import notices

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace, _SubParsersAction


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from ..common.constants import NULL
    from .actions import NullCountAction
    from .helpers import (
        add_parser_create_install_update,
        add_parser_prune,
        add_parser_solver,
        add_parser_update_modifiers,
    )

    summary = "Install a list of packages into a specified conda environment."
    description = dals(
        f"""
        {summary}

        This command accepts a list of package specifications (e.g, bitarray=0.8)
        and installs a set of packages consistent with those specifications and
        compatible with the underlying environment. If full compatibility cannot
        be assured, an error is reported and the environment is not changed.

        Conda attempts to install the newest versions of the requested packages. To
        accomplish this, it may update some packages that are already installed, or
        install additional packages. To prevent existing packages from updating,
        use the --freeze-installed option. This may force conda to install older
        versions of the requested packages, and it does not prevent additional
        dependency packages from being installed.

        If you wish to skip dependency checking altogether, use the '--no-deps'
        option. This may result in an environment with incompatible packages, so
        this option must be used with great caution.

        conda can also be called with a list of explicit conda package filenames
        (e.g. ./lxml-3.2.0-py27_0.tar.bz2). Using conda in this mode implies the
        --no-deps option, and should likewise be used with great caution. Explicit
        filenames and package specifications cannot be mixed in a single command.
        """
    )
    epilog = dals(
        """
        Examples:

        Install the package 'scipy' into the currently-active environment::

            conda install scipy

        Install a list of packages into an environment, myenv::

            conda install -n myenv scipy curl wheel

        Install a specific version of 'python' into an environment, myenv::

            conda install -p path/to/myenv python=3.11

        """
    )

    p = sub_parsers.add_parser(
        "install",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    p.add_argument(
        "--revision",
        action="store",
        help="Revert to the specified REVISION.",
        metavar="REVISION",
    )

    solver_mode_options, package_install_options, _ = add_parser_create_install_update(
        p
    )

    add_parser_prune(solver_mode_options)
    add_parser_solver(solver_mode_options)
    solver_mode_options.add_argument(
        "--force-reinstall",
        action="store_true",
        default=NULL,
        help="Ensure that any user-requested package for the current operation is uninstalled and "
        "reinstalled, even if that package already exists in the environment.",
    )
    add_parser_update_modifiers(solver_mode_options)
    package_install_options.add_argument(
        "-m",
        "--mkdir",
        action=deprecated.action(
            "24.9",
            "25.3",
            _StoreTrueAction,
            addendum="Use `conda create` instead.",
        ),
    )
    package_install_options.add_argument(
        "--clobber",
        action="store_true",
        default=NULL,
        help="Allow clobbering (i.e. overwriting) of overlapping file paths "
        "within packages and suppress related warnings.",
    )
    p.add_argument(
        "--dev",
        action=NullCountAction,
        help="Use `sys.executable -m conda` in wrapper scripts instead of CONDA_EXE. "
        "This is mainly for use during tests where we test new conda sources "
        "against old Python versions.",
        dest="dev",
        default=NULL,
    )
    p.set_defaults(func="conda.cli.main_install.execute")

    return p


@notices
def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.context import context
    from .install import install

    if context.force:
        print(
            "\n\n"
            "WARNING: The --force flag will be removed in a future conda release.\n"
            "         See 'conda install --help' for details about the --force-reinstall\n"
            "         and --clobber flags.\n"
            "\n",
            file=sys.stderr,
        )

    return install(args, parser, "install")


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda search`.

Query channels for packages matching the provided package spec.
"""

from __future__ import annotations

from argparse import SUPPRESS
from collections import defaultdict
from datetime import datetime, timezone
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace, _SubParsersAction

    from ..models.records import PackageRecord


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from ..common.constants import NULL
    from .helpers import (
        add_parser_channels,
        add_parser_json,
        add_parser_known,
        add_parser_networking,
    )

    summary = "Search for packages and display associated information using the MatchSpec format."
    description = dals(
        f"""
        {summary}

        MatchSpec is a query language for conda packages.
        """
    )
    epilog = dals(
        """
        Examples:

        Search for a specific package named 'scikit-learn'::

            conda search scikit-learn

        Search for packages containing 'scikit' in the package name::

            conda search *scikit*

        Note that your shell may expand '*' before handing the command over to conda.
        Therefore, it is sometimes necessary to use single or double quotes around the query::

            conda search '*scikit'
            conda search "*scikit*"

        Search for packages for 64-bit Linux (by default, packages for your current
        platform are shown)::

            conda search numpy[subdir=linux-64]

        Search for a specific version of a package::

            conda search 'numpy>=1.12'

        Search for a package on a specific channel::

            conda search conda-forge::numpy
            conda search 'numpy[channel=conda-forge, subdir=osx-64]'
        """
    )

    p = sub_parsers.add_parser(
        "search",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    p.add_argument(
        "--envs",
        action="store_true",
        help="Search all of the current user's environments. If run as Administrator "
        "(on Windows) or UID 0 (on unix), search all known environments on the system.",
    )
    p.add_argument(
        "-i",
        "--info",
        action="store_true",
        help="Provide detailed information about each package.",
    )
    p.add_argument(
        "--subdir",
        "--platform",
        action="store",
        dest="subdir",
        help="Search the given subdir. Should be formatted like 'osx-64', 'linux-32', "
        "'win-64', and so on. The default is to search the current platform.",
        default=NULL,
    )
    p.add_argument(
        "--skip-flexible-search",
        action="store_true",
        help="Do not perform flexible search if initial search fails.",
    )
    p.add_argument(
        "match_spec",
        default="*",
        nargs="?",
        help=SUPPRESS,
    )
    p.add_argument(
        "--canonical",
        action="store_true",
        help=SUPPRESS,
    )
    p.add_argument(
        "-f",
        "--full-name",
        action="store_true",
        help=SUPPRESS,
    )
    p.add_argument(
        "--names-only",
        action="store_true",
        help=SUPPRESS,
    )
    add_parser_known(p)
    p.add_argument(
        "-o",
        "--outdated",
        action="store_true",
        help=SUPPRESS,
    )
    p.add_argument(
        "--spec",
        action="store_true",
        help=SUPPRESS,
    )
    p.add_argument(
        "--reverse-dependency",
        action="store_true",
        # help="Perform a reverse dependency search. Use 'conda search package --info' "
        #      "to see the dependencies of a package.",
        help=SUPPRESS,  # TODO: re-enable once we have --reverse-dependency working again
    )

    add_parser_channels(p)
    add_parser_networking(p)
    add_parser_json(p)
    p.set_defaults(func="conda.cli.main_search.execute")

    return p


def execute(args: Namespace, parser: ArgumentParser) -> int:
    """
    Implements `conda search` commands.

    `conda search <spec>` searches channels for packages.
    `conda search <spec> --envs` searches environments for packages.

    """
    from ..base.context import context
    from ..cli.common import stdout_json
    from ..common.io import Spinner
    from ..core.envs_manager import query_all_prefixes
    from ..core.index import calculate_channel_urls
    from ..core.subdir_data import SubdirData
    from ..models.match_spec import MatchSpec
    from ..models.records import PackageRecord
    from ..models.version import VersionOrder

    spec = MatchSpec(args.match_spec)
    if spec.get_exact_value("subdir"):
        subdirs = (spec.get_exact_value("subdir"),)
    else:
        subdirs = context.subdirs

    if args.envs:
        with Spinner(
            f"Searching environments for {spec}",
            not context.verbose and not context.quiet,
            context.json,
        ):
            prefix_matches = query_all_prefixes(spec)
            ordered_result = tuple(
                {
                    "location": prefix,
                    "package_records": tuple(
                        sorted(
                            (
                                PackageRecord.from_objects(prefix_rec)
                                for prefix_rec in prefix_recs
                            ),
                            key=lambda prec: prec._pkey,
                        )
                    ),
                }
                for prefix, prefix_recs in prefix_matches
            )
        if context.json:
            stdout_json(ordered_result)
        elif args.info:
            for pkg_group in ordered_result:
                for prec in pkg_group["package_records"]:
                    pretty_record(prec)
        else:
            builder = [
                "# %-13s %15s %15s  %-20s %-20s"
                % (
                    "Name",
                    "Version",
                    "Build",
                    "Channel",
                    "Location",
                )
            ]
            for pkg_group in ordered_result:
                for prec in pkg_group["package_records"]:
                    builder.append(
                        "%-15s %15s %15s  %-20s %-20s"
                        % (
                            prec.name,
                            prec.version,
                            prec.build,
                            prec.channel.name,
                            pkg_group["location"],
                        )
                    )
            print("\n".join(builder))
        return 0

    with Spinner(
        "Loading channels",
        not context.verbose and not context.quiet,
        context.json,
    ):
        spec_channel = spec.get_exact_value("channel")
        channel_urls = (spec_channel,) if spec_channel else context.channels

        matches = sorted(
            SubdirData.query_all(spec, channel_urls, subdirs),
            key=lambda rec: (rec.name, VersionOrder(rec.version), rec.build),
        )
    if not matches and not args.skip_flexible_search and spec.get_exact_value("name"):
        flex_spec = MatchSpec(spec, name=f"*{spec.name}*")
        if not context.json:
            print(f"No match found for: {spec}. Search: {flex_spec}")
        matches = sorted(
            SubdirData.query_all(flex_spec, channel_urls, subdirs),
            key=lambda rec: (rec.name, VersionOrder(rec.version), rec.build),
        )

    if not matches:
        channels_urls = tuple(
            calculate_channel_urls(
                channel_urls=context.channels,
                prepend=not args.override_channels,
                platform=subdirs[0],
                use_local=args.use_local,
            )
        )
        from ..exceptions import PackagesNotFoundError

        raise PackagesNotFoundError((str(spec),), channels_urls)

    if context.json:
        json_obj = defaultdict(list)
        for match in matches:
            json_obj[match.name].append(match)
        stdout_json(json_obj)

    elif args.info:
        for record in matches:
            pretty_record(record)

    else:
        builder = [
            "# %-18s %15s %15s  %-20s"
            % (
                "Name",
                "Version",
                "Build",
                "Channel",
            )
        ]
        for record in matches:
            builder.append(
                "%-20s %15s %15s  %-20s"
                % (
                    record.name,
                    record.version,
                    record.build,
                    record.channel.name,
                )
            )
        print("\n".join(builder))
    return 0


def pretty_record(record: PackageRecord) -> None:
    """
    Pretty prints a `PackageRecord`.

    :param record:  The `PackageRecord` object to print.
    """
    from ..common.io import dashlist
    from ..utils import human_bytes

    def push_line(display_name, attr_name):
        value = getattr(record, attr_name, None)
        if value is not None:
            builder.append("%-12s: %s" % (display_name, value))

    builder = []
    builder.append(record.name + " " + record.version + " " + record.build)
    builder.append("-" * len(builder[0]))

    push_line("file name", "fn")
    push_line("name", "name")
    push_line("version", "version")
    push_line("build", "build")
    push_line("build number", "build_number")
    size = getattr(record, "size", None)
    if size is not None:
        builder.append("%-12s: %s" % ("size", human_bytes(size)))
    push_line("license", "license")
    push_line("subdir", "subdir")
    push_line("url", "url")
    push_line("md5", "md5")
    if record.timestamp:
        date_str = datetime.fromtimestamp(record.timestamp, timezone.utc).strftime(
            "%Y-%m-%d %H:%M:%S %Z"
        )
        builder.append("%-12s: %s" % ("timestamp", date_str))
    if record.track_features:
        builder.append(
            "%-12s: %s" % ("track_features", dashlist(record.track_features))
        )
    if record.constrains:
        builder.append("%-12s: %s" % ("constraints", dashlist(record.constrains)))
    builder.append(
        "%-12s: %s"
        % ("dependencies", dashlist(record.depends) if record.depends else "[]")
    )
    builder.append("\n")
    print("\n".join(builder))


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Entry point for all conda-env subcommands."""

from __future__ import annotations

from argparse import ArgumentParser
from typing import TYPE_CHECKING

from ..deprecations import deprecated
from . import main_export

if TYPE_CHECKING:
    from argparse import Namespace, _SubParsersAction


def configure_parser(sub_parsers: _SubParsersAction | None, **kwargs) -> ArgumentParser:
    from . import (
        main_env_config,
        main_env_create,
        main_env_list,
        main_env_remove,
        main_env_update,
    )

    # This is a backport for the deprecated `conda_env`, see `conda_env.cli.main`
    if sub_parsers is None:
        deprecated.topic(
            "24.9",
            "25.3",
            topic="'conda_env'",
        )
        p = ArgumentParser()

    else:
        p = sub_parsers.add_parser(
            "env",
            **kwargs,
        )

    env_parsers = p.add_subparsers(
        metavar="command",
        dest="cmd",
    )
    main_env_config.configure_parser(env_parsers)
    main_env_create.configure_parser(env_parsers)
    main_export.configure_parser(env_parsers)
    main_env_list.configure_parser(env_parsers)
    main_env_remove.configure_parser(env_parsers)
    main_env_update.configure_parser(env_parsers)

    p.set_defaults(func="conda.cli.main_env.execute")
    return p


def execute(args: Namespace, parser: ArgumentParser) -> int:
    parser.parse_args(["env", "--help"])

    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Mock CLI implementation for `conda deactivate`.

A mock implementation of the deactivate shell command for better UX.
"""

from .. import CondaError


def configure_parser(sub_parsers):
    p = sub_parsers.add_parser(
        "deactivate",
        help="Deactivate the current active conda environment.",
    )
    p.set_defaults(func="conda.cli.main_mock_deactivate.execute")


def execute(args, parser):
    raise CondaError("Run 'conda init' before 'conda deactivate'")


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda run`.

Runs the provided command within the specified environment.
"""

import os
import sys
from argparse import REMAINDER, ArgumentParser, Namespace, _SubParsersAction
from logging import getLogger


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from ..common.constants import NULL
    from .actions import NullCountAction
    from .helpers import add_parser_prefix, add_parser_verbose

    summary = "Run an executable in a conda environment."
    description = summary
    epilog = dals(
        """
        Example::

        $ conda create -y -n my-python-env python=3
        $ conda run -n my-python-env python --version
        """
    )

    p = sub_parsers.add_parser(
        "run",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )

    add_parser_prefix(p)
    add_parser_verbose(p)

    p.add_argument(
        "--dev",
        action=NullCountAction,
        help="Sets `CONDA_EXE` to `python -m conda`, assuming the current "
        "working directory contains the root of conda development sources. "
        "This is mainly for use during tests where we test new conda sources "
        "against old Python versions.",
        dest="dev",
        default=NULL,
    )

    p.add_argument(
        "--debug-wrapper-scripts",
        action=NullCountAction,
        help="When this is set, where implemented, the shell wrapper scripts"
        "will use the echo command to print debugging information to "
        "stderr (standard error).",
        dest="debug_wrapper_scripts",
        default=NULL,
    )
    p.add_argument(
        "--cwd",
        help="Current working directory for command to run in. Defaults to "
        "the user's current working directory if no directory is specified.",
        default=os.getcwd(),
    )
    p.add_argument(
        "--no-capture-output",
        "--live-stream",
        action="store_true",
        help="Don't capture stdout/stderr (standard out/standard error).",
        default=False,
    )

    p.add_argument(
        "executable_call",
        nargs=REMAINDER,
        help="Executable name, with additional arguments to be passed to the executable "
        "on invocation.",
    )

    p.set_defaults(func="conda.cli.main_run.execute")

    return p


def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.context import context
    from ..common.compat import encode_environment
    from ..gateways.disk.delete import rm_rf
    from ..gateways.subprocess import subprocess_call
    from ..utils import wrap_subprocess_call
    from .common import validate_prefix

    # create run script
    script, command = wrap_subprocess_call(
        context.root_prefix,
        validate_prefix(context.target_prefix),  # ensure prefix exists
        args.dev,
        args.debug_wrapper_scripts,
        args.executable_call,
        use_system_tmp_path=True,
    )

    # run script
    response = subprocess_call(
        command,
        env=encode_environment(os.environ.copy()),
        path=args.cwd,
        raise_on_error=False,
        capture_output=not args.no_capture_output,
    )

    # display stdout/stderr if it was captured
    if not args.no_capture_output:
        if response.stdout:
            print(response.stdout, file=sys.stdout)
        if response.stderr:
            print(response.stderr, file=sys.stderr)

    # log error
    if response.rc != 0:
        log = getLogger(__name__)
        log.error(
            f"`conda run {' '.join(args.executable_call)}` failed. (See above for error)"
        )

    # remove script
    if "CONDA_TEST_SAVE_TEMPS" not in os.environ:
        rm_rf(script)
    else:
        log = getLogger(__name__)
        log.warning(f"CONDA_TEST_SAVE_TEMPS :: retaining main_run script {script}")

    return response.rc


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda-env remove`.

Removes the specified conda environment.
"""

from argparse import (
    ArgumentParser,
    Namespace,
    _SubParsersAction,
)


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .helpers import (
        add_output_and_prompt_options,
        add_parser_prefix,
        add_parser_solver,
    )

    summary = "Remove an environment."
    description = dals(
        f"""
        {summary}

        Removes a provided environment.  You must deactivate the existing
        environment before you can remove it.

        """
    )
    epilog = dals(
        """
        Examples::

            conda env remove --name FOO
            conda env remove -n FOO

        """
    )

    p = sub_parsers.add_parser(
        "remove",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )

    add_parser_prefix(p)
    add_parser_solver(p)
    add_output_and_prompt_options(p)

    p.set_defaults(func="conda.cli.main_env_remove.execute")

    return p


def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..base.context import context
    from ..cli.main_remove import execute as remove

    args = vars(args)
    args.update(
        {
            "all": True,
            "channel": None,
            "features": None,
            "override_channels": None,
            "use_local": None,
            "use_cache": None,
            "offline": None,
            "force": True,
            "pinned": None,
            "keep_env": False,
        }
    )
    args = Namespace(**args)

    context.__init__(argparse_args=args)

    remove(args, parser)

    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Conda package installation logic.

Core logic for `conda [create|install|update|remove]` commands.

See conda.cli.main_create, conda.cli.main_install, conda.cli.main_update, and
conda.cli.main_remove for the entry points into this module.
"""

import os
from logging import getLogger
from os.path import abspath, basename, exists, isdir, isfile, join
from pathlib import Path

from boltons.setutils import IndexedSet

from .. import CondaError
from ..auxlib.ish import dals
from ..base.constants import REPODATA_FN, ROOT_ENV_NAME, DepsModifier, UpdateModifier
from ..base.context import context, locate_prefix_by_name
from ..common.constants import NULL
from ..common.io import Spinner
from ..common.path import is_package_file, paths_equal
from ..core.index import (
    _supplement_index_with_prefix,
    calculate_channel_urls,
    get_index,
)
from ..core.link import PrefixSetup, UnlinkLinkTransaction
from ..core.prefix_data import PrefixData
from ..core.solve import diff_for_unlink_link_precs
from ..exceptions import (
    CondaExitZero,
    CondaImportError,
    CondaIndexError,
    CondaOSError,
    CondaSystemExit,
    CondaValueError,
    DirectoryNotACondaEnvironmentError,
    DirectoryNotFoundError,
    DryRunExit,
    EnvironmentLocationNotFound,
    NoBaseEnvironmentError,
    OperationNotAllowed,
    PackageNotInstalledError,
    PackagesNotFoundError,
    ResolvePackageNotFound,
    SpecsConfigurationConflictError,
    TooManyArgumentsError,
    UnsatisfiableError,
)
from ..gateways.disk.create import mkdir_p
from ..gateways.disk.delete import delete_trash, path_is_clean
from ..history import History
from ..misc import _get_best_prec_match, clone_env, explicit, touch_nonadmin
from ..models.match_spec import MatchSpec
from ..models.prefix_graph import PrefixGraph
from . import common
from .common import check_non_admin
from .main_config import set_keys

log = getLogger(__name__)
stderrlog = getLogger("conda.stderr")


def check_prefix(prefix, json=False):
    if os.pathsep in prefix:
        raise CondaValueError(
            f"Cannot create a conda environment with '{os.pathsep}' in the prefix. Aborting."
        )
    name = basename(prefix)
    error = None
    if name == ROOT_ENV_NAME:
        error = f"'{name}' is a reserved environment name"
    if exists(prefix):
        if isdir(prefix) and "conda-meta" not in tuple(
            entry.name for entry in os.scandir(prefix)
        ):
            return None
        error = f"prefix already exists: {prefix}"

    if error:
        raise CondaValueError(error, json)

    if " " in prefix:
        stderrlog.warning(
            "WARNING: A space was detected in your requested environment path:\n"
            f"'{prefix}'\n"
            "Spaces in paths can sometimes be problematic. To minimize issues,\n"
            "make sure you activate your environment before running any executables!\n"
        )


def clone(src_arg, dst_prefix, json=False, quiet=False, index_args=None):
    if os.sep in src_arg:
        src_prefix = abspath(src_arg)
        if not isdir(src_prefix):
            raise DirectoryNotFoundError(src_arg)
    else:
        src_prefix = locate_prefix_by_name(src_arg)

    if not json:
        print(f"Source:      {src_prefix}")
        print(f"Destination: {dst_prefix}")

    actions, untracked_files = clone_env(
        src_prefix, dst_prefix, verbose=not json, quiet=quiet, index_args=index_args
    )

    if json:
        common.stdout_json_success(
            actions=actions,
            untracked_files=list(untracked_files),
            src_prefix=src_prefix,
            dst_prefix=dst_prefix,
        )


def print_activate(env_name_or_prefix):  # pragma: no cover
    if not context.quiet and not context.json:
        if " " in env_name_or_prefix:
            env_name_or_prefix = f'"{env_name_or_prefix}"'
        message = dals(
            f"""
        #
        # To activate this environment, use
        #
        #     $ conda activate {env_name_or_prefix}
        #
        # To deactivate an active environment, use
        #
        #     $ conda deactivate
        """
        )
        print(message)  # TODO: use logger


def get_revision(arg, json=False):
    try:
        return int(arg)
    except ValueError:
        raise CondaValueError(f"expected revision number, not: '{arg}'", json)


def install(args, parser, command="install"):
    """Logic for `conda install`, `conda update`, and `conda create`."""
    context.validate_configuration()
    check_non_admin()
    # this is sort of a hack.  current_repodata.json may not have any .tar.bz2 files,
    #    because it deduplicates records that exist as both formats.  Forcing this to
    #    repodata.json ensures that .tar.bz2 files are available
    if context.use_only_tar_bz2:
        args.repodata_fns = ("repodata.json",)

    newenv = bool(command == "create")
    isupdate = bool(command == "update")
    isinstall = bool(command == "install")
    isremove = bool(command == "remove")
    prefix = context.target_prefix
    if context.force_32bit and prefix == context.root_prefix:
        raise CondaValueError("cannot use CONDA_FORCE_32BIT=1 in base env")
    if isupdate and not (
        args.file
        or args.packages
        or context.update_modifier == UpdateModifier.UPDATE_ALL
    ):
        raise CondaValueError(
            """no package names supplied
# Example: conda update -n myenv scipy
"""
        )

    if newenv:
        check_prefix(prefix, json=context.json)
        if context.subdir != context._native_subdir():
            # We will only allow a different subdir if it's specified by global
            # configuration, environment variable or command line argument. IOW,
            # prevent a non-base env configured for a non-native subdir from leaking
            # its subdir to a newer env.
            context_sources = context.collect_all()
            if context_sources.get("cmd_line", {}).get("subdir") == context.subdir:
                pass  # this is ok
            elif context_sources.get("envvars", {}).get("subdir") == context.subdir:
                pass  # this is ok too
            # config does not come from envvars or cmd_line, it must be a file
            # that's ok as long as it's a base env or a global file
            elif not paths_equal(context.active_prefix, context.root_prefix):
                # this is only ok as long as it's base environment
                active_env_config = next(
                    (
                        config
                        for path, config in context_sources.items()
                        if paths_equal(context.active_prefix, path.parent)
                    ),
                    None,
                )
                if active_env_config.get("subdir") == context.subdir:
                    # In practice this never happens; the subdir info is not even
                    # loaded from the active env for conda create :shrug:
                    msg = dals(
                        f"""
                        Active environment configuration ({context.active_prefix}) is
                        implicitly requesting a non-native platform ({context.subdir}).
                        Please deactivate first or explicitly request the platform via
                        the --platform=[value] command line flag.
                        """
                    )
                    raise OperationNotAllowed(msg)
            log.info(
                "Creating new environment for a non-native platform %s",
                context.subdir,
            )
    elif isdir(prefix):
        delete_trash(prefix)
        if not isfile(join(prefix, "conda-meta", "history")):
            if paths_equal(prefix, context.conda_prefix):
                raise NoBaseEnvironmentError()
            else:
                if not path_is_clean(prefix):
                    raise DirectoryNotACondaEnvironmentError(prefix)
        else:
            # fall-through expected under normal operation
            pass
    elif getattr(args, "mkdir", False):
        # --mkdir is deprecated and marked for removal in conda 25.3
        try:
            mkdir_p(prefix)
        except OSError as e:
            raise CondaOSError(f"Could not create directory: {prefix}", caused_by=e)
    else:
        raise EnvironmentLocationNotFound(prefix)

    args_packages = [s.strip("\"'") for s in args.packages]
    if newenv and not args.no_default_packages:
        # Override defaults if they are specified at the command line
        names = [MatchSpec(pkg).name for pkg in args_packages]
        for default_package in context.create_default_packages:
            if MatchSpec(default_package).name not in names:
                args_packages.append(default_package)

    index_args = {
        "use_cache": args.use_index_cache,
        "channel_urls": context.channels,
        "unknown": args.unknown,
        "prepend": not args.override_channels,
        "use_local": args.use_local,
    }

    num_cp = sum(is_package_file(s) for s in args_packages)
    if num_cp:
        if num_cp == len(args_packages):
            explicit(args_packages, prefix, verbose=not context.quiet)
            if newenv:
                touch_nonadmin(prefix)
                print_activate(args.name or prefix)
            return
        else:
            raise CondaValueError(
                "cannot mix specifications with conda package filenames"
            )

    specs = []
    if args.file:
        for fpath in args.file:
            try:
                specs.extend(common.specs_from_url(fpath, json=context.json))
            except UnicodeError:
                raise CondaError(
                    "Error reading file, file should be a text file containing"
                    " packages \nconda create --help for details"
                )
        if "@EXPLICIT" in specs:
            explicit(specs, prefix, verbose=not context.quiet, index_args=index_args)
            if newenv:
                touch_nonadmin(prefix)
                print_activate(args.name or prefix)
            return
    specs.extend(common.specs_from_args(args_packages, json=context.json))

    if isinstall and args.revision:
        get_revision(args.revision, json=context.json)
    elif isinstall and not (args.file or args_packages):
        raise CondaValueError(
            "too few arguments, must supply command line package specs or --file"
        )

    # for 'conda update', make sure the requested specs actually exist in the prefix
    # and that they are name-only specs
    if isupdate and context.update_modifier != UpdateModifier.UPDATE_ALL:
        prefix_data = PrefixData(prefix)
        for spec in specs:
            spec = MatchSpec(spec)
            if not spec.is_name_only_spec:
                raise CondaError(
                    f"Invalid spec for 'conda update': {spec}\n"
                    "Use 'conda install' instead."
                )
            if not prefix_data.get(spec.name, None):
                raise PackageNotInstalledError(prefix, spec.name)

    if newenv and args.clone:
        if args.packages:
            raise TooManyArgumentsError(
                0,
                len(args.packages),
                list(args.packages),
                "did not expect any arguments for --clone",
            )

        clone(
            args.clone,
            prefix,
            json=context.json,
            quiet=context.quiet,
            index_args=index_args,
        )
        touch_nonadmin(prefix)
        print_activate(args.name or prefix)
        return

    repodata_fns = args.repodata_fns
    if not repodata_fns:
        repodata_fns = list(context.repodata_fns)
    if REPODATA_FN not in repodata_fns:
        repodata_fns.append(REPODATA_FN)

    args_set_update_modifier = (
        hasattr(args, "update_modifier") and args.update_modifier != NULL
    )
    # This helps us differentiate between an update, the --freeze-installed option, and the retry
    # behavior in our initial fast frozen solve
    _should_retry_unfrozen = (
        not args_set_update_modifier
        or args.update_modifier
        not in (UpdateModifier.FREEZE_INSTALLED, UpdateModifier.UPDATE_SPECS)
    ) and not newenv

    for repodata_fn in repodata_fns:
        try:
            if isinstall and args.revision:
                with Spinner(
                    f"Collecting package metadata ({repodata_fn})",
                    not context.verbose and not context.quiet,
                    context.json,
                ):
                    index = get_index(
                        channel_urls=index_args["channel_urls"],
                        prepend=index_args["prepend"],
                        platform=None,
                        use_local=index_args["use_local"],
                        use_cache=index_args["use_cache"],
                        unknown=index_args["unknown"],
                        prefix=prefix,
                        repodata_fn=repodata_fn,
                    )
                revision_idx = get_revision(args.revision)
                with Spinner(
                    f"Reverting to revision {revision_idx}",
                    not context.verbose and not context.quiet,
                    context.json,
                ):
                    unlink_link_transaction = revert_actions(
                        prefix, revision_idx, index
                    )
            else:
                solver_backend = context.plugin_manager.get_cached_solver_backend()
                solver = solver_backend(
                    prefix,
                    context.channels,
                    context.subdirs,
                    specs_to_add=specs,
                    repodata_fn=repodata_fn,
                    command=args.cmd,
                )
                update_modifier = context.update_modifier
                if (isinstall or isremove) and args.update_modifier == NULL:
                    update_modifier = UpdateModifier.FREEZE_INSTALLED
                deps_modifier = context.deps_modifier
                if isupdate:
                    deps_modifier = context.deps_modifier or DepsModifier.UPDATE_SPECS

                unlink_link_transaction = solver.solve_for_transaction(
                    deps_modifier=deps_modifier,
                    update_modifier=update_modifier,
                    force_reinstall=context.force_reinstall or context.force,
                    should_retry_solve=(
                        _should_retry_unfrozen or repodata_fn != repodata_fns[-1]
                    ),
                )
            # we only need one of these to work.  If we haven't raised an exception,
            #   we're good.
            break

        except (ResolvePackageNotFound, PackagesNotFoundError) as e:
            if not getattr(e, "allow_retry", True):
                raise e  # see note in next except block
            # end of the line.  Raise the exception
            if repodata_fn == repodata_fns[-1]:
                # PackagesNotFoundError is the only exception type we want to raise.
                #    Over time, we should try to get rid of ResolvePackageNotFound
                if isinstance(e, PackagesNotFoundError):
                    raise e
                else:
                    channels_urls = tuple(
                        calculate_channel_urls(
                            channel_urls=index_args["channel_urls"],
                            prepend=index_args["prepend"],
                            platform=None,
                            use_local=index_args["use_local"],
                        )
                    )
                    # convert the ResolvePackageNotFound into PackagesNotFoundError
                    raise PackagesNotFoundError(e._formatted_chains, channels_urls)

        except (UnsatisfiableError, SystemExit, SpecsConfigurationConflictError) as e:
            if not getattr(e, "allow_retry", True):
                # TODO: This is a temporary workaround to allow downstream libraries
                # to inject this attribute set to False and skip the retry logic
                # Other solvers might implement their own internal retry logic without
                # depending --freeze-install implicitly like conda classic does. Example
                # retry loop in conda-libmamba-solver:
                # https://github.com/conda-incubator/conda-libmamba-solver/blob/da5b1ba/conda_libmamba_solver/solver.py#L254-L299
                # If we end up raising UnsatisfiableError, we annotate it with `allow_retry`
                # so we don't have go through all the repodatas and freeze-installed logic
                # unnecessarily (see https://github.com/conda/conda/issues/11294). see also:
                # https://github.com/conda-incubator/conda-libmamba-solver/blob/7c698209/conda_libmamba_solver/solver.py#L617
                raise e
            # Quick solve with frozen env or trimmed repodata failed.  Try again without that.
            if not hasattr(args, "update_modifier"):
                if repodata_fn == repodata_fns[-1]:
                    raise e
            elif _should_retry_unfrozen:
                try:
                    unlink_link_transaction = solver.solve_for_transaction(
                        deps_modifier=deps_modifier,
                        update_modifier=UpdateModifier.UPDATE_SPECS,
                        force_reinstall=context.force_reinstall or context.force,
                        should_retry_solve=(repodata_fn != repodata_fns[-1]),
                    )
                except (
                    UnsatisfiableError,
                    SystemExit,
                    SpecsConfigurationConflictError,
                ) as e:
                    # Unsatisfiable package specifications/no such revision/import error
                    if e.args and "could not import" in e.args[0]:
                        raise CondaImportError(str(e))
                    # we want to fall through without raising if we're not at the end of the list
                    #    of fns.  That way, we fall to the next fn.
                    if repodata_fn == repodata_fns[-1]:
                        raise e
            elif repodata_fn != repodata_fns[-1]:
                continue  # if we hit this, we should retry with next repodata source
            else:
                # end of the line.  Raise the exception
                # Unsatisfiable package specifications/no such revision/import error
                if e.args and "could not import" in e.args[0]:
                    raise CondaImportError(str(e))
                raise e
    handle_txn(unlink_link_transaction, prefix, args, newenv)


def revert_actions(prefix, revision=-1, index=None):
    # TODO: If revision raise a revision error, should always go back to a safe revision
    h = History(prefix)
    # TODO: need a History method to get user-requested specs for revision number
    #       Doing a revert right now messes up user-requested spec history.
    #       Either need to wipe out history after ``revision``, or add the correct
    #       history information to the new entry about to be created.
    # TODO: This is wrong!!!!!!!!!!
    user_requested_specs = h.get_requested_specs_map().values()
    try:
        target_state = {
            MatchSpec.from_dist_str(dist_str) for dist_str in h.get_state(revision)
        }
    except IndexError:
        raise CondaIndexError("no such revision: %d" % revision)

    _supplement_index_with_prefix(index, prefix)

    not_found_in_index_specs = set()
    link_precs = set()
    for spec in target_state:
        precs = tuple(prec for prec in index.values() if spec.match(prec))
        if not precs:
            not_found_in_index_specs.add(spec)
        elif len(precs) > 1:
            link_precs.add(_get_best_prec_match(precs))
        else:
            link_precs.add(precs[0])

    if not_found_in_index_specs:
        raise PackagesNotFoundError(not_found_in_index_specs)

    final_precs = IndexedSet(PrefixGraph(link_precs).graph)  # toposort
    unlink_precs, link_precs = diff_for_unlink_link_precs(prefix, final_precs)
    setup = PrefixSetup(prefix, unlink_precs, link_precs, (), user_requested_specs, ())
    return UnlinkLinkTransaction(setup)


def handle_txn(unlink_link_transaction, prefix, args, newenv, remove_op=False):
    if unlink_link_transaction.nothing_to_do:
        if remove_op:
            # No packages found to remove from environment
            raise PackagesNotFoundError(args.package_names)
        elif not newenv:
            if context.json:
                common.stdout_json_success(
                    message="All requested packages already installed."
                )
            else:
                print("\n# All requested packages already installed.\n")
            return

    if not context.json:
        unlink_link_transaction.print_transaction_summary()
        common.confirm_yn()

    elif context.dry_run:
        actions = unlink_link_transaction._make_legacy_action_groups()[0]
        common.stdout_json_success(prefix=prefix, actions=actions, dry_run=True)
        raise DryRunExit()

    try:
        unlink_link_transaction.download_and_extract()
        if context.download_only:
            raise CondaExitZero(
                "Package caches prepared. UnlinkLinkTransaction cancelled with "
                "--download-only option."
            )
        unlink_link_transaction.execute()

    except SystemExit as e:
        raise CondaSystemExit("Exiting", e)

    if newenv:
        touch_nonadmin(prefix)
        if context.subdir != context._native_subdir():
            set_keys(
                ("subdir", context.subdir),
                path=Path(prefix, ".condarc"),
            )
        print_activate(args.name or prefix)

    if context.json:
        actions = unlink_link_transaction._make_legacy_action_groups()[0]
        common.stdout_json_success(prefix=prefix, actions=actions)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Collection of custom argparse actions.
"""

from argparse import Action, _CountAction

from ..common.constants import NULL


class NullCountAction(_CountAction):
    @staticmethod
    def _ensure_value(namespace, name, value):
        if getattr(namespace, name, NULL) in (NULL, None):
            setattr(namespace, name, value)
        return getattr(namespace, name)

    def __call__(self, parser, namespace, values, option_string=None):
        new_count = self._ensure_value(namespace, self.dest, 0) + 1
        setattr(namespace, self.dest, new_count)


class ExtendConstAction(Action):
    """
    A derivative of _AppendConstAction and Python 3.8's _ExtendAction
    """

    def __init__(
        self,
        option_strings,
        dest,
        const,
        default=None,
        type=None,
        choices=None,
        required=False,
        help=None,
        metavar=None,
    ):
        super().__init__(
            option_strings=option_strings,
            dest=dest,
            nargs="*",
            const=const,
            default=default,
            type=type,
            choices=choices,
            required=required,
            help=help,
            metavar=metavar,
        )

    def __call__(self, parser, namespace, values, option_string=None):
        items = getattr(namespace, self.dest, None)
        items = [] if items is None else items[:]
        items.extend(values or [self.const])
        setattr(namespace, self.dest, items)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda rename`.

Renames an existing environment by cloning it and then removing the original environment.
"""

from __future__ import annotations

import os
from functools import partial
from pathlib import Path
from typing import TYPE_CHECKING

from ..deprecations import deprecated

if TYPE_CHECKING:
    from argparse import ArgumentParser, Namespace, _SubParsersAction


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .helpers import add_parser_prefix

    summary = "Rename an existing environment."
    description = dals(
        f"""
        {summary}

        This command renames a conda environment via its name (-n/--name) or
        its prefix (-p/--prefix).

        The base environment and the currently-active environment cannot be renamed.
        """
    )
    epilog = dals(
        """
        Examples::

            conda rename -n test123 test321

            conda rename --name test123 test321

            conda rename -p path/to/test123 test321

            conda rename --prefix path/to/test123 test321

        """
    )

    p = sub_parsers.add_parser(
        "rename",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    # Add name and prefix args
    add_parser_prefix(p)

    p.add_argument("destination", help="New name for the conda environment.")
    # TODO: deprecate --force in favor of --yes
    p.add_argument(
        "--force",
        help="Force rename of an environment.",
        action="store_true",
        default=False,
    )
    p.add_argument(
        "-d",
        "--dry-run",
        help="Only display what would have been done by the current command, arguments, "
        "and other flags.",
        action="store_true",
        default=False,
    )
    p.set_defaults(func="conda.cli.main_rename.execute")

    return p


@deprecated.argument("24.3", "24.9", "name")
@deprecated.argument("24.3", "24.9", "prefix")
def validate_src() -> str:
    """
    Validate that we are receiving at least one valid value for --name or
    --prefix and ensure that the "base" environment is not being renamed
    """
    from ..base.context import context
    from ..exceptions import CondaEnvException

    prefix = Path(context.target_prefix)
    if not prefix.exists():
        raise CondaEnvException(
            "The environment you are trying to rename does not exist."
        )
    if prefix.samefile(context.root_prefix):
        raise CondaEnvException("The 'base' environment cannot be renamed")
    if context.active_prefix and prefix.samefile(context.active_prefix):
        raise CondaEnvException("Cannot rename the active environment")

    return context.target_prefix


def validate_destination(dest: str, force: bool = False) -> str:
    """Ensure that our destination does not exist"""
    from ..base.context import context, validate_prefix_name
    from ..common.path import expand
    from ..exceptions import CondaEnvException

    if os.sep in dest:
        dest = expand(dest)
    else:
        dest = validate_prefix_name(dest, ctx=context, allow_base=False)

    if not force and os.path.exists(dest):
        env_name = os.path.basename(os.path.normpath(dest))
        raise CondaEnvException(
            f"The environment '{env_name}' already exists. Override with --force."
        )
    return dest


def execute(args: Namespace, parser: ArgumentParser) -> int:
    """Executes the command for renaming an existing environment."""
    from ..base.constants import DRY_RUN_PREFIX
    from ..base.context import context
    from ..cli import install
    from ..gateways.disk.delete import rm_rf
    from ..gateways.disk.update import rename_context

    source = validate_src()
    destination = validate_destination(args.destination, force=args.force)

    def clone_and_remove() -> None:
        actions: tuple[partial, ...] = (
            partial(
                install.clone,
                source,
                destination,
                quiet=context.quiet,
                json=context.json,
            ),
            partial(rm_rf, source),
        )

        # We now either run collected actions or print dry run statement
        for func in actions:
            if args.dry_run:
                print(f"{DRY_RUN_PREFIX} {func.func.__name__} {','.join(func.args)}")
            else:
                func()

    if args.force:
        with rename_context(destination, dry_run=args.dry_run):
            clone_and_remove()
    else:
        clone_and_remove()
    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda-env update`.

Updates the conda environments with the specified packages.
"""

import os
from argparse import (
    ArgumentParser,
    Namespace,
    _SubParsersAction,
)

from .. import CondaError
from ..notices import notices


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .helpers import (
        add_parser_json,
        add_parser_prefix,
        add_parser_solver,
    )

    summary = "Update the current environment based on environment file."
    description = summary
    epilog = dals(
        """
        Examples::

            conda env update
            conda env update -n=foo
            conda env update -f=/path/to/environment.yml
            conda env update --name=foo --file=environment.yml
            conda env update vader/deathstar

        """
    )

    p = sub_parsers.add_parser(
        "update",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    add_parser_prefix(p)
    p.add_argument(
        "-f",
        "--file",
        action="store",
        help="environment definition (default: environment.yml)",
        default="environment.yml",
    )
    p.add_argument(
        "--prune",
        action="store_true",
        default=False,
        help="remove installed packages not defined in environment.yml",
    )
    p.add_argument(
        "remote_definition",
        help="remote environment definition / IPython notebook",
        action="store",
        default=None,
        nargs="?",
    )
    add_parser_json(p)
    add_parser_solver(p)
    p.set_defaults(func="conda.cli.main_env_update.execute")

    return p


@notices
def execute(args: Namespace, parser: ArgumentParser) -> int:
    from ..auxlib.ish import dals
    from ..base.context import context, determine_target_prefix
    from ..core.prefix_data import PrefixData
    from ..env import specs as install_specs
    from ..env.env import get_filename, print_result
    from ..env.installers.base import get_installer
    from ..exceptions import CondaEnvException, InvalidInstaller
    from ..misc import touch_nonadmin

    spec = install_specs.detect(
        name=args.name,
        filename=get_filename(args.file),
        directory=os.getcwd(),
        remote_definition=args.remote_definition,
    )
    env = spec.environment

    if not (args.name or args.prefix):
        if not env.name:
            # Note, this is a hack fofr get_prefix that assumes argparse results
            # TODO Refactor common.get_prefix
            name = os.environ.get("CONDA_DEFAULT_ENV", False)
            if not name:
                msg = "Unable to determine environment\n\n"
                instuctions = dals(
                    """
                    Please re-run this command with one of the following options:

                    * Provide an environment name via --name or -n
                    * Re-run this command inside an activated conda environment.
                    """
                )
                msg += instuctions
                # TODO Add json support
                raise CondaEnvException(msg)

        # Note: stubbing out the args object as all of the
        # conda.cli.common code thinks that name will always
        # be specified.
        args.name = env.name

    prefix = determine_target_prefix(context, args)
    # CAN'T Check with this function since it assumes we will create prefix.
    # cli_install.check_prefix(prefix, json=args.json)

    # TODO, add capability
    # common.ensure_override_channels_requires_channel(args)
    # channel_urls = args.channel or ()

    # create installers before running any of them
    # to avoid failure to import after the file being deleted
    # e.g. due to conda_env being upgraded or Python version switched.
    installers = {}

    for installer_type in env.dependencies:
        try:
            installers[installer_type] = get_installer(installer_type)
        except InvalidInstaller:
            raise CondaError(
                dals(
                    f"""
                    Unable to install package for {0}.

                    Please double check and ensure you dependencies file has
                    the correct spelling.  You might also try installing the
                    conda-env-{0} package to see if provides the required
                    installer.
                    """
                )
            )

            return -1

    result = {"conda": None, "pip": None}
    for installer_type, specs in env.dependencies.items():
        installer = installers[installer_type]
        result[installer_type] = installer.install(prefix, specs, args, env)

    if env.variables:
        pd = PrefixData(prefix)
        pd.set_environment_env_vars(env.variables)

    touch_nonadmin(prefix)
    print_result(args, prefix, result)

    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda-env list`, now aliased to `conda info --envs`.

Lists available conda environments.
"""

from argparse import ArgumentParser, Namespace, _SubParsersAction

from conda.deprecations import deprecated


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .helpers import add_parser_json

    summary = "An alias for `conda info --envs`. Lists all conda environments."
    description = summary
    epilog = dals(
        """
        Examples::

            conda env list
            conda env list --json

        """
    )
    p = sub_parsers.add_parser(
        "list",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )

    add_parser_json(p)

    p.set_defaults(
        func="conda.cli.main_info.execute",
        # The following are the necessary default args for the `conda info` command
        envs=True,
        base=False,
        unsafe_channels=False,
        system=False,
        all=False,
    )

    return p


@deprecated("24.9", "25.3", addendum="Use `conda.cli.main_info.execute` instead.")
def execute(args: Namespace, parser: ArgumentParser):
    from conda.cli.main_info import execute as execute_info

    execute_info(args, parser)

    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Common utilities for conda command line tools."""

import re
import sys
from logging import getLogger
from os.path import basename, dirname, isdir, isfile, join, normcase

from ..auxlib.ish import dals
from ..base.constants import ROOT_ENV_NAME
from ..base.context import context, env_name
from ..common.constants import NULL
from ..common.io import swallow_broken_pipe
from ..common.path import paths_equal
from ..common.serialize import json_dump
from ..exceptions import (
    CondaError,
    DirectoryNotACondaEnvironmentError,
    EnvironmentLocationNotFound,
)
from ..models.match_spec import MatchSpec


def confirm(message="Proceed", choices=("yes", "no"), default="yes", dry_run=NULL):
    assert default in choices, default
    if (dry_run is NULL and context.dry_run) or dry_run:
        from ..exceptions import DryRunExit

        raise DryRunExit()

    options = []
    for option in choices:
        if option == default:
            options.append(f"[{option[0]}]")
        else:
            options.append(option[0])
    message = "{} ({})? ".format(message, "/".join(options))
    choices = {alt: choice for choice in choices for alt in [choice, choice[0]]}
    choices[""] = default
    while True:
        # raw_input has a bug and prints to stderr, not desirable
        sys.stdout.write(message)
        sys.stdout.flush()
        try:
            user_choice = sys.stdin.readline().strip().lower()
        except OSError as e:
            raise CondaError(f"cannot read from stdin: {e}")
        if user_choice not in choices:
            print(f"Invalid choice: {user_choice}")
        else:
            sys.stdout.write("\n")
            sys.stdout.flush()
            return choices[user_choice]


def confirm_yn(message="Proceed", default="yes", dry_run=NULL):
    if (dry_run is NULL and context.dry_run) or dry_run:
        from ..exceptions import DryRunExit

        raise DryRunExit()
    if context.always_yes:
        return True
    try:
        choice = confirm(
            message=message, choices=("yes", "no"), default=default, dry_run=dry_run
        )
    except KeyboardInterrupt:  # pragma: no cover
        from ..exceptions import CondaSystemExit

        raise CondaSystemExit("\nOperation aborted.  Exiting.")
    if choice == "no":
        from ..exceptions import CondaSystemExit

        raise CondaSystemExit("Exiting.")
    return True


def is_active_prefix(prefix: str) -> bool:
    """
    Determines whether the args we pass in are pointing to the active prefix.
    Can be used a validation step to make sure operations are not being
    performed on the active prefix.
    """
    if context.active_prefix is None:
        return False
    return (
        paths_equal(prefix, context.active_prefix)
        # normcasing our prefix check for Windows, for case insensitivity
        or normcase(prefix) == normcase(env_name(context.active_prefix))
    )


def arg2spec(arg, json=False, update=False):
    try:
        spec = MatchSpec(arg)
    except:
        from ..exceptions import CondaValueError

        raise CondaValueError(f"invalid package specification: {arg}")

    name = spec.name
    if not spec._is_simple() and update:
        from ..exceptions import CondaValueError

        raise CondaValueError(
            "version specifications not allowed with 'update'; use\n"
            f"    conda update  {name:<{len(arg)}}  or\n"
            f"    conda install {arg:<{len(name)}}"
        )

    return str(spec)


def specs_from_args(args, json=False):
    return [arg2spec(arg, json=json) for arg in args]


spec_pat = re.compile(
    r"""
    (?P<name>[^=<>!\s]+)                # package name
    \s*                                 # ignore spaces
    (
        (?P<cc>=[^=]+(=[^=]+)?)         # conda constraint
        |
        (?P<pc>(?:[=!]=|[><]=?|~=).+)   # new pip-style constraints
    )?$
    """,
    re.VERBOSE,
)


def strip_comment(line):
    return line.split("#")[0].rstrip()


def spec_from_line(line):
    m = spec_pat.match(strip_comment(line))
    if m is None:
        return None
    name, cc, pc = (m.group("name").lower(), m.group("cc"), m.group("pc"))
    if cc:
        return name + cc.replace("=", " ")
    elif pc:
        if pc.startswith("~= "):
            assert (
                pc.count("~=") == 1
            ), f"Overly complex 'Compatible release' spec not handled {line}"
            assert pc.count("."), f"No '.' in 'Compatible release' version {line}"
            ver = pc.replace("~= ", "")
            ver2 = ".".join(ver.split(".")[:-1]) + ".*"
            return name + " >=" + ver + ",==" + ver2
        else:
            return name + " " + pc.replace(" ", "")
    else:
        return name


def specs_from_url(url, json=False):
    from ..gateways.connection.download import TmpDownload

    explicit = False
    with TmpDownload(url, verbose=False) as path:
        specs = []
        try:
            for line in open(path):
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                if line == "@EXPLICIT":
                    explicit = True
                if explicit:
                    specs.append(line)
                    continue
                spec = spec_from_line(line)
                if spec is None:
                    from ..exceptions import CondaValueError

                    raise CondaValueError(f"could not parse '{line}' in: {url}")
                specs.append(spec)
        except OSError as e:
            from ..exceptions import CondaFileIOError

            raise CondaFileIOError(path, e)
    return specs


def names_in_specs(names, specs):
    return any(spec.split()[0] in names for spec in specs)


def disp_features(features):
    if features:
        return "[{}]".format(" ".join(features))
    else:
        return ""


@swallow_broken_pipe
def stdout_json(d):
    getLogger("conda.stdout").info(json_dump(d))


def stdout_json_success(success=True, **kwargs):
    result = {"success": success}
    actions = kwargs.pop("actions", None)
    if actions:
        if "LINK" in actions:
            actions["LINK"] = [prec.dist_fields_dump() for prec in actions["LINK"]]
        if "UNLINK" in actions:
            actions["UNLINK"] = [prec.dist_fields_dump() for prec in actions["UNLINK"]]
        result["actions"] = actions
    result.update(kwargs)
    stdout_json(result)


def print_envs_list(known_conda_prefixes, output=True):
    if output:
        print("# conda environments:")
        print("#")

    def disp_env(prefix):
        fmt = "%-20s  %s  %s"
        active = "*" if prefix == context.active_prefix else " "
        if prefix == context.root_prefix:
            name = ROOT_ENV_NAME
        elif any(
            paths_equal(envs_dir, dirname(prefix)) for envs_dir in context.envs_dirs
        ):
            name = basename(prefix)
        else:
            name = ""
        if output:
            print(fmt % (name, active, prefix))

    for prefix in known_conda_prefixes:
        disp_env(prefix)

    if output:
        print()


def check_non_admin():
    from ..common._os import is_admin

    if not context.non_admin_enabled and not is_admin():
        from ..exceptions import OperationNotAllowed

        raise OperationNotAllowed(
            dals(
                """
            The create, install, update, and remove operations have been disabled
            on your system for non-privileged users.
        """
            )
        )


def validate_prefix(prefix):
    """Verifies the prefix is a valid conda environment.

    :raises EnvironmentLocationNotFound: Non-existent path or not a directory.
    :raises DirectoryNotACondaEnvironmentError: Directory is not a conda environment.
    :returns: Valid prefix.
    :rtype: str
    """
    if isdir(prefix):
        if not isfile(join(prefix, "conda-meta", "history")):
            raise DirectoryNotACondaEnvironmentError(prefix)
    else:
        raise EnvironmentLocationNotFound(prefix)

    return prefix


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
from .main import main  # NOQA


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""CLI implementation for `conda notices`.

Manually retrieves channel notifications, caches them and displays them.
"""

from argparse import ArgumentParser, Namespace, _SubParsersAction


def configure_parser(sub_parsers: _SubParsersAction, **kwargs) -> ArgumentParser:
    from ..auxlib.ish import dals
    from .helpers import add_parser_channels, add_parser_json

    summary = "Retrieve latest channel notifications."
    description = dals(
        f"""
        {summary}

        Conda channel maintainers have the option of setting messages that
        users will see intermittently. Some of these notices are informational
        while others are messages concerning the stability of the channel.

        """
    )
    epilog = dals(
        """
        Examples::

            conda notices

            conda notices -c defaults

        """
    )

    p = sub_parsers.add_parser(
        "notices",
        help=summary,
        description=description,
        epilog=epilog,
        **kwargs,
    )
    add_parser_channels(p)
    add_parser_json(p)

    p.set_defaults(func="conda.cli.main_notices.execute")

    return p


def execute(args: Namespace, parser: ArgumentParser) -> int:
    """Command that retrieves channel notifications, caches them and displays them."""
    from ..exceptions import CondaError
    from ..notices import core as notices

    try:
        channel_notice_set = notices.retrieve_notices()
    except OSError as exc:
        raise CondaError(f"Unable to retrieve notices: {exc}")

    notices.display_notices(channel_notice_set)

    return 0


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""DEPRECATED: Use `conda.cli.main_export` instead.

CLI implementation for `conda-env export`.

Dumps specified environment package specifications to the screen.
"""

# Import from conda.cli.main_export since this module is deprecated.
from conda.cli.main_export import configure_parser, execute  # noqa
from conda.deprecations import deprecated

deprecated.module("24.9", "25.3", addendum="Use `conda.cli.main_export` instead.")


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Anaconda-client (binstar) token management for CondaSession."""

import os
import re
from logging import getLogger
from os.path import isdir, isfile, join
from stat import S_IREAD, S_IWRITE

try:
    from platformdirs import user_config_dir
except ImportError:  # pragma: no cover
    from .._vendor.appdirs import user_data_dir as user_config_dir

from ..common.url import quote_plus, unquote_plus
from ..deprecations import deprecated
from .disk.delete import rm_rf

log = getLogger(__name__)


def replace_first_api_with_conda(url):
    # replace first occurrence of 'api' with 'conda' in url
    return re.sub(r"([./])api([./]|$)", r"\1conda\2", url, count=1)


@deprecated("24.3", "24.9", addendum="Use `platformdirs` instead.")
class EnvAppDirs:
    def __init__(self, appname, appauthor, root_path):
        self.appname = appname
        self.appauthor = appauthor
        self.root_path = root_path

    @property
    def user_data_dir(self):
        return join(self.root_path, "data")

    @property
    def site_data_dir(self):
        return join(self.root_path, "data")

    @property
    def user_cache_dir(self):
        return join(self.root_path, "cache")

    @property
    def user_log_dir(self):
        return join(self.root_path, "log")


def _get_binstar_token_directory():
    if "BINSTAR_CONFIG_DIR" in os.environ:
        return os.path.join(os.environ["BINSTAR_CONFIG_DIR"], "data")
    else:
        return user_config_dir(appname="binstar", appauthor="ContinuumIO")


def read_binstar_tokens():
    tokens = {}
    token_dir = _get_binstar_token_directory()
    if not isdir(token_dir):
        return tokens

    for tkn_entry in os.scandir(token_dir):
        if tkn_entry.name[-6:] != ".token":
            continue
        url = re.sub(r"\.token$", "", unquote_plus(tkn_entry.name))
        with open(tkn_entry.path) as f:
            token = f.read()
        tokens[url] = tokens[replace_first_api_with_conda(url)] = token
    return tokens


def set_binstar_token(url, token):
    token_dir = _get_binstar_token_directory()
    if not isdir(token_dir):
        os.makedirs(token_dir)

    tokenfile = join(token_dir, f"{quote_plus(url)}.token")

    if isfile(tokenfile):
        os.unlink(tokenfile)
    with open(tokenfile, "w") as fd:
        fd.write(token)
    os.chmod(tokenfile, S_IWRITE | S_IREAD)


def remove_binstar_token(url):
    token_dir = _get_binstar_token_directory()
    tokenfile = join(token_dir, f"{quote_plus(url)}.token")
    rm_rf(tokenfile)


if __name__ == "__main__":
    print(read_binstar_tokens())


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Configure logging for conda."""

import logging
import re
import sys
from datetime import datetime
from functools import lru_cache, partial
from logging import (
    DEBUG,
    ERROR,
    INFO,
    WARN,
    Filter,
    Formatter,
    StreamHandler,
    getLogger,
)

from .. import CondaError
from ..common.constants import TRACE
from ..common.io import _FORMATTER, attach_stderr_handler
from ..deprecations import deprecated

log = getLogger(__name__)

_VERBOSITY_LEVELS = {
    0: WARN,  # standard output
    1: WARN,  # -v, detailed output
    2: INFO,  # -vv, info logging
    3: DEBUG,  # -vvv, debug logging
    4: TRACE,  # -vvvv, trace logging
}
deprecated.constant("24.3", "24.9", "VERBOSITY_LEVELS", _VERBOSITY_LEVELS)


class TokenURLFilter(Filter):
    TOKEN_URL_PATTERN = re.compile(
        r"(|https?://)"  # \1  scheme
        r"(|\s"  # \2  space, or
        r"|(?:(?:\d{1,3}\.){3}\d{1,3})"  # ipv4, or
        r"|(?:"  # domain name
        r"(?:[a-zA-Z0-9-]{1,20}\.){0,10}"  # non-tld
        r"(?:[a-zA-Z]{2}[a-zA-Z0-9-]{0,18})"  # tld
        r"))"  # end domain name
        r"(|:\d{1,5})?"  # \3  port
        r"/t/[a-z0-9A-Z-]+/"  # token
    )
    TOKEN_REPLACE = partial(TOKEN_URL_PATTERN.sub, r"\1\2\3/t/<TOKEN>/")

    def filter(self, record):
        """
        Since Python 2's getMessage() is incapable of handling any
        strings that are not unicode when it interpolates the message
        with the arguments, we fix that here by doing it ourselves.

        At the same time we replace tokens in the arguments which was
        not happening until now.
        """
        if not isinstance(record.msg, str):
            # This should always be the case but it's not checked so
            # we avoid any potential logging errors.
            return True
        if record.args:
            record.msg = record.msg % record.args
            record.args = None
        record.msg = self.TOKEN_REPLACE(record.msg)
        return True


class StdStreamHandler(StreamHandler):
    """Log StreamHandler that always writes to the current sys stream."""

    terminator = "\n"

    def __init__(self, sys_stream):
        """
        Args:
            sys_stream: stream name, either "stdout" or "stderr" (attribute of module sys)
        """
        super().__init__(getattr(sys, sys_stream))
        self.sys_stream = sys_stream
        del self.stream

    def __getattr__(self, attr):
        # always get current sys.stdout/sys.stderr, unless self.stream has been set explicitly
        if attr == "stream":
            return getattr(sys, self.sys_stream)
        return super().__getattribute__(attr)

    """
    def emit(self, record):
        # in contrast to the Python 2.7 StreamHandler, this has no special Unicode handling;
        # however, this backports the Python >=3.2 terminator attribute and additionally makes it
        # further customizable by giving record an identically named attribute, e.g., via
        # logger.log(..., extra={"terminator": ""}) or LoggerAdapter(logger, {"terminator": ""}).
        try:
            msg = self.format(record)
            terminator = getattr(record, "terminator", self.terminator)
            stream = self.stream
            stream.write(msg)
            stream.write(terminator)
            self.flush()
        except Exception:
            self.handleError(record)

    """

    # Updated Python 2.7.15's stdlib, with terminator and unicode support.
    def emit(self, record):
        """
        Emit a record.

        If a formatter is specified, it is used to format the record.
        The record is then written to the stream with a trailing newline.  If
        exception information is present, it is formatted using
        traceback.print_exception and appended to the stream.  If the stream
        has an 'encoding' attribute, it is used to determine how to do the
        output to the stream.
        """
        try:
            msg = self.format(record)
            stream = self.stream
            fs = "%s"
            stream.write(fs % msg)
            terminator = getattr(record, "terminator", self.terminator)
            stream.write(terminator)
            self.flush()
        # How does conda handle Ctrl-C? Find out..
        # except (KeyboardInterrupt, SystemExit):
        #     raise
        except Exception:
            self.handleError(record)


# Don't use initialize_logging/initialize_root_logger/set_conda_log_level in
# cli.python_api! There we want the user to have control over their logging,
# e.g., using their own levels, handlers, formatters and propagation settings.


@lru_cache(maxsize=None)
def initialize_logging():
    # 'conda' gets level WARN and does not propagate to root.
    getLogger("conda").setLevel(WARN)
    set_conda_log_level()
    initialize_std_loggers()


def initialize_std_loggers():
    # Set up special loggers 'conda.stdout'/'conda.stderr' which output directly to the
    # corresponding sys streams, filter token urls and don't propagate.
    formatter = Formatter("%(message)s")

    for stream in ("stdout", "stderr"):
        logger = getLogger(f"conda.{stream}")
        logger.handlers = []
        logger.setLevel(INFO)
        handler = StdStreamHandler(stream)
        handler.setLevel(INFO)
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.addFilter(TokenURLFilter())
        logger.propagate = False

        stdlog_logger = getLogger(f"conda.{stream}log")
        stdlog_logger.handlers = []
        stdlog_logger.setLevel(DEBUG)
        stdlog_handler = StdStreamHandler(stream)
        stdlog_handler.terminator = ""
        stdlog_handler.setLevel(DEBUG)
        stdlog_handler.setFormatter(formatter)
        stdlog_logger.addHandler(stdlog_handler)
        stdlog_logger.propagate = False

    verbose_logger = getLogger("conda.stdout.verbose")
    verbose_logger.handlers = []
    verbose_logger.setLevel(INFO)
    verbose_handler = StdStreamHandler("stdout")
    verbose_handler.setLevel(INFO)
    verbose_handler.setFormatter(formatter)
    verbose_handler.addFilter(TokenURLFilter())
    verbose_logger.addHandler(verbose_handler)
    verbose_logger.propagate = False


@deprecated("25.3", "25.9", addendum="Unused.")
def initialize_root_logger(level=ERROR):
    attach_stderr_handler(level=level, filters=[TokenURLFilter()])


def set_conda_log_level(level=WARN):
    attach_stderr_handler(level=level, logger_name="conda", filters=[TokenURLFilter()])


def set_all_logger_level(level=DEBUG):
    formatter = Formatter("%(message)s\n") if level >= INFO else None
    attach_stderr_handler(level, formatter=formatter, filters=[TokenURLFilter()])
    set_conda_log_level(level)
    # 'requests' loggers get their own handlers so that they always output messages in long format
    # regardless of the level.
    attach_stderr_handler(level, "requests", filters=[TokenURLFilter()])
    attach_stderr_handler(
        level, "requests.packages.urllib3", filters=[TokenURLFilter()]
    )


@lru_cache(maxsize=None)
def set_file_logging(logger_name=None, level=DEBUG, path=None):
    if path is None:
        timestamp = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
        path = f".conda.{timestamp}.log"

    conda_logger = getLogger(logger_name)
    handler = logging.FileHandler(path)
    handler.setFormatter(_FORMATTER)
    handler.setLevel(level)
    conda_logger.addHandler(handler)


@deprecated(
    "24.3",
    "24.9",
    addendum="Use `conda.gateways.logging.set_log_level` instead.",
)
def set_verbosity(verbosity: int):
    try:
        set_log_level(_VERBOSITY_LEVELS[verbosity])
    except KeyError:
        raise CondaError(f"Invalid verbosity level: {verbosity}") from None


def set_log_level(log_level: int):
    set_all_logger_level(log_level)
    log.debug("log_level set to %d", log_level)


@deprecated(
    "24.9",
    "25.3",
    addendum="Use `logging.getLogger(__name__)(conda.common.constants.TRACE, ...)` instead.",
)
def trace(self, message, *args, **kwargs):
    if self.isEnabledFor(TRACE):
        self._log(TRACE, message, args, **kwargs)


logging.addLevelName(TRACE, "TRACE")
logging.Logger.trace = trace  # type: ignore[attr-defined]


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Gateways isolate interaction of conda code with the outside world.  Disk manipulation,
database interaction, and remote requests should all be through various gateways.  Functions
and methods in ``conda.gateways`` must use ``conda.models`` for arguments and return values.

Conda modules importable from ``conda.gateways`` are

- ``conda._vendor``
- ``conda.common``
- ``conda.models``
- ``conda.gateways``

Conda modules off limits for import within ``conda.gateways`` are

- ``conda.api``
- ``conda.cli``
- ``conda.client``
- ``conda.core``

Conda modules strictly prohibited from importing ``conda.gateways`` are

- ``conda.api``
- ``conda.cli``
- ``conda.client``

"""


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Helpler functions for subprocess."""

from __future__ import annotations

import os
import sys
from collections import namedtuple
from logging import getLogger
from os.path import abspath
from subprocess import PIPE, CalledProcessError, Popen
from typing import TYPE_CHECKING

from .. import ACTIVE_SUBPROCESSES
from ..auxlib.compat import shlex_split_unicode
from ..auxlib.ish import dals
from ..base.context import context
from ..common.compat import encode_environment, isiterable
from ..common.constants import TRACE
from ..gateways.disk.delete import rm_rf
from ..utils import wrap_subprocess_call

if TYPE_CHECKING:
    from pathlib import Path
    from typing import Sequence

log = getLogger(__name__)
Response = namedtuple("Response", ("stdout", "stderr", "rc"))


def _format_output(command_str, cwd, rc, stdout, stderr):
    return dals(
        """
    $ %s
    ==> cwd: %s <==
    ==> exit code: %d <==
    ==> stdout <==
    %s
    ==> stderr <==
    %s
    """
    ) % (command_str, cwd, rc, stdout, stderr)


def any_subprocess(args, prefix, env=None, cwd=None):
    script_caller, command_args = wrap_subprocess_call(
        context.root_prefix,
        prefix,
        context.dev,
        context.debug,
        args,
    )
    process = Popen(
        command_args,
        cwd=cwd or prefix,
        universal_newlines=False,
        stdout=PIPE,
        stderr=PIPE,
        env=env,
    )
    stdout, stderr = process.communicate()
    if script_caller is not None:
        if "CONDA_TEST_SAVE_TEMPS" not in os.environ:
            rm_rf(script_caller)
        else:
            log.warning(
                f"CONDA_TEST_SAVE_TEMPS :: retaining pip run_script {script_caller}"
            )
    if hasattr(stdout, "decode"):
        stdout = stdout.decode("utf-8", errors="replace")
    if hasattr(stderr, "decode"):
        stderr = stderr.decode("utf-8", errors="replace")
    return stdout, stderr, process.returncode


def subprocess_call(
    command: str | os.PathLike | Path | Sequence[str | os.PathLike | Path],
    env: dict[str, str] | None = None,
    path: str | os.PathLike | Path | None = None,
    stdin: str | None = None,
    raise_on_error: bool = True,
    capture_output: bool = True,
):
    """This utility function should be preferred for all conda subprocessing.
    It handles multiple tricky details.
    """
    env = encode_environment(env or os.environ)
    cwd = sys.prefix if path is None else abspath(path)
    if not isiterable(command):
        command = shlex_split_unicode(command)
    try:
        command_str = os.fspath(command)
    except TypeError:
        # TypeError: command is not a str or PathLike
        command_str = " ".join(map(os.fspath, command))
    log.debug("executing>> %s", command_str)

    pipe = None
    if capture_output:
        pipe = PIPE
    elif stdin:
        raise ValueError("When passing stdin, output needs to be captured")
    else:
        stdin = None

    # spawn subprocess
    process = Popen(
        command,
        cwd=cwd,
        stdin=pipe,
        stdout=pipe,
        stderr=pipe,
        env=env,
        text=True,  # open streams in text mode so that we don't have to decode
        errors="replace",
    )
    ACTIVE_SUBPROCESSES.add(process)

    # decode output, if not PIPE, stdout/stderr will be None
    stdout, stderr = process.communicate(input=stdin)
    rc = process.returncode
    ACTIVE_SUBPROCESSES.remove(process)

    if (raise_on_error and rc != 0) or log.isEnabledFor(TRACE):
        formatted_output = _format_output(command_str, cwd, rc, stdout, stderr)
    if raise_on_error and rc != 0:
        log.info(formatted_output)
        raise CalledProcessError(rc, command, output=formatted_output)
    if log.isEnabledFor(TRACE):
        log.log(TRACE, formatted_output)

    return Response(stdout, stderr, int(rc))


def _subprocess_clean_env(env, clean_python=True, clean_conda=True):
    dels = []
    if clean_python:
        dels.extend(("PYTHONPATH", "PYTHONHOME"))
    if clean_conda:
        dels.extend(
            ("CONDA_ROOT", "CONDA_PROMPT_MODIFIER", "CONDA_EXE", "CONDA_DEFAULT_ENV")
        )
    for key in dels:
        if key in env:
            del env[key]


def subprocess_call_with_clean_env(
    command,
    path=None,
    stdin=None,
    raise_on_error=True,
    clean_python=True,
    clean_conda=True,
):
    # Any of these env vars are likely to mess the whole thing up.
    # This has been seen to be the case with PYTHONPATH.
    env = os.environ.copy()
    _subprocess_clean_env(env, clean_python, clean_conda)
    # env['CONDA_DLL_SEARCH_MODIFICATION_ENABLE'] = '1'
    return subprocess_call(
        command, env=env, path=path, stdin=stdin, raise_on_error=raise_on_error
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Requests session configured with all accepted scheme adapters."""

from __future__ import annotations

from fnmatch import fnmatch
from functools import lru_cache
from logging import getLogger
from threading import local

from ... import CondaError
from ...auxlib.ish import dals
from ...base.constants import CONDA_HOMEPAGE_URL
from ...base.context import context
from ...common.url import (
    add_username_and_password,
    get_proxy_username_and_pass,
    split_anaconda_token,
    urlparse,
)
from ...exceptions import ProxyError
from ...models.channel import Channel
from ..anaconda_client import read_binstar_tokens
from . import (
    AuthBase,
    BaseAdapter,
    Retry,
    Session,
    _basic_auth_str,
    extract_cookies_to_jar,
    get_auth_from_url,
    get_netrc_auth,
)
from .adapters.ftp import FTPAdapter
from .adapters.http import HTTPAdapter
from .adapters.localfs import LocalFSAdapter
from .adapters.s3 import S3Adapter

log = getLogger(__name__)
RETRIES = 3


CONDA_SESSION_SCHEMES = frozenset(
    (
        "http",
        "https",
        "ftp",
        "s3",
        "file",
    )
)


class EnforceUnusedAdapter(BaseAdapter):
    def send(self, request, *args, **kwargs):
        message = dals(
            f"""
        EnforceUnusedAdapter called with url {request.url}
        This command is using a remote connection in offline mode.
        """
        )
        raise RuntimeError(message)

    def close(self):
        raise NotImplementedError()


def get_channel_name_from_url(url: str) -> str | None:
    """
    Given a URL, determine the channel it belongs to and return its name.
    """
    return Channel.from_url(url).canonical_name


@lru_cache(maxsize=None)
def get_session(url: str):
    """
    Function that determines the correct Session object to be returned
    based on the URL that is passed in.
    """
    channel_name = get_channel_name_from_url(url)

    # If for whatever reason a channel name can't be determined, (should be unlikely)
    # we just return the default session object.
    if channel_name is None:
        return CondaSession()

    # We ensure here if there are duplicates defined, we choose the last one
    channel_settings = {}
    for settings in context.channel_settings:
        channel = settings.get("channel", "")
        if channel == channel_name:
            # First we check for exact match
            channel_settings = settings
            continue

        # If we don't have an exact match, we attempt to match a URL pattern
        parsed_url = urlparse(url)
        parsed_setting = urlparse(channel)

        # We require that the schemes must be identical to prevent downgrade attacks.
        # This includes the case of a scheme-less pattern like "*", which is not allowed.
        if parsed_setting.scheme != parsed_url.scheme:
            continue

        url_without_schema = parsed_url.netloc + parsed_url.path
        pattern = parsed_setting.netloc + parsed_setting.path
        if fnmatch(url_without_schema, pattern):
            channel_settings = settings

    auth_handler = channel_settings.get("auth", "").strip() or None

    # Return default session object
    if auth_handler is None:
        return CondaSession()

    auth_handler_cls = context.plugin_manager.get_auth_handler(auth_handler)

    if not auth_handler_cls:
        return CondaSession()

    return CondaSession(auth=auth_handler_cls(channel_name))


def get_session_storage_key(auth) -> str:
    """
    Function that determines which storage key to use for our CondaSession object caching
    """
    if auth is None:
        return "default"

    if isinstance(auth, tuple):
        return hash(auth)

    auth_type = type(auth)

    return f"{auth_type.__module__}.{auth_type.__qualname__}::{auth.channel_name}"


class CondaSessionType(type):
    """
    Takes advice from https://github.com/requests/requests/issues/1871#issuecomment-33327847
    and creates one Session instance per thread.
    """

    def __new__(mcs, name, bases, dct):
        dct["_thread_local"] = local()
        return super().__new__(mcs, name, bases, dct)

    def __call__(cls, **kwargs):
        storage_key = get_session_storage_key(kwargs.get("auth"))

        try:
            return cls._thread_local.sessions[storage_key]
        except AttributeError:
            session = super().__call__(**kwargs)
            cls._thread_local.sessions = {storage_key: session}
        except KeyError:
            session = cls._thread_local.sessions[storage_key] = super().__call__(
                **kwargs
            )

        return session


class CondaSession(Session, metaclass=CondaSessionType):
    def __init__(self, auth: AuthBase | tuple[str, str] | None = None):
        """
        :param auth: Optionally provide ``requests.AuthBase`` compliant objects
        """
        super().__init__()

        self.auth = auth or CondaHttpAuth()

        self.proxies.update(context.proxy_servers)

        ssl_context = None
        if context.ssl_verify == "truststore":
            try:
                import ssl

                import truststore

                ssl_context = truststore.SSLContext(ssl.PROTOCOL_TLS_CLIENT)
            except ImportError:
                raise CondaError(
                    "The `ssl_verify: truststore` setting is only supported on"
                    "Python 3.10 or later."
                )
            self.verify = True
        else:
            self.verify = context.ssl_verify

        if context.offline:
            unused_adapter = EnforceUnusedAdapter()
            self.mount("http://", unused_adapter)
            self.mount("https://", unused_adapter)
            self.mount("ftp://", unused_adapter)
            self.mount("s3://", unused_adapter)

        else:
            # Configure retries
            retry = Retry(
                total=context.remote_max_retries,
                backoff_factor=context.remote_backoff_factor,
                status_forcelist=[413, 429, 500, 503],
                raise_on_status=False,
                respect_retry_after_header=False,
            )
            http_adapter = HTTPAdapter(max_retries=retry, ssl_context=ssl_context)
            self.mount("http://", http_adapter)
            self.mount("https://", http_adapter)
            self.mount("ftp://", FTPAdapter())
            self.mount("s3://", S3Adapter())

        self.mount("file://", LocalFSAdapter())

        self.headers["User-Agent"] = context.user_agent

        if context.client_ssl_cert_key:
            self.cert = (context.client_ssl_cert, context.client_ssl_cert_key)
        elif context.client_ssl_cert:
            self.cert = context.client_ssl_cert

    @classmethod
    def cache_clear(cls):
        try:
            cls._thread_local.sessions.clear()
        except AttributeError:
            # AttributeError: thread's session cache has not been initialized
            pass


class CondaHttpAuth(AuthBase):
    # TODO: make this class thread-safe by adding some of the requests.auth.HTTPDigestAuth() code

    def __call__(self, request):
        request.url = CondaHttpAuth.add_binstar_token(request.url)
        self._apply_basic_auth(request)
        request.register_hook("response", self.handle_407)
        return request

    @staticmethod
    def _apply_basic_auth(request):
        # this logic duplicated from Session.prepare_request and PreparedRequest.prepare_auth
        url_auth = get_auth_from_url(request.url)
        auth = url_auth if any(url_auth) else None

        if auth is None:
            # look for auth information in a .netrc file
            auth = get_netrc_auth(request.url)

        if isinstance(auth, tuple) and len(auth) == 2:
            request.headers["Authorization"] = _basic_auth_str(*auth)

        return request

    @staticmethod
    def add_binstar_token(url):
        clean_url, token = split_anaconda_token(url)
        if not token and context.add_anaconda_token:
            for binstar_url, token in read_binstar_tokens().items():
                if clean_url.startswith(binstar_url):
                    log.debug("Adding anaconda token for url <%s>", clean_url)
                    from ...models.channel import Channel

                    channel = Channel(clean_url)
                    channel.token = token
                    return channel.url(with_credentials=True)
        return url

    @staticmethod
    def handle_407(response, **kwargs):  # pragma: no cover
        """
        Prompts the user for the proxy username and password and modifies the
        proxy in the session object to include it.

        This method is modeled after
          * requests.auth.HTTPDigestAuth.handle_401()
          * requests.auth.HTTPProxyAuth
          * the previous conda.fetch.handle_proxy_407()

        It both adds 'username:password' to the proxy URL, as well as adding a
        'Proxy-Authorization' header.  If any of this is incorrect, please file an issue.

        """
        # kwargs = {'verify': True, 'cert': None, 'proxies': {}, 'stream': False,
        #           'timeout': (3.05, 60)}

        if response.status_code != 407:
            return response

        # Consume content and release the original connection
        # to allow our new request to reuse the same one.
        response.content
        response.close()

        proxies = kwargs.pop("proxies")

        proxy_scheme = urlparse(response.url).scheme
        if proxy_scheme not in proxies:
            raise ProxyError(
                dals(
                    f"""
            Could not find a proxy for {proxy_scheme!r}. See
            {CONDA_HOMEPAGE_URL}/docs/html#configure-conda-for-use-behind-a-proxy-server
            for more information on how to configure proxies.
            """
                )
            )

        # fix-up proxy_url with username & password
        proxy_url = proxies[proxy_scheme]
        username, password = get_proxy_username_and_pass(proxy_scheme)
        proxy_url = add_username_and_password(proxy_url, username, password)
        proxy_authorization_header = _basic_auth_str(username, password)
        proxies[proxy_scheme] = proxy_url
        kwargs["proxies"] = proxies

        prep = response.request.copy()
        extract_cookies_to_jar(prep._cookies, response.request, response.raw)
        prep.prepare_cookies(prep._cookies)
        prep.headers["Proxy-Authorization"] = proxy_authorization_header

        _response = response.connection.send(prep, **kwargs)
        _response.history.append(response)
        _response.request = prep

        return _response


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Download logic for conda indices and packages."""

from __future__ import annotations

import hashlib
import os
import tempfile
import warnings
from contextlib import contextmanager
from logging import DEBUG, getLogger
from os.path import basename, exists, join
from pathlib import Path

from ... import CondaError
from ...auxlib.ish import dals
from ...auxlib.logz import stringify
from ...base.context import context
from ...common.io import time_recorder
from ...exceptions import (
    BasicClobberError,
    ChecksumMismatchError,
    CondaDependencyError,
    CondaHTTPError,
    CondaSSLError,
    CondaValueError,
    ProxyError,
    maybe_raise,
)
from ..disk.delete import rm_rf
from ..disk.lock import lock
from . import (
    ConnectionError,
    HTTPError,
    InsecureRequestWarning,
    InvalidSchema,
    RequestsProxyError,
    SSLError,
)
from .session import get_session

log = getLogger(__name__)


CHUNK_SIZE = 1 << 14


def disable_ssl_verify_warning():
    warnings.simplefilter("ignore", InsecureRequestWarning)


@time_recorder("download")
def download(
    url,
    target_full_path,
    md5=None,
    sha256=None,
    size=None,
    progress_update_callback=None,
):
    if exists(target_full_path):
        maybe_raise(BasicClobberError(target_full_path, url, context), context)
    if not context.ssl_verify:
        disable_ssl_verify_warning()

    with download_http_errors(url):
        download_inner(
            url, target_full_path, md5, sha256, size, progress_update_callback
        )


def download_inner(url, target_full_path, md5, sha256, size, progress_update_callback):
    timeout = context.remote_connect_timeout_secs, context.remote_read_timeout_secs
    session = get_session(url)

    partial = False
    if size and (md5 or sha256):
        partial = True

    streamed_bytes = 0
    size_builder = 0

    # Use `.partial` even for full downloads. Avoid creating incomplete files
    # with the final filename.
    with download_partial_file(
        target_full_path, url=url, md5=md5, sha256=sha256, size=size
    ) as target:
        stat_result = os.fstat(target.fileno())
        if size is not None and stat_result.st_size >= size:
            return  # moves partial onto target_path, checksum will be checked

        headers = {}
        if partial and stat_result.st_size > 0:
            headers = {"Range": f"bytes={stat_result.st_size}-"}

        resp = session.get(
            url, stream=True, headers=headers, proxies=session.proxies, timeout=timeout
        )
        if log.isEnabledFor(DEBUG):
            log.debug(stringify(resp, content_max_len=256))
        resp.raise_for_status()

        # Reset file if we think we're downloading partial content but the
        # server doesn't respond with 206 Partial Content
        if partial and resp.status_code != 206:
            target.seek(0)
            target.truncate()

        content_length = total_content_length = int(
            resp.headers.get("Content-Length", 0)
        )
        if partial and headers:
            # Get total content length, not the range we are currently fetching.
            # ex. Content-Range: bytes 200-1000/67589
            content_range = resp.headers.get("Content-Range", "bytes 0-0/0")
            try:
                total_content_length = int(
                    content_range.split(" ", 1)[1].rsplit("/")[-1]
                )
            except (LookupError, ValueError):
                pass

        for chunk in resp.iter_content(chunk_size=CHUNK_SIZE):
            # chunk could be the decompressed form of the real data
            # but we want the exact number of bytes read till now
            streamed_bytes = resp.raw.tell()
            try:
                target.write(chunk)
            except OSError as e:
                message = "Failed to write to %(target_path)s\n  errno: %(errno)d"
                raise CondaError(message, target_path=target.name, errno=e.errno)
            size_builder += len(chunk)

            if total_content_length and 0 <= streamed_bytes <= content_length:
                if progress_update_callback:
                    progress_update_callback(
                        (stat_result.st_size + streamed_bytes) / total_content_length
                    )

        if content_length and streamed_bytes != content_length:
            # TODO: needs to be a more-specific error type
            message = dals(
                """
            Downloaded bytes did not match Content-Length
                url: %(url)s
                target_path: %(target_path)s
                Content-Length: %(content_length)d
                downloaded bytes: %(downloaded_bytes)d
            """
            )
            raise CondaError(
                message,
                url=url,
                target_path=target_full_path,
                content_length=content_length,
                downloaded_bytes=streamed_bytes,
            )
    # exit context manager, renaming target to target_full_path


@contextmanager
def download_partial_file(
    target_full_path: str | Path, *, url: str, sha256: str, md5: str, size: int
):
    """
    Create or open locked partial download file, moving onto target_full_path
    when finished. Preserve partial file on exception.
    """
    target_full_path = Path(target_full_path)
    parent = target_full_path.parent
    name = Path(target_full_path).name
    partial_name = f"{name}.partial"
    partial_path = parent / partial_name

    def check(target):
        target.seek(0)
        if md5 or sha256:
            checksum_type = "sha256" if sha256 else "md5"
            checksum = sha256 if sha256 else md5
            try:
                checksum_bytes = bytes.fromhex(checksum)
            except (ValueError, TypeError) as exc:
                raise CondaValueError(exc) from exc
            hasher = hashlib.new(checksum_type)
            target.seek(0)
            while read := target.read(CHUNK_SIZE):
                hasher.update(read)

            if hasher.digest() != checksum_bytes:
                actual_checksum = hasher.hexdigest()
                log.debug(
                    "%s mismatch for download: %s (%s != %s)",
                    checksum_type,
                    url,
                    actual_checksum,
                    checksum,
                )
                raise ChecksumMismatchError(
                    url, target_full_path, checksum_type, checksum, actual_checksum
                )
        if size is not None:
            actual_size = os.fstat(target.fileno()).st_size
            if actual_size != size:
                log.debug(
                    "size mismatch for download: %s (%s != %s)",
                    url,
                    actual_size,
                    size,
                )
                raise ChecksumMismatchError(
                    url, target_full_path, "size", size, actual_size
                )

    try:
        with partial_path.open(mode="a+b") as partial, lock(partial):
            yield partial
            check(partial)
    except HTTPError as e:  # before conda error handler wrapper
        # Don't keep `.partial` for errors like 404 not found, or 'Range not
        # Satisfiable' that will never succeed
        try:
            status_code = e.response.status_code
        except AttributeError:
            status_code = None
        if isinstance(status_code, int) and 400 <= status_code < 500:
            partial_path.unlink()
        raise
    except ChecksumMismatchError:
        partial_path.unlink()
        raise

    try:
        partial_path.rename(target_full_path)
    except OSError:  # Windows doesn't rename onto existing paths
        target_full_path.unlink()
        partial_path.rename(target_full_path)


@contextmanager
def download_http_errors(url: str):
    """Exception translator used inside download()"""
    # This complex exception translation strategy is reminiscent of def
    # conda_http_errors(url, repodata_fn): in gateways/repodata

    try:
        yield

    except ConnectionResetError as e:
        log.debug(f"{e}, trying again")
        # where does retry happen?
        raise

    except RequestsProxyError:
        raise ProxyError()  # see #3962

    except InvalidSchema as e:
        if "SOCKS" in str(e):
            message = dals(
                """
                Requests has identified that your current working environment is configured
                to use a SOCKS proxy, but pysocks is not installed.  To proceed, remove your
                proxy configuration, run `conda install pysocks`, and then you can re-enable
                your proxy configuration.
                """
            )
            raise CondaDependencyError(message)
        else:
            raise

    except SSLError as e:
        # SSLError: either an invalid certificate or OpenSSL is unavailable
        try:
            import ssl  # noqa: F401
        except ImportError:
            raise CondaSSLError(
                dals(
                    f"""
                    OpenSSL appears to be unavailable on this machine. OpenSSL is required to
                    download and install packages.

                    Exception: {e}
                    """
                )
            )
        else:
            raise CondaSSLError(
                dals(
                    f"""
                    Encountered an SSL error. Most likely a certificate verification issue.

                    Exception: {e}
                    """
                )
            )

    except (ConnectionError, HTTPError) as e:
        help_message = dals(
            """
        An HTTP error occurred when trying to retrieve this URL.
        HTTP errors are often intermittent, and a simple retry will get you on your way.
        """
        )
        raise CondaHTTPError(
            help_message,
            url,
            getattr(e.response, "status_code", None),
            getattr(e.response, "reason", None),
            getattr(e.response, "elapsed", None),
            e.response,
            caused_by=e,
        )


def download_text(url):
    if not context.ssl_verify:
        disable_ssl_verify_warning()
    with download_http_errors(url):
        timeout = context.remote_connect_timeout_secs, context.remote_read_timeout_secs
        session = get_session(url)
        response = session.get(
            url, stream=True, proxies=session.proxies, timeout=timeout
        )
        if log.isEnabledFor(DEBUG):
            log.debug(stringify(response, content_max_len=256))
        response.raise_for_status()
    return response.text


class TmpDownload:
    """Context manager to handle downloads to a tempfile."""

    def __init__(self, url, verbose=True):
        self.url = url
        self.verbose = verbose

    def __enter__(self):
        if "://" not in self.url:
            # if we provide the file itself, no tmp dir is created
            self.tmp_dir = None
            return self.url
        else:
            self.tmp_dir = tempfile.mkdtemp()
            dst = join(self.tmp_dir, basename(self.url))
            download(self.url, dst)
            return dst

    def __exit__(self, exc_type, exc_value, traceback):
        if self.tmp_dir:
            rm_rf(self.tmp_dir)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
from requests import ConnectionError, HTTPError, Session  # noqa: F401
from requests.adapters import DEFAULT_POOLBLOCK, BaseAdapter, HTTPAdapter  # noqa: F401
from requests.auth import AuthBase, _basic_auth_str  # noqa: F401
from requests.cookies import extract_cookies_to_jar  # noqa: F401
from requests.exceptions import (  # noqa: F401
    ChunkedEncodingError,
    InvalidSchema,
    SSLError,
)
from requests.exceptions import ProxyError as RequestsProxyError  # noqa: F401
from requests.hooks import dispatch_hook  # noqa: F401
from requests.models import PreparedRequest, Response  # noqa: F401
from requests.packages.urllib3.exceptions import InsecureRequestWarning  # noqa: F401
from requests.packages.urllib3.util.retry import Retry  # noqa: F401
from requests.structures import CaseInsensitiveDict  # noqa: F401
from requests.utils import get_auth_from_url, get_netrc_auth  # noqa: F401


# Copyright (C) 2012 Cory Benfield
# SPDX-License-Identifier: Apache-2.0
"""Defines FTP transport adapter for CondaSession (requests.Session).

Taken from requests-ftp (https://github.com/Lukasa/requests-ftp/blob/master/requests_ftp/ftp.py).

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import ftplib
import os
from base64 import b64decode
from io import BytesIO, StringIO
from logging import getLogger

from ....common.url import urlparse
from ....deprecations import deprecated
from ....exceptions import AuthenticationError
from .. import BaseAdapter, Response, dispatch_hook

log = getLogger(__name__)


# After: https://stackoverflow.com/a/44073062/3257826
#   And: https://stackoverflow.com/a/35368154/3257826
_old_makepasv = ftplib.FTP.makepasv


def _new_makepasv(self):
    host, port = _old_makepasv(self)
    host = self.sock.getpeername()[0]
    return host, port


ftplib.FTP.makepasv = _new_makepasv


class FTPAdapter(BaseAdapter):
    """A Requests Transport Adapter that handles FTP urls."""

    def __init__(self):
        super().__init__()

        # Build a dictionary keyed off the methods we support in upper case.
        # The values of this dictionary should be the functions we use to
        # send the specific queries.
        self.func_table = {
            "LIST": self.list,
            "RETR": self.retr,
            "STOR": self.stor,
            "NLST": self.nlst,
            "GET": self.retr,
        }

    def send(self, request, **kwargs):
        """Sends a PreparedRequest object over FTP. Returns a response object."""
        # Get the authentication from the prepared request, if any.
        auth = self.get_username_password_from_header(request)

        # Next, get the host and the path.
        host, port, path = self.get_host_and_path_from_url(request)

        # Sort out the timeout.
        timeout = kwargs.get("timeout", None)
        if not isinstance(timeout, int):
            # https://github.com/conda/conda/pull/3392
            timeout = 10

        # Establish the connection and login if needed.
        self.conn = ftplib.FTP()
        self.conn.connect(host, port, timeout)

        if auth is not None:
            self.conn.login(auth[0], auth[1])
        else:
            self.conn.login()

        # Get the method and attempt to find the function to call.
        resp = self.func_table[request.method](path, request)

        # Return the response.
        return resp

    def close(self):
        """Dispose of any internal state."""
        # Currently this is a no-op.
        pass

    def list(self, path, request):
        """Executes the FTP LIST command on the given path."""
        data = StringIO()

        # To ensure the StringIO gets cleaned up, we need to alias its close
        # method to the release_conn() method. This is a dirty hack, but there
        # you go.
        data.release_conn = data.close

        self.conn.cwd(path)
        code = self.conn.retrbinary("LIST", data_callback_factory(data))

        # When that call has finished executing, we'll have all our data.
        response = build_text_response(request, data, code)

        # Close the connection.
        self.conn.close()

        return response

    def retr(self, path, request):
        """Executes the FTP RETR command on the given path."""
        data = BytesIO()

        # To ensure the BytesIO gets cleaned up, we need to alias its close
        # method. See self.list().
        data.release_conn = data.close

        code = self.conn.retrbinary("RETR " + path, data_callback_factory(data))

        response = build_binary_response(request, data, code)

        # Close the connection.
        self.conn.close()

        return response

    @deprecated("24.3", "24.9")
    def stor(self, path, request):
        """Executes the FTP STOR command on the given path."""
        # First, get the file handle. We assume (bravely)
        # that there is only one file to be sent to a given URL. We also
        # assume that the filename is sent as part of the URL, not as part of
        # the files argument. Both of these assumptions are rarely correct,
        # but they are easy.
        data = parse_multipart_files(request)

        # Split into the path and the filename.
        path, filename = os.path.split(path)

        # Switch directories and upload the data.
        self.conn.cwd(path)
        code = self.conn.storbinary("STOR " + filename, data)

        # Close the connection and build the response.
        self.conn.close()

        response = build_binary_response(request, BytesIO(), code)

        return response

    def nlst(self, path, request):
        """Executes the FTP NLST command on the given path."""
        data = StringIO()

        # Alias the close method.
        data.release_conn = data.close

        self.conn.cwd(path)
        code = self.conn.retrbinary("NLST", data_callback_factory(data))

        # When that call has finished executing, we'll have all our data.
        response = build_text_response(request, data, code)

        # Close the connection.
        self.conn.close()

        return response

    def get_username_password_from_header(self, request):
        """Given a PreparedRequest object, reverse the process of adding HTTP
        Basic auth to obtain the username and password. Allows the FTP adapter
        to piggyback on the basic auth notation without changing the control
        flow.
        """
        auth_header = request.headers.get("Authorization")

        if auth_header:
            # The basic auth header is of the form 'Basic xyz'. We want the
            # second part. Check that we have the right kind of auth though.
            encoded_components = auth_header.split()[:2]
            if encoded_components[0] != "Basic":
                raise AuthenticationError("Invalid form of Authentication used.")
            else:
                encoded = encoded_components[1]

            # Decode the base64 encoded string.
            decoded = b64decode(encoded)

            # The string is of the form 'username:password'. Split on the
            # colon.
            components = decoded.split(":")
            username = components[0]
            password = components[1]
            return (username, password)
        else:
            # No auth header. Return None.
            return None

    def get_host_and_path_from_url(self, request):
        """Given a PreparedRequest object, split the URL in such a manner as to
        determine the host and the path. This is a separate method to wrap some
        of urlparse's craziness.
        """
        url = request.url
        parsed = urlparse(url)
        path = parsed.path

        # If there is a slash on the front of the path, chuck it.
        if path[0] == "/":
            path = path[1:]

        host = parsed.hostname
        port = parsed.port or 0

        return (host, port, path)


def data_callback_factory(variable):
    """Returns a callback suitable for use by the FTP library. This callback
    will repeatedly save data into the variable provided to this function. This
    variable should be a file-like structure.
    """

    def callback(data):
        variable.write(data)

    return callback


def build_text_response(request, data, code):
    """Build a response for textual data."""
    return build_response(request, data, code, "ascii")


def build_binary_response(request, data, code):
    """Build a response for data whose encoding is unknown."""
    return build_response(request, data, code, None)


def build_response(request, data, code, encoding):
    """Builds a response object from the data returned by ftplib, using the
    specified encoding.
    """
    response = Response()

    response.encoding = encoding

    # Fill in some useful fields.
    response.raw = data
    response.url = request.url
    response.request = request
    response.status_code = get_status_code_from_code_response(code)

    # Make sure to seek the file-like raw object back to the start.
    response.raw.seek(0)

    # Run the response hook.
    response = dispatch_hook("response", request.hooks, response)
    return response


@deprecated("24.3", "24.9")
def parse_multipart_files(request):
    """Given a prepared request, return a file-like object containing the
    original data. This is pretty hacky.
    """
    import cgi

    # Start by grabbing the pdict.
    _, pdict = cgi.parse_header(request.headers["Content-Type"])

    # Now, wrap the multipart data in a BytesIO buffer. This is annoying.
    buf = BytesIO()
    buf.write(request.body)
    buf.seek(0)

    # Parse the data. Simply take the first file.
    data = cgi.parse_multipart(buf, pdict)
    _, filedata = data.popitem()
    buf.close()

    # Get a BytesIO now, and write the file into it.
    buf = BytesIO()
    buf.write("".join(filedata))
    buf.seek(0)

    return buf


def get_status_code_from_code_response(code):
    r"""Handle complicated code response, even multi-lines.

    We get the status code in two ways:
    - extracting the code from the last valid line in the response
    - getting it from the 3 first digits in the code
    After a comparison between the two values,
    we can safely set the code or raise a warning.
    Examples:
        - get_status_code_from_code_response('200 Welcome') == 200
        - multi_line_code = '226-File successfully transferred\n226 0.000 seconds'
          get_status_code_from_code_response(multi_line_code) == 226
        - multi_line_with_code_conflicts = '200-File successfully transferred\n226 0.000 seconds'
          get_status_code_from_code_response(multi_line_with_code_conflicts) == 226
    For more detail see RFC 959, page 36, on multi-line responses:
        https://www.ietf.org/rfc/rfc959.txt
        "Thus the format for multi-line replies is that the first line
         will begin with the exact required reply code, followed
         immediately by a Hyphen, "-" (also known as Minus), followed by
         text.  The last line will begin with the same code, followed
         immediately by Space <SP>, optionally some text, and the Telnet
         end-of-line code."
    """
    last_valid_line_from_code = [line for line in code.split("\n") if line][-1]
    status_code_from_last_line = int(last_valid_line_from_code.split()[0])
    status_code_from_first_digits = int(code[:3])
    if status_code_from_last_line != status_code_from_first_digits:
        log.warning(
            "FTP response status code seems to be inconsistent.\n"
            "Code received: %s, extracted: %s and %s",
            code,
            status_code_from_last_line,
            status_code_from_first_digits,
        )
    return status_code_from_last_line


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
# Copyright (c) 2008-2023 The pip developers
# SPDX-License-Identifier: MIT
#
"""Defines HTTP transport adapter for CondaSession (requests.Session).

Closely derived from pip:

https://github.com/pypa/pip/blob/8c24fd2a80bad21aa29aec02fb48bd89a1e8f5e1/src/pip/_internal/network/session.py#L254

Under the MIT license:

Copyright (c) 2008-2023 The pip developers (see AUTHORS.txt file on the pip repository)

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"""

from typing import TYPE_CHECKING, Any, Optional

from .. import DEFAULT_POOLBLOCK
from .. import HTTPAdapter as BaseHTTPAdapter

if TYPE_CHECKING:
    from ssl import SSLContext

    from urllib3 import PoolManager


class _SSLContextAdapterMixin:
    """Mixin to add the ``ssl_context`` constructor argument to HTTP adapters.

    The additional argument is forwarded directly to the pool manager. This allows us
    to dynamically decide what SSL store to use at runtime, which is used to implement
    the optional ``truststore`` backend.
    """

    def __init__(
        self,
        *,
        ssl_context: Optional["SSLContext"] = None,
        **kwargs: Any,
    ) -> None:
        self._ssl_context = ssl_context
        super().__init__(**kwargs)

    def init_poolmanager(
        self,
        connections: int,
        maxsize: int,
        block: bool = DEFAULT_POOLBLOCK,
        **pool_kwargs: Any,
    ) -> "PoolManager":
        if self._ssl_context is not None:
            pool_kwargs.setdefault("ssl_context", self._ssl_context)
        return super().init_poolmanager(  # type: ignore[misc]
            connections=connections,
            maxsize=maxsize,
            block=block,
            **pool_kwargs,
        )


class HTTPAdapter(_SSLContextAdapterMixin, BaseHTTPAdapter):
    pass


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Defines local filesystem transport adapter for CondaSession (requests.Session)."""

import json
from email.utils import formatdate
from logging import getLogger
from mimetypes import guess_type
from os import stat
from tempfile import SpooledTemporaryFile

from ....common.compat import ensure_binary
from ....common.path import url_to_path
from .. import BaseAdapter, CaseInsensitiveDict, Response

log = getLogger(__name__)


class LocalFSAdapter(BaseAdapter):
    def send(
        self, request, stream=None, timeout=None, verify=None, cert=None, proxies=None
    ):
        pathname = url_to_path(request.url)

        resp = Response()
        resp.status_code = 200
        resp.url = request.url

        try:
            stats = stat(pathname)
        except OSError as exc:
            resp.status_code = 404
            message = {
                "error": "file does not exist",
                "path": pathname,
                "exception": repr(exc),
            }
            fh = SpooledTemporaryFile()
            fh.write(ensure_binary(json.dumps(message)))
            fh.seek(0)
            resp.raw = fh
            resp.close = resp.raw.close
        else:
            modified = formatdate(stats.st_mtime, usegmt=True)
            content_type = guess_type(pathname)[0] or "text/plain"
            resp.headers = CaseInsensitiveDict(
                {
                    "Content-Type": content_type,
                    "Content-Length": stats.st_size,
                    "Last-Modified": modified,
                }
            )

            resp.raw = open(pathname, "rb")
            resp.close = resp.raw.close
        return resp

    def close(self):
        pass  # pragma: no cover


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Defines S3 transport adapter for CondaSession (requests.Session)."""

from __future__ import annotations

import json
from logging import LoggerAdapter, getLogger
from tempfile import SpooledTemporaryFile
from typing import TYPE_CHECKING

from ....common.compat import ensure_binary
from ....common.url import url_to_s3_info
from .. import BaseAdapter, CaseInsensitiveDict, Response

if TYPE_CHECKING:
    from .. import PreparedRequest

log = getLogger(__name__)
stderrlog = LoggerAdapter(getLogger("conda.stderrlog"), extra=dict(terminator="\n"))


class S3Adapter(BaseAdapter):
    def send(
        self,
        request: PreparedRequest,
        stream: bool = False,
        timeout: None | float | tuple[float, float] | tuple[float, None] = None,
        verify: bool | str = True,
        cert: None | bytes | str | tuple[bytes | str, bytes | str] = None,
        proxies: dict[str, str] | None = None,
    ) -> Response:
        resp = Response()
        resp.status_code = 200
        resp.url = request.url

        try:
            return self._send_boto3(resp, request)
        except ImportError:
            stderrlog.info(
                "\nError: boto3 is required for S3 channels. "
                "Please install with `conda install boto3`\n"
                "Make sure to run `conda deactivate` if you "
                "are in a conda environment.\n"
            )
            resp.status_code = 404
            return resp

    def close(self):
        pass

    def _send_boto3(self, resp: Response, request: PreparedRequest) -> Response:
        from boto3.session import Session
        from botocore.exceptions import BotoCoreError, ClientError

        bucket_name, key_string = url_to_s3_info(request.url)
        # https://github.com/conda/conda/issues/8993
        # creating a separate boto3 session to make this thread safe
        session = Session()
        # create a resource client using this thread's session object
        s3 = session.resource("s3")
        # finally get the S3 object
        key = s3.Object(bucket_name, key_string[1:])

        try:
            response = key.get()
        except (BotoCoreError, ClientError) as e:
            resp.status_code = 404
            message = {
                "error": "error downloading file from s3",
                "path": request.url,
                "exception": repr(e),
            }
            resp.raw = self._write_tempfile(
                lambda x: x.write(ensure_binary(json.dumps(message)))
            )
            resp.close = resp.raw.close
            return resp

        key_headers = response["ResponseMetadata"]["HTTPHeaders"]
        resp.headers = CaseInsensitiveDict(
            {
                "Content-Type": key_headers.get("content-type", "text/plain"),
                "Content-Length": key_headers["content-length"],
                "Last-Modified": key_headers["last-modified"],
            }
        )

        resp.raw = self._write_tempfile(key.download_fileobj)
        resp.close = resp.raw.close

        return resp

    def _write_tempfile(self, writer_callable):
        fh = SpooledTemporaryFile()
        writer_callable(fh)
        fh.seek(0)
        return fh


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause


# Copyright (C) 2012 Anaconda, Inc & Jason R. Coombs
# SPDX-License-Identifier: BSD-3-Clause, MIT
"""Disk utility functions for symlinking files and folders.

Portions of the code within this module are taken from https://github.com/jaraco/jaraco.windows
which is MIT licensed by Jason R. Coombs.

https://github.com/jaraco/skeleton/issues/1#issuecomment-285448440
"""

import sys
from logging import getLogger
from os import chmod as os_chmod
from os.path import abspath, isdir
from os.path import islink as os_islink
from os.path import lexists as os_lexists

from ...common.compat import on_win
from ...exceptions import CondaOSError, ParseError

__all__ = ("islink", "lchmod", "lexists", "link", "readlink", "symlink")

log = getLogger(__name__)
PYPY = sys.implementation.name == "pypy"


try:
    from os import lchmod as os_lchmod

    lchmod = os_lchmod
except ImportError:  # pragma: no cover

    def lchmod(path, mode):
        # On systems that don't allow permissions on symbolic links, skip
        # links entirely.
        if not islink(path):
            os_chmod(path, mode)


if not on_win:  # pragma: win no cover
    from os import link, symlink

    link = link
    symlink = symlink

else:  # pragma: unix no cover
    from ctypes import windll, wintypes

    CreateHardLink = windll.kernel32.CreateHardLinkW
    CreateHardLink.restype = wintypes.BOOL
    CreateHardLink.argtypes = [wintypes.LPCWSTR, wintypes.LPCWSTR, wintypes.LPVOID]
    try:
        CreateSymbolicLink = windll.kernel32.CreateSymbolicLinkW
        CreateSymbolicLink.restype = wintypes.BOOL
        CreateSymbolicLink.argtypes = [
            wintypes.LPCWSTR,
            wintypes.LPCWSTR,
            wintypes.DWORD,
        ]
    except AttributeError:
        CreateSymbolicLink = None

    def win_hard_link(src, dst):
        """Equivalent to os.link, using the win32 CreateHardLink call."""
        if not CreateHardLink(dst, src, None):
            raise CondaOSError(f"win32 hard link failed\n  src: {src}\n  dst: {dst}")

    def win_soft_link(src, dst):
        """Equivalent to os.symlink, using the win32 CreateSymbolicLink call."""
        if CreateSymbolicLink is None:
            raise CondaOSError("win32 soft link not supported")
        if not CreateSymbolicLink(dst, src, isdir(src)):
            raise CondaOSError(f"win32 soft link failed\n  src: {src}\n  dst: {dst}")

    link = win_hard_link
    symlink = win_soft_link


if not (on_win and PYPY):
    from os import readlink

    islink = os_islink
    lexists = os_lexists
    readlink = readlink

else:  # pragma: no cover
    import builtins
    import inspect
    import sys
    from ctypes import POINTER, Structure, byref, c_uint64, cast, windll, wintypes
    from os import getcwd
    from os.path import isfile

    def islink(path):
        """Determine if the given path is a symlink"""
        return is_reparse_point(path) and is_symlink(path)

    def lexists(path):
        if islink(path):
            return True
        if isdir(path):
            return True
        if isfile(path):
            return True
        return False

    MAX_PATH = 260
    IO_REPARSE_TAG_SYMLINK = 0xA000000C
    INVALID_FILE_ATTRIBUTES = 0xFFFFFFFF
    FILE_ATTRIBUTE_REPARSE_POINT = 0x400
    NULL = 0
    ERROR_NO_MORE_FILES = 0x12

    class WIN32_FIND_DATA(Structure):
        _fields_ = [
            ("file_attributes", wintypes.DWORD),
            ("creation_time", wintypes.FILETIME),
            ("last_access_time", wintypes.FILETIME),
            ("last_write_time", wintypes.FILETIME),
            ("file_size_words", wintypes.DWORD * 2),
            ("reserved", wintypes.DWORD * 2),
            ("filename", wintypes.WCHAR * MAX_PATH),
            ("alternate_filename", wintypes.WCHAR * 14),
        ]

        @property
        def file_size(self):
            return cast(self.file_size_words, POINTER(c_uint64)).contents

    LPWIN32_FIND_DATA = POINTER(WIN32_FIND_DATA)
    FindFirstFile = windll.kernel32.FindFirstFileW
    FindFirstFile.argtypes = (wintypes.LPWSTR, LPWIN32_FIND_DATA)
    FindFirstFile.restype = wintypes.HANDLE
    FindNextFile = windll.kernel32.FindNextFileW
    FindNextFile.argtypes = (wintypes.HANDLE, LPWIN32_FIND_DATA)
    FindNextFile.restype = wintypes.BOOLEAN
    INVALID_HANDLE_VALUE = wintypes.HANDLE(-1).value
    GetFileAttributes = windll.kernel32.GetFileAttributesW
    GetFileAttributes.restype = wintypes.DWORD
    GetFileAttributes.argtypes = (wintypes.LPWSTR,)

    def handle_nonzero_success(result):
        if result == 0:
            raise OSError()

    def format_system_message(errno):
        """
        Call FormatMessage with a system error number to retrieve
        the descriptive error message.
        """
        # first some flags used by FormatMessageW
        ALLOCATE_BUFFER = 0x100
        FROM_SYSTEM = 0x1000

        # Let FormatMessageW allocate the buffer (we'll free it below)
        # Also, let it know we want a system error message.
        flags = ALLOCATE_BUFFER | FROM_SYSTEM
        source = None
        message_id = errno
        language_id = 0
        result_buffer = wintypes.LPWSTR()
        buffer_size = 0
        arguments = None
        bytes = windll.kernel32.FormatMessageW(
            flags,
            source,
            message_id,
            language_id,
            byref(result_buffer),
            buffer_size,
            arguments,
        )
        # note the following will cause an infinite loop if GetLastError
        #  repeatedly returns an error that cannot be formatted, although
        #  this should not happen.
        handle_nonzero_success(bytes)
        message = result_buffer.value
        windll.kernel32.LocalFree(result_buffer)
        return message

    class WindowsError(builtins.WindowsError):
        # more info about errors at http://msdn.microsoft.com/en-us/library/ms681381(VS.85).aspx

        def __init__(self, value=None):
            if value is None:
                value = windll.kernel32.GetLastError()
            strerror = format_system_message(value)
            args = 0, strerror, None, value
            super().__init__(*args)

        @property
        def message(self):
            return self.strerror

        @property
        def code(self):
            return self.winerror

        def __str__(self):
            return self.message

        def __repr__(self):
            return "{self.__class__.__name__}({self.winerror})".format(**vars())

    def _is_symlink(find_data):
        return find_data.reserved[0] == IO_REPARSE_TAG_SYMLINK

    def _patch_path(path):
        r"""
        Paths have a max length of api.MAX_PATH characters (260). If a target path
        is longer than that, it needs to be made absolute and prepended with
        \\?\ in order to work with API calls.
        See http://msdn.microsoft.com/en-us/library/aa365247%28v=vs.85%29.aspx for
        details.
        """  # NOQA
        if path.startswith("\\\\?\\"):
            return path
        path = abspath(path)
        if not path[1] == ":":
            # python doesn't include the drive letter, but \\?\ requires it
            path = getcwd()[:2] + path
        return "\\\\?\\" + path

    def local_format(string):
        """Format the string using variables in the caller's local namespace.

        .. code-block:: pycon
            >>> a = 3
            >>> local_format("{a:5}")
            '    3'
        """
        context = inspect.currentframe().f_back.f_locals
        return string.format_map(context)

    def is_symlink(path):
        """Assuming path is a reparse point, determine if it's a symlink."""
        path = _patch_path(path)
        try:
            return _is_symlink(next(find_files(path)))
        except OSError as orig_error:  # NOQA
            tmpl = "Error accessing {path}: {orig_error.message}"
            raise OSError(local_format(tmpl))

    def find_files(spec):
        r"""
        A pythonic wrapper around the FindFirstFile/FindNextFile win32 api.
        >>> root_files = tuple(find_files(r'c:\*'))
        >>> len(root_files) > 1
        True
        >>> root_files[0].filename == root_files[1].filename
        False
        >>> # This test might fail on a non-standard installation
        >>> 'Windows' in (fd.filename for fd in root_files)
        True
        """  # NOQA
        fd = WIN32_FIND_DATA()
        handle = FindFirstFile(spec, byref(fd))
        while True:
            if handle == INVALID_HANDLE_VALUE:
                raise OSError()
            yield fd
            fd = WIN32_FIND_DATA()
            res = FindNextFile(handle, byref(fd))
            if res == 0:  # error
                error = WindowsError()
                if error.code == ERROR_NO_MORE_FILES:
                    break
                else:
                    raise error
        # todo: how to close handle when generator is destroyed?
        # hint: catch GeneratorExit
        windll.kernel32.FindClose(handle)

    def is_reparse_point(path):
        """
        Determine if the given path is a reparse point.
        Return False if the file does not exist or the file attributes cannot
        be determined.
        """
        res = GetFileAttributes(path)
        return res != INVALID_FILE_ATTRIBUTES and bool(
            res & FILE_ATTRIBUTE_REPARSE_POINT
        )

    OPEN_EXISTING = 3
    FILE_FLAG_OPEN_REPARSE_POINT = 0x00200000
    FILE_FLAG_BACKUP_SEMANTICS = 0x2000000
    FSCTL_GET_REPARSE_POINT = 0x900A8
    LPDWORD = POINTER(wintypes.DWORD)
    LPOVERLAPPED = wintypes.LPVOID
    # VOLUME_NAME_DOS = 0

    class SECURITY_ATTRIBUTES(Structure):
        _fields_ = (
            ("length", wintypes.DWORD),
            ("p_security_descriptor", wintypes.LPVOID),
            ("inherit_handle", wintypes.BOOLEAN),
        )

    LPSECURITY_ATTRIBUTES = POINTER(SECURITY_ATTRIBUTES)

    CreateFile = windll.kernel32.CreateFileW
    CreateFile.argtypes = (
        wintypes.LPWSTR,
        wintypes.DWORD,
        wintypes.DWORD,
        LPSECURITY_ATTRIBUTES,
        wintypes.DWORD,
        wintypes.DWORD,
        wintypes.HANDLE,
    )
    CreateFile.restype = wintypes.HANDLE

    CloseHandle = windll.kernel32.CloseHandle
    CloseHandle.argtypes = (wintypes.HANDLE,)
    CloseHandle.restype = wintypes.BOOLEAN

    from ctypes import Array, c_byte, c_ulong, c_ushort, create_string_buffer, sizeof

    class REPARSE_DATA_BUFFER(Structure):
        _fields_ = [
            ("tag", c_ulong),
            ("data_length", c_ushort),
            ("reserved", c_ushort),
            ("substitute_name_offset", c_ushort),
            ("substitute_name_length", c_ushort),
            ("print_name_offset", c_ushort),
            ("print_name_length", c_ushort),
            ("flags", c_ulong),
            ("path_buffer", c_byte * 1),
        ]

        def get_print_name(self):
            wchar_size = sizeof(wintypes.WCHAR)
            arr_typ = wintypes.WCHAR * (self.print_name_length // wchar_size)
            data = byref(self.path_buffer, self.print_name_offset)
            return cast(data, POINTER(arr_typ)).contents.value

        def get_substitute_name(self):
            wchar_size = sizeof(wintypes.WCHAR)
            arr_typ = wintypes.WCHAR * (self.substitute_name_length // wchar_size)
            data = byref(self.path_buffer, self.substitute_name_offset)
            return cast(data, POINTER(arr_typ)).contents.value

    def readlink(link):
        """Return a string representing the path to which the symbolic link points.

        readlink(link) -> target
        """
        handle = CreateFile(
            link,
            0,
            0,
            None,
            OPEN_EXISTING,
            FILE_FLAG_OPEN_REPARSE_POINT | FILE_FLAG_BACKUP_SEMANTICS,
            None,
        )

        if handle == INVALID_HANDLE_VALUE:
            raise OSError()

        res = reparse_DeviceIoControl(handle, FSCTL_GET_REPARSE_POINT, None, 10240)

        bytes = create_string_buffer(res)
        p_rdb = cast(bytes, POINTER(REPARSE_DATA_BUFFER))
        rdb = p_rdb.contents
        if not rdb.tag == IO_REPARSE_TAG_SYMLINK:
            raise ParseError("Expected IO_REPARSE_TAG_SYMLINK, but got %d" % rdb.tag)

        handle_nonzero_success(CloseHandle(handle))
        return rdb.get_substitute_name()

    DeviceIoControl = windll.kernel32.DeviceIoControl
    DeviceIoControl.argtypes = [
        wintypes.HANDLE,
        wintypes.DWORD,
        wintypes.LPVOID,
        wintypes.DWORD,
        wintypes.LPVOID,
        wintypes.DWORD,
        LPDWORD,
        LPOVERLAPPED,
    ]
    DeviceIoControl.restype = wintypes.BOOL

    def reparse_DeviceIoControl(
        device, io_control_code, in_buffer, out_buffer, overlapped=None
    ):
        if overlapped is not None:
            raise NotImplementedError("overlapped handles not yet supported")

        if isinstance(out_buffer, int):
            out_buffer = create_string_buffer(out_buffer)

        in_buffer_size = len(in_buffer) if in_buffer is not None else 0
        out_buffer_size = len(out_buffer)
        assert isinstance(out_buffer, Array)

        returned_bytes = wintypes.DWORD()

        res = DeviceIoControl(
            device,
            io_control_code,
            in_buffer,
            in_buffer_size,
            out_buffer,
            out_buffer_size,
            returned_bytes,
            overlapped,
        )

        handle_nonzero_success(res)
        handle_nonzero_success(returned_bytes)
        return out_buffer[: returned_bytes.value]


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Record locking to manage potential repodata / repodata metadata file contention
between conda processes. Try to acquire a lock on a single byte in the metadat
file; modify both files; then release the lock.
"""

import time
import warnings
from contextlib import contextmanager

from ...base.context import context

LOCK_BYTE = 21  # mamba interop
LOCK_ATTEMPTS = 10
LOCK_SLEEP = 1


@contextmanager
def _lock_noop(fd):
    """When locking is not available."""
    yield


try:  # pragma: no cover
    import msvcrt

    @contextmanager
    def _lock_impl(fd):  # type: ignore
        tell = fd.tell()
        fd.seek(LOCK_BYTE)
        msvcrt.locking(fd.fileno(), msvcrt.LK_LOCK, 1)  # type: ignore
        try:
            fd.seek(tell)
            yield
        finally:
            fd.seek(LOCK_BYTE)
            msvcrt.locking(fd.fileno(), msvcrt.LK_UNLCK, 1)  # type: ignore

except ImportError:
    try:
        import fcntl
    except ImportError:  # pragma: no cover
        # "fcntl Availibility: not Emscripten, not WASI."
        warnings.warn("file locking not available")

        _lock_impl = _lock_noop  # type: ignore

    else:

        class _lock_impl:
            def __init__(self, fd):
                self.fd = fd

            def __enter__(self):
                for attempt in range(LOCK_ATTEMPTS):
                    try:
                        # msvcrt locking does something similar
                        fcntl.lockf(
                            self.fd, fcntl.LOCK_EX | fcntl.LOCK_NB, 1, LOCK_BYTE
                        )
                        break
                    except OSError:
                        if attempt > LOCK_ATTEMPTS - 2:
                            raise
                        time.sleep(LOCK_SLEEP)

            def __exit__(self, *exc):
                fcntl.lockf(self.fd, fcntl.LOCK_UN, 1, LOCK_BYTE)


def lock(fd):
    if not context.no_lock:
        # locking required for jlap, now default for all
        return _lock_impl(fd)
    return _lock_noop(fd)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Disk utility functions testing path properties (e.g., writable, hardlinks, softlinks, etc.)."""

from functools import lru_cache
from logging import getLogger
from os import W_OK, access
from os.path import basename, dirname, isdir, isfile, join
from uuid import uuid4

from ...base.constants import PREFIX_MAGIC_FILE
from ...common.constants import TRACE
from ...common.path import expand
from ...models.enums import LinkType
from .create import create_link
from .delete import rm_rf
from .link import islink, lexists

log = getLogger(__name__)


def file_path_is_writable(path):
    path = expand(path)
    log.log(TRACE, "checking path is writable %s", path)
    if isdir(dirname(path)):
        path_existed = lexists(path)
        try:
            fh = open(path, "a+")
        except OSError as e:
            log.debug(e)
            return False
        else:
            fh.close()
            if not path_existed:
                rm_rf(path)
            return True
    else:
        # TODO: probably won't work well on Windows
        return access(path, W_OK)


@lru_cache(maxsize=None)
def hardlink_supported(source_file, dest_dir):
    test_file = join(dest_dir, f".tmp.{basename(source_file)}.{str(uuid4())[:8]}")
    assert isfile(source_file), source_file
    assert isdir(dest_dir), dest_dir
    if lexists(test_file):
        rm_rf(test_file)
    assert not lexists(test_file), test_file
    try:
        # BeeGFS is a file system that does not support hard links between files in different
        # directories. Sometimes a soft link will be created with the hard link system call.
        create_link(source_file, test_file, LinkType.hardlink, force=True)
        is_supported = not islink(test_file)
        if is_supported:
            log.log(TRACE, "hard link supported for %s => %s", source_file, dest_dir)
        else:
            log.log(
                TRACE, "hard link IS NOT supported for %s => %s", source_file, dest_dir
            )
        return is_supported
    except OSError:
        log.log(TRACE, "hard link IS NOT supported for %s => %s", source_file, dest_dir)
        return False
    finally:
        rm_rf(test_file)


@lru_cache(maxsize=None)
def softlink_supported(source_file, dest_dir):
    # On Windows, softlink creation is restricted to Administrative users by default. It can
    # optionally be enabled for non-admin users through explicit registry modification.
    log.log(TRACE, "checking soft link capability for %s => %s", source_file, dest_dir)
    test_path = join(dest_dir, ".tmp." + basename(source_file))
    assert isfile(source_file), source_file
    assert isdir(dest_dir), dest_dir
    assert not lexists(test_path), test_path
    try:
        create_link(source_file, test_path, LinkType.softlink, force=True)
        return islink(test_path)
    except OSError:
        return False
    finally:
        rm_rf(test_path)


def is_conda_environment(prefix):
    return isfile(join(prefix, PREFIX_MAGIC_FILE))


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Disk utility functions for modifying file and directory permissions."""

from errno import EACCES, ENOENT, EPERM, EROFS
from itertools import chain
from logging import getLogger
from os import X_OK, access, chmod, lstat, walk
from os.path import isdir, isfile, join
from stat import S_IEXEC, S_IMODE, S_ISDIR, S_ISREG, S_IWRITE, S_IXGRP, S_IXOTH, S_IXUSR

from ...common.compat import on_win
from ...common.constants import TRACE
from . import MAX_TRIES, exp_backoff_fn
from .link import islink, lchmod

log = getLogger(__name__)


def make_writable(path):
    try:
        mode = lstat(path).st_mode
        if S_ISDIR(mode):
            chmod(path, S_IMODE(mode) | S_IWRITE | S_IEXEC)
        elif islink(path):
            lchmod(path, S_IMODE(mode) | S_IWRITE)
        elif S_ISREG(mode):
            chmod(path, S_IMODE(mode) | S_IWRITE)
        else:
            log.debug("path cannot be made writable: %s", path)
        return True
    except Exception as e:
        eno = getattr(e, "errno", None)
        if eno in (ENOENT,):
            log.debug("tried to make writable, but didn't exist: %s", path)
            raise
        elif eno in (EACCES, EPERM, EROFS):
            log.debug("tried make writable but failed: %s\n%r", path, e)
            return False
        else:
            log.warning("Error making path writable: %s\n%r", path, e)
            raise


def make_read_only(path):
    mode = lstat(path).st_mode
    if S_ISDIR(mode):
        chmod(path, S_IMODE(mode) & ~S_IWRITE)
    elif islink(path):
        lchmod(path, S_IMODE(mode) & ~S_IWRITE)
    elif S_ISREG(mode):
        chmod(path, S_IMODE(mode) & ~S_IWRITE)
    else:
        log.debug("path cannot be made read only: %s", path)
    return True


def recursive_make_writable(path, max_tries=MAX_TRIES):
    # The need for this function was pointed out at
    #   https://github.com/conda/conda/issues/3266#issuecomment-239241915
    # Especially on windows, file removal will often fail because it is marked read-only
    if isdir(path):
        for root, dirs, files in walk(path):
            for path in chain.from_iterable((files, dirs)):
                try:
                    exp_backoff_fn(make_writable, join(root, path), max_tries=max_tries)
                except OSError as e:
                    if e.errno == ENOENT:
                        log.debug("no such file or directory: %s", path)
                    else:
                        raise
    else:
        exp_backoff_fn(make_writable, path, max_tries=max_tries)


def make_executable(path):
    if isfile(path):
        mode = lstat(path).st_mode
        log.log(TRACE, "chmod +x %s", path)
        chmod(path, S_IMODE(mode) | S_IXUSR | S_IXGRP | S_IXOTH)
    else:
        log.error("Cannot make path '%s' executable", path)


def is_executable(path):
    if isfile(path):  # for now, leave out `and not islink(path)`
        return path.endswith((".exe", ".bat")) if on_win else access(path, X_OK)
    return False


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Disk utility functions for creating new files or directories."""

import codecs
import os
import sys
import tempfile
import warnings as _warnings
from errno import EACCES, EPERM, EROFS
from logging import getLogger
from os.path import dirname, isdir, isfile, join, splitext
from shutil import copyfileobj, copystat

from ... import CondaError
from ...auxlib.ish import dals
from ...base.constants import CONDA_PACKAGE_EXTENSION_V1, PACKAGE_CACHE_MAGIC_FILE
from ...base.context import context
from ...common.compat import on_linux, on_win
from ...common.constants import TRACE
from ...common.path import ensure_pad, expand, win_path_double_escape, win_path_ok
from ...common.serialize import json_dump
from ...exceptions import BasicClobberError, CondaOSError, maybe_raise
from ...models.enums import LinkType
from . import mkdir_p
from .delete import path_is_clean, rm_rf
from .link import islink, lexists, link, readlink, symlink
from .permissions import make_executable
from .update import touch


# we have our own TemporaryDirectory implementation both for historical reasons and because
#     using our rm_rf function is more robust than the shutil equivalent
class TemporaryDirectory:
    """Create and return a temporary directory.  This has the same
    behavior as mkdtemp but can be used as a context manager.  For
    example:

        with TemporaryDirectory() as tmpdir:
            ...

    Upon exiting the context, the directory and everything contained
    in it are removed.
    """

    # Handle mkdtemp raising an exception
    name = None
    _closed = False

    def __init__(self, suffix="", prefix="tmp", dir=None):
        self.name = tempfile.mkdtemp(suffix, prefix, dir)

    def __repr__(self):
        return f"<{self.__class__.__name__} {self.name!r}>"

    def __enter__(self):
        return self.name

    def cleanup(self, _warn=False, _warnings=_warnings):
        from .delete import rm_rf as _rm_rf

        if self.name and not self._closed:
            try:
                _rm_rf(self.name)
            except (TypeError, AttributeError) as ex:
                if "None" not in f"{ex}":
                    raise
                _rm_rf(self.name)
            self._closed = True

    def __exit__(self, exc, value, tb):
        self.cleanup()

    def __del__(self):
        # Issue a ResourceWarning if implicit cleanup needed
        self.cleanup(_warn=True)


log = getLogger(__name__)
stdoutlog = getLogger("conda.stdoutlog")

# in __init__.py to help with circular imports
mkdir_p = mkdir_p

python_entry_point_template = dals(
    r"""
# -*- coding: utf-8 -*-
import re
import sys

from %(module)s import %(import_name)s

if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
    sys.exit(%(func)s())
"""
)  # NOQA

application_entry_point_template = dals(
    """
# -*- coding: utf-8 -*-
if __name__ == '__main__':
    import os
    import sys
    args = ["%(source_full_path)s"]
    if len(sys.argv) > 1:
        args += sys.argv[1:]
    os.execv(args[0], args)
"""
)


def write_as_json_to_file(file_path, obj):
    log.log(TRACE, "writing json to file %s", file_path)
    with codecs.open(file_path, mode="wb", encoding="utf-8") as fo:
        json_str = json_dump(obj)
        fo.write(json_str)


def create_python_entry_point(target_full_path, python_full_path, module, func):
    if lexists(target_full_path):
        maybe_raise(
            BasicClobberError(
                source_path=None,
                target_path=target_full_path,
                context=context,
            ),
            context,
        )

    import_name = func.split(".")[0]
    pyscript = python_entry_point_template % {
        "module": module,
        "func": func,
        "import_name": import_name,
    }
    if python_full_path is not None:
        from ...core.portability import generate_shebang_for_entry_point

        shebang = generate_shebang_for_entry_point(python_full_path)
    else:
        shebang = None

    with codecs.open(target_full_path, mode="wb", encoding="utf-8") as fo:
        if shebang is not None:
            fo.write(shebang)
        fo.write(pyscript)

    if shebang is not None:
        make_executable(target_full_path)

    return target_full_path


def create_application_entry_point(
    source_full_path, target_full_path, python_full_path
):
    # source_full_path: where the entry point file points to
    # target_full_path: the location of the new entry point file being created
    if lexists(target_full_path):
        maybe_raise(
            BasicClobberError(
                source_path=None,
                target_path=target_full_path,
                context=context,
            ),
            context,
        )

    entry_point = application_entry_point_template % {
        "source_full_path": win_path_double_escape(source_full_path),
    }
    if not isdir(dirname(target_full_path)):
        mkdir_p(dirname(target_full_path))
    with open(target_full_path, "w") as fo:
        if " " in python_full_path:
            python_full_path = ensure_pad(python_full_path, '"')
        fo.write(f"#!{python_full_path}\n")
        fo.write(entry_point)
    make_executable(target_full_path)


class ProgressFileWrapper:
    def __init__(self, fileobj, progress_update_callback):
        self.progress_file = fileobj
        self.progress_update_callback = progress_update_callback
        self.progress_file_size = max(1, os.fstat(fileobj.fileno()).st_size)
        self.progress_max_pos = 0

    def __getattr__(self, name):
        return getattr(self.progress_file, name)

    def __setattr__(self, name, value):
        if name.startswith("progress_"):
            super().__setattr__(name, value)
        else:
            setattr(self.progress_file, name, value)

    def read(self, size=-1):
        data = self.progress_file.read(size)
        self.progress_update()
        return data

    def progress_update(self):
        pos = max(self.progress_max_pos, self.progress_file.tell())
        pos = min(pos, self.progress_file_size)
        self.progress_max_pos = pos
        rel_pos = pos / self.progress_file_size
        self.progress_update_callback(rel_pos)


def extract_tarball(
    tarball_full_path, destination_directory=None, progress_update_callback=None
):
    import conda_package_handling.api

    if destination_directory is None:
        if tarball_full_path[-8:] == CONDA_PACKAGE_EXTENSION_V1:
            destination_directory = tarball_full_path[:-8]
        else:
            destination_directory = tarball_full_path.splitext()[0]
    log.debug("extracting %s\n  to %s", tarball_full_path, destination_directory)

    # the most common reason this happens is due to hard-links, windows thinks
    #    files in the package cache are in-use. rm_rf should have moved them to
    #    have a .conda_trash extension though, so it's ok to just write into
    #    the same existing folder.
    if not path_is_clean(destination_directory):
        log.debug(
            "package folder %s was not empty, but we're writing there.",
            destination_directory,
        )

    conda_package_handling.api.extract(
        tarball_full_path, dest_dir=destination_directory
    )

    if hasattr(conda_package_handling.api, "THREADSAFE_EXTRACT"):
        return  # indicates conda-package-handling 2.x, which implements --no-same-owner

    if on_linux and os.getuid() == 0:  # pragma: no cover
        # When extracting as root, tarfile will by restore ownership
        # of extracted files.  However, we want root to be the owner
        # (our implementation of --no-same-owner).
        for root, dirs, files in os.walk(destination_directory):
            for fn in files:
                p = join(root, fn)
                os.lchown(p, 0, 0)


def make_menu(prefix, file_path, remove=False):
    """
    Create cross-platform menu items (e.g. Windows Start Menu)

    Passes all menu config files %PREFIX%/Menu/*.json to ``menuinst.install``.
    ``remove=True`` will remove the menu items.
    """
    try:
        import menuinst

        menuinst.install(
            join(prefix, win_path_ok(file_path)),
            remove=remove,
            prefix=prefix,
            root_prefix=context.root_prefix,
        )
    except Exception:
        stdoutlog.error("menuinst Exception", exc_info=True)


def create_hard_link_or_copy(src, dst):
    if islink(src):
        message = dals(
            f"""
        Cannot hard link a soft link
          source: {src}
          destination: {dst}
        """
        )
        raise CondaOSError(message)

    try:
        log.log(TRACE, "creating hard link %s => %s", src, dst)
        link(src, dst)
    except OSError:
        log.info("hard link failed, so copying %s => %s", src, dst)
        _do_copy(src, dst)


def _is_unix_executable_using_ORIGIN(path):
    if on_win:
        return False
    else:
        return isfile(path) and not islink(path) and os.access(path, os.X_OK)


def _do_softlink(src, dst):
    if _is_unix_executable_using_ORIGIN(src):
        # for extra details, see https://github.com/conda/conda/pull/4625#issuecomment-280696371
        # We only need to do this copy for executables which have an RPATH containing $ORIGIN
        #   on Linux, so `is_executable()` is currently overly aggressive.
        # A future optimization will be to copy code from @mingwandroid's virtualenv patch.
        copy(src, dst)
    else:
        log.log(TRACE, "soft linking %s => %s", src, dst)
        symlink(src, dst)


def create_fake_executable_softlink(src, dst):
    assert on_win
    src_root, _ = splitext(src)
    # TODO: this open will clobber, consider raising
    with open(dst, "w") as f:
        f.write(f'@echo off\ncall "{src_root}" %*\n')
    return dst


def copy(src, dst):
    # on unix, make sure relative symlinks stay symlinks
    if not on_win and islink(src):
        src_points_to = readlink(src)
        if not src_points_to.startswith("/"):
            # copy relative symlinks as symlinks
            log.log(TRACE, "soft linking %s => %s", src, dst)
            symlink(src_points_to, dst)
            return
    _do_copy(src, dst)


def _do_copy(src, dst):
    log.log(TRACE, "copying %s => %s", src, dst)
    # src and dst are always files. So we can bypass some checks that shutil.copy does.
    # Also shutil.copy calls shutil.copymode, which we can skip because we are explicitly
    # calling copystat.

    # Same size as used by Linux cp command (has performance advantage).
    # Python's default is 16k.
    buffer_size = 4194304  # 4 * 1024 * 1024  == 4 MB
    with open(src, "rb") as fsrc:
        with open(dst, "wb") as fdst:
            copyfileobj(fsrc, fdst, buffer_size)

    try:
        copystat(src, dst)
    except OSError as e:  # pragma: no cover
        # shutil.copystat gives a permission denied when using the os.setxattr function
        # on the security.selinux property.
        log.debug("%r", e)


def create_link(src, dst, link_type=LinkType.hardlink, force=False):
    if link_type == LinkType.directory:
        # A directory is technically not a link.  So link_type is a misnomer.
        #   Naming is hard.
        if lexists(dst) and not isdir(dst):
            if not force:
                maybe_raise(BasicClobberError(src, dst, context), context)
            log.info(f"file exists, but clobbering for directory: {dst!r}")
            rm_rf(dst)
        mkdir_p(dst)
        return

    if not lexists(src):
        raise CondaError(
            f"Cannot link a source that does not exist. {src}\n"
            "Running `conda clean --packages` may resolve your problem."
        )

    if lexists(dst):
        if not force:
            maybe_raise(BasicClobberError(src, dst, context), context)
        log.info(f"file exists, but clobbering: {dst!r}")
        rm_rf(dst)

    if link_type == LinkType.hardlink:
        if isdir(src):
            raise CondaError(f"Cannot hard link a directory. {src}")
        try:
            log.log(TRACE, "hard linking %s => %s", src, dst)
            link(src, dst)
        except OSError as e:
            log.debug("%r", e)
            log.debug(
                "hard-link failed. falling back to copy\n"
                "  error: %r\n"
                "  src: %s\n"
                "  dst: %s",
                e,
                src,
                dst,
            )

            copy(src, dst)
    elif link_type == LinkType.softlink:
        _do_softlink(src, dst)
    elif link_type == LinkType.copy:
        copy(src, dst)
    else:
        raise CondaError(f"Did not expect linktype={link_type!r}")


def compile_multiple_pyc(
    python_exe_full_path, py_full_paths, pyc_full_paths, prefix, py_ver
):
    py_full_paths = tuple(py_full_paths)
    pyc_full_paths = tuple(pyc_full_paths)
    if len(py_full_paths) == 0:
        return []

    fd, filename = tempfile.mkstemp()
    try:
        for f in py_full_paths:
            f = os.path.relpath(f, prefix)
            if hasattr(f, "encode"):
                f = f.encode(sys.getfilesystemencoding(), errors="replace")
            os.write(fd, f + b"\n")
        os.close(fd)
        command = ["-Wi", "-m", "compileall", "-q", "-l", "-i", filename]
        # if the python version in the prefix is 3.5+, we have some extra args.
        #    -j 0 will do the compilation in parallel, with os.cpu_count() cores
        if int(py_ver[0]) >= 3 and int(py_ver.split(".")[1]) > 5:
            command.extend(["-j", "0"])
        command[0:0] = [python_exe_full_path]
        # command[0:0] = ['--cwd', prefix, '--dev', '-p', prefix, python_exe_full_path]
        log.log(TRACE, command)
        from ..subprocess import any_subprocess

        # from ...common.io import env_vars
        # This stack does not maintain its _argparse_args correctly?
        # from ...base.context import stack_context_default
        # with env_vars({}, stack_context_default):
        #     stdout, stderr, rc = run_command(Commands.RUN, *command)
        stdout, stderr, rc = any_subprocess(command, prefix)
    finally:
        os.remove(filename)

    created_pyc_paths = []
    for py_full_path, pyc_full_path in zip(py_full_paths, pyc_full_paths):
        if not isfile(pyc_full_path):
            message = dals(
                """
            pyc file failed to compile successfully (run_command failed)
            python_exe_full_path: %s
            py_full_path: %s
            pyc_full_path: %s
            compile rc: %s
            compile stdout: %s
            compile stderr: %s
            """
            )
            log.info(
                message,
                python_exe_full_path,
                py_full_path,
                pyc_full_path,
                rc,
                stdout,
                stderr,
            )
        else:
            created_pyc_paths.append(pyc_full_path)

    return created_pyc_paths


def create_package_cache_directory(pkgs_dir):
    # returns False if package cache directory cannot be created
    try:
        log.log(TRACE, "creating package cache directory '%s'", pkgs_dir)
        sudo_safe = expand(pkgs_dir).startswith(expand("~"))
        touch(join(pkgs_dir, PACKAGE_CACHE_MAGIC_FILE), mkdir=True, sudo_safe=sudo_safe)
        touch(join(pkgs_dir, "urls"), sudo_safe=sudo_safe)
    except OSError as e:
        if e.errno in (EACCES, EPERM, EROFS):
            log.log(TRACE, "cannot create package cache directory '%s'", pkgs_dir)
            return False
        else:
            raise
    return True


def create_envs_directory(envs_dir):
    # returns False if envs directory cannot be created

    # The magic file being used here could change in the future.  Don't write programs
    # outside this code base that rely on the presence of this file.
    # This value is duplicated in conda.base.context._first_writable_envs_dir().
    envs_dir_magic_file = join(envs_dir, ".conda_envs_dir_test")
    try:
        log.log(TRACE, "creating envs directory '%s'", envs_dir)
        sudo_safe = expand(envs_dir).startswith(expand("~"))
        touch(join(envs_dir, envs_dir_magic_file), mkdir=True, sudo_safe=sudo_safe)
    except OSError as e:
        if e.errno in (EACCES, EPERM, EROFS):
            log.log(TRACE, "cannot create envs directory '%s'", envs_dir)
            return False
        else:
            raise
    return True


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Disk utility functions for deleting files and folders."""

import fnmatch
import shutil
import sys
from errno import ENOENT
from logging import getLogger
from os import environ, getcwd, makedirs, rename, rmdir, scandir, unlink, walk
from os.path import (
    abspath,
    basename,
    dirname,
    exists,
    isdir,
    isfile,
    join,
    normpath,
    split,
)
from subprocess import STDOUT, CalledProcessError, check_output

from ...base.constants import CONDA_TEMP_EXTENSION
from ...base.context import context
from ...common.compat import on_win
from ...common.constants import TRACE
from . import MAX_TRIES, exp_backoff_fn
from .link import islink, lexists
from .permissions import make_writable, recursive_make_writable

if not on_win:
    from shutil import which


log = getLogger(__name__)


def rmtree(path, *args, **kwargs):
    # subprocessing to delete large folders can be quite a bit faster
    path = normpath(path)
    if on_win:
        try:
            # the fastest way seems to be using DEL to recursively delete files
            # https://www.ghacks.net/2017/07/18/how-to-delete-large-folders-in-windows-super-fast/
            # However, this is not entirely safe, as it can end up following symlinks to folders
            # https://superuser.com/a/306618/184799
            # so, we stick with the slower, but hopefully safer way.  Maybe if we figured out how
            #    to scan for any possible symlinks, we could do the faster way.
            # out = check_output('DEL /F/Q/S *.* > NUL 2> NUL'.format(path), shell=True,
            #                    stderr=STDOUT, cwd=path)

            out = check_output(
                f'RD /S /Q "{path}" > NUL 2> NUL', shell=True, stderr=STDOUT
            )
        except:
            try:
                # Try to delete in Unicode
                name = None
                from ...auxlib.compat import Utf8NamedTemporaryFile
                from ...utils import quote_for_shell

                with Utf8NamedTemporaryFile(
                    mode="w", suffix=".bat", delete=False
                ) as batch_file:
                    batch_file.write(f"RD /S {quote_for_shell(path)}\n")
                    batch_file.write("chcp 65001\n")
                    batch_file.write(f"RD /S {quote_for_shell(path)}\n")
                    batch_file.write("EXIT 0\n")
                    name = batch_file.name
                # If the above is bugged we can end up deleting hard-drives, so we check
                # that 'path' appears in it. This is not bulletproof but it could save you (me).
                with open(name) as contents:
                    content = contents.read()
                    assert path in content
                comspec = environ["COMSPEC"]
                CREATE_NO_WINDOW = 0x08000000
                # It is essential that we `pass stdout=None, stderr=None, stdin=None` here because
                # if we do not, then the standard console handles get attached and chcp affects the
                # parent process (and any which share those console handles!)
                out = check_output(
                    [comspec, "/d", "/c", name],
                    shell=False,
                    stdout=None,
                    stderr=None,
                    stdin=None,
                    creationflags=CREATE_NO_WINDOW,
                )

            except CalledProcessError as e:
                if e.returncode != 5:
                    log.error(
                        f"Removing folder {name} the fast way failed.  Output was: {out}"
                    )
                    raise
                else:
                    log.debug(
                        f"removing dir contents the fast way failed.  Output was: {out}"
                    )
    else:
        try:
            makedirs(".empty")
        except:
            pass
        # yes, this looks strange.  See
        #    https://unix.stackexchange.com/a/79656/34459
        #    https://web.archive.org/web/20130929001850/http://linuxnote.net/jianingy/en/linux/a-fast-way-to-remove-huge-number-of-files.html  # NOQA

        if isdir(".empty"):
            rsync = which("rsync")

            if rsync:
                try:
                    out = check_output(
                        [
                            rsync,
                            "-a",
                            "--force",
                            "--delete",
                            join(getcwd(), ".empty") + "/",
                            path + "/",
                        ],
                        stderr=STDOUT,
                    )
                except CalledProcessError:
                    log.debug(
                        f"removing dir contents the fast way failed.  Output was: {out}"
                    )

            shutil.rmtree(".empty")
    shutil.rmtree(path)


def unlink_or_rename_to_trash(path):
    """If files are in use, especially on windows, we can't remove them.
    The fallback path is to rename them (but keep their folder the same),
    which maintains the file handle validity.  See comments at:
    https://serverfault.com/a/503769
    """
    try:
        make_writable(path)
        unlink(path)
    except OSError:
        try:
            rename(path, path + ".conda_trash")
        except OSError:
            if on_win:
                # on windows, it is important to use the rename program, as just using python's
                #    rename leads to permission errors when files are in use.
                condabin_dir = join(context.conda_prefix, "condabin")
                trash_script = join(condabin_dir, "rename_tmp.bat")
                if exists(trash_script):
                    _dirname, _fn = split(path)
                    dest_fn = path + ".conda_trash"
                    counter = 1
                    while isfile(dest_fn):
                        dest_fn = dest_fn.splitext[0] + f".conda_trash_{counter}"
                        counter += 1
                    out = "< empty >"
                    try:
                        out = check_output(
                            [
                                "cmd.exe",
                                "/C",
                                trash_script,
                                _dirname,
                                _fn,
                                basename(dest_fn),
                            ],
                            stderr=STDOUT,
                        )
                    except CalledProcessError:
                        log.debug(
                            f"renaming file path {path} to trash failed.  Output was: {out}"
                        )

                else:
                    log.debug(
                        f"{trash_script} is missing.  Conda was not installed correctly or has been "
                        "corrupted.  Please file an issue on the conda github repo."
                    )
            log.warning(
                f"Could not remove or rename {path}.  Please remove this file manually (you "
                "may need to reboot to free file handles)"
            )


def remove_empty_parent_paths(path):
    # recurse to clean up empty folders that were created to have a nested hierarchy
    parent_path = dirname(path)

    while isdir(parent_path) and not next(scandir(parent_path), None):
        rmdir(parent_path)
        parent_path = dirname(parent_path)


def rm_rf(path, max_retries=5, trash=True, clean_empty_parents=False, *args, **kw):
    """
    Completely delete path
    max_retries is the number of times to retry on failure. The default is 5. This only applies
    to deleting a directory.
    If removing path fails and trash is True, files will be moved to the trash directory.
    """
    try:
        path = abspath(path)
        log.log(TRACE, "rm_rf %s", path)
        if isdir(path) and not islink(path):
            backoff_rmdir(path)
        elif lexists(path):
            unlink_or_rename_to_trash(path)
        else:
            log.log(TRACE, "rm_rf failed. Not a link, file, or directory: %s", path)
    finally:
        if lexists(path):
            log.info("rm_rf failed for %s", path)
            return False
    if isdir(path):
        delete_trash(path)
    if clean_empty_parents:
        remove_empty_parent_paths(path)
    return True


# aliases that all do the same thing (legacy compat)
try_rmdir_all_empty = move_to_trash = move_path_to_trash = rm_rf


def delete_trash(prefix):
    if not prefix:
        prefix = sys.prefix
    exclude = {"envs", "pkgs"}
    for root, dirs, files in walk(prefix, topdown=True):
        dirs[:] = [d for d in dirs if d not in exclude]
        for fn in files:
            if fnmatch.fnmatch(fn, "*.conda_trash*") or fnmatch.fnmatch(
                fn, "*" + CONDA_TEMP_EXTENSION
            ):
                filename = join(root, fn)
                try:
                    unlink(filename)
                    remove_empty_parent_paths(filename)
                except OSError as e:
                    log.debug("%r errno %d\nCannot unlink %s.", e, e.errno, filename)


def backoff_rmdir(dirpath, max_tries=MAX_TRIES):
    if not isdir(dirpath):
        return

    def retry(func, path, exc_info):
        if getattr(exc_info[1], "errno", None) == ENOENT:
            return
        recursive_make_writable(dirname(path), max_tries=max_tries)
        func(path)

    def _rmdir(path):
        try:
            recursive_make_writable(path)
            exp_backoff_fn(rmtree, path, onerror=retry, max_tries=max_tries)
        except OSError as e:
            if e.errno == ENOENT:
                log.log(TRACE, "no such file or directory: %s", path)
            else:
                raise

    try:
        rmtree(dirpath)
    # we don't really care about errors that much.  We'll catch remaining files
    #    with slower python logic.
    except:
        pass

    for root, dirs, files in walk(dirpath, topdown=False):
        for file in files:
            unlink_or_rename_to_trash(join(root, file))


def path_is_clean(path):
    """Sometimes we can't completely remove a path because files are considered in use
    by python (hardlinking confusion).  For our tests, it is sufficient that either the
    folder doesn't exist, or nothing but temporary file copies are left.
    """
    clean = not exists(path)
    if not clean:
        for root, dirs, fns in walk(path):
            for fn in fns:
                if not (
                    fnmatch.fnmatch(fn, "*.conda_trash*")
                    or fnmatch.fnmatch(fn, "*" + CONDA_TEMP_EXTENSION)
                ):
                    return False
    return True


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Disk utility functions for reading and processing file contents."""

from __future__ import annotations

import hashlib
import json
import os
from base64 import b64encode
from collections import namedtuple
from errno import ENOENT
from functools import partial
from itertools import chain
from logging import getLogger
from os.path import isdir, isfile, join  # noqa
from pathlib import Path
from typing import TYPE_CHECKING

from ...auxlib.collection import first
from ...auxlib.compat import shlex_split_unicode
from ...auxlib.ish import dals
from ...base.constants import PREFIX_PLACEHOLDER
from ...common.compat import open
from ...common.pkg_formats.python import (
    PythonDistribution,
    PythonEggInfoDistribution,
    PythonEggLinkDistribution,
    PythonInstalledDistribution,
)
from ...exceptions import CondaUpgradeError, CondaVerificationError, PathNotFoundError
from ...models.channel import Channel
from ...models.enums import FileMode, PackageType, PathType
from ...models.package_info import PackageInfo, PackageMetadata
from ...models.records import PathData, PathDataV1, PathsData, PrefixRecord
from .create import TemporaryDirectory
from .link import islink, lexists  # noqa

if TYPE_CHECKING:
    from typing import Literal

log = getLogger(__name__)

listdir = lambda d: list(entry.name for entry in os.scandir(d))  # noqa


def yield_lines(path):
    """Generator function for lines in file.  Empty generator if path does not exist.

    Args:
        path (str): path to file

    Returns:
        iterator: each line in file, not starting with '#'

    """
    try:
        with open(path) as fh:
            for line in fh:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                yield line
    except OSError as e:
        if e.errno == ENOENT:
            pass
        else:
            raise


def compute_sum(path: str | os.PathLike, algo: Literal["md5", "sha256"]) -> str:
    path = Path(path)
    if not path.is_file():
        raise PathNotFoundError(path)

    # FUTURE: Python 3.11+, replace with hashlib.file_digest
    hasher = hashlib.new(algo)
    with path.open("rb") as fh:
        for chunk in iter(partial(fh.read, 8192), b""):
            hasher.update(chunk)
    return hasher.hexdigest()


# ####################################################
# functions supporting read_package_info()
# ####################################################


def read_package_info(record, package_cache_record):
    epd = package_cache_record.extracted_package_dir
    icondata = read_icondata(epd)
    package_metadata = read_package_metadata(epd)
    paths_data = read_paths_json(epd)

    return PackageInfo(
        extracted_package_dir=epd,
        package_tarball_full_path=package_cache_record.package_tarball_full_path,
        channel=Channel(record.schannel or record.channel),
        repodata_record=record,
        url=package_cache_record.url,
        icondata=icondata,
        package_metadata=package_metadata,
        paths_data=paths_data,
    )


def read_index_json(extracted_package_directory):
    with open(join(extracted_package_directory, "info", "index.json")) as fi:
        return json.load(fi)


def read_index_json_from_tarball(package_tarball_full_path):
    import conda_package_handling.api

    with TemporaryDirectory() as tmpdir:
        conda_package_handling.api.extract(package_tarball_full_path, tmpdir, "info")
        with open(join(tmpdir, "info", "index.json")) as f:
            json_data = json.load(f)
    return json_data


def read_repodata_json(extracted_package_directory):
    with open(join(extracted_package_directory, "info", "repodata_record.json")) as fi:
        return json.load(fi)


def read_icondata(extracted_package_directory):
    icon_file_path = join(extracted_package_directory, "info", "icon.png")
    if isfile(icon_file_path):
        with open(icon_file_path, "rb") as f:
            data = f.read()
        return b64encode(data).decode("utf-8")
    else:
        return None


def read_package_metadata(extracted_package_directory):
    def _paths():
        yield join(extracted_package_directory, "info", "link.json")
        yield join(extracted_package_directory, "info", "package_metadata.json")

    path = first(_paths(), key=isfile)
    if not path:
        return None
    else:
        with open(path) as f:
            data = json.loads(f.read())
            if data.get("package_metadata_version") != 1:
                raise CondaUpgradeError(
                    dals(
                        """
                The current version of conda is too old to install this package. (This version
                only supports link.json schema version 1.)  Please update conda to install
                this package.
                """
                    )
                )
        package_metadata = PackageMetadata(**data)
        return package_metadata


def read_paths_json(extracted_package_directory):
    info_dir = join(extracted_package_directory, "info")
    paths_json_path = join(info_dir, "paths.json")
    if isfile(paths_json_path):
        with open(paths_json_path) as paths_json:
            data = json.load(paths_json)
        if data.get("paths_version") != 1:
            raise CondaUpgradeError(
                dals(
                    """
            The current version of conda is too old to install this package. (This version
            only supports paths.json schema version 1.)  Please update conda to install
            this package."""
                )
            )
        paths_data = PathsData(
            paths_version=1,
            paths=(PathDataV1(**f) for f in data["paths"]),
        )
    else:
        has_prefix_files = read_has_prefix(join(info_dir, "has_prefix"))
        no_link = read_no_link(info_dir)

        def read_files_file():
            files_path = join(info_dir, "files")
            for f in (
                ln for ln in (line.strip() for line in yield_lines(files_path)) if ln
            ):
                path_info = {"_path": f}
                if f in has_prefix_files.keys():
                    path_info["prefix_placeholder"] = has_prefix_files[f][0]
                    path_info["file_mode"] = has_prefix_files[f][1]
                if f in no_link:
                    path_info["no_link"] = True
                if islink(join(extracted_package_directory, f)):
                    path_info["path_type"] = PathType.softlink
                else:
                    path_info["path_type"] = PathType.hardlink
                yield PathData(**path_info)

        paths = tuple(read_files_file())
        paths_data = PathsData(
            paths_version=0,
            paths=paths,
        )
    return paths_data


def read_has_prefix(path):
    """Reads `has_prefix` file and return dict mapping filepaths to tuples(placeholder, FileMode).

    A line in `has_prefix` contains one of:
      * filepath
      * placeholder mode filepath

    Mode values are one of:
      * text
      * binary
    """
    ParseResult = namedtuple("ParseResult", ("placeholder", "filemode", "filepath"))

    def parse_line(line):
        # placeholder, filemode, filepath
        parts = tuple(x.strip("\"'") for x in shlex_split_unicode(line, posix=False))
        if len(parts) == 1:
            return ParseResult(PREFIX_PLACEHOLDER, FileMode.text, parts[0])
        elif len(parts) == 3:
            return ParseResult(parts[0], FileMode(parts[1]), parts[2])
        else:
            raise CondaVerificationError(f"Invalid has_prefix file at path: {path}")

    parsed_lines = (parse_line(line) for line in yield_lines(path))
    return {pr.filepath: (pr.placeholder, pr.filemode) for pr in parsed_lines}


def read_no_link(info_dir):
    return set(
        chain(
            yield_lines(join(info_dir, "no_link")),
            yield_lines(join(info_dir, "no_softlink")),
        )
    )


def read_soft_links(extracted_package_directory, files):
    return tuple(f for f in files if islink(join(extracted_package_directory, f)))


def read_python_record(prefix_path, anchor_file, python_version):
    """
    Convert a python package defined by an anchor file (Metadata information)
    into a conda prefix record object.
    """
    pydist = PythonDistribution.init(prefix_path, anchor_file, python_version)
    depends, constrains = pydist.get_conda_dependencies()

    if isinstance(pydist, PythonInstalledDistribution):
        channel = Channel("pypi")
        build = "pypi_0"
        package_type = PackageType.VIRTUAL_PYTHON_WHEEL

        paths_tups = pydist.get_paths()
        paths_data = PathsData(
            paths_version=1,
            paths=(
                PathDataV1(
                    _path=path,
                    path_type=PathType.hardlink,
                    sha256=checksum,
                    size_in_bytes=size,
                )
                for (path, checksum, size) in paths_tups
            ),
        )
        files = tuple(p[0] for p in paths_tups)

    elif isinstance(pydist, PythonEggLinkDistribution):
        channel = Channel("<develop>")
        build = "dev_0"
        package_type = PackageType.VIRTUAL_PYTHON_EGG_LINK

        paths_data, files = PathsData(paths_version=1, paths=()), ()

    elif isinstance(pydist, PythonEggInfoDistribution):
        channel = Channel("pypi")
        build = "pypi_0"
        if pydist.is_manageable:
            package_type = PackageType.VIRTUAL_PYTHON_EGG_MANAGEABLE

            paths_tups = pydist.get_paths()
            files = tuple(p[0] for p in paths_tups)
            paths_data = PathsData(
                paths_version=1,
                paths=(
                    PathData(_path=path, path_type=PathType.hardlink) for path in files
                ),
            )
        else:
            package_type = PackageType.VIRTUAL_PYTHON_EGG_UNMANAGEABLE
            paths_data, files = PathsData(paths_version=1, paths=()), ()

    else:
        raise NotImplementedError()

    return PrefixRecord(
        package_type=package_type,
        name=pydist.conda_name,
        version=pydist.version,
        channel=channel,
        subdir="pypi",
        fn=pydist.sp_reference,
        build=build,
        build_number=0,
        paths_data=paths_data,
        files=files,
        depends=depends,
        constrains=constrains,
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
import os
import sys
from errno import EACCES, EEXIST, ENOENT, ENOTEMPTY, EPERM, errorcode
from logging import getLogger
from os.path import basename, dirname, isdir
from subprocess import CalledProcessError
from time import sleep

from ...common.compat import on_win
from ...common.constants import TRACE

log = getLogger(__name__)

MAX_TRIES = 7


def exp_backoff_fn(fn, *args, **kwargs):
    """Mostly for retrying file operations that fail on Windows due to virus scanners"""
    max_tries = kwargs.pop("max_tries", MAX_TRIES)
    if not on_win:
        return fn(*args, **kwargs)

    import random

    # with max_tries = 6, max total time ~= 3.2 sec
    # with max_tries = 7, max total time ~= 6.5 sec

    def sleep_some(n, exc):
        if n == max_tries - 1:
            raise
        sleep_time = ((2**n) + random.random()) * 0.1
        caller_frame = sys._getframe(1)
        log.log(
            TRACE,
            "retrying %s/%s %s() in %g sec",
            basename(caller_frame.f_code.co_filename),
            caller_frame.f_lineno,
            fn.__name__,
            sleep_time,
        )
        sleep(sleep_time)

    for n in range(max_tries):
        try:
            result = fn(*args, **kwargs)
        except OSError as e:
            log.log(TRACE, repr(e))
            if e.errno in (EPERM, EACCES):
                sleep_some(n, e)
            elif e.errno in (ENOENT, ENOTEMPTY):
                # errno.ENOENT File not found error / No such file or directory
                # errno.ENOTEMPTY OSError(41, 'The directory is not empty')
                raise
            else:
                log.warning(
                    "Uncaught backoff with errno %s %d", errorcode[e.errno], e.errno
                )
                raise
        except CalledProcessError as e:
            sleep_some(n, e)
        else:
            return result


def mkdir_p(path):
    # putting this here to help with circular imports
    try:
        log.log(TRACE, "making directory %s", path)
        if path:
            os.makedirs(path)
            return isdir(path) and path
    except OSError as e:
        if e.errno == EEXIST and isdir(path):
            return path
        else:
            raise


def mkdir_p_sudo_safe(path):
    if isdir(path):
        return
    base_dir = dirname(path)
    if not isdir(base_dir):
        mkdir_p_sudo_safe(base_dir)
    log.log(TRACE, "making directory %s", path)
    try:
        os.mkdir(path)
    except OSError as e:
        if not (e.errno == EEXIST and isdir(path)):
            raise
    # # per the following issues, removing this code as of 4.6.0:
    # #   - https://github.com/conda/conda/issues/6569
    # #   - https://github.com/conda/conda/issues/6576
    # #   - https://github.com/conda/conda/issues/7109
    # if not on_win and os.environ.get('SUDO_UID') is not None:
    #     uid = int(os.environ['SUDO_UID'])
    #     gid = int(os.environ.get('SUDO_GID', -1))
    #     log.log(TRACE, "chowning %s:%s %s", uid, gid, path)
    #     os.chown(path, uid, gid)
    if not on_win:
        # set newly-created directory permissions to 02775
        # https://github.com/conda/conda/issues/6610#issuecomment-354478489
        try:
            os.chmod(path, 0o2775)
        except OSError as e:
            log.log(
                TRACE,
                "Failed to set permissions to 2775 on %s (%d %d)",
                path,
                e.errno,
                errorcode[e.errno],
            )
            pass


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Disk utility functions for modifying existing files or directories."""

from __future__ import annotations

import os
import re
import tempfile
from contextlib import contextmanager
from errno import EINVAL, EPERM, EXDEV
from logging import getLogger
from os.path import basename, dirname, exists, isdir, join, split
from shutil import move
from subprocess import PIPE, Popen

from ...base.constants import DRY_RUN_PREFIX
from ...base.context import context
from ...common.compat import on_win
from ...common.constants import TRACE
from ...common.path import expand
from ...exceptions import NotWritableError
from . import exp_backoff_fn, mkdir_p, mkdir_p_sudo_safe
from .delete import rm_rf
from .link import lexists

log = getLogger(__name__)

SHEBANG_REGEX = re.compile(rb"^(#!((?:\\ |[^ \n\r])+)(.*))")


class CancelOperation(Exception):
    pass


def update_file_in_place_as_binary(file_full_path, callback):
    # callback should be a callable that takes one positional argument, which is the
    #   content of the file before updating
    # this method updates the file in-place, without releasing the file lock
    fh = None
    try:
        fh = exp_backoff_fn(open, file_full_path, "rb+")
        log.log(TRACE, "in-place update path locked for %s", file_full_path)
        data = fh.read()
        fh.seek(0)
        try:
            fh.write(callback(data))
            fh.truncate()
            return True
        except CancelOperation:
            pass  # NOQA
    finally:
        if fh:
            fh.close()
    return False


def rename(source_path, destination_path, force=False):
    if lexists(destination_path) and force:
        rm_rf(destination_path)
    if lexists(source_path):
        log.log(TRACE, "renaming %s => %s", source_path, destination_path)
        try:
            os.rename(source_path, destination_path)
        except OSError as e:
            if (
                on_win
                and dirname(source_path) == dirname(destination_path)
                and os.path.isfile(source_path)
            ):
                condabin_dir = join(context.conda_prefix, "condabin")
                rename_script = join(condabin_dir, "rename_tmp.bat")
                if exists(rename_script):
                    _dirname, _src_fn = split(source_path)
                    _dest_fn = basename(destination_path)
                    p = Popen(
                        ["cmd.exe", "/C", rename_script, _dirname, _src_fn, _dest_fn],
                        stdout=PIPE,
                        stderr=PIPE,
                    )
                    stdout, stderr = p.communicate()
                else:
                    log.debug(
                        f"{rename_script} is missing.  Conda was not installed correctly or has been "
                        "corrupted.  Please file an issue on the conda github repo."
                    )
            elif e.errno in (EINVAL, EXDEV, EPERM):
                # https://github.com/conda/conda/issues/6811
                # https://github.com/conda/conda/issues/6711
                log.log(
                    TRACE,
                    "Could not rename %s => %s due to errno [%s]. Falling back"
                    " to copy/unlink",
                    source_path,
                    destination_path,
                    e.errno,
                )
                # https://github.com/moby/moby/issues/25409#issuecomment-238537855
                # shutil.move() falls back to copy+unlink
                move(source_path, destination_path)
            else:
                raise
    else:
        log.log(TRACE, "cannot rename; source path does not exist '%s'", source_path)


@contextmanager
def rename_context(source: str, destination: str | None = None, dry_run: bool = False):
    """
    Used for removing a directory when there are dependent actions (i.e. you need to ensure
    other actions succeed before removing it).

    Example:
        with rename_context(directory):
            # Do dependent actions here
    """
    if destination is None:
        destination = tempfile.mkdtemp()

    if dry_run:
        print(f"{DRY_RUN_PREFIX} rename_context {source} > {destination}")
        yield
        return

    try:
        rename(source, destination, force=True)
        yield
    except Exception as exc:
        # Error occurred, roll back change
        rename(destination, source, force=True)
        raise exc


def backoff_rename(source_path, destination_path, force=False):
    exp_backoff_fn(rename, source_path, destination_path, force)


def touch(path, mkdir=False, sudo_safe=False):
    # sudo_safe: use any time `path` is within the user's home directory
    # returns:
    #   True if the file did not exist but was created
    #   False if the file already existed
    # raises: NotWritableError, which is also an OSError having attached errno
    try:
        path = expand(path)
        log.log(TRACE, "touching path %s", path)
        if lexists(path):
            os.utime(path, None)
            return True
        else:
            dirpath = dirname(path)
            if not isdir(dirpath) and mkdir:
                if sudo_safe:
                    mkdir_p_sudo_safe(dirpath)
                else:
                    mkdir_p(dirpath)
            else:
                assert isdir(dirname(path))
            with open(path, "a"):
                pass
            # This chown call causes a false positive PermissionError to be
            # raised (similar to #7109) when called in an environment which
            # comes from sudo -u.
            #
            # if sudo_safe and not on_win and os.environ.get('SUDO_UID') is not None:
            #     uid = int(os.environ['SUDO_UID'])
            #     gid = int(os.environ.get('SUDO_GID', -1))
            #     log.log(TRACE, "chowning %s:%s %s", uid, gid, path)
            #     os.chown(path, uid, gid)
            return False
    except OSError as e:
        raise NotWritableError(path, e.errno, caused_by=e)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Backwards compatibility import.

Moved to prevent circular imports.
"""

from ..disk.lock import lock  # noqa: F401


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Repodata interface."""

from __future__ import annotations

import abc
import datetime
import errno
import hashlib
import json
import logging
import os
import pathlib
import re
import time
import warnings
from collections import UserDict
from contextlib import contextmanager
from os.path import dirname
from typing import TYPE_CHECKING

from ... import CondaError
from ...auxlib.logz import stringify
from ...base.constants import CONDA_HOMEPAGE_URL, REPODATA_FN
from ...base.context import context
from ...common.url import join_url, maybe_unquote
from ...core.package_cache_data import PackageCacheData
from ...exceptions import (
    CondaDependencyError,
    CondaHTTPError,
    CondaSSLError,
    NotWritableError,
    ProxyError,
    UnavailableInvalidChannel,
)
from ...models.channel import Channel
from ..connection import (
    ChunkedEncodingError,
    ConnectionError,
    HTTPError,
    InsecureRequestWarning,
    InvalidSchema,
    RequestsProxyError,
    SSLError,
)
from ..connection.session import get_session
from ..disk import mkdir_p_sudo_safe
from ..disk.lock import lock

if TYPE_CHECKING:
    from pathlib import Path
    from typing import Any

    from ..connection import Response

log = logging.getLogger(__name__)
stderrlog = logging.getLogger("conda.stderrlog")


# if repodata.json.zst or repodata.jlap were unavailable, check again later.
CHECK_ALTERNATE_FORMAT_INTERVAL = datetime.timedelta(days=7)

# repodata.info/state.json keys to keep up with the CEP
LAST_MODIFIED_KEY = "mod"
ETAG_KEY = "etag"
CACHE_CONTROL_KEY = "cache_control"
URL_KEY = "url"
CACHE_STATE_SUFFIX = ".info.json"

# show some unparseable json in error
ERROR_SNIPPET_LENGTH = 32


class RepodataIsEmpty(UnavailableInvalidChannel):
    """
    Subclass used to determine when empty repodata should be cached, e.g. for a
    channel that doesn't provide current_repodata.json
    """


class RepodataOnDisk(Exception):
    """
    Indicate that RepoInterface.repodata() successfully wrote repodata to disk,
    instead of returning a string.
    """


class RepoInterface(abc.ABC):
    # TODO: Support async operations
    # TODO: Support progress bars
    def repodata(self, state: dict) -> str:
        """
        Given a mutable state dictionary with information about the cache,
        return repodata.json (or current_repodata.json) as a str. This function
        also updates state, which is expected to be saved by the caller.
        """
        ...


class Response304ContentUnchanged(Exception):
    pass


def get_repo_interface() -> type[RepoInterface]:
    if "jlap" in context.experimental:
        try:
            from .jlap.interface import JlapRepoInterface

            return JlapRepoInterface
        except ImportError as e:  # pragma: no cover
            warnings.warn(
                "Could not load the configured jlap repo interface. "
                f"Is the required jsonpatch package installed?  {e}"
            )

    if context.repodata_use_zst:
        try:
            from .jlap.interface import ZstdRepoInterface

            return ZstdRepoInterface
        except ImportError:  # pragma: no cover
            pass

    return CondaRepoInterface


class CondaRepoInterface(RepoInterface):
    """Provides an interface for retrieving repodata data from channels."""

    #: Channel URL
    _url: str

    #: Filename of the repodata file; defaults to value of conda.base.constants.REPODATA_FN
    _repodata_fn: str

    def __init__(self, url: str, repodata_fn: str | None, **kwargs) -> None:
        log.debug("Using CondaRepoInterface")
        self._url = url
        self._repodata_fn = repodata_fn or REPODATA_FN

    def repodata(self, state: RepodataState) -> str | None:
        if not context.ssl_verify:
            warnings.simplefilter("ignore", InsecureRequestWarning)

        session = get_session(self._url)

        headers = {}
        etag = state.etag
        last_modified = state.mod
        if etag:
            headers["If-None-Match"] = str(etag)
        if last_modified:
            headers["If-Modified-Since"] = str(last_modified)
        filename = self._repodata_fn

        url = join_url(self._url, filename)

        with conda_http_errors(self._url, filename):
            timeout = (
                context.remote_connect_timeout_secs,
                context.remote_read_timeout_secs,
            )
            response: Response = session.get(
                url, headers=headers, proxies=session.proxies, timeout=timeout
            )
            if log.isEnabledFor(logging.DEBUG):
                log.debug(stringify(response, content_max_len=256))
            response.raise_for_status()

        if response.status_code == 304:
            # should we save cache-control to state here to put another n
            # seconds on the "make a remote request" clock and/or touch cache
            # mtime
            raise Response304ContentUnchanged()

        json_str = response.text

        # We no longer add these tags to the large `resp.content` json
        saved_fields = {"_url": self._url}
        _add_http_value_to_dict(response, "Etag", saved_fields, "_etag")
        _add_http_value_to_dict(response, "Last-Modified", saved_fields, "_mod")
        _add_http_value_to_dict(
            response, "Cache-Control", saved_fields, "_cache_control"
        )

        state.clear()
        state.update(saved_fields)

        return json_str


def _add_http_value_to_dict(resp, http_key, d, dict_key):
    value = resp.headers.get(http_key)
    if value:
        d[dict_key] = value


@contextmanager
def conda_http_errors(url, repodata_fn):
    """Use in a with: statement to translate requests exceptions to conda ones."""
    try:
        yield
    except RequestsProxyError:
        raise ProxyError()  # see #3962

    except InvalidSchema as e:
        if "SOCKS" in str(e):
            message = """\
Requests has identified that your current working environment is configured
to use a SOCKS proxy, but pysocks is not installed.  To proceed, remove your
proxy configuration, run `conda install pysocks`, and then you can re-enable
your proxy configuration.
"""
            raise CondaDependencyError(message)
        else:
            raise

    except SSLError as e:
        # SSLError: either an invalid certificate or OpenSSL is unavailable
        try:
            import ssl  # noqa: F401
        except ImportError:
            raise CondaSSLError(
                f"""\
OpenSSL appears to be unavailable on this machine. OpenSSL is required to
download and install packages.

Exception: {e}
"""
            )
        else:
            raise CondaSSLError(
                f"""\
Encountered an SSL error. Most likely a certificate verification issue.

Exception: {e}
"""
            )

    except (ConnectionError, HTTPError, ChunkedEncodingError) as e:
        status_code = getattr(e.response, "status_code", None)
        if status_code in (403, 404):
            if not url.endswith("/noarch"):
                log.info(
                    "Unable to retrieve repodata (response: %d) for %s",
                    status_code,
                    url + "/" + repodata_fn,
                )
                raise RepodataIsEmpty(
                    Channel(dirname(url)),
                    status_code,
                    response=e.response,
                )
            else:
                if context.allow_non_channel_urls:
                    stderrlog.warning(
                        "Unable to retrieve repodata (response: %d) for %s",
                        status_code,
                        url + "/" + repodata_fn,
                    )
                    raise RepodataIsEmpty(
                        Channel(dirname(url)),
                        status_code,
                        response=e.response,
                    )
                else:
                    raise UnavailableInvalidChannel(
                        Channel(dirname(url)),
                        status_code,
                        response=e.response,
                    )

        elif status_code == 401:
            channel = Channel(url)
            if channel.token:
                help_message = """\
The token '{}' given for the URL is invalid.

If this token was pulled from anaconda-client, you will need to use
anaconda-client to reauthenticate.

If you supplied this token to conda directly, you will need to adjust your
conda configuration to proceed.

Use `conda config --show` to view your configuration's current state.
Further configuration help can be found at <{}>.
""".format(
                    channel.token,
                    join_url(CONDA_HOMEPAGE_URL, "docs/config.html"),
                )

            elif context.channel_alias.location in url:
                # Note, this will not trigger if the binstar configured url does
                # not match the conda configured one.
                help_message = """\
The remote server has indicated you are using invalid credentials for this channel.

If the remote site is anaconda.org or follows the Anaconda Server API, you
will need to
    (a) remove the invalid token from your system with `anaconda logout`, optionally
        followed by collecting a new token with `anaconda login`, or
    (b) provide conda with a valid token directly.

Further configuration help can be found at <{}>.
""".format(join_url(CONDA_HOMEPAGE_URL, "docs/config.html"))

            else:
                help_message = """\
The credentials you have provided for this URL are invalid.

You will need to modify your conda configuration to proceed.
Use `conda config --show` to view your configuration's current state.
Further configuration help can be found at <{}>.
""".format(join_url(CONDA_HOMEPAGE_URL, "docs/config.html"))

        elif status_code is not None and 500 <= status_code < 600:
            help_message = """\
A remote server error occurred when trying to retrieve this URL.

A 500-type error (e.g. 500, 501, 502, 503, etc.) indicates the server failed to
fulfill a valid request.  The problem may be spurious, and will resolve itself if you
try your request again.  If the problem persists, consider notifying the maintainer
of the remote server.
"""

        else:
            if url.startswith("https://repo.anaconda.com/"):
                help_message = f"""\
An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.

If your current network has https://repo.anaconda.com blocked, please file
a support request with your network engineering team.

{maybe_unquote(repr(url))}
"""

            else:
                help_message = f"""\
An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.
{maybe_unquote(repr(url))}
"""

        raise CondaHTTPError(
            help_message,
            join_url(url, repodata_fn),
            status_code,
            getattr(e.response, "reason", None),
            getattr(e.response, "elapsed", None),
            e.response,
            caused_by=e,
        )


class RepodataState(UserDict):
    """Load/save info file that accompanies cached `repodata.json`."""

    # Accept old keys for new serialization
    _aliased = {
        "_mod": LAST_MODIFIED_KEY,
        "_etag": ETAG_KEY,
        "_cache_control": CACHE_CONTROL_KEY,
        "_url": URL_KEY,
    }

    # Enforce string type on these keys
    _strings = {"mod", "etag", "cache_control", "url"}

    def __init__(
        self,
        cache_path_json: Path | str = "",
        cache_path_state: Path | str = "",
        repodata_fn="",
        dict=None,
    ):
        # dict is a positional-only argument in UserDict.
        super().__init__(dict)
        self.cache_path_json = pathlib.Path(cache_path_json)
        self.cache_path_state = pathlib.Path(cache_path_state)
        # XXX may not be that useful/used compared to the full URL
        self.repodata_fn = repodata_fn

    @property
    def mod(self) -> str:
        """
        Last-Modified header or ""
        """
        return self.get(LAST_MODIFIED_KEY) or ""

    @mod.setter
    def mod(self, value):
        self[LAST_MODIFIED_KEY] = value or ""

    @property
    def etag(self) -> str:
        """
        Etag header or ""
        """
        return self.get(ETAG_KEY) or ""

    @etag.setter
    def etag(self, value):
        self[ETAG_KEY] = value or ""

    @property
    def cache_control(self) -> str:
        """
        Cache-Control header or ""
        """
        return self.get(CACHE_CONTROL_KEY) or ""

    @cache_control.setter
    def cache_control(self, value):
        self[CACHE_CONTROL_KEY] = value or ""

    def has_format(self, format: str) -> tuple[bool, datetime.datetime | None]:
        # "has_zst": {
        #     // UTC RFC3999 timestamp of when we last checked whether the file is available or not
        #     // in this case the `repodata.json.zst` file
        #     // Note: same format as conda TUF spec
        #     "last_checked": "2023-01-08T11:45:44Z",
        #     // false = unavailable, true = available
        #     "value": BOOLEAN
        # },

        key = f"has_{format}"
        if key not in self:
            return (True, None)  # we want to check by default

        try:
            obj = self[key]
            last_checked_str = obj["last_checked"]
            if last_checked_str.endswith("Z"):
                last_checked_str = f"{last_checked_str[:-1]}+00:00"
            last_checked = datetime.datetime.fromisoformat(last_checked_str)
            value = bool(obj["value"])
            return (value, last_checked)
        except (KeyError, ValueError, TypeError) as e:
            log.warning(
                f"error parsing `has_` object from `<cache key>{CACHE_STATE_SUFFIX}`",
                exc_info=e,
            )
            self.pop(key)

        return False, datetime.datetime.now(tz=datetime.timezone.utc)

    def set_has_format(self, format: str, value: bool):
        key = f"has_{format}"
        self[key] = {
            "last_checked": datetime.datetime.now(tz=datetime.timezone.utc).isoformat()[
                : -len("+00:00")
            ]
            + "Z",
            "value": value,
        }

    def clear_has_format(self, format: str):
        """Remove 'has_{format}' instead of setting to False."""
        key = f"has_{format}"
        self.pop(key, None)

    def should_check_format(self, format: str) -> bool:
        """Return True if named format should be attempted."""
        has, when = self.has_format(format)
        return (
            has is True
            or isinstance(when, datetime.datetime)
            and datetime.datetime.now(tz=datetime.timezone.utc) - when
            > CHECK_ALTERNATE_FORMAT_INTERVAL
        )

    def __contains__(self, key: str) -> bool:
        key = self._aliased.get(key, key)
        return super().__contains__(key)

    def __setitem__(self, key: str, item: Any) -> None:
        key = self._aliased.get(key, key)
        if key in self._strings and not isinstance(item, str):
            log.debug('Replaced non-str RepodataState[%s] with ""', key)
            item = ""
        return super().__setitem__(key, item)

    def __getitem__(self, key: str) -> Any:
        key = self._aliased.get(key, key)
        return super().__getitem__(key)


class RepodataCache:
    """
    Handle caching for a single repodata.json + repodata.info.json
    (<hex-string>*.json inside `dir`)

    Avoid race conditions while loading, saving repodata.json and cache state.
    """

    def __init__(self, base, repodata_fn):
        """
        base: directory and filename prefix for cache, e.g. /cache/dir/abc123;
        writes /cache/dir/abc123.json
        """
        cache_path_base = pathlib.Path(base)
        self.cache_dir = cache_path_base.parent
        self.name = cache_path_base.name
        # XXX can we skip repodata_fn or include the full url for debugging
        self.repodata_fn = repodata_fn
        self.state = RepodataState(
            self.cache_path_json, self.cache_path_state, repodata_fn
        )

    @property
    def cache_path_json(self):
        return pathlib.Path(
            self.cache_dir,
            self.name + ("1" if context.use_only_tar_bz2 else "") + ".json",
        )

    @property
    def cache_path_state(self):
        """Out-of-band etag and other state needed by the RepoInterface."""
        return self.cache_path_json.with_suffix(CACHE_STATE_SUFFIX)

    def load(self, *, state_only=False) -> str:
        # read state and repodata.json with locking

        # lock {CACHE_STATE_SUFFIX} file
        # read {CACHE_STATES_SUFFIX} file
        # read repodata.json
        # check stat, if wrong clear cache information

        with self.lock("r+") as state_file:
            # cannot use pathlib.read_text / write_text on any locked file, as
            # it will release the lock early
            state = json.loads(state_file.read())

            # json and state files should match. must read json before checking
            # stat (if json_data is to be trusted)
            if state_only:
                json_data = ""
            else:
                json_data = self.cache_path_json.read_text()

            json_stat = self.cache_path_json.stat()
            if not (
                state.get("mtime_ns") == json_stat.st_mtime_ns
                and state.get("size") == json_stat.st_size
            ):
                # clear mod, etag, cache_control to encourage re-download
                state.update(
                    {
                        ETAG_KEY: "",
                        LAST_MODIFIED_KEY: "",
                        CACHE_CONTROL_KEY: "",
                        "size": 0,
                    }
                )
            self.state.clear()
            self.state.update(
                state
            )  # will aliased _mod, _etag (not cleared above) pass through as mod, etag?

        return json_data

        # check repodata.json stat(); mtime_ns must equal value in
        # {CACHE_STATE_SUFFIX} file, or it is stale.
        # read repodata.json
        # check repodata.json stat() again: st_size, st_mtime_ns must be equal

        # repodata.json is okay - use it somewhere

        # repodata.json is not okay - maybe use it, but don't allow cache updates

        # unlock {CACHE_STATE_SUFFIX} file

        # also, add refresh_ns instead of touching repodata.json file

    def load_state(self):
        """
        Update self.state without reading repodata.json.

        Return self.state.
        """
        try:
            self.load(state_only=True)
        except (FileNotFoundError, json.JSONDecodeError) as e:
            if isinstance(e, json.JSONDecodeError):
                log.warning(f"{e.__class__.__name__} loading {self.cache_path_state}")
            self.state.clear()
        return self.state

    def save(self, data: str):
        """Write data to <repodata>.json cache path, synchronize state."""
        temp_path = self.cache_dir / f"{self.name}.{os.urandom(2).hex()}.tmp"

        try:
            with temp_path.open("x") as temp:  # exclusive mode, error if exists
                temp.write(data)

            return self.replace(temp_path)

        finally:
            try:
                temp_path.unlink()
            except OSError:
                pass

    def replace(self, temp_path: Path):
        """
        Rename path onto <repodata>.json path, synchronize state.

        Relies on path's mtime not changing on move. `temp_path` should be
        adjacent to `self.cache_path_json` to be on the same filesystem.
        """
        with self.lock() as state_file:
            # "a+" creates the file if necessary, does not trunctate file.
            state_file.seek(0)
            state_file.truncate()
            stat = temp_path.stat()
            # XXX make sure self.state has the correct etag, etc. for temp_path.
            # UserDict has inscrutable typing, which we ignore
            self.state["mtime_ns"] = stat.st_mtime_ns  # type: ignore
            self.state["size"] = stat.st_size  # type: ignore
            self.state["refresh_ns"] = time.time_ns()  # type: ignore
            try:
                temp_path.rename(self.cache_path_json)
            except FileExistsError:  # Windows
                self.cache_path_json.unlink()
                temp_path.rename(self.cache_path_json)
            state_file.write(json.dumps(dict(self.state), indent=2))

    def refresh(self, refresh_ns=0):
        """
        Update access time in cache info file to indicate a HTTP 304 Not Modified response.
        """
        # Note this is not thread-safe.
        with self.lock() as state_file:
            # "a+" creates the file if necessary, does not trunctate file.
            state_file.seek(0)
            state_file.truncate()
            self.state["refresh_ns"] = refresh_ns or time.time_ns()
            state_file.write(json.dumps(dict(self.state), indent=2))

    @contextmanager
    def lock(self, mode="a+"):
        """
        Lock .info.json file. Hold lock while modifying related files.

        mode: "a+" then seek(0) to write/create; "r+" to read.
        """
        with self.cache_path_state.open(mode) as state_file, lock(state_file):
            yield state_file

    def stale(self):
        """
        Compare state refresh_ns against cache control header and
        context.local_repodata_ttl.
        """
        if context.local_repodata_ttl > 1:
            max_age = context.local_repodata_ttl
        elif context.local_repodata_ttl == 1:
            max_age = get_cache_control_max_age(self.state.cache_control)
        else:
            max_age = 0

        max_age *= 10**9  # nanoseconds
        now = time.time_ns()
        refresh = self.state.get("refresh_ns", 0)
        return (now - refresh) > max_age

    def timeout(self):
        """
        Return number of seconds until cache times out (<= 0 if already timed
        out).
        """
        if context.local_repodata_ttl > 1:
            max_age = context.local_repodata_ttl
        elif context.local_repodata_ttl == 1:
            max_age = get_cache_control_max_age(self.state.cache_control)
        else:
            max_age = 0

        max_age *= 10**9  # nanoseconds
        now = time.time_ns()
        refresh = self.state.get("refresh_ns", 0)
        return ((now - refresh) + max_age) / 1e9


class RepodataFetch:
    """
    Combine RepodataCache and RepoInterface to provide subdir_data.SubdirData()
    with what it needs.

    Provide a variety of formats since some ``RepoInterface`` have to
    ``json.loads(...)`` anyway, and some clients don't need the Python data
    structure at all.
    """

    cache_path_base: Path
    channel: Channel
    repodata_fn: str
    url_w_subdir: str
    url_w_credentials: str
    repo_interface_cls: Any

    def __init__(
        self,
        cache_path_base: Path,
        channel: Channel,
        repodata_fn: str,
        *,
        repo_interface_cls,
    ):
        self.cache_path_base = cache_path_base
        self.channel = channel
        self.repodata_fn = repodata_fn

        self.url_w_subdir = self.channel.url(with_credentials=False) or ""
        self.url_w_credentials = self.channel.url(with_credentials=True) or ""

        self.repo_interface_cls = repo_interface_cls

    def fetch_latest_parsed(self) -> tuple[dict, RepodataState]:
        """
        Retrieve parsed latest or latest-cached repodata as a dict; update
        cache.

        :return: (repodata contents, state including cache headers)
        """
        parsed, state = self.fetch_latest()
        if isinstance(parsed, str):
            try:
                return json.loads(parsed), state
            except json.JSONDecodeError as e:
                e.args = (
                    f'{e.args[0]}; got "{parsed[:ERROR_SNIPPET_LENGTH]}"',
                    *e.args[1:],
                )
                raise
        else:
            return parsed, state

    def fetch_latest_path(self) -> tuple[Path, RepodataState]:
        """
        Retrieve latest or latest-cached repodata; update cache.

        :return: (pathlib.Path to uncompressed repodata contents, RepodataState)
        """
        _, state = self.fetch_latest()
        return self.cache_path_json, state

    @property
    def url_w_repodata_fn(self):
        return self.url_w_subdir + "/" + self.repodata_fn

    @property
    def cache_path_json(self):
        return self.repo_cache.cache_path_json

    @property
    def cache_path_state(self):
        """
        Out-of-band etag and other state needed by the RepoInterface.
        """
        return self.repo_cache.cache_path_state

    @property
    def repo_cache(self) -> RepodataCache:
        return RepodataCache(self.cache_path_base, self.repodata_fn)

    @property
    def _repo(self) -> RepoInterface:
        """
        Changes as we mutate self.repodata_fn.
        """
        return self.repo_interface_cls(
            self.url_w_credentials,
            repodata_fn=self.repodata_fn,
            cache=self.repo_cache,
        )

    def fetch_latest(self) -> tuple[dict | str, RepodataState]:
        """
        Return up-to-date repodata and cache information. Fetch repodata from
        remote if cache has expired; return cached data if cache has not
        expired; return stale cached data or dummy data if in offline mode.
        """
        cache = self.repo_cache
        cache.load_state()

        # XXX cache_path_json and cache_path_state must exist; just try loading
        # it and fall back to this on error?
        if not cache.cache_path_json.exists():
            log.debug(
                "No local cache found for %s at %s",
                self.url_w_repodata_fn,
                self.cache_path_json,
            )
            if context.use_index_cache or (
                context.offline and not self.url_w_subdir.startswith("file://")
            ):
                log.debug(
                    "Using cached data for %s at %s forced. Returning empty repodata.",
                    self.url_w_repodata_fn,
                    self.cache_path_json,
                )
                return (
                    {},
                    cache.state,
                )  # XXX basic properties like info, packages, packages.conda? instead of {}?

        else:
            if context.use_index_cache:
                log.debug(
                    "Using cached repodata for %s at %s because use_cache=True",
                    self.url_w_repodata_fn,
                    self.cache_path_json,
                )

                _internal_state = self.read_cache()
                return _internal_state

            stale = cache.stale()
            if (not stale or context.offline) and not self.url_w_subdir.startswith(
                "file://"
            ):
                timeout = cache.timeout()
                log.debug(
                    "Using cached repodata for %s at %s. Timeout in %d sec",
                    self.url_w_repodata_fn,
                    self.cache_path_json,
                    timeout,
                )
                _internal_state = self.read_cache()
                return _internal_state

            log.debug(
                "Local cache timed out for %s at %s",
                self.url_w_repodata_fn,
                self.cache_path_json,
            )

        try:
            try:
                repo = self._repo
                if hasattr(repo, "repodata_parsed"):
                    raw_repodata = repo.repodata_parsed(cache.state)  # type: ignore
                else:
                    raw_repodata = repo.repodata(cache.state)  # type: ignore
            except RepodataIsEmpty:
                if self.repodata_fn != REPODATA_FN:
                    raise  # is UnavailableInvalidChannel subclass
                # the surrounding try/except/else will cache "{}"
                raw_repodata = None
            except RepodataOnDisk:
                # used as a sentinel, not the raised exception object
                raw_repodata = RepodataOnDisk

        except Response304ContentUnchanged:
            log.debug(
                "304 NOT MODIFIED for '%s'. Updating mtime and loading from disk",
                self.url_w_repodata_fn,
            )
            cache.refresh()
            _internal_state = self.read_cache()
            return _internal_state
        else:
            try:
                if raw_repodata is RepodataOnDisk:
                    # this is handled very similar to a 304. Can the cases be merged?
                    # we may need to read_bytes() and compare a hash to the state, instead.
                    # XXX use self._repo_cache.load() or replace after passing temp path to jlap
                    raw_repodata = self.cache_path_json.read_text()
                    stat = self.cache_path_json.stat()
                    cache.state["size"] = stat.st_size  # type: ignore
                    mtime_ns = stat.st_mtime_ns
                    cache.state["mtime_ns"] = mtime_ns  # type: ignore
                    cache.refresh()
                elif isinstance(raw_repodata, dict):
                    # repo implementation cached it, and parsed it
                    # XXX check size upstream for locking reasons
                    stat = self.cache_path_json.stat()
                    cache.state["size"] = stat.st_size
                    mtime_ns = stat.st_mtime_ns
                    cache.state["mtime_ns"] = mtime_ns  # type: ignore
                    cache.refresh()
                elif isinstance(raw_repodata, (str, type(None))):
                    # Can we pass this information in state or with a sentinel/special exception?
                    if raw_repodata is None:
                        raw_repodata = "{}"
                    cache.save(raw_repodata)
                else:  # pragma: no cover
                    # it can be a dict?
                    assert False, f"Unreachable {raw_repodata}"
            except OSError as e:
                if e.errno in (errno.EACCES, errno.EPERM, errno.EROFS):
                    raise NotWritableError(self.cache_path_json, e.errno, caused_by=e)
                else:
                    raise

            return raw_repodata, cache.state

    def read_cache(self) -> tuple[str, RepodataState]:
        """
        Read repodata from disk, without trying to fetch a fresh version.
        """
        # pickled data is bad or doesn't exist; load cached json
        log.debug(
            "Loading raw json for %s at %s",
            self.url_w_repodata_fn,
            self.cache_path_json,
        )

        cache = self.repo_cache

        try:
            raw_repodata_str = cache.load()
            return raw_repodata_str, cache.state
        except ValueError as e:
            # OSError (locked) may happen here
            # ValueError: Expecting object: line 11750 column 6 (char 303397)
            log.debug("Error for cache path: '%s'\n%r", self.cache_path_json, e)
            message = """An error occurred when loading cached repodata.  Executing
`conda clean --index-cache` will remove cached repodata files
so they can be downloaded again."""
            raise CondaError(message)


try:
    hashlib.md5(b"", usedforsecurity=False)

    def _md5_not_for_security(data):
        return hashlib.md5(data, usedforsecurity=False)

except TypeError:  # pragma: no cover
    # Python < 3.9
    def _md5_not_for_security(data):
        return hashlib.md5(data)


def cache_fn_url(url, repodata_fn=REPODATA_FN):
    # url must be right-padded with '/' to not invalidate any existing caches
    if not url.endswith("/"):
        url += "/"
    # add the repodata_fn in for uniqueness, but keep it off for standard stuff.
    #    It would be more sane to add it for everything, but old programs (Navigator)
    #    are looking for the cache under keys without this.
    if repodata_fn != REPODATA_FN:
        url += repodata_fn

    md5 = _md5_not_for_security(url.encode("utf-8"))
    return f"{md5.hexdigest()[:8]}.json"


def get_cache_control_max_age(cache_control_value: str | None):
    cache_control_value = cache_control_value or ""
    max_age = re.search(r"max-age=(\d+)", cache_control_value)
    return int(max_age.groups()[0]) if max_age else 0


def create_cache_dir():
    cache_dir = os.path.join(PackageCacheData.first_writable().pkgs_dir, "cache")
    mkdir_p_sudo_safe(cache_dir)
    return cache_dir


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""JLAP reader."""

from __future__ import annotations

import logging
from collections import UserList
from hashlib import blake2b
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from typing import Iterable, Iterator

log = logging.getLogger(__name__)


DIGEST_SIZE = 32  # 160 bits a minimum 'for security' length?
DEFAULT_IV = b"\0" * DIGEST_SIZE


def keyed_hash(data: bytes, key: bytes):
    """Keyed hash."""
    return blake2b(data, key=key, digest_size=DIGEST_SIZE)


def line_and_pos(lines: Iterable[bytes], pos=0) -> Iterator[tuple[int, bytes]]:
    r"""
    :param lines: iterator over input split by '\n', with '\n' removed.
    :param pos: initial position
    """
    for line in lines:
        yield pos, line
        pos += len(line) + 1


class JLAP(UserList):
    @classmethod
    def from_lines(cls, lines: Iterable[bytes], iv: bytes, pos=0, verify=True):
        r"""
        :param lines: iterator over input split by b'\n', with b'\n' removed
        :param pos: initial position
        :param iv: initialization vector (first line of .jlap stream, hex
            decoded). Ignored if pos==0.
        :param verify: assert last line equals computed checksum of previous
            line. Useful for writing new .jlap files if False.

        :raises ValueError: if trailing and computed checksums do not match

        :return: list of (offset, line, checksum)
        """
        # save initial iv in case there were no new lines
        buffer: list[tuple[int, str, str]] = [(-1, iv.hex(), iv.hex())]
        initial_pos = pos

        for pos, line in line_and_pos(lines, pos=pos):
            if pos == 0:
                iv = bytes.fromhex(line.decode("utf-8"))
                buffer = [(0, iv.hex(), iv.hex())]
            else:
                iv = keyed_hash(line, iv).digest()
                buffer.append((pos, line.decode("utf-8"), iv.hex()))

        log.debug("%d bytes read", pos - initial_pos)  # maybe + length of last line

        if verify:
            if buffer[-1][1] != buffer[-2][-1]:
                raise ValueError("checksum mismatch")
            else:
                log.info("Checksum OK")

        return cls(buffer)

    @classmethod
    def from_path(cls, path: Path | str, verify=True):
        # in binary mode, line separator is hardcoded as \n
        with Path(path).open("rb") as p:
            return cls.from_lines(
                (line.rstrip(b"\n") for line in p), b"", verify=verify
            )

    def add(self, line: str):
        """
        Add line to buffer, following checksum rules.

        Buffer must not be empty.

        (Remember to pop trailing checksum and possibly trailing metadata line, if
        appending to a complete jlap file)

        Less efficient than creating a new buffer from many lines and our last iv,
        and extending.

        :return: self
        """
        if "\n" in line:
            raise ValueError("\\n not allowed in line")
        pos, last_line, iv = self[-1]
        # include last line's utf-8 encoded length, plus 1 in pos?
        pos += len(last_line.encode("utf-8")) + 1
        self.extend(
            JLAP.from_lines(
                (line.encode("utf-8"),), bytes.fromhex(iv), pos, verify=False
            )[1:]
        )
        return self

    def terminate(self):
        """
        Add trailing checksum to buffer.

        :return: self
        """
        _, _, iv = self[-1]
        self.add(iv)
        return self

    def write(self, path: Path):
        """Write buffer to path."""
        with Path(path).open("w", encoding="utf-8", newline="\n") as p:
            return p.write("\n".join(b[1] for b in self))

    @property
    def body(self):
        """All lines except the first, and last two."""
        return self[1:-2]

    @property
    def penultimate(self):
        """Next-to-last line. Should contain the footer."""
        return self[-2]

    @property
    def last(self):
        """Last line. Should contain the trailing checksum."""
        return self[-1]


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""JLAP consumer."""

from __future__ import annotations

import io
import json
import logging
import pprint
import re
import time
from contextlib import contextmanager
from hashlib import blake2b
from typing import TYPE_CHECKING

import jsonpatch
import zstandard
from requests import HTTPError

from conda.common.url import mask_anaconda_token

from ....base.context import context
from .. import ETAG_KEY, LAST_MODIFIED_KEY, RepodataState
from .core import JLAP

if TYPE_CHECKING:
    import pathlib
    from typing import Iterator

    from ...connection import Response, Session
    from .. import RepodataCache

log = logging.getLogger(__name__)


DIGEST_SIZE = 32  # 256 bits

JLAP_KEY = "jlap"
HEADERS = "headers"
NOMINAL_HASH = "blake2_256_nominal"
ON_DISK_HASH = "blake2_256"
LATEST = "latest"

# save these headers. at least etag, last-modified, cache-control plus a few
# useful extras.
STORE_HEADERS = {
    "etag",
    "last-modified",
    "cache-control",
    "content-range",
    "content-length",
    "date",
    "content-type",
    "content-encoding",
}


def hash():
    """Ordinary hash."""
    return blake2b(digest_size=DIGEST_SIZE)


class Jlap304NotModified(Exception):
    pass


class JlapSkipZst(Exception):
    pass


class JlapPatchNotFound(LookupError):
    pass


def process_jlap_response(response: Response, pos=0, iv=b""):
    # if response is 304 Not Modified, could return a buffer with only the
    # cached footer...
    if response.status_code == 304:
        raise Jlap304NotModified()

    def lines() -> Iterator[bytes]:
        yield from response.iter_lines(delimiter=b"\n")  # type: ignore

    buffer = JLAP.from_lines(lines(), iv, pos)

    # new iv == initial iv if nothing changed
    pos, footer, _ = buffer[-2]
    footer = json.loads(footer)

    new_state = {
        # we need to save etag, last-modified, cache-control
        "headers": {
            k.lower(): v
            for k, v in response.headers.items()
            if k.lower() in STORE_HEADERS
        },
        "iv": buffer[-3][-1],
        "pos": pos,
        "footer": footer,
    }

    return buffer, new_state


def fetch_jlap(url, pos=0, etag=None, iv=b"", ignore_etag=True, session=None):
    response = request_jlap(
        url, pos=pos, etag=etag, ignore_etag=ignore_etag, session=session
    )
    return process_jlap_response(response, pos=pos, iv=iv)


def request_jlap(
    url, pos=0, etag=None, ignore_etag=True, session: Session | None = None
):
    """Return the part of the remote .jlap file we are interested in."""
    headers = {}
    if pos:
        headers["range"] = f"bytes={pos}-"
    if etag and not ignore_etag:
        headers["if-none-match"] = etag

    log.debug("%s %s", mask_anaconda_token(url), headers)

    assert session is not None

    timeout = context.remote_connect_timeout_secs, context.remote_read_timeout_secs
    response = session.get(url, stream=True, headers=headers, timeout=timeout)
    response.raise_for_status()

    if response.request:
        log.debug("request headers: %s", pprint.pformat(response.request.headers))
    else:
        log.debug("response without request.")
    log.debug(
        "response headers: %s",
        pprint.pformat(
            {k: v for k, v in response.headers.items() if k.lower() in STORE_HEADERS}
        ),
    )
    log.debug("status: %d", response.status_code)
    if "range" in headers:
        # 200 is also a possibility that we'd rather not deal with; if the
        # server can't do range requests, also mark jlap as unavailable. Which
        # status codes mean 'try again' instead of 'it will never work'?
        if response.status_code not in (206, 304, 404, 416):
            raise HTTPError(
                f"Unexpected response code for range request {response.status_code}",
                response=response,
            )

    log.info("%s", response)

    return response


def format_hash(hash):
    """Abbreviate hash for formatting."""
    return hash[:16] + "\N{HORIZONTAL ELLIPSIS}"


def find_patches(patches, have, want):
    apply = []
    for patch in reversed(patches):
        if have == want:
            break
        if patch["to"] == want:
            apply.append(patch)
            want = patch["from"]

    if have != want:
        log.debug(f"No patch from local revision {format_hash(have)}")
        raise JlapPatchNotFound(f"No patch from local revision {format_hash(have)}")

    return apply


def apply_patches(data, apply):
    while apply:
        patch = apply.pop()
        log.debug(
            f"{format_hash(patch['from'])} \N{RIGHTWARDS ARROW} {format_hash(patch['to'])}, "
            f"{len(patch['patch'])} steps"
        )
        data = jsonpatch.JsonPatch(patch["patch"]).apply(data, in_place=True)


def withext(url, ext):
    return re.sub(r"(\.\w+)$", ext, url)


@contextmanager
def timeme(message):
    begin = time.monotonic()
    yield
    end = time.monotonic()
    log.debug("%sTook %0.02fs", message, end - begin)


def build_headers(json_path: pathlib.Path, state: RepodataState):
    """Caching headers for a path and state."""
    headers = {}
    # simplify if we require state to be empty when json_path is missing.
    if json_path.exists():
        etag = state.get("_etag")
        if etag:
            headers["if-none-match"] = etag
    return headers


class HashWriter(io.RawIOBase):
    def __init__(self, backing, hasher):
        self.backing = backing
        self.hasher = hasher

    def write(self, b: bytes):
        self.hasher.update(b)
        return self.backing.write(b)

    def close(self):
        self.backing.close()


def download_and_hash(
    hasher,
    url,
    json_path: pathlib.Path,
    session: Session,
    state: RepodataState | None,
    is_zst=False,
    dest_path: pathlib.Path | None = None,
):
    """Download url if it doesn't exist, passing bytes through hasher.update().

    json_path: Path of old cached data (ignore etag if not exists).
    dest_path: Path to write new data.
    """
    if dest_path is None:
        dest_path = json_path
    state = state or RepodataState()
    headers = build_headers(json_path, state)
    timeout = context.remote_connect_timeout_secs, context.remote_read_timeout_secs
    response = session.get(url, stream=True, timeout=timeout, headers=headers)
    log.debug("%s %s", url, response.headers)
    response.raise_for_status()
    length = 0
    # is there a status code for which we must clear the file?
    if response.status_code == 200:
        if is_zst:
            decompressor = zstandard.ZstdDecompressor()
            writer = decompressor.stream_writer(
                HashWriter(dest_path.open("wb"), hasher),  # type: ignore
                closefd=True,
            )
        else:
            writer = HashWriter(dest_path.open("wb"), hasher)
        with writer as repodata:
            for block in response.iter_content(chunk_size=1 << 14):
                repodata.write(block)
    if response.request:
        try:
            length = int(response.headers["Content-Length"])
        except (KeyError, ValueError, AttributeError):
            pass
        log.info("Download %d bytes %r", length, response.request.headers)
    return response  # can be 304 not modified


def _is_http_error_most_400_codes(e: HTTPError) -> bool:
    """
    Determine whether the `HTTPError` is an HTTP 400 error code (except for 416).
    """
    if e.response is None:  # 404 e.response is falsey
        return False
    status_code = e.response.status_code
    return 400 <= status_code < 500 and status_code != 416


def request_url_jlap_state(
    url,
    state: RepodataState,
    full_download=False,
    *,
    session: Session,
    cache: RepodataCache,
    temp_path: pathlib.Path,
) -> dict | None:
    jlap_state = state.get(JLAP_KEY, {})
    headers = jlap_state.get(HEADERS, {})
    json_path = cache.cache_path_json

    buffer = JLAP()  # type checks

    if (
        full_download
        or not (NOMINAL_HASH in state and json_path.exists())
        or not state.should_check_format("jlap")
    ):
        hasher = hash()
        with timeme(f"Download complete {url} "):
            # Don't deal with 304 Not Modified if hash unavailable e.g. if
            # cached without jlap
            if NOMINAL_HASH not in state:
                state.pop(ETAG_KEY, None)
                state.pop(LAST_MODIFIED_KEY, None)

            try:
                if state.should_check_format("zst"):
                    response = download_and_hash(
                        hasher,
                        withext(url, ".json.zst"),
                        json_path,  # makes conditional request if exists
                        dest_path=temp_path,  # writes to
                        session=session,
                        state=state,
                        is_zst=True,
                    )
                else:
                    raise JlapSkipZst()
            except (JlapSkipZst, HTTPError, zstandard.ZstdError) as e:
                if isinstance(e, zstandard.ZstdError):
                    log.warning(
                        "Could not decompress %s as zstd. Fall back to .json. (%s)",
                        mask_anaconda_token(withext(url, ".json.zst")),
                        e,
                    )
                if isinstance(e, HTTPError) and not _is_http_error_most_400_codes(e):
                    raise
                if not isinstance(e, JlapSkipZst):
                    # don't update last-checked timestamp on skip
                    state.set_has_format("zst", False)
                response = download_and_hash(
                    hasher,
                    withext(url, ".json"),
                    json_path,
                    dest_path=temp_path,
                    session=session,
                    state=state,
                )

            # will we use state['headers'] for caching against
            state["_mod"] = response.headers.get("last-modified")
            state["_etag"] = response.headers.get("etag")
            state["_cache_control"] = response.headers.get("cache-control")

        # was not re-hashed if 304 not modified
        if response.status_code == 200:
            state[NOMINAL_HASH] = state[ON_DISK_HASH] = hasher.hexdigest()

        have = state[NOMINAL_HASH]

        # a jlap buffer with zero patches. the buffer format is (position,
        # payload, checksum) where position is the offset from the beginning of
        # the file; payload is the leading or trailing checksum or other data;
        # and checksum is the running checksum for the file up to that point.
        buffer = JLAP([[-1, "", ""], [0, json.dumps({LATEST: have}), ""], [1, "", ""]])

    else:
        have = state[NOMINAL_HASH]
        # have_hash = state.get(ON_DISK_HASH)

        need_jlap = True
        try:
            iv_hex = jlap_state.get("iv", "")
            pos = jlap_state.get("pos", 0)
            etag = headers.get(ETAG_KEY, None)
            jlap_url = withext(url, ".jlap")
            log.debug(
                "Fetch %s from iv=%s, pos=%s",
                mask_anaconda_token(jlap_url),
                iv_hex,
                pos,
            )
            # wrong to read state outside of function, and totally rebuild inside
            buffer, jlap_state = fetch_jlap(
                jlap_url,
                pos=pos,
                etag=etag,
                iv=bytes.fromhex(iv_hex),
                session=session,
                ignore_etag=False,
            )
            state.set_has_format("jlap", True)
            need_jlap = False
        except ValueError:
            log.info("Checksum not OK on JLAP range request. Retry with complete JLAP.")
        except IndexError:
            log.exception("IndexError reading JLAP. Invalid file?")
        except HTTPError as e:
            # If we get a 416 Requested range not satisfiable, the server-side
            # file may have been truncated and we need to fetch from 0
            if _is_http_error_most_400_codes(e):
                state.set_has_format("jlap", False)
                return request_url_jlap_state(
                    url,
                    state,
                    full_download=True,
                    session=session,
                    cache=cache,
                    temp_path=temp_path,
                )
            log.info(
                "Response code %d on JLAP range request. Retry with complete JLAP.",
                e.response.status_code,
            )

        if need_jlap:  # retry whole file, if range failed
            try:
                buffer, jlap_state = fetch_jlap(withext(url, ".jlap"), session=session)
            except (ValueError, IndexError) as e:
                log.exception("Error parsing jlap", exc_info=e)
                # a 'latest' hash that we can't achieve, triggering later error handling
                buffer = JLAP(
                    [[-1, "", ""], [0, json.dumps({LATEST: "0" * 32}), ""], [1, "", ""]]
                )
                state.set_has_format("jlap", False)

        state[JLAP_KEY] = jlap_state

    with timeme("Apply Patches "):
        # buffer[0] == previous iv
        # buffer[1:-2] == patches
        # buffer[-2] == footer = new_state["footer"]
        # buffer[-1] == trailing checksum

        patches = list(json.loads(patch) for _, patch, _ in buffer.body)
        _, footer, _ = buffer.penultimate
        want = json.loads(footer)["latest"]

        try:
            apply = find_patches(patches, have, want)
            log.info(
                f"Apply {len(apply)} patches "
                f"{format_hash(have)} \N{RIGHTWARDS ARROW} {format_hash(want)}"
            )

            if apply:
                with timeme("Load "):
                    # we haven't loaded repodata yet; it could fail to parse, or
                    # have the wrong hash.
                    # if this fails, then we also need to fetch again from 0
                    repodata_json = json.loads(cache.load())
                    # XXX cache.state must equal what we started with, otherwise
                    # bail with 'repodata on disk' (indicating another process
                    # downloaded repodata.json in parallel with us)
                    if have != cache.state.get(NOMINAL_HASH):  # or check mtime_ns?
                        log.warning("repodata cache changed during jlap fetch.")
                        return None

                apply_patches(repodata_json, apply)

                with timeme("Write changed "), temp_path.open("wb") as repodata:
                    hasher = hash()
                    HashWriter(repodata, hasher).write(
                        json.dumps(repodata_json, separators=(",", ":")).encode("utf-8")
                    )

                    # actual hash of serialized json
                    state[ON_DISK_HASH] = hasher.hexdigest()

                    # hash of equivalent upstream json
                    state[NOMINAL_HASH] = want

                    # avoid duplicate parsing
                    return repodata_json
            else:
                assert state[NOMINAL_HASH] == want

        except (JlapPatchNotFound, json.JSONDecodeError) as e:
            if isinstance(e, JlapPatchNotFound):
                # 'have' hash not mentioned in patchset
                #
                # XXX or skip jlap at top of fn; make sure it is not
                # possible to download the complete json twice
                log.info(
                    "Current repodata.json %s not found in patchset. Re-download repodata.json"
                )

            assert not full_download, "Recursion error"  # pragma: no cover

            return request_url_jlap_state(
                url,
                state,
                full_download=True,
                session=session,
                cache=cache,
                temp_path=temp_path,
            )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Incremental repodata feature based on .jlap patch files."""


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""JLAP interface for repodata."""

from __future__ import annotations

import logging
import os
from typing import TYPE_CHECKING

from ....base.context import context
from ...connection.download import disable_ssl_verify_warning
from ...connection.session import get_session
from .. import (
    CACHE_CONTROL_KEY,
    ETAG_KEY,
    LAST_MODIFIED_KEY,
    URL_KEY,
    RepodataOnDisk,
    RepodataState,
    RepoInterface,
    Response304ContentUnchanged,
    conda_http_errors,
)
from . import fetch

if TYPE_CHECKING:
    from .. import RepodataCache

log = logging.getLogger(__name__)


class JlapRepoInterface(RepoInterface):
    def __init__(
        self,
        url: str,
        repodata_fn: str | None,
        *,
        cache: RepodataCache,
        **kwargs,
    ) -> None:
        log.debug("Using %s", self.__class__.__name__)

        self._cache = cache

        self._url = url
        self._repodata_fn = repodata_fn

        self._log = logging.getLogger(__name__)
        self._stderrlog = logging.getLogger("conda.stderrlog")

    def repodata(self, state: dict | RepodataState) -> str | None:
        """
        Fetch newest repodata if necessary.

        Always writes to ``cache_path_json``.
        """
        self.repodata_parsed(state)
        raise RepodataOnDisk()

    def repodata_parsed(self, state: dict | RepodataState) -> dict | None:
        """
        JLAP has to parse the JSON anyway.

        Use this to avoid a redundant parse when repodata is updated.

        When repodata is not updated, it doesn't matter whether this function or
        the caller reads from a file.
        """
        session = get_session(self._url)

        if not context.ssl_verify:
            disable_ssl_verify_warning()

        repodata_url = f"{self._url}/{self._repodata_fn}"

        # XXX won't modify caller's state dict
        state_ = self._repodata_state_copy(state)

        # at this point, self._cache.state == state == state_

        temp_path = (
            self._cache.cache_dir / f"{self._cache.name}.{os.urandom(2).hex()}.tmp"
        )
        try:
            with conda_http_errors(self._url, self._repodata_fn):
                repodata_json_or_none = fetch.request_url_jlap_state(
                    repodata_url,
                    state_,
                    session=session,
                    cache=self._cache,
                    temp_path=temp_path,
                )

                # update caller's state dict-or-RepodataState. Do this before
                # the self._cache.replace() call which also writes state, then
                # signal not to write state to caller.
                state.update(state_)

                state[URL_KEY] = self._url
                headers = state.get("jlap", {}).get(
                    "headers"
                )  # XXX overwrite headers in jlapper.request_url_jlap_state
                if headers:
                    state[ETAG_KEY] = headers.get("etag")
                    state[LAST_MODIFIED_KEY] = headers.get("last-modified")
                    state[CACHE_CONTROL_KEY] = headers.get("cache-control")

                self._cache.state.update(state)

            if temp_path.exists():
                self._cache.replace(temp_path)
        except fetch.Jlap304NotModified:
            raise Response304ContentUnchanged()
        finally:
            # Clean up the temporary file. In the successful case it raises
            # OSError as self._cache_replace() removed temp_file.
            try:
                temp_path.unlink()
            except OSError:
                pass

        if repodata_json_or_none is None:  # common
            # Indicate that subdir_data mustn't rewrite cache_path_json
            raise RepodataOnDisk()
        else:
            return repodata_json_or_none

    def _repodata_state_copy(self, state: dict | RepodataState):
        return RepodataState(dict=state)


class RepodataStateSkipFormat(RepodataState):
    skip_formats: set[str]

    def __init__(self, *args, skip_formats=set(), **kwargs):
        super().__init__(*args, **kwargs)
        self.skip_formats = set(skip_formats)

    def should_check_format(self, format):
        if format in self.skip_formats:
            return False
        return super().should_check_format(format)


class ZstdRepoInterface(JlapRepoInterface):
    """
    Support repodata.json.zst (if available) without checking .jlap
    """

    def _repodata_state_copy(self, state: dict | RepodataState):
        return RepodataStateSkipFormat(dict=state, skip_formats=["jlap"])


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Interface between conda-content-trust and conda."""

from __future__ import annotations

import json
import os
import re
import warnings
from functools import lru_cache
from logging import getLogger
from pathlib import Path

try:
    from conda_content_trust.authentication import verify_delegation, verify_root
    from conda_content_trust.common import (
        SignatureError,
        load_metadata_from_file,
        write_metadata_to_file,
    )
    from conda_content_trust.signing import wrap_as_signable
except ImportError:
    # _SignatureVerification.enabled handles the rest of this state
    class SignatureError(Exception):
        pass


from typing import TYPE_CHECKING

from ..base.constants import CONDA_PACKAGE_EXTENSION_V1, CONDA_PACKAGE_EXTENSION_V2
from ..base.context import context
from ..common.url import join_url
from ..core.subdir_data import SubdirData
from ..gateways.connection import HTTPError, InsecureRequestWarning
from ..gateways.connection.session import get_session
from .constants import INITIAL_TRUST_ROOT, KEY_MGR_FILE

if TYPE_CHECKING:
    from ..models.records import PackageRecord

log = getLogger(__name__)


RE_ROOT_METADATA = re.compile(r"(?P<number>\d+)\.root\.json")


class _SignatureVerification:
    # FUTURE: Python 3.8+, replace with functools.cached_property
    @property
    @lru_cache(maxsize=None)
    def enabled(self) -> bool:
        # safety checks must be enabled
        if not context.extra_safety_checks:
            return False

        # signing url must be defined
        if not context.signing_metadata_url_base:
            log.warning(
                "metadata signature verification requested, "
                "but no metadata URL base has not been specified."
            )
            return False

        # conda_content_trust must be installed
        try:
            import conda_content_trust  # noqa: F401
        except ImportError:
            log.warning(
                "metadata signature verification requested, "
                "but `conda-content-trust` is not installed."
            )
            return False

        # ensure artifact verification directory exists
        Path(context.av_data_dir).mkdir(parents=True, exist_ok=True)

        # ensure the trusted_root exists
        if self.trusted_root is None:
            log.warning(
                "could not find trusted_root data for metadata signature verification"
            )
            return False

        # ensure the key_mgr exists
        if self.key_mgr is None:
            log.warning(
                "could not find key_mgr data for metadata signature verification"
            )
            return False

        # signature verification is enabled
        return True

    # FUTURE: Python 3.8+, replace with functools.cached_property
    @property
    @lru_cache(maxsize=None)
    def trusted_root(self) -> dict:
        # TODO: formalize paths for `*.root.json` and `key_mgr.json` on server-side
        trusted: dict | None = None

        # Load latest trust root metadata from filesystem
        try:
            paths = {
                int(m.group("number")): entry
                for entry in os.scandir(context.av_data_dir)
                if (m := RE_ROOT_METADATA.match(entry.name))
            }
        except (FileNotFoundError, NotADirectoryError, PermissionError):
            # FileNotFoundError: context.av_data_dir does not exist
            # NotADirectoryError: context.av_data_dir is not a directory
            # PermsissionError: context.av_data_dir is not readable
            pass
        else:
            for _, entry in sorted(paths.items(), reverse=True):
                log.info(f"Loading root metadata from {entry}.")
                try:
                    trusted = load_metadata_from_file(entry)
                except (IsADirectoryError, FileNotFoundError, PermissionError):
                    # IsADirectoryError: entry is not a file
                    # FileNotFoundError: entry does not exist
                    # PermsissionError: entry is not readable
                    continue
                else:
                    break

        # Fallback to default root metadata if unable to fetch any
        if not trusted:
            log.debug(
                f"No root metadata in {context.av_data_dir}. "
                "Using built-in root metadata."
            )
            trusted = INITIAL_TRUST_ROOT

        # Refresh trust root metadata
        while True:
            # TODO: caching mechanism to reduce number of refresh requests
            fname = f"{trusted['signed']['version'] + 1}.root.json"
            path = Path(context.av_data_dir, fname)

            try:
                # TODO: support fetching root data with credentials
                untrusted = self._fetch_channel_signing_data(
                    context.signing_metadata_url_base,
                    fname,
                )

                verify_root(trusted, untrusted)
            except HTTPError as err:
                # HTTP 404 implies no updated root.json is available, which is
                # not really an "error" and does not need to be logged.
                if err.response.status_code != 404:
                    log.error(err)
                break
            except Exception as err:
                # TODO: more error handling
                log.error(err)
                break
            else:
                # New trust root metadata checks out
                write_metadata_to_file(trusted := untrusted, path)

        return trusted

    # FUTURE: Python 3.8+, replace with functools.cached_property
    @property
    @lru_cache(maxsize=None)
    def key_mgr(self) -> dict | None:
        trusted: dict | None = None

        # Refresh key manager metadata
        fname = KEY_MGR_FILE
        path = Path(context.av_data_dir, fname)

        try:
            untrusted = self._fetch_channel_signing_data(
                context.signing_metadata_url_base,
                fname,
            )

            verify_delegation("key_mgr", untrusted, self.trusted_root)
        except ConnectionError as err:
            log.warning(err)
        except HTTPError as err:
            # sometimes the HTTPError message is blank, when that occurs include the
            # HTTP status code
            log.warning(
                str(err) or f"{err.__class__.__name__} ({err.response.status_code})"
            )
        else:
            # New key manager metadata checks out
            write_metadata_to_file(trusted := untrusted, path)

        # If key_mgr is unavailable from server, fall back to copy on disk
        if not trusted and path.exists():
            trusted = load_metadata_from_file(path)

        return trusted

    def _fetch_channel_signing_data(
        self, signing_data_url: str, filename: str, etag=None, mod_stamp=None
    ) -> dict:
        session = get_session(signing_data_url)

        if not context.ssl_verify:
            warnings.simplefilter("ignore", InsecureRequestWarning)

        headers = {
            "Accept-Encoding": "gzip, deflate, compress, identity",
            "Content-Type": "application/json",
        }
        if etag:
            headers["If-None-Match"] = etag
        if mod_stamp:
            headers["If-Modified-Since"] = mod_stamp

        saved_token_setting = context.add_anaconda_token
        try:
            # Assume trust metadata is intended to be "generally available",
            # and specifically, _not_ protected by a conda/binstar token.
            # Seems reasonable, since we (probably) don't want the headaches of
            # dealing with protected, per-channel trust metadata.
            #
            # Note: Setting `auth=None` here does allow trust metadata to be
            # protected using standard HTTP basic auth mechanisms, with the
            # login information being provided in the user's netrc file.
            context.add_anaconda_token = False
            resp = session.get(
                join_url(signing_data_url, filename),
                headers=headers,
                proxies=session.proxies,
                auth=None,
                timeout=(
                    context.remote_connect_timeout_secs,
                    context.remote_read_timeout_secs,
                ),
            )
            # TODO: maybe add more sensible error handling
            resp.raise_for_status()
        finally:
            context.add_anaconda_token = saved_token_setting

        # In certain cases (e.g., using `-c` access anaconda.org channels), the
        # `CondaSession.get()` retry logic combined with the remote server's
        # behavior can result in non-JSON content being returned.  Parse returned
        # content here (rather than directly in the return statement) so callers of
        # this function only have to worry about a ValueError being raised.
        try:
            return resp.json()
        except json.decoder.JSONDecodeError as err:  # noqa
            # TODO: additional loading and error handling improvements?
            raise ValueError(
                f"Invalid JSON returned from {signing_data_url}/{filename}"
            )

    def verify(self, repodata_fn: str, record: PackageRecord):
        repodata, _ = SubdirData(
            record.channel,
            repodata_fn=repodata_fn,
        ).repo_fetch.fetch_latest_parsed()

        # short-circuit if no signatures are defined
        if "signatures" not in repodata:
            record.metadata.add(
                f"(no signatures found for {record.channel.canonical_name})"
            )
            return
        signatures = repodata["signatures"]

        # short-circuit if no signature is defined for this package
        if record.fn not in signatures:
            record.metadata.add(f"(no signatures found for {record.fn})")
            return
        signature = signatures[record.fn]

        # extract metadata to be verified
        if record.fn.endswith(CONDA_PACKAGE_EXTENSION_V1):
            info = repodata["packages"][record.fn]
        elif record.fn.endswith(CONDA_PACKAGE_EXTENSION_V2):
            info = repodata["packages.conda"][record.fn]
        else:
            raise ValueError("unknown package extension")

        # create a signable envelope (a dict with the info and signatures)
        envelope = wrap_as_signable(info)
        envelope["signatures"] = signature

        try:
            verify_delegation("pkg_mgr", envelope, self.key_mgr)
        except SignatureError:
            log.warning(f"invalid signature for {record.fn}")
            record.metadata.add("(package metadata is UNTRUSTED)")
        else:
            log.info(f"valid signature for {record.fn}")
            record.metadata.add("(package metadata is TRUSTED)")

    def __call__(
        self,
        repodata_fn: str,
        unlink_precs: tuple[PackageRecord, ...],
        link_precs: tuple[PackageRecord, ...],
    ) -> None:
        if not self.enabled:
            return

        for prec in link_precs:
            self.verify(repodata_fn, prec)

    @classmethod
    def cache_clear(cls) -> None:
        cls.enabled.fget.cache_clear()
        cls.trusted_root.fget.cache_clear()
        cls.key_mgr.fget.cache_clear()


# singleton for caching
signature_verification = _SignatureVerification()


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Context trust constants.

You could argue that the signatures being here is not necessary; indeed, we
are not necessarily going to be able to check them *properly* (based on some
prior expectations) as the user, since this is the beginning of trust
bootstrapping, the first/backup version of the root of trust metadata.
Still, the signatures here are useful for diagnostic purposes, and, more
important, to allow self-consistency checks: that helps us avoid breaking the
chain of trust if someone accidentally lists the wrong keys down the line. (:
The discrepancy can be detected when loading the root data, and we can
decline to cache incorrect trust metadata that would make further root
updates impossible.
"""

INITIAL_TRUST_ROOT = {
    "signatures": {
        "6d4d5888398ad77465e9fd53996309187723e16509144aa6733015c960378e7a": {
            "other_headers": "04001608001d162104d2ca1d4bf5d77e7c312534284dd9c45328b685ec0502605dbb03",  # noqa: E501
            "signature": "b71c9b3aa60e77258c402e574397127bcb4bc15ef3055ada8539b0d1e355bf1415a135fb7cecc9244f839a929f6b1f82844a5b3df8d6225ec9a50b181692490f",  # noqa: E501
        },
        "508debb915ede0b16dc0cff63f250bde73c5923317b44719fcfc25cc95560c44": {
            "other_headers": "04001608001d162104e6dffee4638f24cfa60a08ba03afe1314a3a38fc050260621281",  # noqa: E501
            "signature": "29d53d4e7dbea0a3efb07266d22e57cf4df7abe004453981c631245716e1b737c7a6b4ab95f42592af70be67abf56e97020e1aa1f52b49ef39b37481f05d5701",  # noqa: E501
        },
    },
    "signed": {
        "delegations": {
            "key_mgr": {
                "pubkeys": [
                    "f24c813d23a9b26be665eee5c54680c35321061b337f862385ed6d783b0bedb0"
                ],
                "threshold": 1,
            },
            "root": {
                "pubkeys": [
                    "668a3217d72d4064edb16648435dc4a3e09a172ecee45dcab1464dcd2f402ec6",
                    "508debb915ede0b16dc0cff63f250bde73c5923317b44719fcfc25cc95560c44",
                    "6d4d5888398ad77465e9fd53996309187723e16509144aa6733015c960378e7a",
                    "e0c88b4c0721bd451b7e720dfb0d0bb6b3853f0cbcf5570edd73367d0841be51",
                ],
                "threshold": 2,
            },
        },
        "expiration": "2022-10-31T18:00:00Z",
        "metadata_spec_version": "0.6.0",
        "timestamp": "2021-03-26T00:00:00Z",
        "type": "root",
        "version": 1,
    },
}

KEY_MGR_FILE = "key_mgr.json"


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Tools for managing the packages installed within an environment."""

from __future__ import annotations

import json
import os
import re
from logging import getLogger
from os.path import basename, lexists
from pathlib import Path

from ..auxlib.exceptions import ValidationError
from ..base.constants import (
    CONDA_ENV_VARS_UNSET_VAR,
    CONDA_PACKAGE_EXTENSIONS,
    PREFIX_MAGIC_FILE,
    PREFIX_STATE_FILE,
)
from ..base.context import context
from ..common.constants import NULL
from ..common.io import time_recorder
from ..common.path import get_python_site_packages_short_path, win_path_ok
from ..common.pkg_formats.python import get_site_packages_anchor_files
from ..common.serialize import json_load
from ..common.url import mask_anaconda_token
from ..common.url import remove_auth as url_remove_auth
from ..deprecations import deprecated
from ..exceptions import (
    BasicClobberError,
    CondaDependencyError,
    CorruptedEnvironmentError,
    maybe_raise,
)
from ..gateways.disk.create import write_as_json_to_file
from ..gateways.disk.delete import rm_rf
from ..gateways.disk.read import read_python_record
from ..gateways.disk.test import file_path_is_writable
from ..models.match_spec import MatchSpec
from ..models.prefix_graph import PrefixGraph
from ..models.records import PackageRecord, PrefixRecord

log = getLogger(__name__)


class PrefixDataType(type):
    """Basic caching of PrefixData instance objects."""

    def __call__(
        cls,
        prefix_path: str | os.PathLike | Path,
        pip_interop_enabled: bool | None = None,
    ):
        if isinstance(prefix_path, PrefixData):
            return prefix_path
        elif (prefix_path := Path(prefix_path)) in PrefixData._cache_:
            return PrefixData._cache_[prefix_path]
        else:
            prefix_data_instance = super().__call__(prefix_path, pip_interop_enabled)
            PrefixData._cache_[prefix_path] = prefix_data_instance
            return prefix_data_instance


class PrefixData(metaclass=PrefixDataType):
    _cache_: dict[Path, PrefixData] = {}

    def __init__(
        self,
        prefix_path: Path,
        pip_interop_enabled: bool | None = None,
    ):
        # pip_interop_enabled is a temporary parameter; DO NOT USE
        # TODO: when removing pip_interop_enabled, also remove from meta class
        self.prefix_path = prefix_path
        self.__prefix_records = None
        self.__is_writable = NULL
        self._pip_interop_enabled = (
            pip_interop_enabled
            if pip_interop_enabled is not None
            else context.pip_interop_enabled
        )

    @time_recorder(module_name=__name__)
    def load(self):
        self.__prefix_records = {}
        _conda_meta_dir = self.prefix_path / "conda-meta"
        if lexists(_conda_meta_dir):
            conda_meta_json_paths = (
                p
                for p in (entry.path for entry in os.scandir(_conda_meta_dir))
                if p[-5:] == ".json"
            )
            for meta_file in conda_meta_json_paths:
                self._load_single_record(meta_file)
        if self._pip_interop_enabled:
            self._load_site_packages()

    def reload(self):
        self.load()
        return self

    def _get_json_fn(self, prefix_record):
        fn = prefix_record.fn
        known_ext = False
        # .dist-info is for things installed by pip
        for ext in CONDA_PACKAGE_EXTENSIONS + (".dist-info",):
            if fn.endswith(ext):
                fn = fn.replace(ext, "")
                known_ext = True
        if not known_ext:
            raise ValueError(
                f"Attempted to make prefix record for unknown package type: {fn}"
            )
        return fn + ".json"

    def insert(self, prefix_record, remove_auth=True):
        assert prefix_record.name not in self._prefix_records, (
            f"Prefix record insertion error: a record with name {prefix_record.name} already exists "
            "in the prefix. This is a bug in conda. Please report it at "
            "https://github.com/conda/conda/issues"
        )

        prefix_record_json_path = (
            self.prefix_path / "conda-meta" / self._get_json_fn(prefix_record)
        )
        if lexists(prefix_record_json_path):
            maybe_raise(
                BasicClobberError(
                    source_path=None,
                    target_path=prefix_record_json_path,
                    context=context,
                ),
                context,
            )
            rm_rf(prefix_record_json_path)
        if remove_auth:
            prefix_record_json = prefix_record.dump()
            prefix_record_json["url"] = url_remove_auth(
                mask_anaconda_token(prefix_record.url)
            )
        else:
            prefix_record_json = prefix_record
        write_as_json_to_file(prefix_record_json_path, prefix_record_json)

        self._prefix_records[prefix_record.name] = prefix_record

    def remove(self, package_name):
        assert package_name in self._prefix_records

        prefix_record = self._prefix_records[package_name]

        prefix_record_json_path = (
            self.prefix_path / "conda-meta" / self._get_json_fn(prefix_record)
        )
        if self.is_writable:
            rm_rf(prefix_record_json_path)

        del self._prefix_records[package_name]

    def get(self, package_name, default=NULL):
        try:
            return self._prefix_records[package_name]
        except KeyError:
            if default is not NULL:
                return default
            else:
                raise

    def iter_records(self):
        return iter(self._prefix_records.values())

    def iter_records_sorted(self):
        prefix_graph = PrefixGraph(self.iter_records())
        return iter(prefix_graph.graph)

    def all_subdir_urls(self):
        subdir_urls = set()
        for prefix_record in self.iter_records():
            subdir_url = prefix_record.channel.subdir_url
            if subdir_url and subdir_url not in subdir_urls:
                log.debug("adding subdir url %s for %s", subdir_url, prefix_record)
                subdir_urls.add(subdir_url)
        return subdir_urls

    def query(self, package_ref_or_match_spec):
        # returns a generator
        param = package_ref_or_match_spec
        if isinstance(param, str):
            param = MatchSpec(param)
        if isinstance(param, MatchSpec):
            return (
                prefix_rec
                for prefix_rec in self.iter_records()
                if param.match(prefix_rec)
            )
        else:
            assert isinstance(param, PackageRecord)
            return (
                prefix_rec for prefix_rec in self.iter_records() if prefix_rec == param
            )

    @property
    def _prefix_records(self):
        return self.__prefix_records or self.load() or self.__prefix_records

    def _load_single_record(self, prefix_record_json_path):
        log.debug("loading prefix record %s", prefix_record_json_path)
        with open(prefix_record_json_path) as fh:
            try:
                json_data = json_load(fh.read())
            except (UnicodeDecodeError, json.JSONDecodeError):
                # UnicodeDecodeError: catch horribly corrupt files
                # JSONDecodeError: catch bad json format files
                raise CorruptedEnvironmentError(
                    self.prefix_path, prefix_record_json_path
                )

            # TODO: consider, at least in memory, storing prefix_record_json_path as part
            #       of PrefixRecord
            prefix_record = PrefixRecord(**json_data)

            # check that prefix record json filename conforms to name-version-build
            # apparently implemented as part of #2638 to resolve #2599
            try:
                n, v, b = basename(prefix_record_json_path)[:-5].rsplit("-", 2)
                if (n, v, b) != (
                    prefix_record.name,
                    prefix_record.version,
                    prefix_record.build,
                ):
                    raise ValueError()
            except ValueError:
                log.warning(
                    "Ignoring malformed prefix record at: %s", prefix_record_json_path
                )
                # TODO: consider just deleting here this record file in the future
                return

            self.__prefix_records[prefix_record.name] = prefix_record

    @property
    def is_writable(self):
        if self.__is_writable == NULL:
            test_path = self.prefix_path / PREFIX_MAGIC_FILE
            if not test_path.is_file():
                is_writable = None
            else:
                is_writable = file_path_is_writable(test_path)
            self.__is_writable = is_writable
        return self.__is_writable

    @deprecated("24.3", "24.9")
    def _has_python(self):
        return "python" in self._prefix_records

    @property
    def _python_pkg_record(self):
        """Return the prefix record for the package python."""
        return next(
            (
                prefix_record
                for prefix_record in self.__prefix_records.values()
                if prefix_record.name == "python"
            ),
            None,
        )

    def _load_site_packages(self):
        """
        Load non-conda-installed python packages in the site-packages of the prefix.

        Python packages not handled by conda are installed via other means,
        like using pip or using python setup.py develop for local development.

        Packages found that are not handled by conda are converted into a
        prefix record and handled in memory.

        Packages clobbering conda packages (i.e. the conda-meta record) are
        removed from the in memory representation.
        """
        python_pkg_record = self._python_pkg_record

        if not python_pkg_record:
            return {}

        site_packages_dir = get_python_site_packages_short_path(
            python_pkg_record.version
        )
        site_packages_path = self.prefix_path / win_path_ok(site_packages_dir)

        if not site_packages_path.is_dir():
            return {}

        # Get anchor files for corresponding conda (handled) python packages
        prefix_graph = PrefixGraph(self.iter_records())
        python_records = prefix_graph.all_descendants(python_pkg_record)
        conda_python_packages = get_conda_anchor_files_and_records(
            site_packages_dir, python_records
        )

        # Get all anchor files and compare against conda anchor files to find clobbered conda
        # packages and python packages installed via other means (not handled by conda)
        sp_anchor_files = get_site_packages_anchor_files(
            site_packages_path, site_packages_dir
        )
        conda_anchor_files = set(conda_python_packages)
        clobbered_conda_anchor_files = conda_anchor_files - sp_anchor_files
        non_conda_anchor_files = sp_anchor_files - conda_anchor_files

        # If there's a mismatch for anchor files between what conda expects for a package
        # based on conda-meta, and for what is actually in site-packages, then we'll delete
        # the in-memory record for the conda package.  In the future, we should consider
        # also deleting the record on disk in the conda-meta/ directory.
        for conda_anchor_file in clobbered_conda_anchor_files:
            prefix_rec = self._prefix_records.pop(
                conda_python_packages[conda_anchor_file].name
            )
            try:
                extracted_package_dir = basename(prefix_rec.extracted_package_dir)
            except AttributeError:
                extracted_package_dir = "-".join(
                    (prefix_rec.name, prefix_rec.version, prefix_rec.build)
                )
            prefix_rec_json_path = (
                self.prefix_path / "conda-meta" / f"{extracted_package_dir}.json"
            )
            try:
                rm_rf(prefix_rec_json_path)
            except OSError:
                log.debug(
                    "stale information, but couldn't remove: %s", prefix_rec_json_path
                )
            else:
                log.debug("removed due to stale information: %s", prefix_rec_json_path)

        # Create prefix records for python packages not handled by conda
        new_packages = {}
        for af in non_conda_anchor_files:
            try:
                python_record = read_python_record(
                    self.prefix_path, af, python_pkg_record.version
                )
            except OSError as e:
                log.info(
                    "Python record ignored for anchor path '%s'\n  due to %s", af, e
                )
                continue
            except ValidationError:
                import sys

                exc_type, exc_value, exc_traceback = sys.exc_info()
                import traceback

                tb = traceback.format_exception(exc_type, exc_value, exc_traceback)
                log.warning(
                    "Problem reading non-conda package record at %s. Please verify that you "
                    "still need this, and if so, that this is still installed correctly. "
                    "Reinstalling this package may help.",
                    af,
                )
                log.debug("ValidationError: \n%s\n", "\n".join(tb))
                continue
            if not python_record:
                continue
            self.__prefix_records[python_record.name] = python_record
            new_packages[python_record.name] = python_record

        return new_packages

    def _get_environment_state_file(self):
        env_vars_file = self.prefix_path / PREFIX_STATE_FILE
        if lexists(env_vars_file):
            with open(env_vars_file) as f:
                prefix_state = json.loads(f.read())
        else:
            prefix_state = {}
        return prefix_state

    def _write_environment_state_file(self, state):
        env_vars_file = self.prefix_path / PREFIX_STATE_FILE
        env_vars_file.write_text(
            json.dumps(state, ensure_ascii=False, default=lambda x: x.__dict__)
        )

    def get_environment_env_vars(self):
        prefix_state = self._get_environment_state_file()
        env_vars_all = dict(prefix_state.get("env_vars", {}))
        env_vars = {
            k: v for k, v in env_vars_all.items() if v != CONDA_ENV_VARS_UNSET_VAR
        }
        return env_vars

    def set_environment_env_vars(self, env_vars):
        env_state_file = self._get_environment_state_file()
        current_env_vars = env_state_file.get("env_vars")
        if current_env_vars:
            current_env_vars.update(env_vars)
        else:
            env_state_file["env_vars"] = env_vars
        self._write_environment_state_file(env_state_file)
        return env_state_file.get("env_vars")

    def unset_environment_env_vars(self, env_vars):
        env_state_file = self._get_environment_state_file()
        current_env_vars = env_state_file.get("env_vars")
        if current_env_vars:
            for env_var in env_vars:
                if env_var in current_env_vars.keys():
                    current_env_vars[env_var] = CONDA_ENV_VARS_UNSET_VAR
            self._write_environment_state_file(env_state_file)
        return env_state_file.get("env_vars")


def get_conda_anchor_files_and_records(site_packages_short_path, python_records):
    """Return the anchor files for the conda records of python packages."""
    anchor_file_endings = (".egg-info/PKG-INFO", ".dist-info/RECORD", ".egg-info")
    conda_python_packages = {}

    matcher = re.compile(
        r"^{}/[^/]+(?:{})$".format(
            re.escape(site_packages_short_path),
            r"|".join(re.escape(fn) for fn in anchor_file_endings),
        )
    ).match

    for prefix_record in python_records:
        anchor_paths = tuple(fpath for fpath in prefix_record.files if matcher(fpath))
        if len(anchor_paths) > 1:
            anchor_path = sorted(anchor_paths, key=len)[0]
            log.info(
                "Package %s has multiple python anchor files.\n  Using %s",
                prefix_record.record_id(),
                anchor_path,
            )
            conda_python_packages[anchor_path] = prefix_record
        elif anchor_paths:
            conda_python_packages[anchor_paths[0]] = prefix_record

    return conda_python_packages


def get_python_version_for_prefix(prefix):
    # returns a string e.g. "2.7", "3.4", "3.5" or None
    py_record_iter = (
        rcrd for rcrd in PrefixData(prefix).iter_records() if rcrd.name == "python"
    )
    record = next(py_record_iter, None)
    if record is None:
        return None
    next_record = next(py_record_iter, None)
    if next_record is not None:
        raise CondaDependencyError(f"multiple python records found in prefix {prefix}")
    elif record.version[3].isdigit():
        return record.version[:4]
    else:
        return record.version[:3]


def delete_prefix_from_linked_data(path: str | os.PathLike | Path) -> bool:
    """Here, path may be a complete prefix or a dist inside a prefix"""
    path = Path(path)
    for prefix in sorted(PrefixData._cache_, reverse=True):
        try:
            path.relative_to(prefix)
            del PrefixData._cache_[prefix]
            return True
        except ValueError:
            # ValueError: path is not relative to prefix
            continue
    return False


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""The classic solver implementation."""

from __future__ import annotations

import copy
import sys
from itertools import chain
from logging import DEBUG, getLogger
from os.path import exists, join
from textwrap import dedent
from typing import TYPE_CHECKING

from boltons.setutils import IndexedSet

from .. import CondaError
from .. import __version__ as CONDA_VERSION
from ..auxlib.decorators import memoizedproperty
from ..auxlib.ish import dals
from ..base.constants import REPODATA_FN, UNKNOWN_CHANNEL, DepsModifier, UpdateModifier
from ..base.context import context
from ..common.constants import NULL, TRACE
from ..common.io import Spinner, dashlist, time_recorder
from ..common.iterators import groupby_to_dict as groupby
from ..common.path import get_major_minor_version, paths_equal
from ..exceptions import (
    PackagesNotFoundError,
    SpecsConfigurationConflictError,
    UnsatisfiableError,
)
from ..history import History
from ..models.channel import Channel
from ..models.enums import NoarchType
from ..models.match_spec import MatchSpec
from ..models.prefix_graph import PrefixGraph
from ..models.version import VersionOrder
from ..resolve import Resolve
from .index import _supplement_index_with_system, get_reduced_index
from .link import PrefixSetup, UnlinkLinkTransaction
from .prefix_data import PrefixData
from .subdir_data import SubdirData

try:
    from frozendict import frozendict
except ImportError:
    from ..auxlib.collection import frozendict

if TYPE_CHECKING:
    from typing import Iterable

    from ..models.records import PackageRecord

log = getLogger(__name__)


class Solver:
    """
    A high-level API to conda's solving logic. Three public methods are provided to access a
    solution in various forms.

      * :meth:`solve_final_state`
      * :meth:`solve_for_diff`
      * :meth:`solve_for_transaction`
    """

    def __init__(
        self,
        prefix: str,
        channels: Iterable[Channel],
        subdirs: Iterable[str] = (),
        specs_to_add: Iterable[MatchSpec] = (),
        specs_to_remove: Iterable[MatchSpec] = (),
        repodata_fn: str = REPODATA_FN,
        command=NULL,
    ):
        """
        Args:
            prefix (str):
                The conda prefix / environment location for which the :class:`Solver`
                is being instantiated.
            channels (Sequence[:class:`Channel`]):
                A prioritized list of channels to use for the solution.
            subdirs (Sequence[str]):
                A prioritized list of subdirs to use for the solution.
            specs_to_add (set[:class:`MatchSpec`]):
                The set of package specs to add to the prefix.
            specs_to_remove (set[:class:`MatchSpec`]):
                The set of package specs to remove from the prefix.

        """
        self.prefix = prefix
        self._channels = channels or context.channels
        self.channels = IndexedSet(Channel(c) for c in self._channels)
        self.subdirs = tuple(s for s in subdirs or context.subdirs)
        self.specs_to_add = frozenset(MatchSpec.merge(s for s in specs_to_add))
        self.specs_to_add_names = frozenset(_.name for _ in self.specs_to_add)
        self.specs_to_remove = frozenset(MatchSpec.merge(s for s in specs_to_remove))
        self.neutered_specs = ()
        self._command = command

        assert all(s in context.known_subdirs for s in self.subdirs)
        self._repodata_fn = repodata_fn
        self._index = None
        self._r = None
        self._prepared = False
        self._pool_cache = {}

    def solve_for_transaction(
        self,
        update_modifier=NULL,
        deps_modifier=NULL,
        prune=NULL,
        ignore_pinned=NULL,
        force_remove=NULL,
        force_reinstall=NULL,
        should_retry_solve=False,
    ):
        """Gives an UnlinkLinkTransaction instance that can be used to execute the solution
        on an environment.

        Args:
            deps_modifier (DepsModifier):
                See :meth:`solve_final_state`.
            prune (bool):
                See :meth:`solve_final_state`.
            ignore_pinned (bool):
                See :meth:`solve_final_state`.
            force_remove (bool):
                See :meth:`solve_final_state`.
            force_reinstall (bool):
                See :meth:`solve_for_diff`.
            should_retry_solve (bool):
                See :meth:`solve_final_state`.

        Returns:
            UnlinkLinkTransaction:

        """
        if self.prefix == context.root_prefix and context.enable_private_envs:
            # This path has the ability to generate a multi-prefix transaction. The basic logic
            # is in the commented out get_install_transaction() function below. Exercised at
            # the integration level in the PrivateEnvIntegrationTests in test_create.py.
            raise NotImplementedError()

        # run pre-solve processes here before solving for a solution
        context.plugin_manager.invoke_pre_solves(
            self.specs_to_add,
            self.specs_to_remove,
        )

        unlink_precs, link_precs = self.solve_for_diff(
            update_modifier,
            deps_modifier,
            prune,
            ignore_pinned,
            force_remove,
            force_reinstall,
            should_retry_solve,
        )
        # TODO: Only explicitly requested remove and update specs are being included in
        #   History right now. Do we need to include other categories from the solve?

        # run post-solve processes here before performing the transaction
        context.plugin_manager.invoke_post_solves(
            self._repodata_fn,
            unlink_precs,
            link_precs,
        )

        self._notify_conda_outdated(link_precs)
        return UnlinkLinkTransaction(
            PrefixSetup(
                self.prefix,
                unlink_precs,
                link_precs,
                self.specs_to_remove,
                self.specs_to_add,
                self.neutered_specs,
            )
        )

    def solve_for_diff(
        self,
        update_modifier=NULL,
        deps_modifier=NULL,
        prune=NULL,
        ignore_pinned=NULL,
        force_remove=NULL,
        force_reinstall=NULL,
        should_retry_solve=False,
    ) -> tuple[tuple[PackageRecord, ...], tuple[PackageRecord, ...]]:
        """Gives the package references to remove from an environment, followed by
        the package references to add to an environment.

        Args:
            deps_modifier (DepsModifier):
                See :meth:`solve_final_state`.
            prune (bool):
                See :meth:`solve_final_state`.
            ignore_pinned (bool):
                See :meth:`solve_final_state`.
            force_remove (bool):
                See :meth:`solve_final_state`.
            force_reinstall (bool):
                For requested specs_to_add that are already satisfied in the environment,
                    instructs the solver to remove the package and spec from the environment,
                    and then add it back--possibly with the exact package instance modified,
                    depending on the spec exactness.
            should_retry_solve (bool):
                See :meth:`solve_final_state`.

        Returns:
            tuple[PackageRef], tuple[PackageRef]:
                A two-tuple of PackageRef sequences.  The first is the group of packages to
                remove from the environment, in sorted dependency order from leaves to roots.
                The second is the group of packages to add to the environment, in sorted
                dependency order from roots to leaves.

        """
        final_precs = self.solve_final_state(
            update_modifier,
            deps_modifier,
            prune,
            ignore_pinned,
            force_remove,
            should_retry_solve,
        )
        unlink_precs, link_precs = diff_for_unlink_link_precs(
            self.prefix, final_precs, self.specs_to_add, force_reinstall
        )

        # assert that all unlink_precs are manageable
        unmanageable = groupby(lambda prec: prec.is_unmanageable, unlink_precs).get(
            True
        )
        if unmanageable:
            raise RuntimeError(
                f"Cannot unlink unmanageable packages:{dashlist(prec.record_id() for prec in unmanageable)}"
            )

        return unlink_precs, link_precs

    def solve_final_state(
        self,
        update_modifier=NULL,
        deps_modifier=NULL,
        prune=NULL,
        ignore_pinned=NULL,
        force_remove=NULL,
        should_retry_solve=False,
    ):
        """Gives the final, solved state of the environment.

        Args:
            update_modifier (UpdateModifier):
                An optional flag directing how updates are handled regarding packages already
                existing in the environment.

            deps_modifier (DepsModifier):
                An optional flag indicating special solver handling for dependencies. The
                default solver behavior is to be as conservative as possible with dependency
                updates (in the case the dependency already exists in the environment), while
                still ensuring all dependencies are satisfied.  Options include
                * NO_DEPS
                * ONLY_DEPS
                * UPDATE_DEPS
                * UPDATE_DEPS_ONLY_DEPS
                * FREEZE_INSTALLED
            prune (bool):
                If ``True``, the solution will not contain packages that were
                previously brought into the environment as dependencies but are no longer
                required as dependencies and are not user-requested.
            ignore_pinned (bool):
                If ``True``, the solution will ignore pinned package configuration
                for the prefix.
            force_remove (bool):
                Forces removal of a package without removing packages that depend on it.
            should_retry_solve (bool):
                Indicates whether this solve will be retried. This allows us to control
                whether to call find_conflicts (slow) in ssc.r.solve

        Returns:
            tuple[PackageRef]:
                In sorted dependency order from roots to leaves, the package references for
                the solved state of the environment.

        """
        if prune and update_modifier == UpdateModifier.FREEZE_INSTALLED:
            update_modifier = NULL
        if update_modifier is NULL:
            update_modifier = context.update_modifier
        else:
            update_modifier = UpdateModifier(str(update_modifier).lower())
        if deps_modifier is NULL:
            deps_modifier = context.deps_modifier
        else:
            deps_modifier = DepsModifier(str(deps_modifier).lower())
        ignore_pinned = (
            context.ignore_pinned if ignore_pinned is NULL else ignore_pinned
        )
        force_remove = context.force_remove if force_remove is NULL else force_remove

        log.debug(
            "solving prefix %s\n"
            "  specs_to_remove: %s\n"
            "  specs_to_add: %s\n"
            "  prune: %s",
            self.prefix,
            self.specs_to_remove,
            self.specs_to_add,
            prune,
        )

        retrying = hasattr(self, "ssc")

        if not retrying:
            ssc = SolverStateContainer(
                self.prefix,
                update_modifier,
                deps_modifier,
                prune,
                ignore_pinned,
                force_remove,
                should_retry_solve,
            )
            self.ssc = ssc
        else:
            ssc = self.ssc
            ssc.update_modifier = update_modifier
            ssc.deps_modifier = deps_modifier
            ssc.should_retry_solve = should_retry_solve

        # force_remove is a special case where we return early
        if self.specs_to_remove and force_remove:
            if self.specs_to_add:
                raise NotImplementedError()
            solution = tuple(
                prec
                for prec in ssc.solution_precs
                if not any(spec.match(prec) for spec in self.specs_to_remove)
            )
            return IndexedSet(PrefixGraph(solution).graph)

        # Check if specs are satisfied by current environment. If they are, exit early.
        if (
            update_modifier == UpdateModifier.SPECS_SATISFIED_SKIP_SOLVE
            and not self.specs_to_remove
            and not prune
        ):
            for spec in self.specs_to_add:
                if not next(ssc.prefix_data.query(spec), None):
                    break
            else:
                # All specs match a package in the current environment.
                # Return early, with a solution that should just be PrefixData().iter_records()
                return IndexedSet(PrefixGraph(ssc.solution_precs).graph)

        if not ssc.r:
            with Spinner(
                f"Collecting package metadata ({self._repodata_fn})",
                not context.verbose and not context.quiet and not retrying,
                context.json,
            ):
                ssc = self._collect_all_metadata(ssc)

        if should_retry_solve and update_modifier == UpdateModifier.FREEZE_INSTALLED:
            fail_message = (
                "unsuccessful initial attempt using frozen solve. Retrying"
                " with flexible solve.\n"
            )
        elif self._repodata_fn != REPODATA_FN:
            fail_message = (
                f"unsuccessful attempt using repodata from {self._repodata_fn}, retrying"
                " with next repodata source.\n"
            )
        else:
            fail_message = "failed\n"

        with Spinner(
            "Solving environment",
            not context.verbose and not context.quiet,
            context.json,
            fail_message=fail_message,
        ):
            ssc = self._remove_specs(ssc)
            ssc = self._add_specs(ssc)
            solution_precs = copy.copy(ssc.solution_precs)

            pre_packages = self.get_request_package_in_solution(
                ssc.solution_precs, ssc.specs_map
            )
            ssc = self._find_inconsistent_packages(ssc)
            # this will prune precs that are deps of precs that get removed due to conflicts
            ssc = self._run_sat(ssc)
            post_packages = self.get_request_package_in_solution(
                ssc.solution_precs, ssc.specs_map
            )

            if ssc.update_modifier == UpdateModifier.UPDATE_SPECS:
                constrained = self.get_constrained_packages(
                    pre_packages, post_packages, ssc.index.keys()
                )
                if len(constrained) > 0:
                    for spec in constrained:
                        self.determine_constricting_specs(spec, ssc.solution_precs)

            # if there were any conflicts, we need to add their orphaned deps back in
            if ssc.add_back_map:
                orphan_precs = (
                    set(solution_precs)
                    - set(ssc.solution_precs)
                    - set(ssc.add_back_map)
                )
                solution_prec_names = [_.name for _ in ssc.solution_precs]
                ssc.solution_precs.extend(
                    [
                        _
                        for _ in orphan_precs
                        if _.name not in ssc.specs_map
                        and _.name not in solution_prec_names
                    ]
                )

            ssc = self._post_sat_handling(ssc)

        time_recorder.log_totals()

        ssc.solution_precs = IndexedSet(PrefixGraph(ssc.solution_precs).graph)
        log.debug(
            "solved prefix %s\n  solved_linked_dists:\n    %s\n",
            self.prefix,
            "\n    ".join(prec.dist_str() for prec in ssc.solution_precs),
        )

        return ssc.solution_precs

    def determine_constricting_specs(self, spec, solution_precs):
        highest_version = [
            VersionOrder(sp.version) for sp in solution_precs if sp.name == spec.name
        ][0]
        constricting = []
        for prec in solution_precs:
            if any(j for j in prec.depends if spec.name in j):
                for dep in prec.depends:
                    m_dep = MatchSpec(dep)
                    if (
                        m_dep.name == spec.name
                        and m_dep.version is not None
                        and (m_dep.version.exact_value or "<" in m_dep.version.spec)
                    ):
                        if "," in m_dep.version.spec:
                            constricting.extend(
                                [
                                    (prec.name, MatchSpec(f"{m_dep.name} {v}"))
                                    for v in m_dep.version.tup
                                    if "<" in v.spec
                                ]
                            )
                        else:
                            constricting.append((prec.name, m_dep))

        hard_constricting = [
            i for i in constricting if i[1].version.matcher_vo <= highest_version
        ]
        if len(hard_constricting) == 0:
            return None

        print(f"\n\nUpdating {spec.name} is constricted by \n")
        for const in hard_constricting:
            print(f"{const[0]} -> requires {const[1]}")
        print(
            "\nIf you are sure you want an update of your package either try "
            "`conda update --all` or install a specific version of the "
            "package you want using `conda install <pkg>=<version>`\n"
        )
        return hard_constricting

    def get_request_package_in_solution(self, solution_precs, specs_map):
        requested_packages = {}
        for pkg in self.specs_to_add:
            update_pkg_request = pkg.name

            requested_packages[update_pkg_request] = [
                (i.name, str(i.version))
                for i in solution_precs
                if i.name == update_pkg_request and i.version is not None
            ]
            requested_packages[update_pkg_request].extend(
                [
                    (v.name, str(v.version))
                    for k, v in specs_map.items()
                    if k == update_pkg_request and v.version is not None
                ]
            )

        return requested_packages

    def get_constrained_packages(self, pre_packages, post_packages, index_keys):
        update_constrained = set()

        def empty_package_list(pkg):
            for k, v in pkg.items():
                if len(v) == 0:
                    return True
            return False

        if empty_package_list(pre_packages) or empty_package_list(post_packages):
            return update_constrained

        for pkg in self.specs_to_add:
            if pkg.name.startswith("__"):  # ignore virtual packages
                continue
            current_version = max(i[1] for i in pre_packages[pkg.name])
            if current_version == max(
                i.version for i in index_keys if i.name == pkg.name
            ):
                continue
            else:
                if post_packages == pre_packages:
                    update_constrained = update_constrained | {pkg}
        return update_constrained

    @time_recorder(module_name=__name__)
    def _collect_all_metadata(self, ssc):
        if ssc.prune:
            # When pruning DO NOT consider history of already installed packages when solving.
            prepared_specs = {*self.specs_to_remove, *self.specs_to_add}
        else:
            # add in historically-requested specs
            ssc.specs_map.update(ssc.specs_from_history_map)

            # these are things that we want to keep even if they're not explicitly specified.  This
            #     is to compensate for older installers not recording these appropriately for them
            #     to be preserved.
            for pkg_name in (
                "anaconda",
                "conda",
                "conda-build",
                "python.app",
                "console_shortcut",
                "powershell_shortcut",
            ):
                if pkg_name not in ssc.specs_map and ssc.prefix_data.get(
                    pkg_name, None
                ):
                    ssc.specs_map[pkg_name] = MatchSpec(pkg_name)

            # Add virtual packages so they are taken into account by the solver
            virtual_pkg_index = {}
            _supplement_index_with_system(virtual_pkg_index)
            virtual_pkgs = [p.name for p in virtual_pkg_index.keys()]
            for virtual_pkgs_name in virtual_pkgs:
                if virtual_pkgs_name not in ssc.specs_map:
                    ssc.specs_map[virtual_pkgs_name] = MatchSpec(virtual_pkgs_name)

            for prec in ssc.prefix_data.iter_records():
                # first check: add everything if we have no history to work with.
                #    This happens with "update --all", for example.
                #
                # second check: add in aggressively updated packages
                #
                # third check: add in foreign stuff (e.g. from pip) into the specs
                #    map. We add it so that it can be left alone more. This is a
                #    declaration that it is manually installed, much like the
                #    history map. It may still be replaced if it is in conflict,
                #    but it is not just an indirect dep that can be pruned.
                if (
                    not ssc.specs_from_history_map
                    or MatchSpec(prec.name) in context.aggressive_update_packages
                    or prec.subdir == "pypi"
                ):
                    ssc.specs_map.update({prec.name: MatchSpec(prec.name)})

            prepared_specs = {
                *self.specs_to_remove,
                *self.specs_to_add,
                *ssc.specs_from_history_map.values(),
            }

        index, r = self._prepare(prepared_specs)
        ssc.set_repository_metadata(index, r)
        return ssc

    def _remove_specs(self, ssc):
        if self.specs_to_remove:
            # In a previous implementation, we invoked SAT here via `r.remove()` to help with
            # spec removal, and then later invoking SAT again via `r.solve()`. Rather than invoking
            # SAT for spec removal determination, we can use the PrefixGraph and simple tree
            # traversal if we're careful about how we handle features. We still invoke sat via
            # `r.solve()` later.
            _track_fts_specs = (
                spec for spec in self.specs_to_remove if "track_features" in spec
            )
            feature_names = set(
                chain.from_iterable(
                    spec.get_raw_value("track_features") for spec in _track_fts_specs
                )
            )
            graph = PrefixGraph(ssc.solution_precs, ssc.specs_map.values())

            all_removed_records = []
            no_removed_records_specs = []
            for spec in self.specs_to_remove:
                # If the spec was a track_features spec, then we need to also remove every
                # package with a feature that matches the track_feature. The
                # `graph.remove_spec()` method handles that for us.
                log.log(TRACE, "using PrefixGraph to remove records for %s", spec)
                removed_records = graph.remove_spec(spec)
                if removed_records:
                    all_removed_records.extend(removed_records)
                else:
                    no_removed_records_specs.append(spec)

            # ensure that each spec in specs_to_remove is actually associated with removed records
            unmatched_specs_to_remove = tuple(
                spec
                for spec in no_removed_records_specs
                if not any(spec.match(rec) for rec in all_removed_records)
            )
            if unmatched_specs_to_remove:
                raise PackagesNotFoundError(
                    tuple(sorted(str(s) for s in unmatched_specs_to_remove))
                )

            for rec in all_removed_records:
                # We keep specs (minus the feature part) for the non provides_features packages
                # if they're in the history specs.  Otherwise, we pop them from the specs_map.
                rec_has_a_feature = set(rec.features or ()) & feature_names
                if rec_has_a_feature and rec.name in ssc.specs_from_history_map:
                    spec = ssc.specs_map.get(rec.name, MatchSpec(rec.name))
                    spec._match_components = frozendict(
                        {
                            key: value
                            for key, value in spec._match_components.items()
                            if key != "features"
                        }
                    )
                    ssc.specs_map[spec.name] = spec
                else:
                    ssc.specs_map.pop(rec.name, None)

            ssc.solution_precs = tuple(graph.graph)
        return ssc

    @time_recorder(module_name=__name__)
    def _find_inconsistent_packages(self, ssc):
        # We handle as best as possible environments in inconsistent states. To do this,
        # we remove now from consideration the set of packages causing inconsistencies,
        # and then we add them back in following the main SAT call.
        _, inconsistent_precs = ssc.r.bad_installed(ssc.solution_precs, ())
        if inconsistent_precs:
            # It is possible that the package metadata is incorrect, for example when
            # un-patched metadata from the Miniconda or Anaconda installer is present, see:
            # https://github.com/conda/conda/issues/8076
            # Update the metadata with information from the index and see if that makes the
            # environment consistent.
            ssc.solution_precs = tuple(ssc.index.get(k, k) for k in ssc.solution_precs)
            _, inconsistent_precs = ssc.r.bad_installed(ssc.solution_precs, ())
        if log.isEnabledFor(DEBUG):
            log.debug(
                "inconsistent precs: %s",
                dashlist(inconsistent_precs) if inconsistent_precs else "None",
            )
        if inconsistent_precs:
            print(
                dedent(
                    """
            The environment is inconsistent, please check the package plan carefully
            The following packages are causing the inconsistency:"""
                ),
                file=sys.stderr,
            )
            print(dashlist(inconsistent_precs), file=sys.stderr)
            for prec in inconsistent_precs:
                # pop and save matching spec in specs_map
                spec = ssc.specs_map.pop(prec.name, None)
                ssc.add_back_map[prec.name] = (prec, spec)
                # let the package float.  This is essential to keep the package's dependencies
                #    in the solution
                ssc.specs_map[prec.name] = MatchSpec(prec.name, target=prec.dist_str())
                # inconsistent environments should maintain the python version
                # unless explicitly requested by the user. This along with the logic in
                # _add_specs maintains the major.minor version
                if prec.name == "python" and spec:
                    ssc.specs_map["python"] = spec
            ssc.solution_precs = tuple(
                prec for prec in ssc.solution_precs if prec not in inconsistent_precs
            )
        return ssc

    def _package_has_updates(self, ssc, spec, installed_pool):
        installed_prec = installed_pool.get(spec.name)
        has_update = False

        if installed_prec:
            installed_prec = installed_prec[0]
            for prec in ssc.r.groups.get(spec.name, []):
                if prec.version > installed_prec.version:
                    has_update = True
                    break
                elif (
                    prec.version == installed_prec.version
                    and prec.build_number > installed_prec.build_number
                ):
                    has_update = True
                    break
        # let conda determine the latest version by just adding a name spec
        return (
            MatchSpec(spec.name, version=prec.version, build_number=prec.build_number)
            if has_update
            else spec
        )

    def _should_freeze(
        self, ssc, target_prec, conflict_specs, explicit_pool, installed_pool
    ):
        # never, ever freeze anything if we have no history.
        if not ssc.specs_from_history_map:
            return False
        # never freeze if not in FREEZE_INSTALLED mode
        if ssc.update_modifier != UpdateModifier.FREEZE_INSTALLED:
            return False

        # if all package specs have overlapping package choices (satisfiable in at least one way)
        pkg_name = target_prec.name
        no_conflict = pkg_name not in conflict_specs and (
            pkg_name not in explicit_pool or target_prec in explicit_pool[pkg_name]
        )

        return no_conflict

    def _add_specs(self, ssc):
        # For the remaining specs in specs_map, add target to each spec. `target` is a reference
        # to the package currently existing in the environment. Setting target instructs the
        # solver to not disturb that package if it's not necessary.
        # If the spec.name is being modified by inclusion in specs_to_add, we don't set `target`,
        # since we *want* the solver to modify/update that package.
        #
        # TLDR: when working with MatchSpec objects,
        #  - to minimize the version change, set MatchSpec(name=name, target=prec.dist_str())
        #  - to freeze the package, set all the components of MatchSpec individually

        installed_pool = groupby(lambda x: x.name, ssc.prefix_data.iter_records())

        # the only things we should consider freezing are things that don't conflict with the new
        #    specs being added.
        explicit_pool = ssc.r._get_package_pool(self.specs_to_add)
        if ssc.prune:
            # Ignore installed specs on prune.
            installed_specs = ()
        else:
            installed_specs = [
                record.to_match_spec() for record in ssc.prefix_data.iter_records()
            ]

        conflict_specs = (
            ssc.r.get_conflicting_specs(installed_specs, self.specs_to_add) or tuple()
        )
        conflict_specs = {spec.name for spec in conflict_specs}

        for pkg_name, spec in ssc.specs_map.items():
            matches_for_spec = tuple(
                prec for prec in ssc.solution_precs if spec.match(prec)
            )
            if matches_for_spec:
                if len(matches_for_spec) != 1:
                    raise CondaError(
                        dals(
                            """
                    Conda encountered an error with your environment.  Please report an issue
                    at https://github.com/conda/conda/issues.  In your report, please include
                    the output of 'conda info' and 'conda list' for the active environment, along
                    with the command you invoked that resulted in this error.
                      pkg_name: %s
                      spec: %s
                      matches_for_spec: %s
                    """
                        )
                        % (
                            pkg_name,
                            spec,
                            dashlist((str(s) for s in matches_for_spec), indent=4),
                        )
                    )
                target_prec = matches_for_spec[0]
                if target_prec.is_unmanageable:
                    ssc.specs_map[pkg_name] = target_prec.to_match_spec()
                elif MatchSpec(pkg_name) in context.aggressive_update_packages:
                    ssc.specs_map[pkg_name] = MatchSpec(pkg_name)
                elif self._should_freeze(
                    ssc, target_prec, conflict_specs, explicit_pool, installed_pool
                ):
                    ssc.specs_map[pkg_name] = target_prec.to_match_spec()
                elif pkg_name in ssc.specs_from_history_map:
                    ssc.specs_map[pkg_name] = MatchSpec(
                        ssc.specs_from_history_map[pkg_name],
                        target=target_prec.dist_str(),
                    )
                else:
                    ssc.specs_map[pkg_name] = MatchSpec(
                        pkg_name, target=target_prec.dist_str()
                    )

        pin_overrides = set()
        for s in ssc.pinned_specs:
            if s.name in explicit_pool:
                if s.name not in self.specs_to_add_names and not ssc.ignore_pinned:
                    ssc.specs_map[s.name] = MatchSpec(s, optional=False)
                elif explicit_pool[s.name] & ssc.r._get_package_pool([s]).get(
                    s.name, set()
                ):
                    ssc.specs_map[s.name] = MatchSpec(s, optional=False)
                    pin_overrides.add(s.name)
                else:
                    log.warning(
                        "pinned spec %s conflicts with explicit specs.  "
                        "Overriding pinned spec.",
                        s,
                    )

        # we want to freeze any packages in the env that are not conflicts, so that the
        #     solve goes faster.  This is kind of like an iterative solve, except rather
        #     than just providing a starting place, we are preventing some solutions.
        #     A true iterative solve would probably be better in terms of reaching the
        #     optimal output all the time.  It would probably also get rid of the need
        #     to retry with an unfrozen (UPDATE_SPECS) solve.
        if ssc.update_modifier == UpdateModifier.FREEZE_INSTALLED:
            precs = [
                _ for _ in ssc.prefix_data.iter_records() if _.name not in ssc.specs_map
            ]
            for prec in precs:
                if prec.name not in conflict_specs:
                    ssc.specs_map[prec.name] = prec.to_match_spec()
                else:
                    ssc.specs_map[prec.name] = MatchSpec(
                        prec.name, target=prec.to_match_spec(), optional=True
                    )
        log.debug("specs_map with targets: %s", ssc.specs_map)

        # If we're in UPDATE_ALL mode, we need to drop all the constraints attached to specs,
        # so they can all float and the solver can find the most up-to-date solution. In the case
        # of UPDATE_ALL, `specs_map` wasn't initialized with packages from the current environment,
        # but *only* historically-requested specs.  This lets UPDATE_ALL drop dependencies if
        # they're no longer needed, and their presence would otherwise prevent the updated solution
        # the user most likely wants.
        if ssc.update_modifier == UpdateModifier.UPDATE_ALL:
            # history is preferable because it has explicitly installed stuff in it.
            #   that simplifies our solution.
            if ssc.specs_from_history_map:
                ssc.specs_map = dict(
                    (spec, MatchSpec(spec))
                    if MatchSpec(spec).name not in (_.name for _ in ssc.pinned_specs)
                    else (MatchSpec(spec).name, ssc.specs_map[MatchSpec(spec).name])
                    for spec in ssc.specs_from_history_map
                )
                for prec in ssc.prefix_data.iter_records():
                    # treat pip-installed stuff as explicitly installed, too.
                    if prec.subdir == "pypi":
                        ssc.specs_map.update({prec.name: MatchSpec(prec.name)})
            else:
                ssc.specs_map = {
                    prec.name: (
                        MatchSpec(prec.name)
                        if prec.name not in (_.name for _ in ssc.pinned_specs)
                        else ssc.specs_map[prec.name]
                    )
                    for prec in ssc.prefix_data.iter_records()
                }

        # ensure that our self.specs_to_add are not being held back by packages in the env.
        #    This factors in pins and also ignores specs from the history.  It is unfreezing only
        #    for the indirect specs that otherwise conflict with update of the immediate request
        elif ssc.update_modifier == UpdateModifier.UPDATE_SPECS:
            skip = lambda x: (
                (
                    x.name not in pin_overrides
                    and any(x.name == _.name for _ in ssc.pinned_specs)
                    and not ssc.ignore_pinned
                )
                or x.name in ssc.specs_from_history_map
            )

            specs_to_add = tuple(
                self._package_has_updates(ssc, _, installed_pool)
                for _ in self.specs_to_add
                if not skip(_)
            )
            # the index is sorted, so the first record here gives us what we want.
            conflicts = ssc.r.get_conflicting_specs(
                tuple(MatchSpec(_) for _ in ssc.specs_map.values()), specs_to_add
            )
            for conflict in conflicts or ():
                # neuter the spec due to a conflict
                if (
                    conflict.name in ssc.specs_map
                    and (
                        # add optional because any pinned specs will include it
                        MatchSpec(conflict, optional=True) not in ssc.pinned_specs
                        or ssc.ignore_pinned
                    )
                    and conflict.name not in ssc.specs_from_history_map
                ):
                    ssc.specs_map[conflict.name] = MatchSpec(conflict.name)

        # As a business rule, we never want to update python beyond the current minor version,
        # unless that's requested explicitly by the user (which we actively discourage).
        py_in_prefix = any(_.name == "python" for _ in ssc.solution_precs)
        py_requested_explicitly = any(s.name == "python" for s in self.specs_to_add)
        if py_in_prefix and not py_requested_explicitly:
            python_prefix_rec = ssc.prefix_data.get("python")
            freeze_installed = ssc.update_modifier == UpdateModifier.FREEZE_INSTALLED
            if "python" not in conflict_specs and freeze_installed:
                ssc.specs_map["python"] = python_prefix_rec.to_match_spec()
            else:
                # will our prefix record conflict with any explicit spec?  If so, don't add
                #     anything here - let python float when it hasn't been explicitly specified
                python_spec = ssc.specs_map.get("python", MatchSpec("python"))
                if not python_spec.get("version"):
                    pinned_version = (
                        get_major_minor_version(python_prefix_rec.version) + ".*"
                    )
                    python_spec = MatchSpec(python_spec, version=pinned_version)

                spec_set = (python_spec,) + tuple(self.specs_to_add)
                if ssc.r.get_conflicting_specs(spec_set, self.specs_to_add):
                    if self._command != "install" or (
                        self._repodata_fn == REPODATA_FN
                        and (not ssc.should_retry_solve or not freeze_installed)
                    ):
                        # raises a hopefully helpful error message
                        ssc.r.find_conflicts(spec_set)
                    else:
                        raise UnsatisfiableError({})
                ssc.specs_map["python"] = python_spec

        # For the aggressive_update_packages configuration parameter, we strip any target
        # that's been set.
        if not context.offline:
            for spec in context.aggressive_update_packages:
                if spec.name in ssc.specs_map:
                    ssc.specs_map[spec.name] = spec

        # add in explicitly requested specs from specs_to_add
        # this overrides any name-matching spec already in the spec map
        ssc.specs_map.update(
            (s.name, s) for s in self.specs_to_add if s.name not in pin_overrides
        )

        # As a business rule, we never want to downgrade conda below the current version,
        # unless that's requested explicitly by the user (which we actively discourage).
        if "conda" in ssc.specs_map and paths_equal(self.prefix, context.conda_prefix):
            conda_prefix_rec = ssc.prefix_data.get("conda")
            if conda_prefix_rec:
                version_req = f">={conda_prefix_rec.version}"
                conda_requested_explicitly = any(
                    s.name == "conda" for s in self.specs_to_add
                )
                conda_spec = ssc.specs_map["conda"]
                conda_in_specs_to_add_version = ssc.specs_map.get("conda", {}).get(
                    "version"
                )
                if not conda_in_specs_to_add_version:
                    conda_spec = MatchSpec(conda_spec, version=version_req)
                if context.auto_update_conda and not conda_requested_explicitly:
                    conda_spec = MatchSpec("conda", version=version_req, target=None)
                ssc.specs_map["conda"] = conda_spec

        return ssc

    @time_recorder(module_name=__name__)
    def _run_sat(self, ssc):
        final_environment_specs = IndexedSet(
            (
                *ssc.specs_map.values(),
                *ssc.track_features_specs,
                # pinned specs removed here - added to specs_map in _add_specs instead
            )
        )

        absent_specs = [s for s in ssc.specs_map.values() if not ssc.r.find_matches(s)]
        if absent_specs:
            raise PackagesNotFoundError(absent_specs)

        # We've previously checked `solution` for consistency (which at that point was the
        # pre-solve state of the environment). Now we check our compiled set of
        # `final_environment_specs` for the possibility of a solution.  If there are conflicts,
        # we can often avoid them by neutering specs that have a target (e.g. removing version
        # constraint) and also making them optional. The result here will be less cases of
        # `UnsatisfiableError` handed to users, at the cost of more packages being modified
        # or removed from the environment.
        #
        # get_conflicting_specs() returns a "minimal unsatisfiable subset" which
        # may not be the only unsatisfiable subset. We may have to call get_conflicting_specs()
        # several times, each time making modifications to loosen constraints.

        conflicting_specs = set(
            ssc.r.get_conflicting_specs(
                tuple(final_environment_specs), self.specs_to_add
            )
            or []
        )
        while conflicting_specs:
            specs_modified = False
            if log.isEnabledFor(DEBUG):
                log.debug(
                    "conflicting specs: %s",
                    dashlist(s.target or s for s in conflicting_specs),
                )

            # Are all conflicting specs in specs_map? If not, that means they're in
            # track_features_specs or pinned_specs, which we should raise an error on.
            specs_map_set = set(ssc.specs_map.values())
            grouped_specs = groupby(lambda s: s in specs_map_set, conflicting_specs)
            # force optional to true. This is what it is originally in
            # pinned_specs, but we override that in _add_specs to make it
            # non-optional when there's a name match in the explicit package
            # pool
            conflicting_pinned_specs = groupby(
                lambda s: MatchSpec(s, optional=True) in ssc.pinned_specs,
                conflicting_specs,
            )

            if conflicting_pinned_specs.get(True):
                in_specs_map = grouped_specs.get(True, ())
                pinned_conflicts = conflicting_pinned_specs.get(True, ())
                in_specs_map_or_specs_to_add = (
                    set(in_specs_map) | set(self.specs_to_add)
                ) - set(pinned_conflicts)

                raise SpecsConfigurationConflictError(
                    sorted(s.__str__() for s in in_specs_map_or_specs_to_add),
                    sorted(s.__str__() for s in {s for s in pinned_conflicts}),
                    self.prefix,
                )
            for spec in conflicting_specs:
                if spec.target and not spec.optional:
                    specs_modified = True
                    final_environment_specs.remove(spec)
                    if spec.get("version"):
                        neutered_spec = MatchSpec(spec.name, version=spec.version)
                    else:
                        neutered_spec = MatchSpec(spec.name)
                    final_environment_specs.add(neutered_spec)
                    ssc.specs_map[spec.name] = neutered_spec
            if specs_modified:
                conflicting_specs = set(
                    ssc.r.get_conflicting_specs(
                        tuple(final_environment_specs), self.specs_to_add
                    )
                )
            else:
                # Let r.solve() use r.find_conflicts() to report conflict chains.
                break

        # Finally! We get to call SAT.
        if log.isEnabledFor(DEBUG):
            log.debug(
                "final specs to add: %s",
                dashlist(sorted(str(s) for s in final_environment_specs)),
            )

        # this will raise for unsatisfiable stuff.  We can
        if not conflicting_specs or context.unsatisfiable_hints:
            ssc.solution_precs = ssc.r.solve(
                tuple(final_environment_specs),
                specs_to_add=self.specs_to_add,
                history_specs=ssc.specs_from_history_map,
                should_retry_solve=ssc.should_retry_solve,
            )
        else:
            # shortcut to raise an unsat error without needing another solve step when
            # unsatisfiable_hints is off
            raise UnsatisfiableError({})

        self.neutered_specs = tuple(
            v
            for k, v in ssc.specs_map.items()
            if k in ssc.specs_from_history_map
            and v.strictness < ssc.specs_from_history_map[k].strictness
        )

        # add back inconsistent packages to solution
        if ssc.add_back_map:
            for name, (prec, spec) in ssc.add_back_map.items():
                # spec here will only be set if the conflicting prec was in the original specs_map
                #    if it isn't there, then we restore the conflict.  If it is there, though,
                #    we keep the new, consistent solution
                if not spec:
                    # filter out solution precs and reinsert the conflict.  Any resolution
                    #    of the conflict should be explicit (i.e. it must be in ssc.specs_map)
                    ssc.solution_precs = [
                        _ for _ in ssc.solution_precs if _.name != name
                    ]
                    ssc.solution_precs.append(prec)
                    final_environment_specs.add(spec)

        ssc.final_environment_specs = final_environment_specs
        return ssc

    def _post_sat_handling(self, ssc):
        # Special case handling for various DepsModifier flags.
        final_environment_specs = ssc.final_environment_specs
        if ssc.deps_modifier == DepsModifier.NO_DEPS:
            # In the NO_DEPS case, we need to start with the original list of packages in the
            # environment, and then only modify packages that match specs_to_add or
            # specs_to_remove.
            #
            # Help information notes that use of NO_DEPS is expected to lead to broken
            # environments.
            _no_deps_solution = IndexedSet(ssc.prefix_data.iter_records())
            only_remove_these = {
                prec
                for spec in self.specs_to_remove
                for prec in _no_deps_solution
                if spec.match(prec)
            }
            _no_deps_solution -= only_remove_these

            only_add_these = {
                prec
                for spec in self.specs_to_add
                for prec in ssc.solution_precs
                if spec.match(prec)
            }
            remove_before_adding_back = {prec.name for prec in only_add_these}
            _no_deps_solution = IndexedSet(
                prec
                for prec in _no_deps_solution
                if prec.name not in remove_before_adding_back
            )
            _no_deps_solution |= only_add_these
            ssc.solution_precs = _no_deps_solution

            # TODO: check if solution is satisfiable, and emit warning if it's not

        elif (
            ssc.deps_modifier == DepsModifier.ONLY_DEPS
            and ssc.update_modifier != UpdateModifier.UPDATE_DEPS
        ):
            # Using a special instance of PrefixGraph to remove youngest child nodes that match
            # the original specs_to_add.  It's important to remove only the *youngest* child nodes,
            # because a typical use might be `conda install --only-deps python=2 flask`, and in
            # that case we'd want to keep python.
            #
            # What are we supposed to do if flask was already in the environment?
            # We can't be removing stuff here that's already in the environment.
            #
            # What should be recorded for the user-requested specs in this case? Probably all
            # direct dependencies of flask.
            graph = PrefixGraph(ssc.solution_precs, self.specs_to_add)
            removed_nodes = graph.remove_youngest_descendant_nodes_with_specs()
            self.specs_to_add = set(self.specs_to_add)
            for prec in removed_nodes:
                for dep in prec.depends:
                    dep = MatchSpec(dep)
                    if dep.name not in ssc.specs_map:
                        self.specs_to_add.add(dep)
            # unfreeze
            self.specs_to_add = frozenset(self.specs_to_add)

            # Add back packages that are already in the prefix.
            specs_to_remove_names = {spec.name for spec in self.specs_to_remove}
            add_back = tuple(
                ssc.prefix_data.get(node.name, None)
                for node in removed_nodes
                if node.name not in specs_to_remove_names
            )
            ssc.solution_precs = tuple(
                PrefixGraph((*graph.graph, *filter(None, add_back))).graph
            )

            # TODO: check if solution is satisfiable, and emit warning if it's not

        elif ssc.update_modifier == UpdateModifier.UPDATE_DEPS:
            # Here we have to SAT solve again :(  It's only now that we know the dependency
            # chain of specs_to_add.
            #
            # UPDATE_DEPS is effectively making each spec in the dependency chain a user-requested
            # spec.  We don't modify pinned_specs, track_features_specs, or specs_to_add.  For
            # all other specs, we drop all information but name, drop target, and add them to
            # the specs_to_add that gets recorded in the history file.
            #
            # It's like UPDATE_ALL, but only for certain dependency chains.
            graph = PrefixGraph(ssc.solution_precs)
            update_names = set()
            for spec in self.specs_to_add:
                node = graph.get_node_by_name(spec.name)
                update_names.update(
                    ancest_rec.name for ancest_rec in graph.all_ancestors(node)
                )
            specs_map = {name: MatchSpec(name) for name in update_names}

            # Remove pinned_specs and any python spec (due to major-minor pinning business rule).
            # Add in the original specs_to_add on top.
            for spec in ssc.pinned_specs:
                specs_map.pop(spec.name, None)
            if "python" in specs_map:
                python_rec = ssc.prefix_data.get("python")
                py_ver = ".".join(python_rec.version.split(".")[:2]) + ".*"
                specs_map["python"] = MatchSpec(name="python", version=py_ver)
            specs_map.update({spec.name: spec for spec in self.specs_to_add})
            new_specs_to_add = tuple(specs_map.values())

            # It feels wrong/unsafe to modify this instance, but I guess let's go with it for now.
            self.specs_to_add = new_specs_to_add
            ssc.solution_precs = self.solve_final_state(
                update_modifier=UpdateModifier.UPDATE_SPECS,
                deps_modifier=ssc.deps_modifier,
                prune=ssc.prune,
                ignore_pinned=ssc.ignore_pinned,
                force_remove=ssc.force_remove,
            )
            ssc.prune = False

        if ssc.prune:
            graph = PrefixGraph(ssc.solution_precs, final_environment_specs)
            graph.prune()
            ssc.solution_precs = tuple(graph.graph)

        return ssc

    def _notify_conda_outdated(self, link_precs):
        if not context.notify_outdated_conda or context.quiet:
            return
        current_conda_prefix_rec = PrefixData(context.conda_prefix).get("conda", None)
        if current_conda_prefix_rec:
            channel_name = current_conda_prefix_rec.channel.canonical_name
            if channel_name == UNKNOWN_CHANNEL:
                channel_name = "defaults"

            # only look for a newer conda in the channel conda is currently installed from
            conda_newer_spec = MatchSpec(f"{channel_name}::conda>{CONDA_VERSION}")

            if paths_equal(self.prefix, context.conda_prefix):
                if any(conda_newer_spec.match(prec) for prec in link_precs):
                    return

            conda_newer_precs = sorted(
                SubdirData.query_all(
                    conda_newer_spec,
                    self.channels,
                    self.subdirs,
                    repodata_fn=self._repodata_fn,
                ),
                key=lambda x: VersionOrder(x.version),
                # VersionOrder is fine here rather than r.version_key because all precs
                # should come from the same channel
            )
            if conda_newer_precs:
                latest_version = conda_newer_precs[-1].version
                # If conda comes from defaults, ensure we're giving instructions to users
                # that should resolve release timing issues between defaults and conda-forge.
                print(
                    dedent(
                        f"""

                ==> WARNING: A newer version of conda exists. <==
                  current version: {CONDA_VERSION}
                  latest version: {latest_version}

                Please update conda by running

                    $ conda update -n base -c {channel_name} conda

                Or to minimize the number of packages updated during conda update use

                     conda install conda={latest_version}

                """
                    ),
                    file=sys.stderr,
                )

    def _prepare(self, prepared_specs):
        # All of this _prepare() method is hidden away down here. Someday we may want to further
        # abstract away the use of `index` or the Resolve object.

        if self._prepared and prepared_specs == self._prepared_specs:
            return self._index, self._r

        if hasattr(self, "_index") and self._index:
            # added in install_actions for conda-build back-compat
            self._prepared_specs = prepared_specs
            _supplement_index_with_system(self._index)
            self._r = Resolve(self._index, channels=self.channels)
        else:
            # add in required channels that aren't explicitly given in the channels list
            # For correctness, we should probably add to additional_channels any channel that
            #  is given by PrefixData(self.prefix).all_subdir_urls().  However that causes
            #  usability problems with bad / expired tokens.

            additional_channels = set()
            for spec in self.specs_to_add:
                # TODO: correct handling for subdir isn't yet done
                channel = spec.get_exact_value("channel")
                if channel:
                    additional_channels.add(Channel(channel))

            self.channels.update(additional_channels)

            reduced_index = get_reduced_index(
                self.prefix,
                self.channels,
                self.subdirs,
                prepared_specs,
                self._repodata_fn,
            )
            _supplement_index_with_system(reduced_index)

            self._prepared_specs = prepared_specs
            self._index = reduced_index
            self._r = Resolve(reduced_index, channels=self.channels)

        self._prepared = True
        return self._index, self._r


class SolverStateContainer:
    # A mutable container with defined attributes to help keep method signatures clean
    # and also keep track of important state variables.

    def __init__(
        self,
        prefix,
        update_modifier,
        deps_modifier,
        prune,
        ignore_pinned,
        force_remove,
        should_retry_solve,
    ):
        # prefix, channels, subdirs, specs_to_add, specs_to_remove
        # self.prefix = prefix
        # self.channels = channels
        # self.subdirs = subdirs
        # self.specs_to_add = specs_to_add
        # self.specs_to_remove = specs_to_remove

        # Group 1. Behavior flags
        self.update_modifier = update_modifier
        self.deps_modifier = deps_modifier
        self.prune = prune
        self.ignore_pinned = ignore_pinned
        self.force_remove = force_remove
        self.should_retry_solve = should_retry_solve

        # Group 2. System state
        self.prefix = prefix
        # self.prefix_data = None
        # self.specs_from_history_map = None
        # self.track_features_specs = None
        # self.pinned_specs = None

        # Group 3. Repository metadata
        self.index = None
        self.r = None

        # Group 4. Mutable working containers
        self.specs_map = {}
        self.solution_precs = None
        self._init_solution_precs()
        self.add_back_map = {}  # name: (prec, spec)
        self.final_environment_specs = None

    @memoizedproperty
    def prefix_data(self):
        return PrefixData(self.prefix)

    @memoizedproperty
    def specs_from_history_map(self):
        return History(self.prefix).get_requested_specs_map()

    @memoizedproperty
    def track_features_specs(self):
        return tuple(MatchSpec(x + "@") for x in context.track_features)

    @memoizedproperty
    def pinned_specs(self):
        return () if self.ignore_pinned else get_pinned_specs(self.prefix)

    def set_repository_metadata(self, index, r):
        self.index, self.r = index, r

    def _init_solution_precs(self):
        if self.prune:
            # DO NOT add existing prefix data to solution on prune
            self.solution_precs = tuple()
        else:
            self.solution_precs = tuple(self.prefix_data.iter_records())

    def working_state_reset(self):
        self.specs_map = {}
        self._init_solution_precs()
        self.add_back_map = {}  # name: (prec, spec)
        self.final_environment_specs = None


def get_pinned_specs(prefix):
    """Find pinned specs from file and return a tuple of MatchSpec."""
    pinfile = join(prefix, "conda-meta", "pinned")
    if exists(pinfile):
        with open(pinfile) as f:
            from_file = (
                i
                for i in f.read().strip().splitlines()
                if i and not i.strip().startswith("#")
            )
    else:
        from_file = ()

    return tuple(
        MatchSpec(spec, optional=True)
        for spec in (*context.pinned_packages, *from_file)
    )


def diff_for_unlink_link_precs(
    prefix,
    final_precs,
    specs_to_add=(),
    force_reinstall=NULL,
) -> tuple[tuple[PackageRecord, ...], tuple[PackageRecord, ...]]:
    # Ensure final_precs supports the IndexedSet interface
    if not isinstance(final_precs, IndexedSet):
        assert hasattr(
            final_precs, "__getitem__"
        ), "final_precs must support list indexing"
        assert hasattr(
            final_precs, "__sub__"
        ), "final_precs must support set difference"

    previous_records = IndexedSet(PrefixGraph(PrefixData(prefix).iter_records()).graph)
    force_reinstall = (
        context.force_reinstall if force_reinstall is NULL else force_reinstall
    )

    unlink_precs = previous_records - final_precs
    link_precs = final_precs - previous_records

    def _add_to_unlink_and_link(rec):
        link_precs.add(rec)
        if prec in previous_records:
            unlink_precs.add(rec)

    # If force_reinstall is enabled, make sure any package in specs_to_add is unlinked then
    # re-linked
    if force_reinstall:
        for spec in specs_to_add:
            prec = next((rec for rec in final_precs if spec.match(rec)), None)
            assert prec
            _add_to_unlink_and_link(prec)

    # add back 'noarch: python' packages to unlink and link if python version changes
    python_spec = MatchSpec("python")
    prev_python = next(
        (rec for rec in previous_records if python_spec.match(rec)), None
    )
    curr_python = next((rec for rec in final_precs if python_spec.match(rec)), None)
    gmm = get_major_minor_version
    if (
        prev_python
        and curr_python
        and gmm(prev_python.version) != gmm(curr_python.version)
    ):
        noarch_python_precs = (p for p in final_precs if p.noarch == NoarchType.python)
        for prec in noarch_python_precs:
            _add_to_unlink_and_link(prec)

    unlink_precs = IndexedSet(
        reversed(sorted(unlink_precs, key=lambda x: previous_records.index(x)))
    )
    link_precs = IndexedSet(sorted(link_precs, key=lambda x: final_precs.index(x)))
    return tuple(unlink_precs), tuple(link_precs)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Package installation implemented as a series of link/unlink transactions."""

from __future__ import annotations

import itertools
import os
import sys
import warnings
from collections import defaultdict
from itertools import chain
from logging import getLogger
from os.path import basename, dirname, isdir, join
from pathlib import Path
from textwrap import indent
from traceback import format_exception_only
from typing import TYPE_CHECKING, NamedTuple

from .. import CondaError, CondaMultiError, conda_signal_handler
from ..auxlib.collection import first
from ..auxlib.ish import dals
from ..base.constants import DEFAULTS_CHANNEL_NAME, PREFIX_MAGIC_FILE, SafetyChecks
from ..base.context import context
from ..cli.common import confirm_yn
from ..common.compat import ensure_text_type, on_win
from ..common.io import (
    DummyExecutor,
    Spinner,
    ThreadLimitedThreadPoolExecutor,
    dashlist,
    time_recorder,
)
from ..common.path import (
    explode_directories,
    get_all_directories,
    get_major_minor_version,
    get_python_site_packages_short_path,
)
from ..common.signals import signal_handler
from ..exceptions import (
    CondaSystemExit,
    DisallowedPackageError,
    EnvironmentNotWritableError,
    KnownPackageClobberError,
    LinkError,
    RemoveError,
    SharedLinkPathClobberError,
    UnknownPackageClobberError,
    maybe_raise,
)
from ..gateways.disk import mkdir_p
from ..gateways.disk.delete import rm_rf
from ..gateways.disk.read import isfile, lexists, read_package_info
from ..gateways.disk.test import (
    hardlink_supported,
    is_conda_environment,
    softlink_supported,
)
from ..gateways.subprocess import subprocess_call
from ..models.enums import LinkType
from ..models.version import VersionOrder
from ..resolve import MatchSpec
from ..utils import get_comspec, human_bytes, wrap_subprocess_call
from .package_cache_data import PackageCacheData
from .path_actions import (
    AggregateCompileMultiPycAction,
    CompileMultiPycAction,
    CreateNonadminAction,
    CreatePrefixRecordAction,
    CreatePythonEntryPointAction,
    LinkPathAction,
    MakeMenuAction,
    RegisterEnvironmentLocationAction,
    RemoveLinkedPackageRecordAction,
    RemoveMenuAction,
    UnlinkPathAction,
    UnregisterEnvironmentLocationAction,
    UpdateHistoryAction,
)
from .prefix_data import PrefixData, get_python_version_for_prefix

if TYPE_CHECKING:
    from typing import Iterable

    from ..models.package_info import PackageInfo
    from ..models.records import PackageRecord
    from .path_actions import _Action

log = getLogger(__name__)


def determine_link_type(extracted_package_dir, target_prefix):
    source_test_file = join(extracted_package_dir, "info", "index.json")
    if context.always_copy:
        return LinkType.copy
    if context.always_softlink:
        return LinkType.softlink
    if hardlink_supported(source_test_file, target_prefix):
        return LinkType.hardlink
    if context.allow_softlinks and softlink_supported(source_test_file, target_prefix):
        return LinkType.softlink
    return LinkType.copy


def make_unlink_actions(transaction_context, target_prefix, prefix_record):
    # no side effects in this function!
    unlink_path_actions = tuple(
        UnlinkPathAction(transaction_context, prefix_record, target_prefix, trgt)
        for trgt in prefix_record.files
    )

    try:
        extracted_package_dir = basename(prefix_record.extracted_package_dir)
    except AttributeError:
        try:
            extracted_package_dir = basename(prefix_record.link.source)
        except AttributeError:
            # for backward compatibility only
            extracted_package_dir = (
                f"{prefix_record.name}-{prefix_record.version}-{prefix_record.build}"
            )

    meta_short_path = "{}/{}".format("conda-meta", extracted_package_dir + ".json")
    remove_conda_meta_actions = (
        RemoveLinkedPackageRecordAction(
            transaction_context, prefix_record, target_prefix, meta_short_path
        ),
    )

    _all_d = get_all_directories(axn.target_short_path for axn in unlink_path_actions)
    all_directories = sorted(explode_directories(_all_d), reverse=True)
    directory_remove_actions = tuple(
        UnlinkPathAction(
            transaction_context, prefix_record, target_prefix, d, LinkType.directory
        )
        for d in all_directories
    )

    # unregister_private_package_actions = UnregisterPrivateEnvAction.create_actions(
    #     transaction_context, package_cache_record, target_prefix
    # )

    return (
        *unlink_path_actions,
        *directory_remove_actions,
        # *unregister_private_package_actions,
        *remove_conda_meta_actions,
    )


def match_specs_to_dists(packages_info_to_link, specs):
    matched_specs = [None for _ in range(len(packages_info_to_link))]
    for spec in specs or ():
        spec = MatchSpec(spec)
        idx = next(
            (
                q
                for q, pkg_info in enumerate(packages_info_to_link)
                if pkg_info.repodata_record.name == spec.name
            ),
            None,
        )
        if idx is not None:
            matched_specs[idx] = spec
    return tuple(matched_specs)


class PrefixSetup(NamedTuple):
    target_prefix: str
    unlink_precs: tuple[PackageRecord, ...]
    link_precs: tuple[PackageRecord, ...]
    remove_specs: tuple[MatchSpec, ...]
    update_specs: tuple[MatchSpec, ...]
    neutered_specs: tuple[MatchSpec, ...]


class ActionGroup(NamedTuple):
    type: str
    pkg_data: PackageInfo | None
    actions: Iterable[_Action]
    target_prefix: str


class PrefixActionGroup(NamedTuple):
    remove_menu_action_groups: Iterable[ActionGroup]
    unlink_action_groups: Iterable[ActionGroup]
    unregister_action_groups: Iterable[ActionGroup]
    link_action_groups: Iterable[ActionGroup]
    register_action_groups: Iterable[ActionGroup]
    compile_action_groups: Iterable[ActionGroup]
    make_menu_action_groups: Iterable[ActionGroup]
    entry_point_action_groups: Iterable[ActionGroup]
    prefix_record_groups: Iterable[ActionGroup]


class ChangeReport(NamedTuple):
    prefix: str
    specs_to_remove: Iterable[MatchSpec]
    specs_to_add: Iterable[MatchSpec]
    removed_precs: Iterable[PackageRecord]
    new_precs: Iterable[PackageRecord]
    updated_precs: Iterable[PackageRecord]
    downgraded_precs: Iterable[PackageRecord]
    superseded_precs: Iterable[PackageRecord]
    fetch_precs: Iterable[PackageRecord]


class UnlinkLinkTransaction:
    def __init__(self, *setups):
        self.prefix_setups = {stp.target_prefix: stp for stp in setups}
        self.prefix_action_groups = {}

        for stp in self.prefix_setups.values():
            log.info(
                "initializing UnlinkLinkTransaction with\n"
                "  target_prefix: %s\n"
                "  unlink_precs:\n"
                "    %s\n"
                "  link_precs:\n"
                "    %s\n",
                stp.target_prefix,
                "\n    ".join(prec.dist_str() for prec in stp.unlink_precs),
                "\n    ".join(prec.dist_str() for prec in stp.link_precs),
            )

        self._pfe = None
        self._prepared = False
        self._verified = False
        # this can be CPU-bound.  Use ProcessPoolExecutor.
        self.verify_executor = (
            DummyExecutor()
            if context.debug or context.verify_threads == 1
            else ThreadLimitedThreadPoolExecutor(context.verify_threads)
        )
        # this is more I/O bound.  Use ThreadPoolExecutor.
        self.execute_executor = (
            DummyExecutor()
            if context.debug or context.execute_threads == 1
            else ThreadLimitedThreadPoolExecutor(context.execute_threads)
        )

    @property
    def nothing_to_do(self):
        return not any(
            (stp.unlink_precs or stp.link_precs) for stp in self.prefix_setups.values()
        ) and all(
            is_conda_environment(stp.target_prefix)
            for stp in self.prefix_setups.values()
        )

    def download_and_extract(self):
        if self._pfe is None:
            self._get_pfe()
        if not self._pfe._executed:
            self._pfe.execute()

    def prepare(self):
        if self._pfe is None:
            self._get_pfe()
        if not self._pfe._executed:
            self._pfe.execute()

        if self._prepared:
            return

        self.transaction_context = {}

        with Spinner(
            "Preparing transaction",
            not context.verbose and not context.quiet,
            context.json,
        ):
            for stp in self.prefix_setups.values():
                grps = self._prepare(
                    self.transaction_context,
                    stp.target_prefix,
                    stp.unlink_precs,
                    stp.link_precs,
                    stp.remove_specs,
                    stp.update_specs,
                    stp.neutered_specs,
                )
                self.prefix_action_groups[stp.target_prefix] = PrefixActionGroup(*grps)

        self._prepared = True

    @time_recorder("unlink_link_prepare_and_verify")
    def verify(self):
        if not self._prepared:
            self.prepare()

        assert not context.dry_run

        if context.safety_checks == SafetyChecks.disabled:
            self._verified = True
            return

        with Spinner(
            "Verifying transaction",
            not context.verbose and not context.quiet,
            context.json,
        ):
            exceptions = self._verify(self.prefix_setups, self.prefix_action_groups)
            if exceptions:
                try:
                    maybe_raise(CondaMultiError(exceptions), context)
                except:
                    rm_rf(self.transaction_context["temp_dir"])
                    raise
                log.info(exceptions)
        try:
            self._verify_pre_link_message(
                itertools.chain(
                    *(
                        act.link_action_groups
                        for act in self.prefix_action_groups.values()
                    )
                )
            )
        except CondaSystemExit:
            rm_rf(self.transaction_context["temp_dir"])
            raise
        self._verified = True

    def _verify_pre_link_message(self, all_link_groups):
        flag_pre_link = False
        for act in all_link_groups:
            prelink_msg_dir = (
                Path(act.pkg_data.extracted_package_dir) / "info" / "prelink_messages"
            )
            all_msg_subdir = list(
                item for item in prelink_msg_dir.glob("**/*") if item.is_file()
            )
            if prelink_msg_dir.is_dir() and all_msg_subdir:
                print("\n\nThe following PRELINK MESSAGES are INCLUDED:\n\n")
                flag_pre_link = True

                for msg_file in all_msg_subdir:
                    print(f"  File {msg_file.name}:\n")
                    print(indent(msg_file.read_text(), "  "))
                    print()
        if flag_pre_link:
            confirm_yn()

    def execute(self):
        if not self._verified:
            self.verify()

        assert not context.dry_run
        try:
            # innermost dict.values() is an iterable of PrefixActionGroup namedtuple
            # zip() is an iterable of each PrefixActionGroup namedtuple key
            self._execute(
                tuple(chain(*chain(*zip(*self.prefix_action_groups.values()))))
            )
        finally:
            rm_rf(self.transaction_context["temp_dir"])

    def _get_pfe(self):
        from .package_cache_data import ProgressiveFetchExtract

        if self._pfe is not None:
            pfe = self._pfe
        elif not self.prefix_setups:
            self._pfe = pfe = ProgressiveFetchExtract(())
        else:
            link_precs = set(
                chain.from_iterable(
                    stp.link_precs for stp in self.prefix_setups.values()
                )
            )
            self._pfe = pfe = ProgressiveFetchExtract(link_precs)
        return pfe

    @classmethod
    def _prepare(
        cls,
        transaction_context,
        target_prefix,
        unlink_precs,
        link_precs,
        remove_specs,
        update_specs,
        neutered_specs,
    ):
        # make sure prefix directory exists
        if not isdir(target_prefix):
            try:
                mkdir_p(target_prefix)
            except OSError as e:
                log.debug(repr(e))
                raise CondaError(
                    f"Unable to create prefix directory '{target_prefix}'.\n"
                    "Check that you have sufficient permissions."
                    ""
                )

        # gather information from disk and caches
        prefix_data = PrefixData(target_prefix)
        prefix_recs_to_unlink = (prefix_data.get(prec.name) for prec in unlink_precs)
        # NOTE: load_meta can return None
        # TODO: figure out if this filter shouldn't be an assert not None
        prefix_recs_to_unlink = tuple(lpd for lpd in prefix_recs_to_unlink if lpd)
        pkg_cache_recs_to_link = tuple(
            PackageCacheData.get_entry_to_link(prec) for prec in link_precs
        )
        assert all(pkg_cache_recs_to_link)
        packages_info_to_link = tuple(
            read_package_info(prec, pcrec)
            for prec, pcrec in zip(link_precs, pkg_cache_recs_to_link)
        )

        link_types = tuple(
            determine_link_type(pkg_info.extracted_package_dir, target_prefix)
            for pkg_info in packages_info_to_link
        )

        # make all the path actions
        # no side effects allowed when instantiating these action objects
        python_version = cls._get_python_version(
            target_prefix, prefix_recs_to_unlink, packages_info_to_link
        )
        transaction_context["target_python_version"] = python_version
        sp = get_python_site_packages_short_path(python_version)
        transaction_context["target_site_packages_short_path"] = sp

        transaction_context["temp_dir"] = join(target_prefix, ".condatmp")

        remove_menu_action_groups = []
        unlink_action_groups = []
        for prefix_rec in prefix_recs_to_unlink:
            unlink_action_groups.append(
                ActionGroup(
                    "unlink",
                    prefix_rec,
                    make_unlink_actions(transaction_context, target_prefix, prefix_rec),
                    target_prefix,
                )
            )

            remove_menu_action_groups.append(
                ActionGroup(
                    "remove_menus",
                    prefix_rec,
                    RemoveMenuAction.create_actions(
                        transaction_context, prefix_rec, target_prefix
                    ),
                    target_prefix,
                )
            )

        if unlink_action_groups:
            axns = (
                UnregisterEnvironmentLocationAction(transaction_context, target_prefix),
            )
            unregister_action_groups = [
                ActionGroup("unregister", None, axns, target_prefix)
            ]
        else:
            unregister_action_groups = ()

        matchspecs_for_link_dists = match_specs_to_dists(
            packages_info_to_link, update_specs
        )
        link_action_groups = []
        entry_point_action_groups = []
        compile_action_groups = []
        make_menu_action_groups = []
        record_axns = []
        for pkg_info, lt, spec in zip(
            packages_info_to_link, link_types, matchspecs_for_link_dists
        ):
            link_ag = ActionGroup(
                "link",
                pkg_info,
                cls._make_link_actions(
                    transaction_context, pkg_info, target_prefix, lt, spec
                ),
                target_prefix,
            )
            link_action_groups.append(link_ag)

            entry_point_ag = ActionGroup(
                "entry_point",
                pkg_info,
                cls._make_entry_point_actions(
                    transaction_context,
                    pkg_info,
                    target_prefix,
                    lt,
                    spec,
                    link_action_groups,
                ),
                target_prefix,
            )
            entry_point_action_groups.append(entry_point_ag)

            compile_ag = ActionGroup(
                "compile",
                pkg_info,
                cls._make_compile_actions(
                    transaction_context,
                    pkg_info,
                    target_prefix,
                    lt,
                    spec,
                    link_action_groups,
                ),
                target_prefix,
            )
            compile_action_groups.append(compile_ag)

            make_menu_ag = ActionGroup(
                "make_menus",
                pkg_info,
                MakeMenuAction.create_actions(
                    transaction_context, pkg_info, target_prefix, lt
                ),
                target_prefix,
            )
            make_menu_action_groups.append(make_menu_ag)

            all_link_path_actions = (
                *link_ag.actions,
                *compile_ag.actions,
                *entry_point_ag.actions,
                *make_menu_ag.actions,
            )
            record_axns.extend(
                CreatePrefixRecordAction.create_actions(
                    transaction_context,
                    pkg_info,
                    target_prefix,
                    lt,
                    spec,
                    all_link_path_actions,
                )
            )

        prefix_record_groups = [ActionGroup("record", None, record_axns, target_prefix)]

        # We're post solve here.  The update_specs are explicit requests.  We need to neuter
        #    any historic spec that was neutered prior to the solve.
        history_actions = UpdateHistoryAction.create_actions(
            transaction_context,
            target_prefix,
            remove_specs,
            update_specs,
            neutered_specs,
        )
        register_actions = (
            RegisterEnvironmentLocationAction(transaction_context, target_prefix),
        )
        register_action_groups = [
            ActionGroup(
                "register", None, register_actions + history_actions, target_prefix
            )
        ]
        return PrefixActionGroup(
            remove_menu_action_groups,
            unlink_action_groups,
            unregister_action_groups,
            link_action_groups,
            register_action_groups,
            compile_action_groups,
            make_menu_action_groups,
            entry_point_action_groups,
            prefix_record_groups,
        )

    @staticmethod
    def _verify_individual_level(prefix_action_group):
        all_actions = chain.from_iterable(
            axngroup.actions
            for action_groups in prefix_action_group
            for axngroup in action_groups
        )

        # run all per-action (per-package) verify methods
        #   one of the more important of these checks is to verify that a file listed in
        #   the packages manifest (i.e. info/files) is actually contained within the package
        error_results = []
        for axn in all_actions:
            if axn.verified:
                continue
            error_result = axn.verify()
            if error_result:
                formatted_error = "".join(
                    format_exception_only(type(error_result), error_result)
                )
                log.debug("Verification error in action %s\n%s", axn, formatted_error)
                error_results.append(error_result)
        return error_results

    @staticmethod
    def _verify_prefix_level(target_prefix_AND_prefix_action_group_tuple):
        # further verification of the whole transaction
        # for each path we are creating in link_actions, we need to make sure
        #   1. each path either doesn't already exist in the prefix, or will be unlinked
        #   2. there's only a single instance of each path
        #   3. if the target is a private env, leased paths need to be verified
        #   4. make sure conda-meta/history file is writable
        #   5. make sure envs/catalog.json is writable; done with RegisterEnvironmentLocationAction
        # TODO: 3, 4

        # this strange unpacking is to help the parallel execution work.  Unpacking
        #    tuples in the map call could be done with a lambda, but that is then not picklable,
        #    which precludes the use of ProcessPoolExecutor (but not ThreadPoolExecutor)
        target_prefix, prefix_action_group = target_prefix_AND_prefix_action_group_tuple

        unlink_action_groups = prefix_action_group.unlink_action_groups
        prefix_record_groups = prefix_action_group.prefix_record_groups

        lower_on_win = lambda p: p.lower() if on_win else p
        unlink_paths = {
            lower_on_win(axn.target_short_path)
            for grp in unlink_action_groups
            for axn in grp.actions
            if isinstance(axn, UnlinkPathAction)
        }
        # we can get all of the paths being linked by looking only at the
        #   CreateLinkedPackageRecordAction actions
        create_lpr_actions = (
            axn
            for grp in prefix_record_groups
            for axn in grp.actions
            if isinstance(axn, CreatePrefixRecordAction)
        )

        error_results = []
        # Verification 1. each path either doesn't already exist in the prefix, or will be unlinked
        link_paths_dict = defaultdict(list)
        for axn in create_lpr_actions:
            for link_path_action in axn.all_link_path_actions:
                if isinstance(link_path_action, CompileMultiPycAction):
                    target_short_paths = link_path_action.target_short_paths
                elif isinstance(link_path_action, CreateNonadminAction):
                    continue
                else:
                    target_short_paths = (
                        (link_path_action.target_short_path,)
                        if not hasattr(link_path_action, "link_type")
                        or link_path_action.link_type != LinkType.directory
                        else ()
                    )
                for path in target_short_paths:
                    path = lower_on_win(path)
                    link_paths_dict[path].append(axn)
                    if path not in unlink_paths and lexists(join(target_prefix, path)):
                        # we have a collision; at least try to figure out where it came from
                        colliding_prefix_rec = first(
                            (
                                prefix_rec
                                for prefix_rec in PrefixData(
                                    target_prefix
                                ).iter_records()
                            ),
                            key=lambda prefix_rec: path in prefix_rec.files,
                        )
                        if colliding_prefix_rec:
                            error_results.append(
                                KnownPackageClobberError(
                                    path,
                                    axn.package_info.repodata_record.dist_str(),
                                    colliding_prefix_rec.dist_str(),
                                    context,
                                )
                            )
                        else:
                            error_results.append(
                                UnknownPackageClobberError(
                                    path,
                                    axn.package_info.repodata_record.dist_str(),
                                    context,
                                )
                            )

        # Verification 2. there's only a single instance of each path
        for path, axns in link_paths_dict.items():
            if len(axns) > 1:
                error_results.append(
                    SharedLinkPathClobberError(
                        path,
                        tuple(
                            axn.package_info.repodata_record.dist_str() for axn in axns
                        ),
                        context,
                    )
                )
        return error_results

    @staticmethod
    def _verify_transaction_level(prefix_setups):
        # 1. make sure we're not removing conda from conda's env
        # 2. make sure we're not removing a conda dependency from conda's env
        # 3. enforce context.disallowed_packages
        # 4. make sure we're not removing pinned packages without no-pin flag
        # 5. make sure conda-meta/history for each prefix is writable
        # TODO: Verification 4

        conda_prefixes = (
            join(context.root_prefix, "envs", "_conda_"),
            context.root_prefix,
        )
        conda_setups = tuple(
            setup
            for setup in prefix_setups.values()
            if setup.target_prefix in conda_prefixes
        )

        conda_unlinked = any(
            prec.name == "conda"
            for setup in conda_setups
            for prec in setup.unlink_precs
        )

        conda_prec, conda_final_setup = next(
            (
                (prec, setup)
                for setup in conda_setups
                for prec in setup.link_precs
                if prec.name == "conda"
            ),
            (None, None),
        )

        if conda_unlinked and conda_final_setup is None:
            # means conda is being unlinked and not re-linked anywhere
            # this should never be able to be skipped, even with --force
            yield RemoveError(
                "This operation will remove conda without replacing it with\n"
                "another version of conda."
            )

        if conda_final_setup is None:
            # means we're not unlinking then linking a new package, so look up current conda record
            conda_final_prefix = context.conda_prefix
            pd = PrefixData(conda_final_prefix)
            pkg_names_already_lnkd = tuple(rec.name for rec in pd.iter_records())
            pkg_names_being_lnkd = ()
            pkg_names_being_unlnkd = ()
            conda_linked_depends = next(
                (
                    record.depends
                    for record in pd.iter_records()
                    if record.name == "conda"
                ),
                (),
            )
        else:
            conda_final_prefix = conda_final_setup.target_prefix
            pd = PrefixData(conda_final_prefix)
            pkg_names_already_lnkd = tuple(rec.name for rec in pd.iter_records())
            pkg_names_being_lnkd = tuple(
                prec.name for prec in conda_final_setup.link_precs or ()
            )
            pkg_names_being_unlnkd = tuple(
                prec.name for prec in conda_final_setup.unlink_precs or ()
            )
            conda_linked_depends = conda_prec.depends

        if conda_final_prefix in prefix_setups:
            for conda_dependency in conda_linked_depends:
                dep_name = MatchSpec(conda_dependency).name
                if dep_name not in pkg_names_being_lnkd and (
                    dep_name not in pkg_names_already_lnkd
                    or dep_name in pkg_names_being_unlnkd
                ):
                    yield RemoveError(
                        f"'{dep_name}' is a dependency of conda and cannot be removed from\n"
                        "conda's operating environment."
                    )

        # Verification 3. enforce disallowed_packages
        disallowed = tuple(MatchSpec(s) for s in context.disallowed_packages)
        for prefix_setup in prefix_setups.values():
            for prec in prefix_setup.link_precs:
                if any(d.match(prec) for d in disallowed):
                    yield DisallowedPackageError(prec)

        # Verification 5. make sure conda-meta/history for each prefix is writable
        for prefix_setup in prefix_setups.values():
            test_path = join(prefix_setup.target_prefix, PREFIX_MAGIC_FILE)
            test_path_existed = lexists(test_path)
            dir_existed = None
            try:
                dir_existed = mkdir_p(dirname(test_path))
                open(test_path, "a").close()
            except OSError:
                if dir_existed is False:
                    rm_rf(dirname(test_path))
                yield EnvironmentNotWritableError(prefix_setup.target_prefix)
            else:
                if not dir_existed:
                    rm_rf(dirname(test_path))
                elif not test_path_existed:
                    rm_rf(test_path)

    def _verify(self, prefix_setups, prefix_action_groups):
        transaction_exceptions = tuple(
            exc
            for exc in UnlinkLinkTransaction._verify_transaction_level(prefix_setups)
            if exc
        )
        if transaction_exceptions:
            return transaction_exceptions

        exceptions = []
        for exc in self.verify_executor.map(
            UnlinkLinkTransaction._verify_individual_level,
            prefix_action_groups.values(),
        ):
            if exc:
                exceptions.extend(exc)
        for exc in self.verify_executor.map(
            UnlinkLinkTransaction._verify_prefix_level, prefix_action_groups.items()
        ):
            if exc:
                exceptions.extend(exc)
        return exceptions

    def _execute(self, all_action_groups):
        # unlink unlink_action_groups and unregister_action_groups
        unlink_actions = tuple(
            group for group in all_action_groups if group.type == "unlink"
        )
        # link unlink_action_groups and register_action_groups
        link_actions = list(
            group for group in all_action_groups if group.type == "link"
        )
        compile_actions = list(
            group for group in all_action_groups if group.type == "compile"
        )
        entry_point_actions = list(
            group for group in all_action_groups if group.type == "entry_point"
        )
        record_actions = list(
            group for group in all_action_groups if group.type == "record"
        )
        make_menu_actions = list(
            group for group in all_action_groups if group.type == "make_menus"
        )
        remove_menu_actions = list(
            group for group in all_action_groups if group.type == "remove_menus"
        )

        with signal_handler(conda_signal_handler), time_recorder("unlink_link_execute"):
            exceptions = []
            with Spinner(
                "Executing transaction",
                not context.verbose and not context.quiet,
                context.json,
            ):
                # Execute unlink actions
                for group, register_group, install_side in (
                    (unlink_actions, "unregister", False),
                    (link_actions, "register", True),
                ):
                    if not install_side:
                        # uninstalling menus must happen prior to unlinking, or else they might
                        #   call something that isn't there anymore
                        for axngroup in remove_menu_actions:
                            UnlinkLinkTransaction._execute_actions(axngroup)

                    for axngroup in group:
                        is_unlink = axngroup.type == "unlink"
                        target_prefix = axngroup.target_prefix
                        prec = axngroup.pkg_data
                        run_script(
                            target_prefix if is_unlink else prec.extracted_package_dir,
                            prec,
                            "pre-unlink" if is_unlink else "pre-link",
                            target_prefix,
                        )

                    # parallel block 1:
                    for exc in self.execute_executor.map(
                        UnlinkLinkTransaction._execute_actions, group
                    ):
                        if exc:
                            exceptions.append(exc)

                    # post link scripts may employ entry points.  Do them before post-link.
                    if install_side:
                        for axngroup in entry_point_actions:
                            UnlinkLinkTransaction._execute_actions(axngroup)

                    # Run post-link or post-unlink scripts and registering AFTER link/unlink,
                    #    because they may depend on files in the prefix.  Additionally, run
                    #    them serially, just in case order matters (hopefully not)
                    for axngroup in group:
                        exc = UnlinkLinkTransaction._execute_post_link_actions(axngroup)
                        if exc:
                            exceptions.append(exc)

                    # parallel block 2:
                    composite_ag = []
                    if install_side:
                        composite_ag.extend(record_actions)
                        # consolidate compile actions into one big'un for better efficiency
                        individual_actions = [
                            axn for ag in compile_actions for axn in ag.actions
                        ]
                        if individual_actions:
                            composite = AggregateCompileMultiPycAction(
                                *individual_actions
                            )
                            composite_ag.append(
                                ActionGroup(
                                    "compile",
                                    None,
                                    [composite],
                                    composite.target_prefix,
                                )
                            )
                    # functions return None unless there was an exception
                    for exc in self.execute_executor.map(
                        UnlinkLinkTransaction._execute_actions, composite_ag
                    ):
                        if exc:
                            exceptions.append(exc)

                    # must do the register actions AFTER all link/unlink is done
                    register_actions = tuple(
                        group
                        for group in all_action_groups
                        if group.type == register_group
                    )
                    for axngroup in register_actions:
                        exc = UnlinkLinkTransaction._execute_actions(axngroup)
                        if exc:
                            exceptions.append(exc)
                    if exceptions:
                        break
                    if install_side:
                        # uninstalling menus must happen prior to unlinking, or else they might
                        #   call something that isn't there anymore
                        for axngroup in make_menu_actions:
                            UnlinkLinkTransaction._execute_actions(axngroup)
            if exceptions:
                # might be good to show all errors, but right now we only show the first
                e = exceptions[0]
                axngroup = e.errors[1]

                action, is_unlink = (None, axngroup.type == "unlink")
                prec = axngroup.pkg_data

                if prec:
                    log.error(
                        "An error occurred while {} package '{}'.".format(
                            "uninstalling" if is_unlink else "installing",
                            prec.dist_str(),
                        )
                    )

                # reverse all executed packages except the one that failed
                rollback_excs = []
                if context.rollback_enabled:
                    with Spinner(
                        "Rolling back transaction",
                        not context.verbose and not context.quiet,
                        context.json,
                    ):
                        reverse_actions = reversed(tuple(all_action_groups))
                        for axngroup in reverse_actions:
                            excs = UnlinkLinkTransaction._reverse_actions(axngroup)
                            rollback_excs.extend(excs)

                raise CondaMultiError(
                    (
                        *(
                            (e.errors[0], e.errors[2:])
                            if isinstance(e, CondaMultiError)
                            else (e,)
                        ),
                        *rollback_excs,
                    )
                )
            else:
                for axngroup in all_action_groups:
                    for action in axngroup.actions:
                        action.cleanup()

    @staticmethod
    def _execute_actions(axngroup):
        target_prefix = axngroup.target_prefix
        prec = axngroup.pkg_data

        conda_meta_dir = join(target_prefix, "conda-meta")
        if not isdir(conda_meta_dir):
            mkdir_p(conda_meta_dir)

        try:
            if axngroup.type == "unlink":
                log.info(
                    "===> UNLINKING PACKAGE: %s <===\n  prefix=%s\n",
                    prec.dist_str(),
                    target_prefix,
                )

            elif axngroup.type == "link":
                log.info(
                    "===> LINKING PACKAGE: %s <===\n  prefix=%s\n  source=%s\n",
                    prec.dist_str(),
                    target_prefix,
                    prec.extracted_package_dir,
                )

            for action in axngroup.actions:
                action.execute()
        except Exception as e:  # this won't be a multi error
            # reverse this package
            reverse_excs = ()
            if context.rollback_enabled:
                reverse_excs = UnlinkLinkTransaction._reverse_actions(axngroup)
            return CondaMultiError(
                (
                    e,
                    axngroup,
                    *reverse_excs,
                )
            )

    @staticmethod
    def _execute_post_link_actions(axngroup):
        target_prefix = axngroup.target_prefix
        is_unlink = axngroup.type == "unlink"
        prec = axngroup.pkg_data
        if prec:
            try:
                run_script(
                    target_prefix,
                    prec,
                    "post-unlink" if is_unlink else "post-link",
                    activate=True,
                )
            except Exception as e:  # this won't be a multi error
                # reverse this package
                reverse_excs = ()
                if context.rollback_enabled:
                    reverse_excs = UnlinkLinkTransaction._reverse_actions(axngroup)
                return CondaMultiError(
                    (
                        e,
                        axngroup,
                        *reverse_excs,
                    )
                )

    @staticmethod
    def _reverse_actions(axngroup, reverse_from_idx=-1):
        target_prefix = axngroup.target_prefix

        # reverse_from_idx = -1 means reverse all actions
        prec = axngroup.pkg_data

        if axngroup.type == "unlink":
            log.info(
                "===> REVERSING PACKAGE UNLINK: %s <===\n  prefix=%s\n",
                prec.dist_str(),
                target_prefix,
            )

        elif axngroup.type == "link":
            log.info(
                "===> REVERSING PACKAGE LINK: %s <===\n  prefix=%s\n",
                prec.dist_str(),
                target_prefix,
            )

        exceptions = []
        if reverse_from_idx < 0:
            reverse_actions = axngroup.actions
        else:
            reverse_actions = axngroup.actions[: reverse_from_idx + 1]
        for axn_idx, action in reversed(tuple(enumerate(reverse_actions))):
            try:
                action.reverse()
            except Exception as e:
                log.debug("action.reverse() error in action %r", action, exc_info=True)
                exceptions.append(e)
        return exceptions

    @staticmethod
    def _get_python_version(target_prefix, pcrecs_to_unlink, packages_info_to_link):
        # this method determines the python version that will be present at the
        # end of the transaction
        linking_new_python = next(
            (
                package_info
                for package_info in packages_info_to_link
                if package_info.repodata_record.name == "python"
            ),
            None,
        )
        if linking_new_python:
            # is python being linked? we're done
            full_version = linking_new_python.repodata_record.version
            assert full_version
            log.debug("found in current transaction python version %s", full_version)
            return get_major_minor_version(full_version)

        # is python already linked and not being unlinked? that's ok too
        linked_python_version = get_python_version_for_prefix(target_prefix)
        if linked_python_version:
            find_python = (
                lnkd_pkg_data
                for lnkd_pkg_data in pcrecs_to_unlink
                if lnkd_pkg_data.name == "python"
            )
            unlinking_this_python = next(find_python, None)
            if unlinking_this_python is None:
                # python is not being unlinked
                log.debug(
                    "found in current prefix python version %s", linked_python_version
                )
                return linked_python_version

        # there won't be any python in the finished environment
        log.debug("no python version found in prefix")
        return None

    @staticmethod
    def _make_link_actions(
        transaction_context,
        package_info,
        target_prefix,
        requested_link_type,
        requested_spec,
    ):
        required_quad = (
            transaction_context,
            package_info,
            target_prefix,
            requested_link_type,
        )

        file_link_actions = LinkPathAction.create_file_link_actions(*required_quad)
        create_directory_actions = LinkPathAction.create_directory_actions(
            *required_quad, file_link_actions=file_link_actions
        )
        create_nonadmin_actions = CreateNonadminAction.create_actions(*required_quad)

        # the ordering here is significant
        return (
            *create_directory_actions,
            *file_link_actions,
            *create_nonadmin_actions,
        )

    @staticmethod
    def _make_entry_point_actions(
        transaction_context,
        package_info,
        target_prefix,
        requested_link_type,
        requested_spec,
        link_action_groups,
    ):
        required_quad = (
            transaction_context,
            package_info,
            target_prefix,
            requested_link_type,
        )
        return CreatePythonEntryPointAction.create_actions(*required_quad)

    @staticmethod
    def _make_compile_actions(
        transaction_context,
        package_info,
        target_prefix,
        requested_link_type,
        requested_spec,
        link_action_groups,
    ):
        required_quad = (
            transaction_context,
            package_info,
            target_prefix,
            requested_link_type,
        )
        link_action_group = next(
            ag for ag in link_action_groups if ag.pkg_data == package_info
        )
        return CompileMultiPycAction.create_actions(
            *required_quad, file_link_actions=link_action_group.actions
        )

    def _make_legacy_action_groups(self):
        # this code reverts json output for plan back to previous behavior
        #   relied on by Anaconda Navigator and nb_conda
        legacy_action_groups = []

        if self._pfe is None:
            self._get_pfe()

        for q, (prefix, setup) in enumerate(self.prefix_setups.items()):
            actions = defaultdict(list)
            if q == 0:
                self._pfe.prepare()
                download_urls = {axn.url for axn in self._pfe.cache_actions}
                actions["FETCH"].extend(
                    prec for prec in self._pfe.link_precs if prec.url in download_urls
                )

            actions["PREFIX"] = setup.target_prefix
            for prec in setup.unlink_precs:
                actions["UNLINK"].append(prec)
            for prec in setup.link_precs:
                # TODO (AV): maybe add warnings about unverified packages here;
                # be warned that doing so may break compatibility with other
                # applications.
                actions["LINK"].append(prec)

            legacy_action_groups.append(actions)

        return legacy_action_groups

    def print_transaction_summary(self):
        legacy_action_groups = self._make_legacy_action_groups()

        download_urls = {axn.url for axn in self._pfe.cache_actions}

        for actions, (prefix, stp) in zip(
            legacy_action_groups, self.prefix_setups.items()
        ):
            change_report = self._calculate_change_report(
                prefix,
                stp.unlink_precs,
                stp.link_precs,
                download_urls,
                stp.remove_specs,
                stp.update_specs,
            )
            change_report_str = self._change_report_str(change_report)
            print(ensure_text_type(change_report_str))

        return legacy_action_groups

    def _change_report_str(self, change_report):
        # TODO (AV): add warnings about unverified packages in this function
        builder = ["", "## Package Plan ##\n"]
        builder.append(f"  environment location: {change_report.prefix}")
        builder.append("")
        if change_report.specs_to_remove:
            builder.append(
                "  removed specs:{}".format(
                    dashlist(
                        sorted(str(s) for s in change_report.specs_to_remove), indent=4
                    )
                )
            )
            builder.append("")
        if change_report.specs_to_add:
            builder.append(
                f"  added / updated specs:{dashlist(sorted(str(s) for s in change_report.specs_to_add), indent=4)}"
            )
            builder.append("")

        def channel_filt(s):
            if context.show_channel_urls is False:
                return ""
            if context.show_channel_urls is None and s == DEFAULTS_CHANNEL_NAME:
                return ""
            return s

        def print_dists(dists_extras):
            lines = []
            fmt = "    %-27s|%17s"
            lines.append(fmt % ("package", "build"))
            lines.append(fmt % ("-" * 27, "-" * 17))
            for prec, extra in dists_extras:
                line = fmt % (
                    strip_global(prec.namekey) + "-" + prec.version,
                    prec.build,
                )
                if extra:
                    line += extra
                lines.append(line)
            return lines

        convert_namekey = lambda x: ("0:" + x[7:]) if x.startswith("global:") else x
        strip_global = lambda x: x[7:] if x.startswith("global:") else x

        if change_report.fetch_precs:
            builder.append("\nThe following packages will be downloaded:\n")

            disp_lst = []
            total_download_bytes = 0
            for prec in sorted(
                change_report.fetch_precs, key=lambda x: convert_namekey(x.namekey)
            ):
                size = prec.size
                extra = "%15s" % human_bytes(size)
                total_download_bytes += size
                schannel = channel_filt(str(prec.channel.canonical_name))
                if schannel:
                    extra += "  " + schannel
                disp_lst.append((prec, extra))
            builder.extend(print_dists(disp_lst))

            builder.append(" " * 4 + "-" * 60)
            builder.append(" " * 43 + "Total: %14s" % human_bytes(total_download_bytes))

        def diff_strs(unlink_prec, link_prec):
            channel_change = unlink_prec.channel.name != link_prec.channel.name
            subdir_change = unlink_prec.subdir != link_prec.subdir
            version_change = unlink_prec.version != link_prec.version
            build_change = unlink_prec.build != link_prec.build

            builder_left = []
            builder_right = []

            if channel_change or subdir_change:
                if unlink_prec.channel.name is not None:
                    builder_left.append(unlink_prec.channel.name)
                if link_prec.channel.name is not None:
                    builder_right.append(link_prec.channel.name)
            if subdir_change:
                builder_left.append("/" + unlink_prec.subdir)
                builder_right.append("/" + link_prec.subdir)
            if (channel_change or subdir_change) and (version_change or build_change):
                builder_left.append("::" + unlink_prec.name + "-")
                builder_right.append("::" + link_prec.name + "-")
            if version_change or build_change:
                builder_left.append(unlink_prec.version + "-" + unlink_prec.build)
                builder_right.append(link_prec.version + "-" + link_prec.build)

            return "".join(builder_left), "".join(builder_right)

        def add_single(display_key, disp_str):
            if len(display_key) > 18:
                display_key = display_key[:17] + "~"
            builder.append("  %-18s %s" % (display_key, disp_str))

        def add_double(display_key, left_str, right_str):
            if len(display_key) > 18:
                display_key = display_key[:17] + "~"
            if len(left_str) > 38:
                left_str = left_str[:37] + "~"
            builder.append("  %-18s %38s --> %s" % (display_key, left_str, right_str))

        if change_report.new_precs:
            builder.append("\nThe following NEW packages will be INSTALLED:\n")
            for namekey in sorted(change_report.new_precs, key=convert_namekey):
                link_prec = change_report.new_precs[namekey]
                add_single(
                    strip_global(namekey),
                    f"{link_prec.record_id()} {' '.join(link_prec.metadata)}",
                )

        if change_report.removed_precs:
            builder.append("\nThe following packages will be REMOVED:\n")
            for namekey in sorted(change_report.removed_precs, key=convert_namekey):
                unlink_prec = change_report.removed_precs[namekey]
                builder.append(
                    f"  {unlink_prec.name}-{unlink_prec.version}-{unlink_prec.build}"
                )

        if change_report.updated_precs:
            builder.append("\nThe following packages will be UPDATED:\n")
            for namekey in sorted(change_report.updated_precs, key=convert_namekey):
                unlink_prec, link_prec = change_report.updated_precs[namekey]
                left_str, right_str = diff_strs(unlink_prec, link_prec)
                add_double(
                    strip_global(namekey),
                    left_str,
                    f"{right_str} {' '.join(link_prec.metadata)}",
                )

        if change_report.superseded_precs:
            builder.append(
                "\nThe following packages will be SUPERSEDED "
                "by a higher-priority channel:\n"
            )
            for namekey in sorted(change_report.superseded_precs, key=convert_namekey):
                unlink_prec, link_prec = change_report.superseded_precs[namekey]
                left_str, right_str = diff_strs(unlink_prec, link_prec)
                add_double(
                    strip_global(namekey),
                    left_str,
                    f"{right_str} {' '.join(link_prec.metadata)}",
                )

        if change_report.downgraded_precs:
            builder.append("\nThe following packages will be DOWNGRADED:\n")
            for namekey in sorted(change_report.downgraded_precs, key=convert_namekey):
                unlink_prec, link_prec = change_report.downgraded_precs[namekey]
                left_str, right_str = diff_strs(unlink_prec, link_prec)
                add_double(
                    strip_global(namekey),
                    left_str,
                    f"{right_str} {' '.join(link_prec.metadata)}",
                )
        builder.append("")
        builder.append("")
        return "\n".join(builder)

    @staticmethod
    def _calculate_change_report(
        prefix, unlink_precs, link_precs, download_urls, specs_to_remove, specs_to_add
    ):
        unlink_map = {prec.namekey: prec for prec in unlink_precs}
        link_map = {prec.namekey: prec for prec in link_precs}
        unlink_namekeys, link_namekeys = set(unlink_map), set(link_map)

        removed_precs = {
            namekey: unlink_map[namekey]
            for namekey in (unlink_namekeys - link_namekeys)
        }
        new_precs = {
            namekey: link_map[namekey] for namekey in (link_namekeys - unlink_namekeys)
        }

        # updated means a version increase, or a build number increase
        # downgraded means a version decrease, or build number decrease, but channel canonical_name
        #   has to be the same
        # superseded then should be everything else left over
        updated_precs = {}
        downgraded_precs = {}
        superseded_precs = {}

        common_namekeys = link_namekeys & unlink_namekeys
        for namekey in common_namekeys:
            unlink_prec, link_prec = unlink_map[namekey], link_map[namekey]
            unlink_vo = VersionOrder(unlink_prec.version)
            link_vo = VersionOrder(link_prec.version)
            build_number_increases = link_prec.build_number > unlink_prec.build_number
            if link_vo == unlink_vo and build_number_increases or link_vo > unlink_vo:
                updated_precs[namekey] = (unlink_prec, link_prec)
            elif (
                link_prec.channel.name == unlink_prec.channel.name
                and link_prec.subdir == unlink_prec.subdir
            ):
                if link_prec == unlink_prec:
                    # noarch: python packages are re-linked on a python version change
                    # just leave them out of the package report
                    continue
                downgraded_precs[namekey] = (unlink_prec, link_prec)
            else:
                superseded_precs[namekey] = (unlink_prec, link_prec)

        fetch_precs = {prec for prec in link_precs if prec.url in download_urls}
        change_report = ChangeReport(
            prefix,
            specs_to_remove,
            specs_to_add,
            removed_precs,
            new_precs,
            updated_precs,
            downgraded_precs,
            superseded_precs,
            fetch_precs,
        )
        return change_report


def run_script(
    prefix: str,
    prec,
    action: str = "post-link",
    env_prefix: str = None,
    activate: bool = False,
) -> bool:
    """
    Call the post-link (or pre-unlink) script, returning True on success,
    False on failure.
    """
    path = join(
        prefix,
        "Scripts" if on_win else "bin",
        ".{}-{}.{}".format(prec.name, action, "bat" if on_win else "sh"),
    )
    if not isfile(path):
        return True

    env = os.environ.copy()

    if action == "pre-link":  # pragma: no cover
        # old no-arch support; deprecated
        is_old_noarch = False
        try:
            with open(path) as f:
                script_text = ensure_text_type(f.read())
            if (
                on_win and "%PREFIX%\\python.exe %SOURCE_DIR%\\link.py" in script_text
            ) or "$PREFIX/bin/python $SOURCE_DIR/link.py" in script_text:
                is_old_noarch = True
        except Exception as e:
            log.debug(e, exc_info=True)

        env["SOURCE_DIR"] = prefix
        if not is_old_noarch:
            warnings.warn(
                dals(
                    """
            Package %s uses a pre-link script. Pre-link scripts are potentially dangerous.
            This is because pre-link scripts have the ability to change the package contents in the
            package cache, and therefore modify the underlying files for already-created conda
            environments.  Future versions of conda may deprecate and ignore pre-link scripts.
            """
                )
                % prec.dist_str()
            )

    script_caller = None
    if on_win:
        try:
            comspec = get_comspec()  # fail early with KeyError if undefined
        except KeyError:
            log.info(
                "failed to run %s for %s due to COMSPEC KeyError",
                action,
                prec.dist_str(),
            )
            return False
        if activate:
            script_caller, command_args = wrap_subprocess_call(
                context.root_prefix,
                prefix,
                context.dev,
                False,
                ("@CALL", path),
            )
        else:
            command_args = [comspec, "/d", "/c", path]
    else:
        shell_path = "sh" if "bsd" in sys.platform else "bash"
        if activate:
            script_caller, command_args = wrap_subprocess_call(
                context.root_prefix,
                prefix,
                context.dev,
                False,
                (".", path),
            )
        else:
            shell_path = "sh" if "bsd" in sys.platform else "bash"
            command_args = [shell_path, "-x", path]

    env["ROOT_PREFIX"] = context.root_prefix
    env["PREFIX"] = env_prefix or prefix
    env["PKG_NAME"] = prec.name
    env["PKG_VERSION"] = prec.version
    env["PKG_BUILDNUM"] = prec.build_number
    env["PATH"] = os.pathsep.join((dirname(path), env.get("PATH", "")))

    log.debug(
        "for %s at %s, executing script: $ %s",
        prec.dist_str(),
        env["PREFIX"],
        " ".join(command_args),
    )
    try:
        response = subprocess_call(
            command_args, env=env, path=dirname(path), raise_on_error=False
        )
        if response.rc != 0:
            m = messages(prefix)
            if action in ("pre-link", "post-link"):
                if "openssl" in prec.dist_str():
                    # this is a hack for conda-build string parsing in the conda_build/build.py
                    #   create_env function
                    message = f"{action} failed for: {prec}"
                else:
                    message = dals(
                        """
                    %s script failed for package %s
                    location of failed script: %s
                    ==> script messages <==
                    %s
                    ==> script output <==
                    stdout: %s
                    stderr: %s
                    return code: %s
                    """
                    ) % (
                        action,
                        prec.dist_str(),
                        path,
                        m or "<None>",
                        response.stdout,
                        response.stderr,
                        response.rc,
                    )
                raise LinkError(message)
            else:
                log.warning(
                    "%s script failed for package %s\n"
                    "consider notifying the package maintainer",
                    action,
                    prec.dist_str(),
                )
                return False
        else:
            messages(prefix)
            return True
    finally:
        if script_caller is not None:
            if "CONDA_TEST_SAVE_TEMPS" not in os.environ:
                rm_rf(script_caller)
            else:
                log.warning(
                    f"CONDA_TEST_SAVE_TEMPS :: retaining run_script {script_caller}"
                )


def messages(prefix):
    path = join(prefix, ".messages.txt")
    try:
        if isfile(path):
            with open(path) as fi:
                m = fi.read()
                if hasattr(m, "decode"):
                    m = m.decode("utf-8")
                print(m, file=sys.stderr if context.json else sys.stdout)
                return m
    finally:
        rm_rf(path)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Tools for managing the package cache (previously downloaded packages)."""

from __future__ import annotations

import codecs
import os
from collections import defaultdict
from concurrent.futures import CancelledError, ThreadPoolExecutor, as_completed
from errno import EACCES, ENOENT, EPERM, EROFS
from functools import partial
from itertools import chain
from json import JSONDecodeError
from logging import getLogger
from os import scandir
from os.path import basename, dirname, getsize, join
from sys import platform
from tarfile import ReadError
from typing import TYPE_CHECKING

from .. import CondaError, CondaMultiError, conda_signal_handler
from ..auxlib.collection import first
from ..auxlib.decorators import memoizemethod
from ..auxlib.entity import ValidationError
from ..base.constants import (
    CONDA_PACKAGE_EXTENSION_V1,
    CONDA_PACKAGE_EXTENSION_V2,
    CONDA_PACKAGE_EXTENSIONS,
    PACKAGE_CACHE_MAGIC_FILE,
)
from ..base.context import context
from ..common.constants import NULL, TRACE
from ..common.io import IS_INTERACTIVE, ProgressBar, time_recorder
from ..common.iterators import groupby_to_dict as groupby
from ..common.path import expand, strip_pkg_extension, url_to_path
from ..common.signals import signal_handler
from ..common.url import path_to_url
from ..deprecations import deprecated
from ..exceptions import NotWritableError, NoWritablePkgsDirError
from ..gateways.disk.create import (
    create_package_cache_directory,
    extract_tarball,
    write_as_json_to_file,
)
from ..gateways.disk.delete import rm_rf
from ..gateways.disk.read import (
    compute_sum,
    isdir,
    isfile,
    islink,
    read_index_json,
    read_index_json_from_tarball,
    read_repodata_json,
)
from ..gateways.disk.test import file_path_is_writable
from ..models.match_spec import MatchSpec
from ..models.records import PackageCacheRecord, PackageRecord
from ..utils import human_bytes
from .path_actions import CacheUrlAction, ExtractPackageAction

if TYPE_CHECKING:
    from concurrent.futures import Future
    from pathlib import Path

log = getLogger(__name__)

FileNotFoundError = IOError

try:
    from conda_package_handling.api import THREADSAFE_EXTRACT
except ImportError:
    THREADSAFE_EXTRACT = False
# On the machines we tested, extraction doesn't get any faster after 3 threads
EXTRACT_THREADS = min(os.cpu_count() or 1, 3) if THREADSAFE_EXTRACT else 1


class PackageCacheType(type):
    """This metaclass does basic caching of PackageCache instance objects."""

    def __call__(cls, pkgs_dir: str | os.PathLike | Path):
        if isinstance(pkgs_dir, PackageCacheData):
            return pkgs_dir
        elif (pkgs_dir := str(pkgs_dir)) in PackageCacheData._cache_:
            return PackageCacheData._cache_[pkgs_dir]
        else:
            package_cache_instance = super().__call__(pkgs_dir)
            PackageCacheData._cache_[pkgs_dir] = package_cache_instance
            return package_cache_instance


class PackageCacheData(metaclass=PackageCacheType):
    _cache_: dict[str, PackageCacheData] = {}

    def __init__(self, pkgs_dir):
        self.pkgs_dir = pkgs_dir
        self.__package_cache_records = None
        self.__is_writable = NULL

        self._urls_data = UrlsData(pkgs_dir)

    def insert(self, package_cache_record):
        meta = join(
            package_cache_record.extracted_package_dir, "info", "repodata_record.json"
        )
        write_as_json_to_file(meta, PackageRecord.from_objects(package_cache_record))

        self._package_cache_records[package_cache_record] = package_cache_record

    def load(self):
        self.__package_cache_records = _package_cache_records = {}
        self._check_writable()  # called here to create the cache if it doesn't exist
        if not isdir(self.pkgs_dir):
            # no directory exists, and we didn't have permissions to create it
            return

        _CONDA_TARBALL_EXTENSIONS = CONDA_PACKAGE_EXTENSIONS
        pkgs_dir_contents = tuple(entry.name for entry in scandir(self.pkgs_dir))
        for base_name in self._dedupe_pkgs_dir_contents(pkgs_dir_contents):
            full_path = join(self.pkgs_dir, base_name)
            if islink(full_path):
                continue
            elif (
                isdir(full_path)
                and isfile(join(full_path, "info", "index.json"))
                or isfile(full_path)
                and full_path.endswith(_CONDA_TARBALL_EXTENSIONS)
            ):
                try:
                    package_cache_record = self._make_single_record(base_name)
                except ValidationError as err:
                    # ValidationError: package fields are invalid
                    log.warning(
                        f"Failed to create package cache record for '{base_name}'. {err}"
                    )
                    package_cache_record = None

                # if package_cache_record is None, it means we couldn't create a record, ignore
                if package_cache_record:
                    _package_cache_records[package_cache_record] = package_cache_record

    def reload(self):
        self.load()
        return self

    def get(self, package_ref, default=NULL):
        assert isinstance(package_ref, PackageRecord)
        try:
            return self._package_cache_records[package_ref]
        except KeyError:
            if default is not NULL:
                return default
            else:
                raise

    def remove(self, package_ref, default=NULL):
        if default is NULL:
            return self._package_cache_records.pop(package_ref)
        else:
            return self._package_cache_records.pop(package_ref, default)

    def query(self, package_ref_or_match_spec):
        # returns a generator
        param = package_ref_or_match_spec
        if isinstance(param, str):
            param = MatchSpec(param)
        if isinstance(param, MatchSpec):
            return (
                pcrec
                for pcrec in self._package_cache_records.values()
                if param.match(pcrec)
            )
        else:
            assert isinstance(param, PackageRecord)
            return (
                pcrec
                for pcrec in self._package_cache_records.values()
                if pcrec == param
            )

    def iter_records(self):
        return iter(self._package_cache_records)

    @classmethod
    def query_all(cls, package_ref_or_match_spec, pkgs_dirs=None):
        if pkgs_dirs is None:
            pkgs_dirs = context.pkgs_dirs

        return chain.from_iterable(
            pcache.query(package_ref_or_match_spec)
            for pcache in cls.all_caches_writable_first(pkgs_dirs)
        )

    # ##########################################################################################
    # these class methods reach across all package cache directories (usually context.pkgs_dirs)
    # ##########################################################################################

    @classmethod
    def first_writable(cls, pkgs_dirs=None):
        # Calling this method will *create* a package cache directory if one does not already
        # exist. Any caller should intend to *use* that directory for *writing*, not just reading.
        if pkgs_dirs is None:
            pkgs_dirs = context.pkgs_dirs
        for pkgs_dir in pkgs_dirs:
            package_cache = cls(pkgs_dir)
            i_wri = package_cache.is_writable
            if i_wri is True:
                return package_cache
            elif i_wri is None:
                # means package cache directory doesn't exist, need to try to create it
                try:
                    created = create_package_cache_directory(package_cache.pkgs_dir)
                except NotWritableError:
                    continue
                if created:
                    package_cache.__is_writable = True
                    return package_cache

        raise NoWritablePkgsDirError(pkgs_dirs)

    @classmethod
    def writable_caches(cls, pkgs_dirs=None):
        if pkgs_dirs is None:
            pkgs_dirs = context.pkgs_dirs
        writable_caches = tuple(
            filter(lambda c: c.is_writable, (cls(pd) for pd in pkgs_dirs))
        )
        return writable_caches

    @classmethod
    def read_only_caches(cls, pkgs_dirs=None):
        if pkgs_dirs is None:
            pkgs_dirs = context.pkgs_dirs
        read_only_caches = tuple(
            filter(lambda c: not c.is_writable, (cls(pd) for pd in pkgs_dirs))
        )
        return read_only_caches

    @classmethod
    def all_caches_writable_first(cls, pkgs_dirs=None):
        if pkgs_dirs is None:
            pkgs_dirs = context.pkgs_dirs
        pc_groups = groupby(lambda pc: pc.is_writable, (cls(pd) for pd in pkgs_dirs))
        return (*pc_groups.get(True, ()), *pc_groups.get(False, ()))

    @classmethod
    def get_all_extracted_entries(cls):
        package_caches = (cls(pd) for pd in context.pkgs_dirs)
        return tuple(
            pc_entry
            for pc_entry in chain.from_iterable(
                package_cache.values() for package_cache in package_caches
            )
            if pc_entry.is_extracted
        )

    @classmethod
    def get_entry_to_link(cls, package_ref):
        pc_entry = next(
            (pcrec for pcrec in cls.query_all(package_ref) if pcrec.is_extracted), None
        )
        if pc_entry is not None:
            return pc_entry

        # this can happen with `conda install path/to/package.tar.bz2`
        #   because dist has channel '<unknown>'
        # if ProgressiveFetchExtract did its job correctly, what we're looking for
        #   should be the matching dist_name in the first writable package cache
        # we'll search all caches for a match, but search writable caches first
        dist_str = package_ref.dist_str().rsplit(":", 1)[-1]
        pc_entry = next(
            (
                cache._scan_for_dist_no_channel(dist_str)
                for cache in cls.all_caches_writable_first()
                if cache
            ),
            None,
        )
        if pc_entry is not None:
            return pc_entry
        raise CondaError(
            f"No package '{package_ref.dist_str()}' found in cache directories."
        )

    @classmethod
    def tarball_file_in_cache(cls, tarball_path, md5sum=None, exclude_caches=()):
        tarball_full_path, md5sum = cls._clean_tarball_path_and_get_md5sum(
            tarball_path, md5sum
        )
        pc_entry = first(
            cls(pkgs_dir).tarball_file_in_this_cache(tarball_full_path, md5sum)
            for pkgs_dir in context.pkgs_dirs
            if pkgs_dir not in exclude_caches
        )
        return pc_entry

    @classmethod
    def clear(cls):
        cls._cache_.clear()

    def tarball_file_in_this_cache(self, tarball_path, md5sum=None):
        tarball_full_path, md5sum = self._clean_tarball_path_and_get_md5sum(
            tarball_path, md5sum
        )
        tarball_basename = basename(tarball_full_path)
        pc_entry = first(
            (pc_entry for pc_entry in self.values()),
            key=lambda pce: pce.tarball_basename == tarball_basename
            and pce.md5 == md5sum,
        )
        return pc_entry

    @property
    def _package_cache_records(self):
        # don't actually populate _package_cache_records until we need it
        if self.__package_cache_records is None:
            self.load()
        return self.__package_cache_records

    @property
    def is_writable(self):
        # returns None if package cache directory does not exist / has not been created
        if self.__is_writable is NULL:
            return self._check_writable()
        return self.__is_writable

    def _check_writable(self):
        magic_file = join(self.pkgs_dir, PACKAGE_CACHE_MAGIC_FILE)
        if isfile(magic_file):
            i_wri = file_path_is_writable(join(self.pkgs_dir, PACKAGE_CACHE_MAGIC_FILE))
            self.__is_writable = i_wri
            log.debug("package cache directory '%s' writable: %s", self.pkgs_dir, i_wri)
        else:
            log.log(TRACE, "package cache directory '%s' does not exist", self.pkgs_dir)
            self.__is_writable = i_wri = None
        return i_wri

    @staticmethod
    def _clean_tarball_path_and_get_md5sum(tarball_path, md5sum=None):
        if tarball_path.startswith("file:/"):
            tarball_path = url_to_path(tarball_path)
        tarball_full_path = expand(tarball_path)

        if isfile(tarball_full_path) and md5sum is None:
            md5sum = compute_sum(tarball_full_path, "md5")

        return tarball_full_path, md5sum

    def _scan_for_dist_no_channel(self, dist_str):
        return next(
            (
                pcrec
                for pcrec in self._package_cache_records
                if pcrec.dist_str().rsplit(":", 1)[-1] == dist_str
            ),
            None,
        )

    def itervalues(self):
        return iter(self.values())

    def values(self):
        return self._package_cache_records.values()

    def __repr__(self):
        args = (f"{key}={getattr(self, key)!r}" for key in ("pkgs_dir",))
        return "{}({})".format(self.__class__.__name__, ", ".join(args))

    def _make_single_record(self, package_filename):
        # delay-load this to help make sure libarchive can be found
        from conda_package_handling.api import InvalidArchiveError

        package_tarball_full_path = join(self.pkgs_dir, package_filename)
        log.log(TRACE, "adding to package cache %s", package_tarball_full_path)
        extracted_package_dir, pkg_ext = strip_pkg_extension(package_tarball_full_path)

        # try reading info/repodata_record.json
        try:
            repodata_record = read_repodata_json(extracted_package_dir)
            package_cache_record = PackageCacheRecord.from_objects(
                repodata_record,
                package_tarball_full_path=package_tarball_full_path,
                extracted_package_dir=extracted_package_dir,
            )
            return package_cache_record
        except (OSError, JSONDecodeError, ValueError, FileNotFoundError) as e:
            # EnvironmentError if info/repodata_record.json doesn't exists
            # JsonDecodeError if info/repodata_record.json is partially extracted or corrupted
            #   python 2.7 raises ValueError instead of JsonDecodeError
            #   ValueError("No JSON object could be decoded")
            log.debug(
                "unable to read %s\n  because %r",
                join(extracted_package_dir, "info", "repodata_record.json"),
                e,
            )

            # try reading info/index.json
            try:
                raw_json_record = read_index_json(extracted_package_dir)
            except (OSError, JSONDecodeError, ValueError, FileNotFoundError) as e:
                # EnvironmentError if info/index.json doesn't exist
                # JsonDecodeError if info/index.json is partially extracted or corrupted
                #   python 2.7 raises ValueError instead of JsonDecodeError
                #   ValueError("No JSON object could be decoded")
                log.debug(
                    "unable to read %s\n  because %r",
                    join(extracted_package_dir, "info", "index.json"),
                    e,
                )

                if isdir(extracted_package_dir) and not isfile(
                    package_tarball_full_path
                ):
                    # We have a directory that looks like a conda package, but without
                    # (1) info/repodata_record.json or info/index.json, and (2) a conda package
                    # tarball, there's not much we can do.  We'll just ignore it.
                    return None

                try:
                    if self.is_writable:
                        if isdir(extracted_package_dir):
                            # We have a partially unpacked conda package directory. Best thing
                            # to do is remove it and try extracting.
                            rm_rf(extracted_package_dir)
                        try:
                            extract_tarball(
                                package_tarball_full_path, extracted_package_dir
                            )
                        except (OSError, InvalidArchiveError) as e:
                            if e.errno == ENOENT:
                                # FileNotFoundError(2, 'No such file or directory')
                                # At this point, we can assume the package tarball is bad.
                                # Remove everything and move on.
                                # see https://github.com/conda/conda/issues/6707
                                rm_rf(package_tarball_full_path)
                                rm_rf(extracted_package_dir)
                                return None
                        try:
                            raw_json_record = read_index_json(extracted_package_dir)
                        except (OSError, JSONDecodeError, FileNotFoundError):
                            # At this point, we can assume the package tarball is bad.
                            # Remove everything and move on.
                            rm_rf(package_tarball_full_path)
                            rm_rf(extracted_package_dir)
                            return None
                    else:
                        raw_json_record = read_index_json_from_tarball(
                            package_tarball_full_path
                        )
                except (
                    EOFError,
                    ReadError,
                    FileNotFoundError,
                    InvalidArchiveError,
                ) as e:
                    # EOFError: Compressed file ended before the end-of-stream marker was reached
                    # tarfile.ReadError: file could not be opened successfully
                    # We have a corrupted tarball. Remove the tarball so it doesn't affect
                    # anything, and move on.
                    log.debug(
                        "unable to extract info/index.json from %s\n  because %r",
                        package_tarball_full_path,
                        e,
                    )
                    rm_rf(package_tarball_full_path)
                    return None

            # we were able to read info/index.json, so let's continue
            if isfile(package_tarball_full_path):
                md5 = compute_sum(package_tarball_full_path, "md5")
            else:
                md5 = None

            url = self._urls_data.get_url(package_filename)
            package_cache_record = PackageCacheRecord.from_objects(
                raw_json_record,
                url=url,
                fn=basename(package_tarball_full_path),
                md5=md5,
                size=getsize(package_tarball_full_path),
                package_tarball_full_path=package_tarball_full_path,
                extracted_package_dir=extracted_package_dir,
            )

            # write the info/repodata_record.json file so we can short-circuit this next time
            if self.is_writable:
                repodata_record = PackageRecord.from_objects(package_cache_record)
                repodata_record_path = join(
                    extracted_package_dir, "info", "repodata_record.json"
                )
                try:
                    write_as_json_to_file(repodata_record_path, repodata_record)
                except OSError as e:
                    if e.errno in (EACCES, EPERM, EROFS) and isdir(
                        dirname(repodata_record_path)
                    ):
                        raise NotWritableError(
                            repodata_record_path, e.errno, caused_by=e
                        )
                    else:
                        raise

            return package_cache_record

    @staticmethod
    def _dedupe_pkgs_dir_contents(pkgs_dir_contents):
        # if both 'six-1.10.0-py35_0/' and 'six-1.10.0-py35_0.tar.bz2' are in pkgs_dir,
        #   only 'six-1.10.0-py35_0.tar.bz2' will be in the return contents
        if not pkgs_dir_contents:
            return []
        _CONDA_TARBALL_EXTENSION_V1 = CONDA_PACKAGE_EXTENSION_V1
        _CONDA_TARBALL_EXTENSION_V2 = CONDA_PACKAGE_EXTENSION_V2
        _strip_pkg_extension = strip_pkg_extension
        groups = defaultdict(set)
        any(
            groups[ext].add(fn_root)
            for fn_root, ext in (_strip_pkg_extension(fn) for fn in pkgs_dir_contents)
        )
        conda_extensions = groups[_CONDA_TARBALL_EXTENSION_V2]
        tar_bz2_extensions = groups[_CONDA_TARBALL_EXTENSION_V1] - conda_extensions
        others = groups[None] - conda_extensions - tar_bz2_extensions
        return sorted(
            (
                *(path + _CONDA_TARBALL_EXTENSION_V2 for path in conda_extensions),
                *(path + _CONDA_TARBALL_EXTENSION_V1 for path in tar_bz2_extensions),
                *others,
            )
        )


class UrlsData:
    # this is a class to manage urls.txt
    # it should basically be thought of as a sequence
    # in this class I'm breaking the rule that all disk access goes through conda.gateways

    def __init__(self, pkgs_dir):
        self.pkgs_dir = pkgs_dir
        self.urls_txt_path = urls_txt_path = join(pkgs_dir, "urls.txt")
        if isfile(urls_txt_path):
            with open(urls_txt_path, "rb") as fh:
                self._urls_data = [line.strip().decode("utf-8") for line in fh]
                self._urls_data.reverse()
        else:
            self._urls_data = []

    def __contains__(self, url):
        return url in self._urls_data

    def __iter__(self):
        return iter(self._urls_data)

    def add_url(self, url):
        with codecs.open(self.urls_txt_path, mode="ab", encoding="utf-8") as fh:
            linefeed = "\r\n" if platform == "win32" else "\n"
            fh.write(url + linefeed)
        self._urls_data.insert(0, url)

    @memoizemethod
    def get_url(self, package_path):
        # package path can be a full path or just a basename
        #   can be either an extracted directory or tarball
        package_path = basename(package_path)
        # NOTE: This makes an assumption that all extensionless packages came from a .tar.bz2.
        #       That's probably a good assumption going forward, because we should now always
        #       be recording the extension in urls.txt.  The extensionless situation should be
        #       legacy behavior only.
        if not package_path.endswith(CONDA_PACKAGE_EXTENSIONS):
            package_path += CONDA_PACKAGE_EXTENSION_V1
        return first(self, lambda url: basename(url) == package_path)


# ##############################
# downloading
# ##############################


class ProgressiveFetchExtract:
    @staticmethod
    def make_actions_for_record(pref_or_spec):
        assert pref_or_spec is not None
        # returns a cache_action and extract_action

        # if the pref or spec has an md5 value
        # look in all caches for package cache record that is
        #   (1) already extracted, and
        #   (2) matches the md5
        # If one exists, no actions are needed.
        sha256 = pref_or_spec.get("sha256")
        size = pref_or_spec.get("size")
        md5 = pref_or_spec.get("md5")
        legacy_bz2_size = pref_or_spec.get("legacy_bz2_size")
        legacy_bz2_md5 = pref_or_spec.get("legacy_bz2_md5")

        def pcrec_matches(pcrec):
            matches = True
            # sha256 is overkill for things that are already in the package cache.
            #     It's just a quick match.
            # if sha256 is not None and pcrec.sha256 is not None:
            #     matches = sha256 == pcrec.sha256
            if size is not None and pcrec.get("size") is not None:
                matches = pcrec.size in (size, legacy_bz2_size)
            if matches and md5 is not None and pcrec.get("md5") is not None:
                matches = pcrec.md5 in (md5, legacy_bz2_md5)
            return matches

        extracted_pcrec = next(
            (
                pcrec
                for pcrec in chain.from_iterable(
                    PackageCacheData(pkgs_dir).query(pref_or_spec)
                    for pkgs_dir in context.pkgs_dirs
                )
                if pcrec.is_extracted
            ),
            None,
        )
        if (
            extracted_pcrec
            and pcrec_matches(extracted_pcrec)
            and extracted_pcrec.get("url")
        ):
            return None, None

        # there is no extracted dist that can work, so now we look for tarballs that
        #   aren't extracted
        # first we look in all writable caches, and if we find a match, we extract in place
        # otherwise, if we find a match in a non-writable cache, we link it to the first writable
        #   cache, and then extract
        pcrec_from_writable_cache = next(
            (
                pcrec
                for pcrec in chain.from_iterable(
                    pcache.query(pref_or_spec)
                    for pcache in PackageCacheData.writable_caches()
                )
                if pcrec.is_fetched
            ),
            None,
        )
        if (
            pcrec_from_writable_cache
            and pcrec_matches(pcrec_from_writable_cache)
            and pcrec_from_writable_cache.get("url")
        ):
            # extract in place
            extract_action = ExtractPackageAction(
                source_full_path=pcrec_from_writable_cache.package_tarball_full_path,
                target_pkgs_dir=dirname(
                    pcrec_from_writable_cache.package_tarball_full_path
                ),
                target_extracted_dirname=basename(
                    pcrec_from_writable_cache.extracted_package_dir
                ),
                record_or_spec=pcrec_from_writable_cache,
                sha256=pcrec_from_writable_cache.sha256 or sha256,
                size=pcrec_from_writable_cache.size or size,
                md5=pcrec_from_writable_cache.md5 or md5,
            )
            return None, extract_action

        pcrec_from_read_only_cache = next(
            (
                pcrec
                for pcrec in chain.from_iterable(
                    pcache.query(pref_or_spec)
                    for pcache in PackageCacheData.read_only_caches()
                )
                if pcrec.is_fetched
            ),
            None,
        )

        first_writable_cache = PackageCacheData.first_writable()
        if pcrec_from_read_only_cache and pcrec_matches(pcrec_from_read_only_cache):
            # we found a tarball, but it's in a read-only package cache
            # we need to link the tarball into the first writable package cache,
            #   and then extract
            cache_action = CacheUrlAction(
                url=path_to_url(pcrec_from_read_only_cache.package_tarball_full_path),
                target_pkgs_dir=first_writable_cache.pkgs_dir,
                target_package_basename=pcrec_from_read_only_cache.fn,
                sha256=pcrec_from_read_only_cache.get("sha256") or sha256,
                size=pcrec_from_read_only_cache.get("size") or size,
                md5=pcrec_from_read_only_cache.get("md5") or md5,
            )
            trgt_extracted_dirname = strip_pkg_extension(pcrec_from_read_only_cache.fn)[
                0
            ]
            extract_action = ExtractPackageAction(
                source_full_path=cache_action.target_full_path,
                target_pkgs_dir=first_writable_cache.pkgs_dir,
                target_extracted_dirname=trgt_extracted_dirname,
                record_or_spec=pcrec_from_read_only_cache,
                sha256=pcrec_from_read_only_cache.get("sha256") or sha256,
                size=pcrec_from_read_only_cache.get("size") or size,
                md5=pcrec_from_read_only_cache.get("md5") or md5,
            )
            return cache_action, extract_action

        # if we got here, we couldn't find a matching package in the caches
        #   we'll have to download one; fetch and extract
        url = pref_or_spec.get("url")
        assert url

        cache_action = CacheUrlAction(
            url=url,
            target_pkgs_dir=first_writable_cache.pkgs_dir,
            target_package_basename=pref_or_spec.fn,
            sha256=sha256,
            size=size,
            md5=md5,
        )
        extract_action = ExtractPackageAction(
            source_full_path=cache_action.target_full_path,
            target_pkgs_dir=first_writable_cache.pkgs_dir,
            target_extracted_dirname=strip_pkg_extension(pref_or_spec.fn)[0],
            record_or_spec=pref_or_spec,
            sha256=sha256,
            size=size,
            md5=md5,
        )
        return cache_action, extract_action

    def __init__(self, link_prefs):
        """
        Args:
            link_prefs (tuple[PackageRecord]):
                A sequence of :class:`PackageRecord`s to ensure available in a known
                package cache, typically for a follow-on :class:`UnlinkLinkTransaction`.
                Here, "available" means the package tarball is both downloaded and extracted
                to a package directory.
        """
        self.link_precs = link_prefs

        log.debug(
            "instantiating ProgressiveFetchExtract with\n  %s\n",
            "\n  ".join(pkg_rec.dist_str() for pkg_rec in link_prefs),
        )

        self.paired_actions = {}  # Map[pref, Tuple(CacheUrlAction, ExtractPackageAction)]

        self._prepared = False
        self._executed = False

    @time_recorder("fetch_extract_prepare")
    def prepare(self):
        if self._prepared:
            return

        # Download largest first
        def by_size(prec: PackageRecord | MatchSpec):
            # the test suite passes MatchSpec in here, is that an intentional
            # feature?
            try:
                return int(prec.size)  # type: ignore
            except (LookupError, ValueError, AttributeError):
                return 0

        largest_first = sorted(self.link_precs, key=by_size, reverse=True)

        self.paired_actions.update(
            (prec, self.make_actions_for_record(prec)) for prec in largest_first
        )
        self._prepared = True

    @property
    def cache_actions(self):
        return tuple(axns[0] for axns in self.paired_actions.values() if axns[0])

    @property
    def extract_actions(self):
        return tuple(axns[1] for axns in self.paired_actions.values() if axns[1])

    def execute(self):
        """
        Run each action in self.paired_actions. Each action in cache_actions
        runs before its corresponding extract_actions.
        """
        if self._executed:
            return
        if not self._prepared:
            self.prepare()

        assert not context.dry_run

        if not self.paired_actions:
            return

        if not context.verbose and not context.quiet and not context.json:
            print(
                "\nDownloading and Extracting Packages:",
                end="\n" if IS_INTERACTIVE else " ...working...",
            )
        else:
            log.debug(
                "prepared package cache actions:\n"
                "  cache_actions:\n"
                "    %s\n"
                "  extract_actions:\n"
                "    %s\n",
                "\n    ".join(str(ca) for ca in self.cache_actions),
                "\n    ".join(str(ea) for ea in self.extract_actions),
            )

        exceptions = []
        progress_bars = {}
        futures: list[Future] = []

        cancelled_flag = False

        def cancelled():
            """
            Used to cancel download threads.
            """
            nonlocal cancelled_flag
            return cancelled_flag

        with signal_handler(conda_signal_handler), time_recorder(
            "fetch_extract_execute"
        ), ThreadPoolExecutor(
            context.fetch_threads
        ) as fetch_executor, ThreadPoolExecutor(EXTRACT_THREADS) as extract_executor:
            for prec_or_spec, (
                cache_action,
                extract_action,
            ) in self.paired_actions.items():
                if cache_action is None and extract_action is None:
                    # Not sure when this is reached.
                    continue

                progress_bar = self._progress_bar(prec_or_spec, leave=False)

                progress_bars[prec_or_spec] = progress_bar

                future = fetch_executor.submit(
                    do_cache_action,
                    prec_or_spec,
                    cache_action,
                    progress_bar,
                    cancelled=cancelled,
                )

                future.add_done_callback(
                    partial(
                        done_callback,
                        actions=(cache_action,),
                        exceptions=exceptions,
                        progress_bar=progress_bar,
                        finish=False,
                    )
                )
                futures.append(future)

            try:
                for completed_future in as_completed(futures):
                    futures.remove(completed_future)
                    prec_or_spec = completed_future.result()

                    cache_action, extract_action = self.paired_actions[prec_or_spec]
                    extract_future = extract_executor.submit(
                        do_extract_action,
                        prec_or_spec,
                        extract_action,
                        progress_bars[prec_or_spec],
                    )
                    extract_future.add_done_callback(
                        partial(
                            done_callback,
                            actions=(cache_action, extract_action),
                            exceptions=exceptions,
                            progress_bar=progress_bars[prec_or_spec],
                            finish=True,
                        )
                    )
            except BaseException as e:
                # We are interested in KeyboardInterrupt delivered to
                # as_completed() while waiting, or any exception raised from
                # completed_future.result(). cancelled_flag is checked in the
                # progress callback to stop running transfers, shutdown() should
                # prevent new downloads from starting.
                cancelled_flag = True
                for future in futures:  # needed on top of .shutdown()
                    future.cancel()
                # Has a Python >=3.9 cancel_futures= parameter that does not
                # replace the above loop:
                fetch_executor.shutdown(wait=False)
                exceptions.append(e)

        for bar in progress_bars.values():
            bar.close()

        if not context.verbose and not context.quiet and not context.json:
            if IS_INTERACTIVE:
                print("\r")  # move to column 0
            else:
                print(" done")

        if exceptions:
            # avoid printing one CancelledError() per pending download
            not_cancelled = [e for e in exceptions if not isinstance(e, CancelledError)]
            raise CondaMultiError(not_cancelled)

        self._executed = True

    @staticmethod
    def _progress_bar(prec_or_spec, position=None, leave=False) -> ProgressBar:
        desc = ""
        if prec_or_spec.name and prec_or_spec.version:
            desc = "{}-{}".format(prec_or_spec.name or "", prec_or_spec.version or "")
        size = getattr(prec_or_spec, "size", None)
        size_str = size and human_bytes(size) or ""
        if len(desc) > 0:
            desc = "%-20.20s | " % desc
        if len(size_str) > 0:
            desc += "%-9s | " % size_str

        progress_bar = ProgressBar(
            desc,
            not context.verbose and not context.quiet and IS_INTERACTIVE,
            context.json,
            position=position,
            leave=leave,
        )

        return progress_bar

    def __hash__(self):
        return hash(self.link_precs)

    def __eq__(self, other):
        return hash(self) == hash(other)


def do_cache_action(prec, cache_action, progress_bar, download_total=1.0, *, cancelled):
    """This function gets called from `ProgressiveFetchExtract.execute`."""
    # pass None if already cached (simplifies code)
    if not cache_action:
        return prec
    cache_action.verify()

    if not cache_action.url.startswith("file:/"):

        def progress_update_cache_action(pct_completed):
            if cancelled():
                """
                Used to cancel dowload threads when parent thread is interrupted.
                """
                raise CancelledError()
            progress_bar.update_to(pct_completed * download_total)

    else:
        download_total = 0
        progress_update_cache_action = None

    cache_action.execute(progress_update_cache_action)
    return prec


def do_extract_action(prec, extract_action, progress_bar):
    """This function gets called after do_cache_action completes."""
    # pass None if already extracted (simplifies code)
    if not extract_action:
        return prec
    extract_action.verify()
    # currently unable to do updates on extract;
    # likely too fast to bother
    extract_action.execute(None)
    progress_bar.update_to(1.0)
    return prec


def do_cleanup(actions):
    for action in actions:
        if action:
            action.cleanup()


def do_reverse(actions):
    for action in actions:
        if action:
            action.reverse()


def done_callback(
    future: Future,
    actions: tuple[CacheUrlAction | ExtractPackageAction, ...],
    progress_bar: ProgressBar,
    exceptions: list[Exception],
    finish: bool = False,
):
    try:
        future.result()
    except Exception as e:
        # if it was interrupted with CTRL-C this might be BaseException and not
        # get caught here, but conda's signal handler also converts that to
        # CondaError which is just Exception.
        do_reverse(reversed(actions))
        exceptions.append(e)
    else:
        do_cleanup(actions)
        if finish:
            progress_bar.finish()
            progress_bar.refresh()


@deprecated("24.3", "24.9")
def rm_fetched(dist):
    """
    Checks to see if the requested package is in the cache; and if so, it removes both
    the package itself and its extracted contents.
    """
    # in conda/exports.py and conda_build/conda_interface.py, but not actually
    #   used in conda-build
    raise NotImplementedError()


@deprecated(
    "24.3",
    "24.9",
    addendum="Use `conda.gateways.connection.download.download` instead.",
)
def download(url, dst_path, session=None, md5sum=None, urlstxt=False, retries=3):
    from ..gateways.connection.download import download as gateway_download

    gateway_download(url, dst_path, md5sum)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Tools for managing conda environments."""

from __future__ import annotations

import os
from errno import EACCES, ENOENT, EROFS
from logging import getLogger
from os.path import dirname, isdir, isfile, join, normpath
from typing import TYPE_CHECKING

from ..base.context import context
from ..common._os import is_admin
from ..common.compat import ensure_text_type, on_win, open
from ..common.path import expand
from ..gateways.disk.read import yield_lines
from ..gateways.disk.test import is_conda_environment
from .prefix_data import PrefixData

if TYPE_CHECKING:
    from typing import Iterator

log = getLogger(__name__)


def get_user_environments_txt_file(userhome: str = "~") -> str:
    """
    Gets the path to the user's environments.txt file.

    :param userhome: The home directory of the user.
    :type userhome: str
    :return: Path to the environments.txt file.
    :rtype: str
    """
    return expand(join(userhome, ".conda", "environments.txt"))


def register_env(location: str) -> None:
    """
    Registers an environment by adding it to environments.txt file.

    :param location: The file path of the environment to register.
    :type location: str
    :return: None
    """
    if not context.register_envs:
        return

    user_environments_txt_file = get_user_environments_txt_file()
    location = normpath(location)
    folder = dirname(location)
    try:
        os.makedirs(folder)
    except:
        pass

    if (
        "placehold_pl" in location
        or "skeleton_" in location
        or user_environments_txt_file == os.devnull
    ):
        # Don't record envs created by conda-build.
        return

    if location in yield_lines(user_environments_txt_file):
        # Nothing to do. Location is already recorded in a known environments.txt file.
        return

    user_environments_txt_directory = os.path.dirname(user_environments_txt_file)
    try:
        os.makedirs(user_environments_txt_directory, exist_ok=True)
    except OSError as exc:
        log.warning(
            "Unable to register environment. "
            f"Could not create {user_environments_txt_directory}. "
            f"Reason: {exc}"
        )
        return

    try:
        with open(user_environments_txt_file, "a") as fh:
            fh.write(ensure_text_type(location))
            fh.write("\n")
    except OSError as e:
        if e.errno in (EACCES, EROFS, ENOENT):
            log.warning(
                "Unable to register environment. Path not writable or missing.\n"
                "  environment location: %s\n"
                "  registry file: %s",
                location,
                user_environments_txt_file,
            )
        else:
            raise


def unregister_env(location: str) -> None:
    """
    Unregisters an environment by removing its entry from the environments.txt file if certain conditions are met.

    The environment is only unregistered if its associated 'conda-meta' directory exists and contains no significant files other than 'history'. If these conditions are met, the environment's path is removed from environments.txt.

    :param location: The file path of the environment to unregister.
    :type location: str
    :return: None
    """
    if isdir(location):
        meta_dir = join(location, "conda-meta")
        if isdir(meta_dir):
            meta_dir_contents = tuple(entry.name for entry in os.scandir(meta_dir))
            if len(meta_dir_contents) > 1:
                # if there are any files left other than 'conda-meta/history'
                #   then don't unregister
                return

    _clean_environments_txt(get_user_environments_txt_file(), location)


def list_all_known_prefixes() -> list[str]:
    """
    Lists all known conda environment prefixes.

    :return: A list of all known conda environment prefixes.
    :rtype: List[str]
    """
    all_env_paths = set()
    # If the user is an admin, load environments from all user home directories
    if is_admin():
        if on_win:
            home_dir_dir = dirname(expand("~"))
            search_dirs = tuple(entry.path for entry in os.scandir(home_dir_dir))
        else:
            from pwd import getpwall

            search_dirs = tuple(pwentry.pw_dir for pwentry in getpwall()) or (
                expand("~"),
            )
    else:
        search_dirs = (expand("~"),)
    for home_dir in filter(None, search_dirs):
        environments_txt_file = get_user_environments_txt_file(home_dir)
        if isfile(environments_txt_file):
            try:
                # When the user is an admin, some environments.txt files might
                # not be readable (if on network file system for example)
                all_env_paths.update(_clean_environments_txt(environments_txt_file))
            except PermissionError:
                log.warning(f"Unable to access {environments_txt_file}")

    # in case environments.txt files aren't complete, also add all known conda environments in
    # all envs_dirs
    envs_dirs = (envs_dir for envs_dir in context.envs_dirs if isdir(envs_dir))
    all_env_paths.update(
        path
        for path in (
            entry.path for envs_dir in envs_dirs for entry in os.scandir(envs_dir)
        )
        if path not in all_env_paths and is_conda_environment(path)
    )

    all_env_paths.add(context.root_prefix)
    return sorted(all_env_paths)


def query_all_prefixes(spec: str) -> Iterator[tuple[str, tuple]]:
    """
    Queries all known prefixes for a given specification.

    :param spec: The specification to query for.
    :type spec: str
    :return: An iterator of tuples containing the prefix and the query results.
    :rtype: Iterator[Tuple[str, Tuple]]
    """
    for prefix in list_all_known_prefixes():
        prefix_recs = tuple(PrefixData(prefix).query(spec))
        if prefix_recs:
            yield prefix, prefix_recs


def _clean_environments_txt(
    environments_txt_file: str,
    remove_location: str | None = None,
) -> tuple[str, ...]:
    """
    Cleans the environments.txt file by removing specified locations.

    :param environments_txt_file: The file path of environments.txt.
    :param remove_location: Optional location to remove from the file.
    :type environments_txt_file: str
    :type remove_location: Optional[str]
    :return: A tuple of the cleaned lines.
    :rtype: Tuple[str, ...]
    """
    if not isfile(environments_txt_file):
        return ()

    if remove_location:
        remove_location = normpath(remove_location)
    environments_txt_lines = tuple(yield_lines(environments_txt_file))
    environments_txt_lines_cleaned = tuple(
        prefix
        for prefix in environments_txt_lines
        if prefix != remove_location and is_conda_environment(prefix)
    )
    if environments_txt_lines_cleaned != environments_txt_lines:
        _rewrite_environments_txt(environments_txt_file, environments_txt_lines_cleaned)
    return environments_txt_lines_cleaned


def _rewrite_environments_txt(environments_txt_file: str, prefixes: list[str]) -> None:
    """
    Rewrites the environments.txt file with the specified prefixes.

    :param environments_txt_file: The file path of environments.txt.
    :param prefixes: List of prefixes to write into the file.
    :type environments_txt_file: str
    :type prefixes: List[str]
    :return: None
    """
    try:
        with open(environments_txt_file, "w") as fh:
            fh.write("\n".join(prefixes))
            fh.write("\n")
    except OSError as e:
        log.info("File not cleaned: %s", environments_txt_file)
        log.debug("%r", e, exc_info=True)


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Backend logic for `conda init`.

Sections in this module are

  1. top-level functions
  2. plan creators
  3. plan runners
  4. individual operations
  5. helper functions

The top-level functions compose and execute full plans.

A plan is created by composing various individual operations.  The plan data structure is a
list of dicts, where each dict represents an individual operation.  The dict contains two
keys--`function` and `kwargs`--where function is the name of the individual operation function
within this module.

Each individual operation must

  a) return a `Result` (i.e. NEEDS_SUDO, MODIFIED, or NO_CHANGE)
  b) have no side effects if context.dry_run is True
  c) be verbose and descriptive about the changes being made or proposed is context.verbose

The plan runner functions take the plan (list of dicts) as an argument, and then coordinate the
execution of each individual operation.  The docstring for `run_plan_elevated()` has details on
how that strategy is implemented.

"""

import json
import os
import re
import struct
import sys
from difflib import unified_diff
from errno import ENOENT
from glob import glob
from itertools import chain
from logging import getLogger
from os.path import abspath, basename, dirname, exists, expanduser, isdir, isfile, join
from pathlib import Path
from random import randint

from .. import CONDA_PACKAGE_ROOT, CondaError
from .. import __version__ as CONDA_VERSION
from ..activate import (
    CshActivator,
    FishActivator,
    PosixActivator,
    PowerShellActivator,
    XonshActivator,
)
from ..auxlib.compat import Utf8NamedTemporaryFile
from ..auxlib.ish import dals
from ..base.context import context
from ..common.compat import (
    ensure_binary,
    ensure_text_type,
    ensure_utf8_encoding,
    on_mac,
    on_win,
    open,
)
from ..common.path import (
    expand,
    get_bin_directory_short_path,
    get_python_short_path,
    get_python_site_packages_short_path,
    win_path_ok,
)
from ..exceptions import CondaValueError
from ..gateways.disk.create import copy, mkdir_p
from ..gateways.disk.delete import rm_rf
from ..gateways.disk.link import lexists
from ..gateways.disk.permissions import make_executable
from ..gateways.disk.read import compute_sum
from ..gateways.subprocess import subprocess_call
from .portability import generate_shebang_for_entry_point

if on_win:  # pragma: no cover
    import winreg

    # Use v1 import paths to avoid bootstrapping issues
    # TODO: Remove once fully deployed (one release after merge)
    from menuinst.knownfolders import FOLDERID, get_folder_path
    from menuinst.winshortcut import create_shortcut


log = getLogger(__name__)

CONDA_INITIALIZE_RE_BLOCK = (
    r"^# >>> conda initialize >>>(?:\n|\r\n)"
    r"([\s\S]*?)"
    r"# <<< conda initialize <<<(?:\n|\r\n)?"
)

CONDA_INITIALIZE_PS_RE_BLOCK = (
    r"^#region conda initialize(?:\n|\r\n)([\s\S]*?)#endregion(?:\n|\r\n)?"
)


class Result:
    NEEDS_SUDO = "needs sudo"
    MODIFIED = "modified"
    NO_CHANGE = "no change"


# #####################################################
# top-level functions
# #####################################################


def install(conda_prefix):
    plan = make_install_plan(conda_prefix)
    run_plan(plan)
    if not context.dry_run:
        assert not any(step["result"] == Result.NEEDS_SUDO for step in plan)
    print_plan_results(plan)
    return 0


def initialize(
    conda_prefix, shells, for_user, for_system, anaconda_prompt, reverse=False
):
    plan1 = []
    if os.getenv("CONDA_PIP_UNINITIALIZED") == "true":
        plan1 = make_install_plan(conda_prefix)
        run_plan(plan1)
        if not context.dry_run:
            run_plan_elevated(plan1)

    plan2 = make_initialize_plan(
        conda_prefix, shells, for_user, for_system, anaconda_prompt, reverse=reverse
    )
    run_plan(plan2)
    if not context.dry_run:
        run_plan_elevated(plan2)

    plan = plan1 + plan2
    print_plan_results(plan)

    if any(step["result"] == Result.NEEDS_SUDO for step in plan):
        print("Operation failed.", file=sys.stderr)
        return 1


def initialize_dev(shell, dev_env_prefix=None, conda_source_root=None):
    # > alias conda-dev='eval "$(python -m conda init --dev)"'
    # > eval "$(python -m conda init --dev)"

    prefix = expand(dev_env_prefix or sys.prefix)
    conda_source_root = expand(conda_source_root or os.getcwd())

    python_exe, python_version, site_packages_dir = _get_python_info(prefix)

    if not isfile(join(conda_source_root, "conda", "__main__.py")):
        raise CondaValueError(
            f"Directory is not a conda source root: {conda_source_root}"
        )

    plan = make_install_plan(prefix)
    plan.append(
        {
            "function": remove_conda_in_sp_dir.__name__,
            "kwargs": {
                "target_path": site_packages_dir,
            },
        }
    )
    plan.append(
        {
            "function": make_conda_egg_link.__name__,
            "kwargs": {
                "target_path": join(site_packages_dir, "conda.egg-link"),
                "conda_source_root": conda_source_root,
            },
        }
    )
    plan.append(
        {
            "function": modify_easy_install_pth.__name__,
            "kwargs": {
                "target_path": join(site_packages_dir, "easy-install.pth"),
                "conda_source_root": conda_source_root,
            },
        }
    )
    plan.append(
        {
            "function": make_dev_egg_info_file.__name__,
            "kwargs": {
                "target_path": join(conda_source_root, "conda.egg-info"),
            },
        }
    )

    run_plan(plan)

    if context.dry_run or context.verbose:
        print_plan_results(plan, sys.stderr)

    if any(step["result"] == Result.NEEDS_SUDO for step in plan):  # pragma: no cover
        raise CondaError(
            "Operation failed. Privileged install disallowed for 'conda init --dev'."
        )

    env_vars = {
        "PYTHONHASHSEED": randint(0, 4294967296),
        "PYTHON_MAJOR_VERSION": python_version[0],
        "TEST_PLATFORM": "win" if on_win else "unix",
    }
    unset_env_vars = (
        "CONDA_DEFAULT_ENV",
        "CONDA_EXE",
        "_CE_M",
        "_CE_CONDA",
        "CONDA_PREFIX",
        "CONDA_PREFIX_1",
        "CONDA_PREFIX_2",
        "CONDA_PYTHON_EXE",
        "CONDA_PROMPT_MODIFIER",
        "CONDA_SHLVL",
    )

    if shell == "bash":
        print("\n".join(_initialize_dev_bash(prefix, env_vars, unset_env_vars)))
    elif shell == "cmd.exe":
        script = _initialize_dev_cmdexe(prefix, env_vars, unset_env_vars)
        if not context.dry_run:
            with open("dev-init.bat", "w") as fh:
                fh.write("\n".join(script))
        if context.verbose:
            print("\n".join(script))
        print("now run  > .\\dev-init.bat")
    else:
        raise NotImplementedError()
    return 0


def _initialize_dev_bash(prefix, env_vars, unset_env_vars):
    sys_executable = abspath(sys.executable)
    if on_win:
        sys_executable = f"$(cygpath '{sys_executable}')"

    # unset/set environment variables
    yield from (f"unset {envvar}" for envvar in unset_env_vars)
    yield from (
        f"export {envvar}='{value}'" for envvar, value in sorted(env_vars.items())
    )

    # initialize shell interface
    yield f'eval "$("{sys_executable}" -m conda shell.bash hook)"'

    # optionally activate environment
    if context.auto_activate_base:
        yield f"conda activate '{prefix}'"


def _initialize_dev_cmdexe(prefix, env_vars, unset_env_vars):
    dev_arg = ""
    if context.dev:
        dev_arg = "--dev"
    condabin = Path(prefix, "condabin")

    yield (
        '@IF NOT "%CONDA_PROMPT_MODIFIER%" == "" '
        '@CALL SET "PROMPT=%%PROMPT:%CONDA_PROMPT_MODIFIER%=%_empty_not_set_%%%"'
    )

    # unset/set environment variables
    yield from (f"@SET {envvar}=" for envvar in unset_env_vars)
    yield from (
        f'@SET "{envvar}={value}"' for envvar, value in sorted(env_vars.items())
    )

    # initialize shell interface
    yield f'@CALL "{condabin / "conda_hook.bat"}" {dev_arg}'
    yield "@IF %ERRORLEVEL% NEQ 0 @EXIT /B %ERRORLEVEL%"

    # optionally activate environment
    if context.auto_activate_base:
        yield f'@CALL "{condabin / "conda.bat"}" activate {dev_arg} "{prefix}"'
        yield "@IF %ERRORLEVEL% NEQ 0 @EXIT /B %ERRORLEVEL%"


# #####################################################
# plan creators
# #####################################################


def make_install_plan(conda_prefix):
    try:
        python_exe, python_version, site_packages_dir = _get_python_info(conda_prefix)
    except OSError:
        python_exe, python_version, site_packages_dir = None, None, None  # NOQA

    plan = []

    # ######################################
    # executables
    # ######################################
    if on_win:
        conda_exe_path = join(conda_prefix, "Scripts", "conda-script.py")
        conda_env_exe_path = join(conda_prefix, "Scripts", "conda-env-script.py")
        plan.append(
            {
                "function": make_entry_point_exe.__name__,
                "kwargs": {
                    "target_path": join(conda_prefix, "Scripts", "conda.exe"),
                    "conda_prefix": conda_prefix,
                },
            }
        )
        plan.append(
            {
                "function": make_entry_point_exe.__name__,
                "kwargs": {
                    "target_path": join(conda_prefix, "Scripts", "conda-env.exe"),
                    "conda_prefix": conda_prefix,
                },
            }
        )
    else:
        # We can't put a conda.exe in condabin on Windows. It'll conflict with conda.bat.
        plan.append(
            {
                "function": make_entry_point.__name__,
                "kwargs": {
                    "target_path": join(conda_prefix, "condabin", "conda"),
                    "conda_prefix": conda_prefix,
                    "module": "conda.cli",
                    "func": "main",
                },
            }
        )
        conda_exe_path = join(conda_prefix, "bin", "conda")
        conda_env_exe_path = join(conda_prefix, "bin", "conda-env")

    plan.append(
        {
            "function": make_entry_point.__name__,
            "kwargs": {
                "target_path": conda_exe_path,
                "conda_prefix": conda_prefix,
                "module": "conda.cli",
                "func": "main",
            },
        }
    )
    plan.append(
        {
            "function": make_entry_point.__name__,
            "kwargs": {
                "target_path": conda_env_exe_path,
                "conda_prefix": conda_prefix,
                # TODO: Remove upon full deprecation in 25.3
                "module": "conda_env.cli.main",
                "func": "main",
            },
        }
    )

    # ######################################
    # shell wrappers
    # ######################################
    if on_win:
        plan.append(
            {
                "function": install_condabin_conda_bat.__name__,
                "kwargs": {
                    "target_path": join(conda_prefix, "condabin", "conda.bat"),
                    "conda_prefix": conda_prefix,
                },
            }
        )
        plan.append(
            {
                "function": install_library_bin_conda_bat.__name__,
                "kwargs": {
                    "target_path": join(conda_prefix, "Library", "bin", "conda.bat"),
                    "conda_prefix": conda_prefix,
                },
            }
        )
        plan.append(
            {
                "function": install_condabin_conda_activate_bat.__name__,
                "kwargs": {
                    "target_path": join(
                        conda_prefix, "condabin", "_conda_activate.bat"
                    ),
                    "conda_prefix": conda_prefix,
                },
            }
        )
        plan.append(
            {
                "function": install_condabin_rename_tmp_bat.__name__,
                "kwargs": {
                    "target_path": join(conda_prefix, "condabin", "rename_tmp.bat"),
                    "conda_prefix": conda_prefix,
                },
            }
        )
        plan.append(
            {
                "function": install_condabin_conda_auto_activate_bat.__name__,
                "kwargs": {
                    "target_path": join(
                        conda_prefix, "condabin", "conda_auto_activate.bat"
                    ),
                    "conda_prefix": conda_prefix,
                },
            }
        )
        plan.append(
            {
                "function": install_condabin_hook_bat.__name__,
                "kwargs": {
                    "target_path": join(conda_prefix, "condabin", "conda_hook.bat"),
                    "conda_prefix": conda_prefix,
                },
            }
        )
        plan.append(
            {
                "function": install_Scripts_activate_bat.__name__,
                "kwargs": {
                    "target_path": join(conda_prefix, "Scripts", "activate.bat"),
                    "conda_prefix": conda_prefix,
                },
            }
        )
        plan.append(
            {
                "function": install_activate_bat.__name__,
                "kwargs": {
                    "target_path": join(conda_prefix, "condabin", "activate.bat"),
                    "conda_prefix": conda_prefix,
                },
            }
        )
        plan.append(
            {
                "function": install_deactivate_bat.__name__,
                "kwargs": {
                    "target_path": join(conda_prefix, "condabin", "deactivate.bat"),
                    "conda_prefix": conda_prefix,
                },
            }
        )

    plan.append(
        {
            "function": install_activate.__name__,
            "kwargs": {
                "target_path": join(
                    conda_prefix, get_bin_directory_short_path(), "activate"
                ),
                "conda_prefix": conda_prefix,
            },
        }
    )
    plan.append(
        {
            "function": install_deactivate.__name__,
            "kwargs": {
                "target_path": join(
                    conda_prefix, get_bin_directory_short_path(), "deactivate"
                ),
                "conda_prefix": conda_prefix,
            },
        }
    )

    plan.append(
        {
            "function": install_conda_sh.__name__,
            "kwargs": {
                "target_path": join(conda_prefix, "etc", "profile.d", "conda.sh"),
                "conda_prefix": conda_prefix,
            },
        }
    )
    plan.append(
        {
            "function": install_conda_fish.__name__,
            "kwargs": {
                "target_path": join(
                    conda_prefix, "etc", "fish", "conf.d", "conda.fish"
                ),
                "conda_prefix": conda_prefix,
            },
        }
    )
    plan.append(
        {
            "function": install_conda_psm1.__name__,
            "kwargs": {
                "target_path": join(conda_prefix, "shell", "condabin", "Conda.psm1"),
                "conda_prefix": conda_prefix,
            },
        }
    )
    plan.append(
        {
            "function": install_conda_hook_ps1.__name__,
            "kwargs": {
                "target_path": join(
                    conda_prefix, "shell", "condabin", "conda-hook.ps1"
                ),
                "conda_prefix": conda_prefix,
            },
        }
    )
    if site_packages_dir:
        plan.append(
            {
                "function": install_conda_xsh.__name__,
                "kwargs": {
                    "target_path": join(site_packages_dir, "xontrib", "conda.xsh"),
                    "conda_prefix": conda_prefix,
                },
            }
        )
    else:
        print(
            "WARNING: Cannot install xonsh wrapper without a python interpreter in prefix: "
            f"{conda_prefix}",
            file=sys.stderr,
        )
    plan.append(
        {
            "function": install_conda_csh.__name__,
            "kwargs": {
                "target_path": join(conda_prefix, "etc", "profile.d", "conda.csh"),
                "conda_prefix": conda_prefix,
            },
        }
    )
    return plan


def make_initialize_plan(
    conda_prefix, shells, for_user, for_system, anaconda_prompt, reverse=False
):
    """
    Creates a plan for initializing conda in shells.

    Bash:
    On Linux, when opening the terminal, .bashrc is sourced (because it is an interactive shell).
    On macOS on the other hand, the .bash_profile gets sourced by default when executing it in
    Terminal.app. Some other programs do the same on macOS so that's why we're initializing conda
    in .bash_profile.
    On Windows, there are multiple ways to open bash depending on how it was installed. Git Bash,
    Cygwin, and MSYS2 all use .bash_profile by default.

    PowerShell:
    There's several places PowerShell can store its path, depending on if it's Windows PowerShell,
    PowerShell Core on Windows, or PowerShell Core on macOS/Linux. The easiest way to resolve it
    is to just ask different possible installations of PowerShell where their profiles are.
    """
    plan = make_install_plan(conda_prefix)
    shells = set(shells)
    if shells & {"bash", "zsh"}:
        if "bash" in shells and for_user:
            bashrc_path = expand(
                join("~", ".bash_profile" if (on_mac or on_win) else ".bashrc")
            )
            plan.append(
                {
                    "function": init_sh_user.__name__,
                    "kwargs": {
                        "target_path": bashrc_path,
                        "conda_prefix": conda_prefix,
                        "shell": "bash",
                        "reverse": reverse,
                    },
                }
            )

        if "zsh" in shells and for_user:
            if "ZDOTDIR" in os.environ:
                zshrc_path = expand(join("$ZDOTDIR", ".zshrc"))
            else:
                zshrc_path = expand(join("~", ".zshrc"))
            plan.append(
                {
                    "function": init_sh_user.__name__,
                    "kwargs": {
                        "target_path": zshrc_path,
                        "conda_prefix": conda_prefix,
                        "shell": "zsh",
                        "reverse": reverse,
                    },
                }
            )

        if for_system:
            plan.append(
                {
                    "function": init_sh_system.__name__,
                    "kwargs": {
                        "target_path": "/etc/profile.d/conda.sh",
                        "conda_prefix": conda_prefix,
                        "reverse": reverse,
                    },
                }
            )

    if "fish" in shells:
        if for_user:
            config_fish_path = expand(join("~", ".config", "fish", "config.fish"))
            plan.append(
                {
                    "function": init_fish_user.__name__,
                    "kwargs": {
                        "target_path": config_fish_path,
                        "conda_prefix": conda_prefix,
                        "reverse": reverse,
                    },
                }
            )

        if for_system:
            config_fish_path = expand(join("~", ".config", "fish", "config.fish"))
            plan.append(
                {
                    "function": init_fish_user.__name__,
                    "kwargs": {
                        "target_path": config_fish_path,
                        "conda_prefix": conda_prefix,
                        "reverse": reverse,
                    },
                }
            )

    if "xonsh" in shells:
        if for_user:
            config_xonsh_path = expand(join("~", ".xonshrc"))
            plan.append(
                {
                    "function": init_xonsh_user.__name__,
                    "kwargs": {
                        "target_path": config_xonsh_path,
                        "conda_prefix": conda_prefix,
                        "reverse": reverse,
                    },
                }
            )

        if for_system:
            if on_win:
                config_xonsh_path = expand(
                    join("%ALLUSERSPROFILE%", "xonsh", "xonshrc")
                )
            else:
                config_xonsh_path = "/etc/xonshrc"
            plan.append(
                {
                    "function": init_xonsh_user.__name__,
                    "kwargs": {
                        "target_path": config_xonsh_path,
                        "conda_prefix": conda_prefix,
                        "reverse": reverse,
                    },
                }
            )

    if "tcsh" in shells and for_user:
        tcshrc_path = expand(join("~", ".tcshrc"))
        plan.append(
            {
                "function": init_sh_user.__name__,
                "kwargs": {
                    "target_path": tcshrc_path,
                    "conda_prefix": conda_prefix,
                    "shell": "tcsh",
                    "reverse": reverse,
                },
            }
        )

    if "powershell" in shells:
        if for_user:
            profile = "$PROFILE.CurrentUserAllHosts"

        if for_system:
            profile = "$PROFILE.AllUsersAllHosts"

        def find_powershell_paths(*exe_names):
            for exe_name in exe_names:
                try:
                    yield subprocess_call(
                        (exe_name, "-NoProfile", "-Command", profile)
                    ).stdout.strip()
                except Exception:
                    pass

        config_powershell_paths = set(
            find_powershell_paths("powershell", "pwsh", "pwsh-preview")
        )

        for config_path in config_powershell_paths:
            if config_path is not None:
                plan.append(
                    {
                        "function": init_powershell_user.__name__,
                        "kwargs": {
                            "target_path": config_path,
                            "conda_prefix": conda_prefix,
                            "reverse": reverse,
                        },
                    }
                )

    if "cmd.exe" in shells:
        if for_user:
            plan.append(
                {
                    "function": init_cmd_exe_registry.__name__,
                    "kwargs": {
                        "target_path": "HKEY_CURRENT_USER\\Software\\Microsoft\\"
                        "Command Processor\\AutoRun",
                        "conda_prefix": conda_prefix,
                        "reverse": reverse,
                    },
                }
            )
        if for_system:
            plan.append(
                {
                    "function": init_cmd_exe_registry.__name__,
                    "kwargs": {
                        "target_path": "HKEY_LOCAL_MACHINE\\Software\\Microsoft\\"
                        "Command Processor\\AutoRun",
                        "conda_prefix": conda_prefix,
                        "reverse": reverse,
                    },
                }
            )
            # it would be nice to enable this on a user-level basis, but unfortunately, it is
            #    a system-level key only.
            plan.append(
                {
                    "function": init_long_path.__name__,
                    "kwargs": {
                        "target_path": "HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\"
                        "FileSystem\\LongPathsEnabled"
                    },
                }
            )
        if anaconda_prompt:
            plan.append(
                {
                    "function": install_anaconda_prompt.__name__,
                    "kwargs": {
                        "target_path": join(
                            conda_prefix, "condabin", "Anaconda Prompt.lnk"
                        ),
                        "conda_prefix": conda_prefix,
                        "reverse": reverse,
                    },
                }
            )
            if on_win:
                desktop_dir, exception = get_folder_path(FOLDERID.Desktop)
                assert not exception
            else:
                desktop_dir = join(expanduser("~"), "Desktop")
            plan.append(
                {
                    "function": install_anaconda_prompt.__name__,
                    "kwargs": {
                        "target_path": join(desktop_dir, "Anaconda Prompt.lnk"),
                        "conda_prefix": conda_prefix,
                        "reverse": reverse,
                    },
                }
            )

    return plan


# #####################################################
# plan runners
# #####################################################


def run_plan(plan):
    for step in plan:
        previous_result = step.get("result", None)
        if previous_result in (Result.MODIFIED, Result.NO_CHANGE):
            continue
        try:
            result = globals()[step["function"]](
                *step.get("args", ()), **step.get("kwargs", {})
            )
        except OSError as e:
            log.info("%s: %r", step["function"], e, exc_info=True)
            result = Result.NEEDS_SUDO
        step["result"] = result


def run_plan_elevated(plan):
    """
    The strategy of this function differs between unix and Windows.  Both strategies use a
    subprocess call, where the subprocess is run with elevated privileges.  The executable
    invoked with the subprocess is `python -m conda.core.initialize`, so see the
    `if __name__ == "__main__"` at the bottom of this module.

    For unix platforms, we convert the plan list to json, and then call this module with
    `sudo python -m conda.core.initialize` while piping the plan json to stdin.  We collect json
    from stdout for the results of the plan execution with elevated privileges.

    For Windows, we create a temporary file that holds the json content of the plan.  The
    subprocess reads the content of the file, modifies the content of the file with updated
    execution status, and then closes the file.  This process then reads the content of that file
    for the individual operation execution results, and then deletes the file.
    """
    if any(step["result"] == Result.NEEDS_SUDO for step in plan):
        if on_win:
            from ..common._os.windows import run_as_admin

            temp_path = None
            try:
                with Utf8NamedTemporaryFile("w+", suffix=".json", delete=False) as tf:
                    # the default mode is 'w+b', and universal new lines don't work in that mode
                    tf.write(
                        json.dumps(
                            plan, ensure_ascii=False, default=lambda x: x.__dict__
                        )
                    )
                    temp_path = tf.name
                python_exe = f'"{abspath(sys.executable)}"'
                hinstance, error_code = run_as_admin(
                    (python_exe, "-m", "conda.core.initialize", f'"{temp_path}"')
                )
                if error_code is not None:
                    print(
                        f"ERROR during elevated execution.\n  rc: {error_code}",
                        file=sys.stderr,
                    )

                with open(temp_path) as fh:
                    _plan = json.loads(ensure_text_type(fh.read()))

            finally:
                if temp_path and lexists(temp_path):
                    rm_rf(temp_path)

        else:
            stdin = json.dumps(plan, ensure_ascii=False, default=lambda x: x.__dict__)
            result = subprocess_call(
                f"sudo {sys.executable} -m conda.core.initialize",
                env={},
                path=os.getcwd(),
                stdin=stdin,
            )
            stderr = result.stderr.strip()
            if stderr:
                print(stderr, file=sys.stderr)
            _plan = json.loads(result.stdout.strip())

        del plan[:]
        plan.extend(_plan)


def run_plan_from_stdin():
    stdin = sys.stdin.read().strip()
    plan = json.loads(stdin)
    run_plan(plan)
    sys.stdout.write(json.dumps(plan))


def run_plan_from_temp_file(temp_path):
    with open(temp_path) as fh:
        plan = json.loads(ensure_text_type(fh.read()))
    run_plan(plan)
    with open(temp_path, "w+b") as fh:
        fh.write(ensure_binary(json.dumps(plan, ensure_ascii=False)))


def print_plan_results(plan, stream=None):
    if not stream:
        stream = sys.stdout
    for step in plan:
        print(
            "%-14s%s" % (step.get("result"), step["kwargs"]["target_path"]), file=stream
        )

    changed = any(step.get("result") == Result.MODIFIED for step in plan)
    if changed:
        print(
            "\n==> For changes to take effect, close and re-open your current shell. <==\n",
            file=stream,
        )
    else:
        print("No action taken.", file=stream)


# #####################################################
# individual operations
# #####################################################


def make_entry_point(target_path, conda_prefix, module, func):
    # 'ep' in this function refers to 'entry point'
    # target_path: join(conda_prefix, 'bin', 'conda')
    conda_ep_path = target_path

    if isfile(conda_ep_path):
        with open(conda_ep_path) as fh:
            original_ep_content = fh.read()
    else:
        original_ep_content = ""

    if on_win:
        # no shebang needed on windows
        new_ep_content = ""
    else:
        python_path = join(conda_prefix, get_python_short_path())
        new_ep_content = generate_shebang_for_entry_point(
            python_path, with_usr_bin_env=True
        )

    conda_extra = dals(
        """
    # Before any more imports, leave cwd out of sys.path for internal 'conda shell.*' commands.
    # see https://github.com/conda/conda/issues/6549
    if len(sys.argv) > 1 and sys.argv[1].startswith('shell.') and sys.path and sys.path[0] == '':
        # The standard first entry in sys.path is an empty string,
        # and os.path.abspath('') expands to os.getcwd().
        del sys.path[0]
    """
    )

    new_ep_content += dals(
        """
    # -*- coding: utf-8 -*-
    import sys
    %(extra)s
    if __name__ == '__main__':
        from %(module)s import %(func)s
        sys.exit(%(func)s())
    """
    ) % {
        "extra": conda_extra if module == "conda.cli" else "",
        "module": module,
        "func": func,
    }

    if new_ep_content != original_ep_content:
        if context.verbose:
            print("\n")
            print(target_path)
            print(make_diff(original_ep_content, new_ep_content))
        if not context.dry_run:
            mkdir_p(dirname(conda_ep_path))
            with open(conda_ep_path, "w") as fdst:
                fdst.write(new_ep_content)
            if not on_win:
                make_executable(conda_ep_path)
        return Result.MODIFIED
    else:
        return Result.NO_CHANGE


def make_entry_point_exe(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'Scripts', 'conda.exe')
    exe_path = target_path
    bits = 8 * struct.calcsize("P")
    source_exe_path = join(CONDA_PACKAGE_ROOT, "shell", "cli-%d.exe" % bits)
    if isfile(exe_path):
        if compute_sum(exe_path, "md5") == compute_sum(source_exe_path, "md5"):
            return Result.NO_CHANGE

    if not context.dry_run:
        if not isdir(dirname(exe_path)):
            mkdir_p(dirname(exe_path))
        # prefer copy() over create_hard_link_or_copy() because of windows file deletion issues
        # with open processes
        copy(source_exe_path, exe_path)
    return Result.MODIFIED


def install_anaconda_prompt(target_path, conda_prefix, reverse):
    # target_path: join(conda_prefix, 'condabin', 'Anaconda Prompt.lnk')
    # target: join(os.environ["HOMEPATH"], "Desktop", "Anaconda Prompt.lnk")
    icon_path = join(CONDA_PACKAGE_ROOT, "shell", "conda_icon.ico")
    target = join(os.environ["HOMEPATH"], "Desktop", "Anaconda Prompt.lnk")

    args = (
        "/K",
        '""{}" && "{}""'.format(
            join(conda_prefix, "condabin", "conda_hook.bat"),
            join(conda_prefix, "condabin", "conda_auto_activate.bat"),
        ),
    )
    # The API for the call to 'create_shortcut' has 3
    # required arguments (path, description, filename)
    # and 4 optional ones (args, working_dir, icon_path, icon_index).
    result = Result.NO_CHANGE
    if not context.dry_run:
        create_shortcut(
            "%windir%\\System32\\cmd.exe",
            "Anconda Prompt",
            "" + target_path,
            " ".join(args),
            "" + expanduser("~"),
            "" + icon_path,
        )
        result = Result.MODIFIED
    if reverse:
        if os.path.isfile(target):
            os.remove(target)
            result = Result.MODIFIED
    return result


def _install_file(target_path, file_content):
    if isfile(target_path):
        with open(target_path) as fh:
            original_content = fh.read()
    else:
        original_content = ""

    new_content = file_content

    if new_content != original_content:
        if context.verbose:
            print("\n")
            print(target_path)
            print(make_diff(original_content, new_content))
        if not context.dry_run:
            mkdir_p(dirname(target_path))
            with open(target_path, "w") as fdst:
                fdst.write(new_content)
        return Result.MODIFIED
    else:
        return Result.NO_CHANGE


def install_conda_sh(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'etc', 'profile.d', 'conda.sh')
    file_content = PosixActivator().hook(auto_activate_base=False)
    return _install_file(target_path, file_content)


def install_Scripts_activate_bat(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'Scripts', 'activate.bat')
    src_path = join(CONDA_PACKAGE_ROOT, "shell", "Scripts", "activate.bat")
    with open(src_path) as fsrc:
        file_content = fsrc.read()
    return _install_file(target_path, file_content)


def install_activate_bat(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'condabin', 'activate.bat')
    src_path = join(CONDA_PACKAGE_ROOT, "shell", "condabin", "activate.bat")
    with open(src_path) as fsrc:
        file_content = fsrc.read()
    return _install_file(target_path, file_content)


def install_deactivate_bat(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'condabin', 'deactivate.bat')
    src_path = join(CONDA_PACKAGE_ROOT, "shell", "condabin", "deactivate.bat")
    with open(src_path) as fsrc:
        file_content = fsrc.read()
    return _install_file(target_path, file_content)


def install_activate(target_path, conda_prefix):
    # target_path: join(conda_prefix, get_bin_directory_short_path(), 'activate')
    src_path = join(CONDA_PACKAGE_ROOT, "shell", "bin", "activate")
    file_content = f'#!/bin/sh\n_CONDA_ROOT="{conda_prefix}"\n'
    with open(src_path) as fsrc:
        file_content += fsrc.read()
    return _install_file(target_path, file_content)


def install_deactivate(target_path, conda_prefix):
    # target_path: join(conda_prefix, get_bin_directory_short_path(), 'deactivate')
    src_path = join(CONDA_PACKAGE_ROOT, "shell", "bin", "deactivate")
    file_content = f'#!/bin/sh\n_CONDA_ROOT="{conda_prefix}"\n'
    with open(src_path) as fsrc:
        file_content += fsrc.read()
    return _install_file(target_path, file_content)


def install_condabin_conda_bat(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'condabin', 'conda.bat')
    conda_bat_src_path = join(CONDA_PACKAGE_ROOT, "shell", "condabin", "conda.bat")
    with open(conda_bat_src_path) as fsrc:
        file_content = fsrc.read()
    return _install_file(target_path, file_content)


def install_library_bin_conda_bat(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'Library', 'bin', 'conda.bat')
    conda_bat_src_path = join(
        CONDA_PACKAGE_ROOT, "shell", "Library", "bin", "conda.bat"
    )
    with open(conda_bat_src_path) as fsrc:
        file_content = fsrc.read()
    return _install_file(target_path, file_content)


def install_condabin_conda_activate_bat(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'condabin', '_conda_activate.bat')
    conda_bat_src_path = join(
        CONDA_PACKAGE_ROOT, "shell", "condabin", "_conda_activate.bat"
    )
    with open(conda_bat_src_path) as fsrc:
        file_content = fsrc.read()
    return _install_file(target_path, file_content)


def install_condabin_rename_tmp_bat(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'condabin', 'rename_tmp.bat')
    conda_bat_src_path = join(CONDA_PACKAGE_ROOT, "shell", "condabin", "rename_tmp.bat")
    with open(conda_bat_src_path) as fsrc:
        file_content = fsrc.read()
    return _install_file(target_path, file_content)


def install_condabin_conda_auto_activate_bat(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'condabin', 'conda_auto_activate.bat')
    conda_bat_src_path = join(
        CONDA_PACKAGE_ROOT, "shell", "condabin", "conda_auto_activate.bat"
    )
    with open(conda_bat_src_path) as fsrc:
        file_content = fsrc.read()
    return _install_file(target_path, file_content)


def install_condabin_hook_bat(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'condabin', 'conda_hook.bat')
    conda_bat_src_path = join(CONDA_PACKAGE_ROOT, "shell", "condabin", "conda_hook.bat")
    with open(conda_bat_src_path) as fsrc:
        file_content = fsrc.read()
    return _install_file(target_path, file_content)


def install_conda_fish(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'etc', 'fish', 'conf.d', 'conda.fish')
    file_content = FishActivator().hook(auto_activate_base=False)
    return _install_file(target_path, file_content)


def install_conda_psm1(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'shell', 'condabin', 'Conda.psm1')
    conda_psm1_path = join(CONDA_PACKAGE_ROOT, "shell", "condabin", "Conda.psm1")
    with open(conda_psm1_path) as fsrc:
        file_content = fsrc.read()
    return _install_file(target_path, file_content)


def install_conda_hook_ps1(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'shell', 'condabin', 'conda-hook.ps1')
    file_content = PowerShellActivator().hook(auto_activate_base=False)
    return _install_file(target_path, file_content)


def install_conda_xsh(target_path, conda_prefix):
    # target_path: join(site_packages_dir, 'xonsh', 'conda.xsh')
    file_content = XonshActivator().hook(auto_activate_base=False)
    return _install_file(target_path, file_content)


def install_conda_csh(target_path, conda_prefix):
    # target_path: join(conda_prefix, 'etc', 'profile.d', 'conda.csh')
    file_content = CshActivator().hook(auto_activate_base=False)
    return _install_file(target_path, file_content)


def _config_fish_content(conda_prefix):
    if on_win:
        from ..activate import native_path_to_unix

        conda_exe = native_path_to_unix(join(conda_prefix, "Scripts", "conda.exe"))
    else:
        conda_exe = join(conda_prefix, "bin", "conda")
    conda_initialize_content = dals(
        """
        # >>> conda initialize >>>
        # !! Contents within this block are managed by 'conda init' !!
        if test -f %(conda_exe)s
            eval %(conda_exe)s "shell.fish" "hook" $argv | source
        else
            if test -f "%(conda_prefix)s/etc/fish/conf.d/conda.fish"
                . "%(conda_prefix)s/etc/fish/conf.d/conda.fish"
            else
                set -x PATH "%(conda_prefix)s/bin" $PATH
            end
        end
        # <<< conda initialize <<<
        """
    ) % {
        "conda_exe": conda_exe,
        "conda_prefix": conda_prefix,
    }
    return conda_initialize_content


def init_fish_user(target_path, conda_prefix, reverse):
    # target_path: ~/.config/config.fish
    user_rc_path = target_path

    try:
        with open(user_rc_path) as fh:
            rc_content = fh.read()
    except FileNotFoundError:
        rc_content = ""
    except:
        raise

    rc_original_content = rc_content

    conda_init_comment = "# commented out by conda initialize"
    conda_initialize_content = _config_fish_content(conda_prefix)
    if reverse:
        # uncomment any lines that were commented by prior conda init run
        rc_content = re.sub(
            rf"#\s(.*?)\s*{conda_init_comment}",
            r"\1",
            rc_content,
            flags=re.MULTILINE,
        )

        # remove any conda init sections added
        rc_content = re.sub(
            r"^\s*" + CONDA_INITIALIZE_RE_BLOCK,
            "",
            rc_content,
            flags=re.DOTALL | re.MULTILINE,
        )
    else:
        if not on_win:
            rc_content = re.sub(
                rf"^[ \t]*?(set -gx PATH ([\'\"]?).*?{basename(conda_prefix)}\/bin\2 [^\n]*?\$PATH)"
                r"",
                rf"# \1  {conda_init_comment}",
                rc_content,
                flags=re.MULTILINE,
            )

        rc_content = re.sub(
            r"^[ \t]*[^#\n]?[ \t]*((?:source|\.) .*etc\/fish\/conf\.d\/conda\.fish.*?)\n"
            r"(conda activate.*?)$",
            rf"# \1  {conda_init_comment}\n# \2  {conda_init_comment}",
            rc_content,
            flags=re.MULTILINE,
        )
        rc_content = re.sub(
            r"^[ \t]*[^#\n]?[ \t]*((?:source|\.) .*etc\/fish\/conda\.d\/conda\.fish.*?)$",
            rf"# \1  {conda_init_comment}",
            rc_content,
            flags=re.MULTILINE,
        )

        replace_str = "__CONDA_REPLACE_ME_123__"
        rc_content = re.sub(
            CONDA_INITIALIZE_RE_BLOCK,
            replace_str,
            rc_content,
            flags=re.MULTILINE,
        )
        # TODO: maybe remove all but last of replace_str, if there's more than one occurrence
        rc_content = rc_content.replace(replace_str, conda_initialize_content)

        if "# >>> conda initialize >>>" not in rc_content:
            rc_content += f"\n{conda_initialize_content}\n"

    if rc_content != rc_original_content:
        if context.verbose:
            print("\n")
            print(target_path)
            print(make_diff(rc_original_content, rc_content))
        if not context.dry_run:
            # Make the directory if needed.
            if not exists(dirname(user_rc_path)):
                mkdir_p(dirname(user_rc_path))
            with open(user_rc_path, "w") as fh:
                fh.write(rc_content)
        return Result.MODIFIED
    else:
        return Result.NO_CHANGE


def _config_xonsh_content(conda_prefix):
    if on_win:
        from ..activate import native_path_to_unix

        conda_exe = native_path_to_unix(join(conda_prefix, "Scripts", "conda.exe"))
    else:
        conda_exe = join(conda_prefix, "bin", "conda")
    conda_initialize_content = dals(
        """
    # >>> conda initialize >>>
    # !! Contents within this block are managed by 'conda init' !!
    if !(test -f "{conda_exe}"):
        import sys as _sys
        from types import ModuleType as _ModuleType
        _mod = _ModuleType("xontrib.conda",
                        "Autogenerated from $({conda_exe} shell.xonsh hook)")
        __xonsh__.execer.exec($("{conda_exe}" "shell.xonsh" "hook"),
                            glbs=_mod.__dict__,
                            filename="$({conda_exe} shell.xonsh hook)")
        _sys.modules["xontrib.conda"] = _mod
        del _sys, _mod, _ModuleType
    # <<< conda initialize <<<
    """
    ).format(conda_exe=conda_exe)
    return conda_initialize_content


def init_xonsh_user(target_path, conda_prefix, reverse):
    # target_path: ~/.xonshrc
    user_rc_path = target_path

    try:
        with open(user_rc_path) as fh:
            rc_content = fh.read()
    except FileNotFoundError:
        rc_content = ""
    except:
        raise

    rc_original_content = rc_content

    conda_init_comment = "# commented out by conda initialize"
    conda_initialize_content = _config_xonsh_content(conda_prefix)
    if reverse:
        # uncomment any lines that were commented by prior conda init run
        rc_content = re.sub(
            rf"#\s(.*?)\s*{conda_init_comment}",
            r"\1",
            rc_content,
            flags=re.MULTILINE,
        )

        # remove any conda init sections added
        rc_content = re.sub(
            r"^\s*" + CONDA_INITIALIZE_RE_BLOCK,
            "",
            rc_content,
            flags=re.DOTALL | re.MULTILINE,
        )
    else:
        replace_str = "__CONDA_REPLACE_ME_123__"
        rc_content = re.sub(
            CONDA_INITIALIZE_RE_BLOCK,
            replace_str,
            rc_content,
            flags=re.MULTILINE,
        )
        # TODO: maybe remove all but last of replace_str, if there's more than one occurrence
        rc_content = rc_content.replace(replace_str, conda_initialize_content)

        if "# >>> conda initialize >>>" not in rc_content:
            rc_content += f"\n{conda_initialize_content}\n"

    if rc_content != rc_original_content:
        if context.verbose:
            print("\n")
            print(target_path)
            print(make_diff(rc_original_content, rc_content))
        if not context.dry_run:
            # Make the directory if needed.
            if not exists(dirname(user_rc_path)):
                mkdir_p(dirname(user_rc_path))
            with open(user_rc_path, "w") as fh:
                fh.write(rc_content)
        return Result.MODIFIED
    else:
        return Result.NO_CHANGE


def _bashrc_content(conda_prefix, shell):
    if on_win:
        from ..activate import native_path_to_unix

        conda_exe = native_path_to_unix(join(conda_prefix, "Scripts", "conda.exe"))
        conda_initialize_content = dals(
            """
        # >>> conda initialize >>>
        # !! Contents within this block are managed by 'conda init' !!
        if [ -f '%(conda_exe)s' ]; then
            eval "$('%(conda_exe)s' 'shell.%(shell)s' 'hook')"
        fi
        # <<< conda initialize <<<
        """
        ) % {
            "conda_exe": conda_exe,
            "shell": shell,
        }
    else:
        conda_exe = join(conda_prefix, "bin", "conda")
        if shell in ("csh", "tcsh"):
            conda_initialize_content = dals(
                """
            # >>> conda initialize >>>
            # !! Contents within this block are managed by 'conda init' !!
            if ( -f "%(conda_prefix)s/etc/profile.d/conda.csh" ) then
                source "%(conda_prefix)s/etc/profile.d/conda.csh"
            else
                setenv PATH "%(conda_bin)s:$PATH"
            endif
            # <<< conda initialize <<<
            """
            ) % {
                "conda_exe": conda_exe,
                "shell": shell,
                "conda_bin": dirname(conda_exe),
                "conda_prefix": conda_prefix,
            }
        else:
            conda_initialize_content = dals(
                """
            # >>> conda initialize >>>
            # !! Contents within this block are managed by 'conda init' !!
            __conda_setup="$('%(conda_exe)s' 'shell.%(shell)s' 'hook' 2> /dev/null)"
            if [ $? -eq 0 ]; then
                eval "$__conda_setup"
            else
                if [ -f "%(conda_prefix)s/etc/profile.d/conda.sh" ]; then
                    . "%(conda_prefix)s/etc/profile.d/conda.sh"
                else
                    export PATH="%(conda_bin)s:$PATH"
                fi
            fi
            unset __conda_setup
            # <<< conda initialize <<<
            """
            ) % {
                "conda_exe": conda_exe,
                "shell": shell,
                "conda_bin": dirname(conda_exe),
                "conda_prefix": conda_prefix,
            }
    return conda_initialize_content


def init_sh_user(target_path, conda_prefix, shell, reverse=False):
    # target_path: ~/.bash_profile
    user_rc_path = target_path

    try:
        with open(user_rc_path) as fh:
            rc_content = fh.read()
    except FileNotFoundError:
        rc_content = ""
    except:
        raise

    rc_original_content = rc_content

    conda_initialize_content = _bashrc_content(conda_prefix, shell)
    conda_init_comment = "# commented out by conda initialize"

    if reverse:
        # uncomment any lines that were commented by prior conda init run
        rc_content = re.sub(
            rf"#\s(.*?)\s*{conda_init_comment}",
            r"\1",
            rc_content,
            flags=re.MULTILINE,
        )

        # remove any conda init sections added
        rc_content = re.sub(
            r"^\s*" + CONDA_INITIALIZE_RE_BLOCK,
            "",
            rc_content,
            flags=re.DOTALL | re.MULTILINE,
        )
    else:
        if not on_win:
            rc_content = re.sub(
                rf"^[ \t]*?(export PATH=[\'\"].*?{basename(conda_prefix)}\/bin:\$PATH[\'\"])"
                r"",
                rf"# \1  {conda_init_comment}",
                rc_content,
                flags=re.MULTILINE,
            )

        rc_content = re.sub(
            r"^[ \t]*[^#\n]?[ \t]*((?:source|\.) .*etc\/profile\.d\/conda\.sh.*?)\n"
            r"(conda activate.*?)$",
            rf"# \1  {conda_init_comment}\n# \2  {conda_init_comment}",
            rc_content,
            flags=re.MULTILINE,
        )
        rc_content = re.sub(
            r"^[ \t]*[^#\n]?[ \t]*((?:source|\.) .*etc\/profile\.d\/conda\.sh.*?)$",
            rf"# \1  {conda_init_comment}",
            rc_content,
            flags=re.MULTILINE,
        )

        if on_win:
            rc_content = re.sub(
                r"^[ \t]*^[ \t]*[^#\n]?[ \t]*((?:source|\.) .*Scripts[/\\]activate.*?)$",
                r"# \1  # commented out by conda initialize",
                rc_content,
                flags=re.MULTILINE,
            )
        else:
            rc_content = re.sub(
                r"^[ \t]*^[ \t]*[^#\n]?[ \t]*((?:source|\.) .*bin/activate.*?)$",
                r"# \1  # commented out by conda initialize",
                rc_content,
                flags=re.MULTILINE,
            )

        replace_str = "__CONDA_REPLACE_ME_123__"
        rc_content = re.sub(
            CONDA_INITIALIZE_RE_BLOCK,
            replace_str,
            rc_content,
            flags=re.MULTILINE,
        )
        # TODO: maybe remove all but last of replace_str, if there's more than one occurrence
        rc_content = rc_content.replace(replace_str, conda_initialize_content)

        if "# >>> conda initialize >>>" not in rc_content:
            rc_content += f"\n{conda_initialize_content}\n"

    if rc_content != rc_original_content:
        if context.verbose:
            print("\n")
            print(target_path)
            print(make_diff(rc_original_content, rc_content))
        if not context.dry_run:
            with open(user_rc_path, "w") as fh:
                fh.write(rc_content)
        return Result.MODIFIED
    else:
        return Result.NO_CHANGE


def init_sh_system(target_path, conda_prefix, reverse=False):
    # target_path: '/etc/profile.d/conda.sh'
    conda_sh_system_path = target_path

    if exists(conda_sh_system_path):
        with open(conda_sh_system_path) as fh:
            conda_sh_system_contents = fh.read()
    else:
        conda_sh_system_contents = ""
    if reverse:
        if exists(conda_sh_system_path):
            os.remove(conda_sh_system_path)
            return Result.MODIFIED
    else:
        conda_sh_contents = _bashrc_content(conda_prefix, "posix")
        if conda_sh_system_contents != conda_sh_contents:
            if context.verbose:
                print("\n")
                print(target_path)
                print(make_diff(conda_sh_contents, conda_sh_system_contents))
            if not context.dry_run:
                if lexists(conda_sh_system_path):
                    rm_rf(conda_sh_system_path)
                mkdir_p(dirname(conda_sh_system_path))
                with open(conda_sh_system_path, "w") as fh:
                    fh.write(conda_sh_contents)
            return Result.MODIFIED
    return Result.NO_CHANGE


def _read_windows_registry(target_path):  # pragma: no cover
    # HKEY_LOCAL_MACHINE\Software\Microsoft\Command Processor\AutoRun
    # HKEY_CURRENT_USER\Software\Microsoft\Command Processor\AutoRun
    # returns value_value, value_type  -or-  None, None if target does not exist
    main_key, the_rest = target_path.split("\\", 1)
    subkey_str, value_name = the_rest.rsplit("\\", 1)
    main_key = getattr(winreg, main_key)

    try:
        key = winreg.OpenKey(main_key, subkey_str, 0, winreg.KEY_READ)
    except OSError as e:
        if e.errno != ENOENT:
            raise
        return None, None

    try:
        value_tuple = winreg.QueryValueEx(key, value_name)
        value_value = value_tuple[0]
        if isinstance(value_value, str):
            value_value = value_value.strip()
        value_type = value_tuple[1]
        return value_value, value_type
    except Exception:
        # [WinError 2] The system cannot find the file specified
        winreg.CloseKey(key)
        return None, None
    finally:
        winreg.CloseKey(key)


def _write_windows_registry(target_path, value_value, value_type):  # pragma: no cover
    main_key, the_rest = target_path.split("\\", 1)
    subkey_str, value_name = the_rest.rsplit("\\", 1)
    main_key = getattr(winreg, main_key)
    try:
        key = winreg.OpenKey(main_key, subkey_str, 0, winreg.KEY_WRITE)
    except OSError as e:
        if e.errno != ENOENT:
            raise
        key = winreg.CreateKey(main_key, subkey_str)
    try:
        winreg.SetValueEx(key, value_name, 0, value_type, value_value)
    finally:
        winreg.CloseKey(key)


def init_cmd_exe_registry(target_path, conda_prefix, reverse=False):
    # HKEY_LOCAL_MACHINE\Software\Microsoft\Command Processor\AutoRun
    # HKEY_CURRENT_USER\Software\Microsoft\Command Processor\AutoRun

    prev_value, value_type = _read_windows_registry(target_path)
    if prev_value is None:
        prev_value = ""
        value_type = winreg.REG_EXPAND_SZ

    old_hook_path = '"{}"'.format(join(conda_prefix, "condabin", "conda_hook.bat"))
    new_hook = f"if exist {old_hook_path} {old_hook_path}"
    if reverse:
        # we can't just reset it to None and remove it, because there may be other contents here.
        #   We need to strip out our part, and if there's nothing left, remove the key.
        # Break up string by parts joined with "&"
        autorun_parts = prev_value.split("&")
        autorun_parts = [part.strip() for part in autorun_parts if new_hook not in part]
        # We must remove the old hook path too if it is there
        autorun_parts = [
            part.strip() for part in autorun_parts if old_hook_path not in part
        ]
        new_value = " & ".join(autorun_parts)
    else:
        replace_str = "__CONDA_REPLACE_ME_123__"
        # Replace new (if exist checked) hook
        new_value = re.sub(
            r"(if exist \"[^\"]*?conda[-_]hook\.bat\" \"[^\"]*?conda[-_]hook\.bat\")",
            replace_str,
            prev_value,
            count=1,
            flags=re.IGNORECASE | re.UNICODE,
        )
        # Replace old hook
        new_value = re.sub(
            r"(\"[^\"]*?conda[-_]hook\.bat\")",
            replace_str,
            new_value,
            flags=re.IGNORECASE | re.UNICODE,
        )

        # Fold repeats of 'HOOK & HOOK'
        new_value_2 = new_value.replace(replace_str + " & " + replace_str, replace_str)
        while new_value_2 != new_value:
            new_value = new_value_2
            new_value_2 = new_value.replace(
                replace_str + " & " + replace_str, replace_str
            )
        new_value = new_value_2.replace(replace_str, new_hook)
        if new_hook not in new_value:
            if new_value:
                new_value += " & " + new_hook
            else:
                new_value = new_hook

    if prev_value != new_value:
        if context.verbose:
            print("\n")
            print(target_path)
            print(make_diff(prev_value, new_value))
        if not context.dry_run:
            _write_windows_registry(target_path, new_value, value_type)
        return Result.MODIFIED
    else:
        return Result.NO_CHANGE


def init_long_path(target_path):
    win_ver, _, win_rev = context.os_distribution_name_version[1].split(".")
    # win10, build 14352 was the first preview release that supported this
    if int(win_ver) >= 10 and int(win_rev) >= 14352:
        prev_value, value_type = _read_windows_registry(target_path)
        if str(prev_value) != "1":
            if context.verbose:
                print("\n")
                print(target_path)
                print(make_diff(str(prev_value), "1"))
            if not context.dry_run:
                _write_windows_registry(target_path, 1, winreg.REG_DWORD)
            return Result.MODIFIED
        else:
            return Result.NO_CHANGE
    else:
        if context.verbose:
            print("\n")
            print(
                "Not setting long path registry key; Windows version must be at least 10 with "
                'the fall 2016 "Anniversary update" or newer.'
            )
            return Result.NO_CHANGE


def _powershell_profile_content(conda_prefix):
    if on_win:
        conda_exe = join(conda_prefix, "Scripts", "conda.exe")
    else:
        conda_exe = join(conda_prefix, "bin", "conda")

    conda_powershell_module = dals(
        f"""
    #region conda initialize
    # !! Contents within this block are managed by 'conda init' !!
    If (Test-Path "{conda_exe}") {{
        (& "{conda_exe}" "shell.powershell" "hook") | Out-String | ?{{$_}} | Invoke-Expression
    }}
    #endregion
    """
    )

    return conda_powershell_module


def init_powershell_user(target_path, conda_prefix, reverse):
    # target_path: $PROFILE
    profile_path = target_path

    # NB: the user may not have created a profile. We need to check
    #     if the file exists first.
    if os.path.exists(profile_path):
        with open(profile_path) as fp:
            profile_content = fp.read()
    else:
        profile_content = ""

    profile_original_content = profile_content

    # TODO: comment out old ipmos and Import-Modules.

    if reverse:
        profile_content = re.sub(
            CONDA_INITIALIZE_PS_RE_BLOCK,
            "",
            profile_content,
            count=1,
            flags=re.DOTALL | re.MULTILINE,
        )
    else:
        # Find what content we need to add.
        conda_initialize_content = _powershell_profile_content(conda_prefix)

        if "#region conda initialize" not in profile_content:
            profile_content += f"\n{conda_initialize_content}\n"
        else:
            profile_content = re.sub(
                CONDA_INITIALIZE_PS_RE_BLOCK,
                "__CONDA_REPLACE_ME_123__",
                profile_content,
                count=1,
                flags=re.DOTALL | re.MULTILINE,
            ).replace("__CONDA_REPLACE_ME_123__", conda_initialize_content)

    if profile_content != profile_original_content:
        if context.verbose:
            print("\n")
            print(target_path)
            print(make_diff(profile_original_content, profile_content))
        if not context.dry_run:
            # Make the directory if needed.
            if not exists(dirname(profile_path)):
                mkdir_p(dirname(profile_path))
            with open(profile_path, "w") as fp:
                fp.write(profile_content)
        return Result.MODIFIED
    else:
        return Result.NO_CHANGE


def remove_conda_in_sp_dir(target_path):
    # target_path: site_packages_dir
    modified = False
    site_packages_dir = target_path
    rm_rf_these = chain.from_iterable(
        (
            glob(join(site_packages_dir, "conda-*info")),
            glob(join(site_packages_dir, "conda.*")),
            glob(join(site_packages_dir, "conda-*.egg")),
        )
    )
    rm_rf_these = (p for p in rm_rf_these if not p.endswith("conda.egg-link"))
    for fn in rm_rf_these:
        print(f"rm -rf {join(site_packages_dir, fn)}", file=sys.stderr)
        if not context.dry_run:
            rm_rf(join(site_packages_dir, fn))
        modified = True
    others = (
        "conda",
        "conda_env",
    )
    for other in others:
        path = join(site_packages_dir, other)
        if lexists(path):
            print(f"rm -rf {path}", file=sys.stderr)
            if not context.dry_run:
                rm_rf(path)
            modified = True
    if modified:
        return Result.MODIFIED
    else:
        return Result.NO_CHANGE


def make_conda_egg_link(target_path, conda_source_root):
    # target_path: join(site_packages_dir, 'conda.egg-link')
    conda_egg_link_contents = conda_source_root + os.linesep

    if isfile(target_path):
        with open(target_path, "rb") as fh:
            conda_egg_link_contents_old = fh.read()
    else:
        conda_egg_link_contents_old = ""

    if conda_egg_link_contents_old != conda_egg_link_contents:
        if context.verbose:
            print("\n", file=sys.stderr)
            print(target_path, file=sys.stderr)
            print(
                make_diff(conda_egg_link_contents_old, conda_egg_link_contents),
                file=sys.stderr,
            )
        if not context.dry_run:
            with open(target_path, "wb") as fh:
                fh.write(ensure_utf8_encoding(conda_egg_link_contents))
        return Result.MODIFIED
    else:
        return Result.NO_CHANGE


def modify_easy_install_pth(target_path, conda_source_root):
    # target_path: join(site_packages_dir, 'easy-install.pth')
    easy_install_new_line = conda_source_root

    if isfile(target_path):
        with open(target_path) as fh:
            old_contents = fh.read()
    else:
        old_contents = ""

    old_contents_lines = old_contents.splitlines()
    if easy_install_new_line in old_contents_lines:
        return Result.NO_CHANGE

    ln_end = os.sep + "conda"
    old_contents_lines = tuple(
        ln for ln in old_contents_lines if not ln.endswith(ln_end)
    )
    new_contents = (
        easy_install_new_line
        + os.linesep
        + os.linesep.join(old_contents_lines)
        + os.linesep
    )

    if context.verbose:
        print("\n", file=sys.stderr)
        print(target_path, file=sys.stderr)
        print(make_diff(old_contents, new_contents), file=sys.stderr)
    if not context.dry_run:
        with open(target_path, "wb") as fh:
            fh.write(ensure_utf8_encoding(new_contents))
    return Result.MODIFIED


def make_dev_egg_info_file(target_path):
    # target_path: join(conda_source_root, 'conda.egg-info')

    if isfile(target_path):
        with open(target_path) as fh:
            old_contents = fh.read()
    else:
        old_contents = ""

    new_contents = (
        dals(
            """
    Metadata-Version: 1.1
    Name: conda
    Version: %s
    Platform: UNKNOWN
    Summary: OS-agnostic, system-level binary package manager.
    """
        )
        % CONDA_VERSION
    )

    if old_contents == new_contents:
        return Result.NO_CHANGE

    if context.verbose:
        print("\n", file=sys.stderr)
        print(target_path, file=sys.stderr)
        print(make_diff(old_contents, new_contents), file=sys.stderr)
    if not context.dry_run:
        if lexists(target_path):
            rm_rf(target_path)
        with open(target_path, "w") as fh:
            fh.write(new_contents)
    return Result.MODIFIED


# #####################################################
# helper functions
# #####################################################


def make_diff(old, new):
    return "\n".join(unified_diff(old.splitlines(), new.splitlines()))


def _get_python_info(prefix):
    python_exe = join(prefix, get_python_short_path())
    result = subprocess_call(f"{python_exe} --version")
    stdout, stderr = result.stdout.strip(), result.stderr.strip()
    if stderr:
        python_version = stderr.split()[1]
    elif stdout:  # pragma: no cover
        python_version = stdout.split()[1]
    else:  # pragma: no cover
        raise ValueError("No python version information available.")

    site_packages_dir = join(
        prefix, win_path_ok(get_python_site_packages_short_path(python_version))
    )
    return python_exe, python_version, site_packages_dir


if __name__ == "__main__":
    if on_win:
        temp_path = sys.argv[1]
        run_plan_from_temp_file(temp_path)
    else:
        run_plan_from_stdin()


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Tools for managing a subdir's repodata.json."""

from __future__ import annotations

import json
import pickle
from collections import UserList, defaultdict
from functools import partial
from itertools import chain
from logging import getLogger
from os.path import exists, getmtime, isfile, join, splitext
from pathlib import Path
from time import time
from typing import TYPE_CHECKING

from boltons.setutils import IndexedSet

from ..auxlib.ish import dals
from ..base.constants import CONDA_PACKAGE_EXTENSION_V1, REPODATA_FN
from ..base.context import context
from ..common.io import DummyExecutor, ThreadLimitedThreadPoolExecutor, dashlist
from ..common.iterators import groupby_to_dict as groupby
from ..common.path import url_to_path
from ..common.url import join_url
from ..deprecations import deprecated
from ..exceptions import ChannelError, CondaUpgradeError, UnavailableInvalidChannel
from ..gateways.disk.delete import rm_rf
from ..gateways.repodata import (
    CACHE_STATE_SUFFIX,
    CondaRepoInterface,
    RepodataFetch,
    RepodataState,
    cache_fn_url,
    create_cache_dir,
    get_repo_interface,
)
from ..gateways.repodata import (
    get_cache_control_max_age as _get_cache_control_max_age,
)
from ..models.channel import Channel, all_channel_urls
from ..models.match_spec import MatchSpec
from ..models.records import PackageRecord

if TYPE_CHECKING:
    from ..gateways.repodata import RepodataCache, RepoInterface

log = getLogger(__name__)

REPODATA_PICKLE_VERSION = 30
MAX_REPODATA_VERSION = 2
REPODATA_HEADER_RE = b'"(_etag|_mod|_cache_control)":[ ]?"(.*?[^\\\\])"[,}\\s]'  # NOQA


@deprecated(
    "24.3",
    "24.9",
    addendum="Use `conda.gateways.repodata.get_cache_control_max_age` instead.",
)
def get_cache_control_max_age(cache_control_value: str) -> int:
    return _get_cache_control_max_age(cache_control_value)


class SubdirDataType(type):
    def __call__(cls, channel, repodata_fn=REPODATA_FN):
        assert channel.subdir
        assert not channel.package_filename
        assert type(channel) is Channel
        now = time()
        repodata_fn = repodata_fn or REPODATA_FN
        cache_key = channel.url(with_credentials=True), repodata_fn
        if cache_key in SubdirData._cache_:
            cache_entry = SubdirData._cache_[cache_key]
            if cache_key[0] and cache_key[0].startswith("file://"):
                channel_url = channel.url()
                if channel_url:
                    file_path = url_to_path(channel_url + "/" + repodata_fn)
                    if exists(file_path) and cache_entry._mtime >= getmtime(file_path):
                        return cache_entry
            else:
                return cache_entry
        subdir_data_instance = super().__call__(
            channel, repodata_fn, RepoInterface=get_repo_interface()
        )
        subdir_data_instance._mtime = now
        SubdirData._cache_[cache_key] = subdir_data_instance
        return subdir_data_instance


class PackageRecordList(UserList):
    """Lazily convert dicts to PackageRecord."""

    def __getitem__(self, i):
        if isinstance(i, slice):
            return self.__class__(self.data[i])
        else:
            record = self.data[i]
            if not isinstance(record, PackageRecord):
                record = PackageRecord(**record)
                self.data[i] = record
            return record


class SubdirData(metaclass=SubdirDataType):
    _cache_ = {}

    @classmethod
    def clear_cached_local_channel_data(cls, exclude_file=True):
        # This should only ever be needed during unit tests, when
        # CONDA_USE_ONLY_TAR_BZ2 may change during process lifetime.
        if exclude_file:
            cls._cache_ = {
                k: v for k, v in cls._cache_.items() if not k[0].startswith("file://")
            }
        else:
            cls._cache_.clear()

    @staticmethod
    def query_all(
        package_ref_or_match_spec, channels=None, subdirs=None, repodata_fn=REPODATA_FN
    ):
        from .index import check_allowlist  # TODO: fix in-line import

        # ensure that this is not called by threaded code
        create_cache_dir()
        if channels is None:
            channels = context.channels
        if subdirs is None:
            subdirs = context.subdirs
        channel_urls = all_channel_urls(channels, subdirs=subdirs)
        if context.offline:
            grouped_urls = groupby(lambda url: url.startswith("file://"), channel_urls)
            ignored_urls = grouped_urls.get(False, ())
            if ignored_urls:
                log.info(
                    "Ignoring the following channel urls because mode is offline.%s",
                    dashlist(ignored_urls),
                )
            channel_urls = IndexedSet(grouped_urls.get(True, ()))

        check_allowlist(channel_urls)

        def subdir_query(url):
            return tuple(
                SubdirData(Channel(url), repodata_fn=repodata_fn).query(
                    package_ref_or_match_spec
                )
            )

        # TODO test timing with ProcessPoolExecutor
        Executor = (
            DummyExecutor
            if context.debug or context.repodata_threads == 1
            else partial(
                ThreadLimitedThreadPoolExecutor, max_workers=context.repodata_threads
            )
        )
        with Executor() as executor:
            result = tuple(
                chain.from_iterable(executor.map(subdir_query, channel_urls))
            )
        return result

    def query(self, package_ref_or_match_spec):
        if not self._loaded:
            self.load()
        param = package_ref_or_match_spec
        if isinstance(param, str):
            param = MatchSpec(param)  # type: ignore
        if isinstance(param, MatchSpec):
            if param.get_exact_value("name"):
                package_name = param.get_exact_value("name")
                for prec in self._iter_records_by_name(package_name):
                    if param.match(prec):
                        yield prec
            else:
                for prec in self.iter_records():
                    if param.match(prec):
                        yield prec
        else:
            assert isinstance(param, PackageRecord)
            for prec in self._iter_records_by_name(param.name):
                if prec == param:
                    yield prec

    def __init__(
        self, channel, repodata_fn=REPODATA_FN, RepoInterface=CondaRepoInterface
    ):
        assert channel.subdir
        # metaclass __init__ asserts no package_filename
        if channel.package_filename:  # pragma: no cover
            parts = channel.dump()
            del parts["package_filename"]
            channel = Channel(**parts)
        self.channel = channel
        # disallow None (typing)
        self.url_w_subdir = self.channel.url(with_credentials=False) or ""
        self.url_w_credentials = self.channel.url(with_credentials=True) or ""
        # these can be overriden by repodata.json v2
        self._base_url = self.url_w_subdir
        self._base_url_w_credentials = self.url_w_credentials
        # whether or not to try using the new, trimmed-down repodata
        self.repodata_fn = repodata_fn
        self.RepoInterface = RepoInterface
        self._loaded = False
        self._key_mgr = None

    @property
    def _repo(self) -> RepoInterface:
        """
        Changes as we mutate self.repodata_fn.
        """
        return self.repo_fetch._repo

    @property
    def repo_cache(self) -> RepodataCache:
        return self.repo_fetch.repo_cache

    @property
    def repo_fetch(self) -> RepodataFetch:
        """
        Object to get repodata. Not cached since self.repodata_fn is mutable.

        Replaces self._repo & self.repo_cache.
        """
        return RepodataFetch(
            Path(self.cache_path_base),
            self.channel,
            self.repodata_fn,
            repo_interface_cls=self.RepoInterface,
        )

    def reload(self):
        self._loaded = False
        self.load()
        return self

    @property
    def cache_path_base(self):
        return join(
            create_cache_dir(),
            splitext(cache_fn_url(self.url_w_credentials, self.repodata_fn))[0],
        )

    @property
    def url_w_repodata_fn(self):
        return self.url_w_subdir + "/" + self.repodata_fn

    @property
    def cache_path_json(self):
        return Path(
            self.cache_path_base + ("1" if context.use_only_tar_bz2 else "") + ".json"
        )

    @property
    def cache_path_state(self):
        """Out-of-band etag and other state needed by the RepoInterface."""
        return Path(
            self.cache_path_base
            + ("1" if context.use_only_tar_bz2 else "")
            + CACHE_STATE_SUFFIX
        )

    @property
    def cache_path_pickle(self):
        return self.cache_path_base + ("1" if context.use_only_tar_bz2 else "") + ".q"

    def load(self):
        _internal_state = self._load()
        if _internal_state.get("repodata_version", 0) > MAX_REPODATA_VERSION:
            raise CondaUpgradeError(
                dals(
                    """
                The current version of conda is too old to read repodata from

                    %s

                (This version only supports repodata_version 1 and 2.)
                Please update conda to use this channel.
                """
                )
                % self.url_w_repodata_fn
            )
        self._base_url = _internal_state.get("base_url", self.url_w_subdir)
        self._base_url_w_credentials = _internal_state.get(
            "base_url_w_credentials", self.url_w_credentials
        )
        self._internal_state = _internal_state
        self._package_records = _internal_state["_package_records"]
        self._names_index = _internal_state["_names_index"]
        # Unused since early 2023:
        self._track_features_index = _internal_state["_track_features_index"]
        self._loaded = True
        return self

    def iter_records(self):
        if not self._loaded:
            self.load()
        return iter(self._package_records)
        # could replace self._package_records with fully-converted UserList.data
        # after going through entire list

    def _iter_records_by_name(self, name):
        for i in self._names_index[name]:
            yield self._package_records[i]

    def _load(self):
        """
        Try to load repodata. If e.g. we are downloading
        `current_repodata.json`, fall back to `repodata.json` when the former is
        unavailable.
        """
        try:
            fetcher = self.repo_fetch
            repodata, state = fetcher.fetch_latest_parsed()
            return self._process_raw_repodata(repodata, state)
        except UnavailableInvalidChannel:
            if self.repodata_fn != REPODATA_FN:
                self.repodata_fn = REPODATA_FN
                return self._load()
            else:
                raise

    def _pickle_me(self):
        try:
            log.debug(
                "Saving pickled state for %s at %s",
                self.url_w_repodata_fn,
                self.cache_path_pickle,
            )
            with open(self.cache_path_pickle, "wb") as fh:
                pickle.dump(self._internal_state, fh, pickle.HIGHEST_PROTOCOL)
        except Exception:
            log.debug("Failed to dump pickled repodata.", exc_info=True)

    def _read_local_repodata(self, state: RepodataState):
        # first try reading pickled data
        _pickled_state = self._read_pickled(state)
        if _pickled_state:
            return _pickled_state

        raw_repodata_str, state = self.repo_fetch.read_cache()
        _internal_state = self._process_raw_repodata_str(raw_repodata_str, state)
        # taken care of by _process_raw_repodata():
        assert self._internal_state is _internal_state
        self._pickle_me()
        return _internal_state

    def _pickle_valid_checks(self, pickled_state, mod, etag):
        """Throw away the pickle if these don't all match."""
        yield "_url", pickled_state.get("_url"), self.url_w_credentials
        yield "_schannel", pickled_state.get("_schannel"), self.channel.canonical_name
        yield (
            "_add_pip",
            pickled_state.get("_add_pip"),
            context.add_pip_as_python_dependency,
        )
        yield "_mod", pickled_state.get("_mod"), mod
        yield "_etag", pickled_state.get("_etag"), etag
        yield (
            "_pickle_version",
            pickled_state.get("_pickle_version"),
            REPODATA_PICKLE_VERSION,
        )
        yield "fn", pickled_state.get("fn"), self.repodata_fn

    def _read_pickled(self, state: RepodataState):
        if not isinstance(state, RepodataState):
            state = RepodataState(
                self.cache_path_json,
                self.cache_path_state,
                self.repodata_fn,
                dict=state,
            )

        if not isfile(self.cache_path_pickle) or not isfile(self.cache_path_json):
            # Don't trust pickled data if there is no accompanying json data
            return None

        try:
            if isfile(self.cache_path_pickle):
                log.debug("found pickle file %s", self.cache_path_pickle)
            with open(self.cache_path_pickle, "rb") as fh:
                _pickled_state = pickle.load(fh)
        except Exception:
            log.debug("Failed to load pickled repodata.", exc_info=True)
            rm_rf(self.cache_path_pickle)
            return None

        def checks():
            return self._pickle_valid_checks(_pickled_state, state.mod, state.etag)

        def _check_pickled_valid():
            for _, left, right in checks():
                yield left == right

        if not all(_check_pickled_valid()):
            log.debug(
                "Pickle load validation failed for %s at %s. %r",
                self.url_w_repodata_fn,
                self.cache_path_json,
                tuple(checks()),
            )
            return None

        return _pickled_state

    def _process_raw_repodata_str(
        self,
        raw_repodata_str,
        state: RepodataState | None = None,
    ):
        """State contains information that was previously in-band in raw_repodata_str."""
        json_obj = json.loads(raw_repodata_str or "{}")
        return self._process_raw_repodata(json_obj, state=state)

    def _process_raw_repodata(self, repodata: dict, state: RepodataState | None = None):
        if not isinstance(state, RepodataState):
            state = RepodataState(
                self.cache_path_json,
                self.cache_path_state,
                self.repodata_fn,
                dict=state,
            )

        subdir = repodata.get("info", {}).get("subdir") or self.channel.subdir
        assert subdir == self.channel.subdir
        add_pip = context.add_pip_as_python_dependency
        schannel = self.channel.canonical_name

        self._package_records = _package_records = PackageRecordList()
        self._names_index = _names_index = defaultdict(list)
        self._track_features_index = _track_features_index = defaultdict(list)
        base_url = self._get_base_url(repodata, with_credentials=False)
        base_url_w_credentials = self._get_base_url(repodata, with_credentials=True)

        _internal_state = {
            "channel": self.channel,
            "url_w_subdir": self.url_w_subdir,
            "url_w_credentials": self.url_w_credentials,
            "base_url": base_url,
            "base_url_w_credentials": base_url_w_credentials,
            "cache_path_base": self.cache_path_base,
            "fn": self.repodata_fn,
            "_package_records": _package_records,
            "_names_index": _names_index,
            "_track_features_index": _track_features_index,
            "_etag": state.get("_etag"),
            "_mod": state.get("_mod"),
            "_cache_control": state.get("_cache_control"),
            "_url": state.get("_url"),
            "_add_pip": add_pip,
            "_pickle_version": REPODATA_PICKLE_VERSION,
            "_schannel": schannel,
            "repodata_version": state.get("repodata_version", 0),
        }
        if _internal_state["repodata_version"] > MAX_REPODATA_VERSION:
            raise CondaUpgradeError(
                dals(
                    """
                The current version of conda is too old to read repodata from

                    %s

                (This version only supports repodata_version 1 and 2.)
                Please update conda to use this channel.
                """
                )
                % self.url_w_subdir
            )

        meta_in_common = {  # just need to make this once, then apply with .update()
            "arch": repodata.get("info", {}).get("arch"),
            "channel": self.channel,
            "platform": repodata.get("info", {}).get("platform"),
            "schannel": schannel,
            "subdir": subdir,
        }

        legacy_packages = repodata.get("packages", {})
        conda_packages = (
            {} if context.use_only_tar_bz2 else repodata.get("packages.conda", {})
        )

        _tar_bz2 = CONDA_PACKAGE_EXTENSION_V1
        use_these_legacy_keys = set(legacy_packages.keys()) - {
            k[:-6] + _tar_bz2 for k in conda_packages.keys()
        }

        for group, copy_legacy_md5 in (
            (conda_packages.items(), True),
            (((k, legacy_packages[k]) for k in use_these_legacy_keys), False),
        ):
            for fn, info in group:
                if copy_legacy_md5:
                    counterpart = fn.replace(".conda", ".tar.bz2")
                    if counterpart in legacy_packages:
                        info["legacy_bz2_md5"] = legacy_packages[counterpart].get("md5")
                        info["legacy_bz2_size"] = legacy_packages[counterpart].get(
                            "size"
                        )
                if (
                    add_pip
                    and info["name"] == "python"
                    and info["version"].startswith(("2.", "3."))
                ):
                    info["depends"].append("pip")
                info.update(meta_in_common)
                if info.get("record_version", 0) > 1:
                    log.debug(
                        "Ignoring record_version %d from %s",
                        info["record_version"],
                        info["url"],
                    )
                    continue

                # lazy
                # package_record = PackageRecord(**info)
                info["fn"] = fn
                info["url"] = join_url(base_url_w_credentials, fn)
                _package_records.append(info)
                record_index = len(_package_records) - 1
                _names_index[info["name"]].append(record_index)

        self._internal_state = _internal_state
        return _internal_state

    def _get_base_url(self, repodata: dict, with_credentials: bool = True) -> str:
        """
        In repodata_version=1, .tar.bz2 and .conda artifacts are assumed to
        be colocated next to repodata.json, in the same server and directory.

        In repodata_version=2, repodata.json files can define a 'base_url' field
        to override that default assumption. See CEP-15 for more details.

        This method deals with both cases and returns the appropriate value.
        """
        maybe_base_url = repodata.get("info", {}).get("base_url")
        if maybe_base_url:  # repodata defines base_url field
            try:
                base_url_parts = Channel(maybe_base_url).dump()
            except ValueError as exc:
                raise ChannelError(
                    f"Subdir for {self.channel.canonical_name} at url '{self.url_w_subdir}' "
                    "has invalid 'base_url'"
                ) from exc
            if with_credentials and self.url_w_credentials != self.url_w_subdir:
                # We don't check for .token or .auth because those are not well defined
                # in multichannel objects. It's safer to compare the resulting URLs.
                # Note that base_url is assumed to have the same authentication as the repodata
                channel_parts = self.channel.dump()
                for key in ("auth", "token"):
                    if base_url_parts.get(key):
                        raise ChannelError(
                            f"'{self.url_w_subdir}' has 'base_url' with credentials. "
                            "This is not supported."
                        )
                    channel_creds = channel_parts.get(key)
                    if channel_creds:
                        base_url_parts[key] = channel_creds
                return Channel(**base_url_parts).url(with_credentials=True)
            return maybe_base_url
        if with_credentials:
            return self.url_w_credentials
        return self.url_w_subdir


def make_feature_record(feature_name):
    # necessary for the SAT solver to do the right thing with features
    pkg_name = f"{feature_name}@"
    return PackageRecord(
        name=pkg_name,
        version="0",
        build="0",
        channel="@",
        subdir=context.subdir,
        md5="12345678901234567890123456789012",
        track_features=(feature_name,),
        build_number=0,
        fn=pkg_name,
    )


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Tools for cross-OS portability."""

from __future__ import annotations

import os
import re
import struct
import subprocess
from logging import getLogger
from os.path import basename, realpath

from ..auxlib.ish import dals
from ..base.constants import PREFIX_PLACEHOLDER
from ..base.context import context
from ..common.compat import on_linux, on_mac, on_win
from ..exceptions import BinaryPrefixReplacementError, CondaIOError
from ..gateways.disk.update import CancelOperation, update_file_in_place_as_binary
from ..models.enums import FileMode

log = getLogger(__name__)


# three capture groups: whole_shebang, executable, options
SHEBANG_REGEX = (
    rb"^(#!"  # pretty much the whole match string
    rb"(?:[ ]*)"  # allow spaces between #! and beginning of the executable path
    rb"(/(?:\\ |[^ \n\r\t])*)"  # the executable is the next text block without an escaped space or non-space whitespace character  # NOQA
    rb"(.*)"  # the rest of the line can contain option flags
    rb")$"
)  # end whole_shebang group

MAX_SHEBANG_LENGTH = 127 if on_linux else 512  # Not used on Windows

# These are the most common file encodings that we run across when having to replace our
# PREFIX_PLACEHOLDER string. They apply to binary and text formats.
# More information/discussion: https://github.com/conda/conda/pull/9946
POPULAR_ENCODINGS = (
    "utf-8",
    "utf-16-le",
    "utf-16-be",
    "utf-32-le",
    "utf-32-be",
)


class _PaddingError(Exception):
    pass


def _subdir_is_win(subdir: str) -> bool:
    if "-" in subdir:
        os, _ = subdir.lower().split("-", 1)
        return os == "win"
    else:
        # For noarch, check that we are running on windows
        return on_win


def update_prefix(
    path,
    new_prefix,
    placeholder=PREFIX_PLACEHOLDER,
    mode=FileMode.text,
    subdir=context.subdir,
):
    if _subdir_is_win(subdir) and mode == FileMode.text:
        # force all prefix replacements to forward slashes to simplify need to escape backslashes
        # replace with unix-style path separators
        new_prefix = new_prefix.replace("\\", "/")

    def _update_prefix(original_data):
        # Step 1. do all prefix replacement
        data = replace_prefix(mode, original_data, placeholder, new_prefix, subdir)

        # Step 2. if the shebang is too long or the new prefix contains spaces, shorten it using
        # /usr/bin/env trick -- NOTE: this trick assumes the environment WILL BE activated
        if not _subdir_is_win(subdir):
            data = replace_long_shebang(mode, data)

        # Step 3. if the before and after content is the same, skip writing
        if data == original_data:
            raise CancelOperation()

        # Step 4. if we have a binary file, make sure the byte size is the same before
        #         and after the update
        if mode == FileMode.binary and len(data) != len(original_data):
            raise BinaryPrefixReplacementError(
                path, placeholder, new_prefix, len(original_data), len(data)
            )

        return data

    updated = update_file_in_place_as_binary(realpath(path), _update_prefix)

    if updated and mode == FileMode.binary and subdir == "osx-arm64" and on_mac:
        # Apple arm64 needs signed executables
        subprocess.run(
            ["/usr/bin/codesign", "-s", "-", "-f", realpath(path)], capture_output=True
        )


def replace_prefix(
    mode: FileMode,
    data: bytes,
    placeholder: str,
    new_prefix: str,
    subdir: str = "noarch",
) -> bytes:
    """
    Replaces `placeholder` text with the `new_prefix` provided. The `mode` provided can
    either be text or binary.

    We use the `POPULAR_ENCODINGS` module level constant defined above to make several
    passes at replacing the placeholder. We do this to account for as many encodings as
    possible. If this causes any performance problems in the future, it could potentially
    be removed (i.e. just using the most popular "utf-8" encoding").

    More information/discussion available here: https://github.com/conda/conda/pull/9946
    """
    for encoding in POPULAR_ENCODINGS:
        if mode == FileMode.text:
            if not _subdir_is_win(subdir):
                # if new_prefix contains spaces, it might break the shebang!
                # handle this by escaping the spaces early, which will trigger a
                # /usr/bin/env replacement later on
                newline_pos = data.find(b"\n")
                if newline_pos > -1:
                    shebang_line, rest_of_data = data[:newline_pos], data[newline_pos:]
                    shebang_placeholder = f"#!{placeholder}".encode(encoding)
                    if shebang_placeholder in shebang_line:
                        escaped_shebang = f"#!{new_prefix}".replace(" ", "\\ ").encode(
                            encoding
                        )
                        shebang_line = shebang_line.replace(
                            shebang_placeholder, escaped_shebang
                        )
                        data = shebang_line + rest_of_data
            # the rest of the file can be replaced normally
            data = data.replace(
                placeholder.encode(encoding), new_prefix.encode(encoding)
            )
        elif mode == FileMode.binary:
            data = binary_replace(
                data,
                placeholder.encode(encoding),
                new_prefix.encode(encoding),
                encoding=encoding,
                subdir=subdir,
            )
        else:
            raise CondaIOError(f"Invalid mode: {mode!r}")
    return data


def binary_replace(
    data: bytes,
    search: bytes,
    replacement: bytes,
    encoding: str = "utf-8",
    subdir: str = "noarch",
) -> bytes:
    """
    Perform a binary replacement of `data`, where the placeholder `search` is
    replaced with `replacement` and the remaining string is padded with null characters.
    All input arguments are expected to be bytes objects.

    Parameters
    ----------
    data:
        The bytes object that will be searched and replaced
    search:
        The bytes object to find
    replacement:
        The bytes object that will replace `search`
    encoding: str
        The encoding of the expected string in the binary.
    """
    zeros = "\0".encode(encoding)
    if _subdir_is_win(subdir):
        # on Windows for binary files, we currently only replace a pyzzer-type entry point
        #   we skip all other prefix replacement
        if has_pyzzer_entry_point(data):
            return replace_pyzzer_entry_point_shebang(data, search, replacement)
        else:
            return data

    def replace(match):
        occurrences = match.group().count(search)
        padding = (len(search) - len(replacement)) * occurrences
        if padding < 0:
            raise _PaddingError
        return match.group().replace(search, replacement) + b"\0" * padding

    original_data_len = len(data)
    pat = re.compile(
        re.escape(search) + b"(?:(?!(?:" + zeros + b")).)*" + zeros, flags=re.DOTALL
    )
    data = pat.sub(replace, data)
    assert len(data) == original_data_len

    return data


def has_pyzzer_entry_point(data):
    pos = data.rfind(b"PK\x05\x06")
    return pos >= 0


def replace_pyzzer_entry_point_shebang(all_data, placeholder, new_prefix):
    """Code adapted from pyzzer.  This is meant to deal with entry point exe's created by distlib,
    which consist of a launcher, then a shebang, then a zip archive of the entry point code to run.
    We need to change the shebang.
    https://bitbucket.org/vinay.sajip/pyzzer/src/5d5740cb04308f067d5844a56fbe91e7a27efccc/pyzzer/__init__.py?at=default&fileviewer=file-view-default#__init__.py-112  # NOQA
    """
    # Copyright (c) 2013 Vinay Sajip.
    #
    # Permission is hereby granted, free of charge, to any person obtaining a copy
    # of this software and associated documentation files (the "Software"), to deal
    # in the Software without restriction, including without limitation the rights
    # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    # copies of the Software, and to permit persons to whom the Software is
    # furnished to do so, subject to the following conditions:
    #
    # The above copyright notice and this permission notice shall be included in
    # all copies or substantial portions of the Software.
    #
    # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
    # THE SOFTWARE.
    launcher = shebang = None
    pos = all_data.rfind(b"PK\x05\x06")
    if pos >= 0:
        end_cdr = all_data[pos + 12 : pos + 20]
        cdr_size, cdr_offset = struct.unpack("<LL", end_cdr)
        arc_pos = pos - cdr_size - cdr_offset
        data = all_data[arc_pos:]
        if arc_pos > 0:
            pos = all_data.rfind(b"#!", 0, arc_pos)
            if pos >= 0:
                shebang = all_data[pos:arc_pos]
                if pos > 0:
                    launcher = all_data[:pos]

        if data and shebang and launcher:
            if hasattr(placeholder, "encode"):
                placeholder = placeholder.encode("utf-8")
            if hasattr(new_prefix, "encode"):
                new_prefix = new_prefix.encode("utf-8")
            shebang = shebang.replace(placeholder, new_prefix)
            all_data = b"".join([launcher, shebang, data])
    return all_data


def replace_long_shebang(mode, data):
    # this function only changes a shebang line if it exists and is greater than 127 characters
    if mode == FileMode.text:
        if not isinstance(data, bytes):
            try:
                data = bytes(data, encoding="utf-8")
            except:
                data = data.encode("utf-8")

        shebang_match = re.match(SHEBANG_REGEX, data, re.MULTILINE)
        if shebang_match:
            whole_shebang, executable, options = shebang_match.groups()
            prefix, executable_name = executable.decode("utf-8").rsplit("/", 1)
            if len(whole_shebang) > MAX_SHEBANG_LENGTH or "\\ " in prefix:
                new_shebang = (
                    f"#!/usr/bin/env {executable_name}{options.decode('utf-8')}"
                )
                data = data.replace(whole_shebang, new_shebang.encode("utf-8"))

    else:
        # TODO: binary shebangs exist; figure this out in the future if text works well
        pass
    return data


def generate_shebang_for_entry_point(executable, with_usr_bin_env=False):
    """
    This function can be used to generate a shebang line for Python entry points.

    Use cases:
    - At install/link time, to generate the `noarch: python` entry points.
    - conda init uses it to create its own entry point during conda-build
    """
    shebang = f"#!{executable}\n"
    if os.environ.get("CONDA_BUILD") == "1" and "/_h_env_placehold" in executable:
        # This is being used during a conda-build process,
        # which uses long prefixes on purpose. This will be replaced
        # with the real environment prefix at install time. Do not
        # do nothing for now.
        return shebang

    # In principle, the naive shebang will work as long as the path
    # to the python executable does not contain spaces AND it's not
    # longer than 127 characters. Otherwise, we must fix it
    if len(shebang) > MAX_SHEBANG_LENGTH or " " in executable:
        if with_usr_bin_env:
            # This approach works well for all cases BUT it requires
            # the executable to be in PATH. In other words, the environment
            # needs to be activated!
            shebang = f"#!/usr/bin/env {basename(executable)}\n"
        else:
            # This approach follows a method inspired by `pypa/distlib`
            # https://github.com/pypa/distlib/blob/91aa92e64/distlib/scripts.py#L129
            # Explanation: these lines are both valid Python and shell :)
            # 1. Python will read it as a triple-quoted string; end of story
            # 2. The shell will see:
            #    * '' (empty string)
            #    * 'exec' "path/with spaces/to/python" "this file" "arguments"
            #    * # ''' (inline comment with three quotes, ignored by shell)
            # This method works well BUT in some shells, $PS1 is dropped, which
            # makes the prompt disappear. This is very problematic for the conda
            # entry point! Details: https://github.com/conda/conda/issues/11885
            shebang = dals(
                f"""
                #!/bin/sh
                '''exec' "{executable}" "$0" "$@" #'''
                """
            )

    return shebang


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Atomic actions that make up a package installation or removal transaction."""

import re
import sys
from abc import ABCMeta, abstractmethod, abstractproperty
from itertools import chain
from json import JSONDecodeError
from logging import getLogger
from os.path import basename, dirname, getsize, isdir, join
from uuid import uuid4

from .. import CondaError
from ..auxlib.ish import dals
from ..base.constants import CONDA_TEMP_EXTENSION
from ..base.context import context
from ..common.compat import on_win
from ..common.constants import TRACE
from ..common.path import (
    get_bin_directory_short_path,
    get_leaf_directories,
    get_python_noarch_target_path,
    get_python_short_path,
    parse_entry_point_def,
    pyc_path,
    url_to_path,
    win_path_ok,
)
from ..common.url import has_platform, path_to_url
from ..exceptions import (
    CondaUpgradeError,
    CondaVerificationError,
    NotWritableError,
    PaddingError,
    SafetyError,
)
from ..gateways.connection.download import download
from ..gateways.disk.create import (
    compile_multiple_pyc,
    copy,
    create_hard_link_or_copy,
    create_link,
    create_python_entry_point,
    extract_tarball,
    make_menu,
    mkdir_p,
    write_as_json_to_file,
)
from ..gateways.disk.delete import rm_rf
from ..gateways.disk.permissions import make_writable
from ..gateways.disk.read import compute_sum, islink, lexists, read_index_json
from ..gateways.disk.update import backoff_rename, touch
from ..history import History
from ..models.channel import Channel
from ..models.enums import LinkType, NoarchType, PathType
from ..models.match_spec import MatchSpec
from ..models.records import (
    Link,
    PackageCacheRecord,
    PackageRecord,
    PathDataV1,
    PathsData,
    PrefixRecord,
)
from .envs_manager import get_user_environments_txt_file, register_env, unregister_env
from .portability import _PaddingError, update_prefix
from .prefix_data import PrefixData

try:
    FileNotFoundError
except NameError:
    FileNotFoundError = IOError

log = getLogger(__name__)

_MENU_RE = re.compile(r"^menu/.*\.json$", re.IGNORECASE)
REPR_IGNORE_KWARGS = (
    "transaction_context",
    "package_info",
    "hold_path",
)


class _Action(metaclass=ABCMeta):
    _verified = False

    @abstractmethod
    def verify(self):
        # if verify fails, it should return an exception object rather than raise
        #  at the end of a verification run, all errors will be raised as a CondaMultiError
        # after successful verification, the verify method should set self._verified = True
        raise NotImplementedError()

    @abstractmethod
    def execute(self):
        raise NotImplementedError()

    @abstractmethod
    def reverse(self):
        raise NotImplementedError()

    @abstractmethod
    def cleanup(self):
        raise NotImplementedError()

    @property
    def verified(self):
        return self._verified

    def __repr__(self):
        args = (
            f"{key}={value!r}"
            for key, value in vars(self).items()
            if key not in REPR_IGNORE_KWARGS
        )
        return "{}({})".format(self.__class__.__name__, ", ".join(args))


class PathAction(_Action, metaclass=ABCMeta):
    @abstractproperty
    def target_full_path(self):
        raise NotImplementedError()


class MultiPathAction(_Action, metaclass=ABCMeta):
    @abstractproperty
    def target_full_paths(self):
        raise NotImplementedError()


class PrefixPathAction(PathAction, metaclass=ABCMeta):
    def __init__(self, transaction_context, target_prefix, target_short_path):
        self.transaction_context = transaction_context
        self.target_prefix = target_prefix
        self.target_short_path = target_short_path

    @property
    def target_short_paths(self):
        return (self.target_short_path,)

    @property
    def target_full_path(self):
        trgt, shrt_pth = self.target_prefix, self.target_short_path
        if trgt is not None and shrt_pth is not None:
            return join(trgt, win_path_ok(shrt_pth))
        else:
            return None


# ######################################################
#  Creation of Paths within a Prefix
# ######################################################


class CreateInPrefixPathAction(PrefixPathAction, metaclass=ABCMeta):
    # All CreatePathAction subclasses must create a SINGLE new path
    #   the short/in-prefix version of that path must be returned by execute()

    def __init__(
        self,
        transaction_context,
        package_info,
        source_prefix,
        source_short_path,
        target_prefix,
        target_short_path,
    ):
        super().__init__(transaction_context, target_prefix, target_short_path)
        self.package_info = package_info
        self.source_prefix = source_prefix
        self.source_short_path = source_short_path

    def verify(self):
        self._verified = True

    def cleanup(self):
        # create actions typically won't need cleanup
        pass

    @property
    def source_full_path(self):
        prfx, shrt_pth = self.source_prefix, self.source_short_path
        return join(prfx, win_path_ok(shrt_pth)) if prfx and shrt_pth else None


class LinkPathAction(CreateInPrefixPathAction):
    @classmethod
    def create_file_link_actions(
        cls, transaction_context, package_info, target_prefix, requested_link_type
    ):
        def get_prefix_replace(source_path_data):
            if source_path_data.path_type == PathType.softlink:
                link_type = LinkType.copy
                prefix_placehoder, file_mode = "", None
            elif source_path_data.prefix_placeholder:
                link_type = LinkType.copy
                prefix_placehoder = source_path_data.prefix_placeholder
                file_mode = source_path_data.file_mode
            elif source_path_data.no_link:
                link_type = LinkType.copy
                prefix_placehoder, file_mode = "", None
            else:
                link_type = requested_link_type
                prefix_placehoder, file_mode = "", None

            return link_type, prefix_placehoder, file_mode

        def make_file_link_action(source_path_data):
            # TODO: this inner function is still kind of a mess
            noarch = package_info.repodata_record.noarch
            if noarch is None and package_info.package_metadata is not None:
                # Look in package metadata in case it was omitted from repodata (see issue #8311)
                noarch = package_info.package_metadata.noarch
                if noarch is not None:
                    noarch = noarch.type
            if noarch == NoarchType.python:
                sp_dir = transaction_context["target_site_packages_short_path"]
                if sp_dir is None:
                    raise CondaError(
                        "Unable to determine python site-packages "
                        "dir in target_prefix!\nPlease make sure "
                        f"python is installed in {target_prefix}"
                    )
                target_short_path = get_python_noarch_target_path(
                    source_path_data.path, sp_dir
                )
            elif noarch is None or noarch == NoarchType.generic:
                target_short_path = source_path_data.path
            else:
                raise CondaUpgradeError(
                    dals(
                        """
                The current version of conda is too old to install this package.
                Please update conda."""
                    )
                )

            link_type, placeholder, fmode = get_prefix_replace(source_path_data)

            if placeholder:
                return PrefixReplaceLinkAction(
                    transaction_context,
                    package_info,
                    package_info.extracted_package_dir,
                    source_path_data.path,
                    target_prefix,
                    target_short_path,
                    requested_link_type,
                    placeholder,
                    fmode,
                    source_path_data,
                )
            else:
                return LinkPathAction(
                    transaction_context,
                    package_info,
                    package_info.extracted_package_dir,
                    source_path_data.path,
                    target_prefix,
                    target_short_path,
                    link_type,
                    source_path_data,
                )

        return tuple(
            make_file_link_action(spi) for spi in package_info.paths_data.paths
        )

    @classmethod
    def create_directory_actions(
        cls,
        transaction_context,
        package_info,
        target_prefix,
        requested_link_type,
        file_link_actions,
    ):
        leaf_directories = get_leaf_directories(
            axn.target_short_path for axn in file_link_actions
        )
        return tuple(
            cls(
                transaction_context,
                package_info,
                None,
                None,
                target_prefix,
                directory_short_path,
                LinkType.directory,
                None,
            )
            for directory_short_path in leaf_directories
        )

    @classmethod
    def create_python_entry_point_windows_exe_action(
        cls,
        transaction_context,
        package_info,
        target_prefix,
        requested_link_type,
        entry_point_def,
    ):
        source_directory = context.conda_prefix
        source_short_path = "Scripts/conda.exe"
        command, _, _ = parse_entry_point_def(entry_point_def)
        target_short_path = f"Scripts/{command}.exe"
        source_path_data = PathDataV1(
            _path=target_short_path,
            path_type=PathType.windows_python_entry_point_exe,
        )
        return cls(
            transaction_context,
            package_info,
            source_directory,
            source_short_path,
            target_prefix,
            target_short_path,
            requested_link_type,
            source_path_data,
        )

    def __init__(
        self,
        transaction_context,
        package_info,
        extracted_package_dir,
        source_short_path,
        target_prefix,
        target_short_path,
        link_type,
        source_path_data,
    ):
        super().__init__(
            transaction_context,
            package_info,
            extracted_package_dir,
            source_short_path,
            target_prefix,
            target_short_path,
        )
        self.link_type = link_type
        self._execute_successful = False
        self.source_path_data = source_path_data
        self.prefix_path_data = None

    def verify(self):
        if self.link_type != LinkType.directory and not lexists(
            self.source_full_path
        ):  # pragma: no cover  # NOQA
            return CondaVerificationError(
                dals(
                    f"""
            The package for {self.package_info.repodata_record.name} located at {self.package_info.extracted_package_dir}
            appears to be corrupted. The path '{self.source_short_path}'
            specified in the package manifest cannot be found.
            """
                )
            )

        source_path_data = self.source_path_data
        try:
            source_path_type = source_path_data.path_type
        except AttributeError:
            source_path_type = None
        if source_path_type in PathType.basic_types:
            # this let's us keep the non-generic path types like windows_python_entry_point_exe
            source_path_type = None

        if self.link_type == LinkType.directory:
            self.prefix_path_data = None
        elif self.link_type == LinkType.softlink:
            self.prefix_path_data = PathDataV1.from_objects(
                self.source_path_data,
                path_type=source_path_type or PathType.softlink,
            )
        elif (
            self.link_type == LinkType.copy
            and source_path_data.path_type == PathType.softlink
        ):
            self.prefix_path_data = PathDataV1.from_objects(
                self.source_path_data,
                path_type=source_path_type or PathType.softlink,
            )

        elif source_path_data.path_type == PathType.hardlink:
            try:
                reported_size_in_bytes = source_path_data.size_in_bytes
            except AttributeError:
                reported_size_in_bytes = None
            source_size_in_bytes = 0
            if reported_size_in_bytes:
                source_size_in_bytes = getsize(self.source_full_path)
                if reported_size_in_bytes != source_size_in_bytes:
                    return SafetyError(
                        dals(
                            f"""
                    The package for {self.package_info.repodata_record.name} located at {self.package_info.extracted_package_dir}
                    appears to be corrupted. The path '{self.source_short_path}'
                    has an incorrect size.
                      reported size: {reported_size_in_bytes} bytes
                      actual size: {source_size_in_bytes} bytes
                    """
                        )
                    )

            try:
                reported_sha256 = source_path_data.sha256
            except AttributeError:
                reported_sha256 = None
            # sha256 is expensive.  Only run if file sizes agree, and then only if enabled
            if (
                source_size_in_bytes
                and reported_size_in_bytes == source_size_in_bytes
                and context.extra_safety_checks
            ):
                source_sha256 = compute_sum(self.source_full_path, "sha256")

                if reported_sha256 and reported_sha256 != source_sha256:
                    return SafetyError(
                        dals(
                            f"""
                    The package for {self.package_info.repodata_record.name} located at {self.package_info.extracted_package_dir}
                    appears to be corrupted. The path '{self.source_short_path}'
                    has a sha256 mismatch.
                    reported sha256: {reported_sha256}
                    actual sha256: {source_sha256}
                    """
                        )
                    )
            self.prefix_path_data = PathDataV1.from_objects(
                source_path_data,
                sha256=reported_sha256,
                sha256_in_prefix=reported_sha256,
                path_type=source_path_type or PathType.hardlink,
            )
        elif source_path_data.path_type == PathType.windows_python_entry_point_exe:
            self.prefix_path_data = source_path_data
        else:
            raise NotImplementedError()

        self._verified = True

    def execute(self):
        log.log(TRACE, "linking %s => %s", self.source_full_path, self.target_full_path)
        create_link(
            self.source_full_path,
            self.target_full_path,
            self.link_type,
            force=context.force,
        )
        self._execute_successful = True

    def reverse(self):
        if self._execute_successful:
            log.log(TRACE, "reversing link creation %s", self.target_prefix)
            if not isdir(self.target_full_path):
                rm_rf(self.target_full_path, clean_empty_parents=True)


class PrefixReplaceLinkAction(LinkPathAction):
    def __init__(
        self,
        transaction_context,
        package_info,
        extracted_package_dir,
        source_short_path,
        target_prefix,
        target_short_path,
        link_type,
        prefix_placeholder,
        file_mode,
        source_path_data,
    ):
        # This link_type used in execute(). Make sure we always respect LinkType.copy request.
        link_type = LinkType.copy if link_type == LinkType.copy else LinkType.hardlink
        super().__init__(
            transaction_context,
            package_info,
            extracted_package_dir,
            source_short_path,
            target_prefix,
            target_short_path,
            link_type,
            source_path_data,
        )
        self.prefix_placeholder = prefix_placeholder
        self.file_mode = file_mode
        self.intermediate_path = None

    def verify(self):
        validation_error = super().verify()
        if validation_error:
            return validation_error

        if islink(self.source_full_path):
            log.log(
                TRACE,
                "ignoring prefix update for symlink with source path %s",
                self.source_full_path,
            )
            # return
            assert False, "I don't think this is the right place to ignore this"

        mkdir_p(self.transaction_context["temp_dir"])
        self.intermediate_path = join(
            self.transaction_context["temp_dir"], str(uuid4())
        )

        log.log(
            TRACE, "copying %s => %s", self.source_full_path, self.intermediate_path
        )
        create_link(self.source_full_path, self.intermediate_path, LinkType.copy)
        make_writable(self.intermediate_path)

        try:
            log.log(TRACE, "rewriting prefixes in %s", self.target_full_path)
            update_prefix(
                self.intermediate_path,
                context.target_prefix_override or self.target_prefix,
                self.prefix_placeholder,
                self.file_mode,
                subdir=self.package_info.repodata_record.subdir,
            )
        except _PaddingError:
            raise PaddingError(
                self.target_full_path,
                self.prefix_placeholder,
                len(self.prefix_placeholder),
            )

        sha256_in_prefix = compute_sum(self.intermediate_path, "sha256")

        self.prefix_path_data = PathDataV1.from_objects(
            self.prefix_path_data,
            file_mode=self.file_mode,
            path_type=PathType.hardlink,
            prefix_placeholder=self.prefix_placeholder,
            sha256_in_prefix=sha256_in_prefix,
        )

        self._verified = True

    def execute(self):
        if not self._verified:
            self.verify()
        source_path = self.intermediate_path or self.source_full_path
        log.log(TRACE, "linking %s => %s", source_path, self.target_full_path)
        create_link(source_path, self.target_full_path, self.link_type)
        self._execute_successful = True


class MakeMenuAction(CreateInPrefixPathAction):
    @classmethod
    def create_actions(
        cls, transaction_context, package_info, target_prefix, requested_link_type
    ):
        shorcuts_lower = [name.lower() for name in (context.shortcuts_only or ())]
        if context.shortcuts and (
            not context.shortcuts_only
            or (shorcuts_lower and package_info.name.lower() in shorcuts_lower)
        ):
            return tuple(
                cls(transaction_context, package_info, target_prefix, spi.path)
                for spi in package_info.paths_data.paths
                if bool(_MENU_RE.match(spi.path))
            )
        else:
            return ()

    def __init__(
        self, transaction_context, package_info, target_prefix, target_short_path
    ):
        super().__init__(
            transaction_context,
            package_info,
            None,
            None,
            target_prefix,
            target_short_path,
        )
        self._execute_successful = False

    def execute(self):
        log.log(TRACE, "making menu for %s", self.target_full_path)
        make_menu(self.target_prefix, self.target_short_path, remove=False)
        self._execute_successful = True

    def reverse(self):
        if self._execute_successful:
            log.log(TRACE, "removing menu for %s", self.target_full_path)
            make_menu(self.target_prefix, self.target_short_path, remove=True)


class CreateNonadminAction(CreateInPrefixPathAction):
    @classmethod
    def create_actions(
        cls, transaction_context, package_info, target_prefix, requested_link_type
    ):
        if on_win and lexists(join(context.root_prefix, ".nonadmin")):
            return (cls(transaction_context, package_info, target_prefix),)
        else:
            return ()

    def __init__(self, transaction_context, package_info, target_prefix):
        super().__init__(
            transaction_context, package_info, None, None, target_prefix, ".nonadmin"
        )
        self._file_created = False

    def execute(self):
        log.log(TRACE, "touching nonadmin %s", self.target_full_path)
        self._file_created = touch(self.target_full_path)

    def reverse(self):
        if self._file_created:
            log.log(TRACE, "removing nonadmin file %s", self.target_full_path)
            rm_rf(self.target_full_path)


class CompileMultiPycAction(MultiPathAction):
    @classmethod
    def create_actions(
        cls,
        transaction_context,
        package_info,
        target_prefix,
        requested_link_type,
        file_link_actions,
    ):
        noarch = package_info.package_metadata and package_info.package_metadata.noarch
        if noarch is not None and noarch.type == NoarchType.python:
            noarch_py_file_re = re.compile(r"^site-packages[/\\][^\t\n\r\f\v]+\.py$")
            py_ver = transaction_context["target_python_version"]
            py_files = tuple(
                axn.target_short_path
                for axn in file_link_actions
                if getattr(axn, "source_short_path")
                and noarch_py_file_re.match(axn.source_short_path)
            )
            pyc_files = tuple(pyc_path(pf, py_ver) for pf in py_files)
            return (
                cls(
                    transaction_context,
                    package_info,
                    target_prefix,
                    py_files,
                    pyc_files,
                ),
            )
        else:
            return ()

    def __init__(
        self,
        transaction_context,
        package_info,
        target_prefix,
        source_short_paths,
        target_short_paths,
    ):
        self.transaction_context = transaction_context
        self.package_info = package_info
        self.target_prefix = target_prefix
        self.source_short_paths = source_short_paths
        self.target_short_paths = target_short_paths
        self.prefix_path_data = None
        self.prefix_paths_data = [
            PathDataV1(
                _path=p,
                path_type=PathType.pyc_file,
            )
            for p in self.target_short_paths
        ]
        self._execute_successful = False

    @property
    def target_full_paths(self):
        def join_or_none(prefix, short_path):
            if prefix is None or short_path is None:
                return None
            else:
                return join(prefix, win_path_ok(short_path))

        return (join_or_none(self.target_prefix, p) for p in self.target_short_paths)

    @property
    def source_full_paths(self):
        def join_or_none(prefix, short_path):
            if prefix is None or short_path is None:
                return None
            else:
                return join(prefix, win_path_ok(short_path))

        return (join_or_none(self.target_prefix, p) for p in self.source_short_paths)

    def verify(self):
        self._verified = True

    def cleanup(self):
        # create actions typically won't need cleanup
        pass

    def execute(self):
        # compile_pyc is sometimes expected to fail, for example a python 3.6 file
        #   installed into a python 2 environment, but no code paths actually importing it
        # technically then, this file should be removed from the manifest in conda-meta, but
        #   at the time of this writing that's not currently happening
        log.log(TRACE, "compiling %s", " ".join(self.target_full_paths))
        target_python_version = self.transaction_context["target_python_version"]
        python_short_path = get_python_short_path(target_python_version)
        python_full_path = join(self.target_prefix, win_path_ok(python_short_path))
        compile_multiple_pyc(
            python_full_path,
            self.source_full_paths,
            self.target_full_paths,
            self.target_prefix,
            self.transaction_context["target_python_version"],
        )
        self._execute_successful = True

    def reverse(self):
        # this removes all pyc files even if they were not created
        if self._execute_successful:
            log.log(
                TRACE, "reversing pyc creation %s", " ".join(self.target_full_paths)
            )
            for target_full_path in self.target_full_paths:
                rm_rf(target_full_path)


class AggregateCompileMultiPycAction(CompileMultiPycAction):
    """Bunch up all of our compile actions, so that they all get carried out at once.
    This avoids clobbering and is faster when we have several individual packages requiring
    compilation.
    """

    def __init__(self, *individuals, **kw):
        transaction_context = individuals[0].transaction_context
        # not used; doesn't matter
        package_info = individuals[0].package_info
        target_prefix = individuals[0].target_prefix
        source_short_paths = set()
        target_short_paths = set()
        for individual in individuals:
            source_short_paths.update(individual.source_short_paths)
            target_short_paths.update(individual.target_short_paths)
        super().__init__(
            transaction_context,
            package_info,
            target_prefix,
            source_short_paths,
            target_short_paths,
        )


class CreatePythonEntryPointAction(CreateInPrefixPathAction):
    @classmethod
    def create_actions(
        cls, transaction_context, package_info, target_prefix, requested_link_type
    ):
        noarch = package_info.package_metadata and package_info.package_metadata.noarch
        if noarch is not None and noarch.type == NoarchType.python:

            def this_triplet(entry_point_def):
                command, module, func = parse_entry_point_def(entry_point_def)
                target_short_path = f"{get_bin_directory_short_path()}/{command}"
                if on_win:
                    target_short_path += "-script.py"
                return target_short_path, module, func

            actions = tuple(
                cls(
                    transaction_context,
                    package_info,
                    target_prefix,
                    *this_triplet(ep_def),
                )
                for ep_def in noarch.entry_points or ()
            )

            if on_win:  # pragma: unix no cover
                actions += tuple(
                    LinkPathAction.create_python_entry_point_windows_exe_action(
                        transaction_context,
                        package_info,
                        target_prefix,
                        requested_link_type,
                        ep_def,
                    )
                    for ep_def in noarch.entry_points or ()
                )

            return actions
        else:
            return ()

    def __init__(
        self,
        transaction_context,
        package_info,
        target_prefix,
        target_short_path,
        module,
        func,
    ):
        super().__init__(
            transaction_context,
            package_info,
            None,
            None,
            target_prefix,
            target_short_path,
        )
        self.module = module
        self.func = func

        if on_win:
            path_type = PathType.windows_python_entry_point_script
        else:
            path_type = PathType.unix_python_entry_point
        self.prefix_path_data = PathDataV1(
            _path=self.target_short_path,
            path_type=path_type,
        )

        self._execute_successful = False

    def execute(self):
        log.log(TRACE, "creating python entry point %s", self.target_full_path)
        if on_win:
            python_full_path = None
        else:
            target_python_version = self.transaction_context["target_python_version"]
            python_short_path = get_python_short_path(target_python_version)
            python_full_path = join(
                context.target_prefix_override or self.target_prefix,
                win_path_ok(python_short_path),
            )

        create_python_entry_point(
            self.target_full_path, python_full_path, self.module, self.func
        )
        self._execute_successful = True

    def reverse(self):
        if self._execute_successful:
            log.log(
                TRACE, "reversing python entry point creation %s", self.target_full_path
            )
            rm_rf(self.target_full_path)


class CreatePrefixRecordAction(CreateInPrefixPathAction):
    # this is the action that creates a packages json file in the conda-meta/ directory

    @classmethod
    def create_actions(
        cls,
        transaction_context,
        package_info,
        target_prefix,
        requested_link_type,
        requested_spec,
        all_link_path_actions,
    ):
        extracted_package_dir = package_info.extracted_package_dir
        target_short_path = f"conda-meta/{basename(extracted_package_dir)}.json"
        return (
            cls(
                transaction_context,
                package_info,
                target_prefix,
                target_short_path,
                requested_link_type,
                requested_spec,
                all_link_path_actions,
            ),
        )

    def __init__(
        self,
        transaction_context,
        package_info,
        target_prefix,
        target_short_path,
        requested_link_type,
        requested_spec,
        all_link_path_actions,
    ):
        super().__init__(
            transaction_context,
            package_info,
            None,
            None,
            target_prefix,
            target_short_path,
        )
        self.requested_link_type = requested_link_type
        self.requested_spec = requested_spec
        self.all_link_path_actions = list(all_link_path_actions)
        self._execute_successful = False

    def execute(self):
        link = Link(
            source=self.package_info.extracted_package_dir,
            type=self.requested_link_type,
        )
        extracted_package_dir = self.package_info.extracted_package_dir
        package_tarball_full_path = self.package_info.package_tarball_full_path

        def files_from_action(link_path_action):
            if isinstance(link_path_action, CompileMultiPycAction):
                return link_path_action.target_short_paths
            else:
                return (
                    (link_path_action.target_short_path,)
                    if isinstance(link_path_action, CreateInPrefixPathAction)
                    and (
                        not hasattr(link_path_action, "link_type")
                        or link_path_action.link_type != LinkType.directory
                    )
                    else ()
                )

        def paths_from_action(link_path_action):
            if isinstance(link_path_action, CompileMultiPycAction):
                return link_path_action.prefix_paths_data
            else:
                if (
                    not hasattr(link_path_action, "prefix_path_data")
                    or link_path_action.prefix_path_data is None
                ):
                    return ()
                else:
                    return (link_path_action.prefix_path_data,)

        files = list(
            chain.from_iterable(
                files_from_action(x) for x in self.all_link_path_actions if x
            )
        )
        paths_data = PathsData(
            paths_version=1,
            paths=chain.from_iterable(
                paths_from_action(x) for x in self.all_link_path_actions if x
            ),
        )

        self.prefix_record = PrefixRecord.from_objects(
            self.package_info.repodata_record,
            # self.package_info.index_json_record,
            self.package_info.package_metadata,
            requested_spec=str(self.requested_spec),
            paths_data=paths_data,
            files=files,
            link=link,
            url=self.package_info.url,
            extracted_package_dir=extracted_package_dir,
            package_tarball_full_path=package_tarball_full_path,
        )

        log.log(TRACE, "creating linked package record %s", self.target_full_path)
        PrefixData(self.target_prefix).insert(self.prefix_record)
        self._execute_successful = True

    def reverse(self):
        log.log(
            TRACE, "reversing linked package record creation %s", self.target_full_path
        )
        if self._execute_successful:
            PrefixData(self.target_prefix).remove(
                self.package_info.repodata_record.name
            )


class UpdateHistoryAction(CreateInPrefixPathAction):
    @classmethod
    def create_actions(
        cls,
        transaction_context,
        target_prefix,
        remove_specs,
        update_specs,
        neutered_specs,
    ):
        target_short_path = join("conda-meta", "history")
        return (
            cls(
                transaction_context,
                target_prefix,
                target_short_path,
                remove_specs,
                update_specs,
                neutered_specs,
            ),
        )

    def __init__(
        self,
        transaction_context,
        target_prefix,
        target_short_path,
        remove_specs,
        update_specs,
        neutered_specs,
    ):
        super().__init__(
            transaction_context, None, None, None, target_prefix, target_short_path
        )
        self.remove_specs = remove_specs
        self.update_specs = update_specs
        self.neutered_specs = neutered_specs

        self.hold_path = self.target_full_path + CONDA_TEMP_EXTENSION

    def execute(self):
        log.log(TRACE, "updating environment history %s", self.target_full_path)

        if lexists(self.target_full_path):
            copy(self.target_full_path, self.hold_path)

        h = History(self.target_prefix)
        h.update()
        h.write_specs(self.remove_specs, self.update_specs, self.neutered_specs)

    def reverse(self):
        if lexists(self.hold_path):
            log.log(TRACE, "moving %s => %s", self.hold_path, self.target_full_path)
            backoff_rename(self.hold_path, self.target_full_path, force=True)

    def cleanup(self):
        rm_rf(self.hold_path)


class RegisterEnvironmentLocationAction(PathAction):
    def __init__(self, transaction_context, target_prefix):
        self.transaction_context = transaction_context
        self.target_prefix = target_prefix

        self._execute_successful = False

    def verify(self):
        user_environments_txt_file = get_user_environments_txt_file()
        try:
            touch(user_environments_txt_file, mkdir=True, sudo_safe=True)
            self._verified = True
        except NotWritableError:
            log.warning(
                "Unable to create environments file. Path not writable.\n"
                "  environment location: %s\n",
                user_environments_txt_file,
            )

    def execute(self):
        log.log(TRACE, "registering environment in catalog %s", self.target_prefix)

        register_env(self.target_prefix)
        self._execute_successful = True

    def reverse(self):
        pass

    def cleanup(self):
        pass

    @property
    def target_full_path(self):
        raise NotImplementedError()


# ######################################################
#  Removal of Paths within a Prefix
# ######################################################


class RemoveFromPrefixPathAction(PrefixPathAction, metaclass=ABCMeta):
    def __init__(
        self, transaction_context, linked_package_data, target_prefix, target_short_path
    ):
        super().__init__(transaction_context, target_prefix, target_short_path)
        self.linked_package_data = linked_package_data

    def verify(self):
        # inability to remove will trigger a rollback
        # can't definitely know if path can be removed until it's attempted and failed
        self._verified = True


class UnlinkPathAction(RemoveFromPrefixPathAction):
    def __init__(
        self,
        transaction_context,
        linked_package_data,
        target_prefix,
        target_short_path,
        link_type=LinkType.hardlink,
    ):
        super().__init__(
            transaction_context, linked_package_data, target_prefix, target_short_path
        )
        self.holding_short_path = self.target_short_path + CONDA_TEMP_EXTENSION
        self.holding_full_path = self.target_full_path + CONDA_TEMP_EXTENSION
        self.link_type = link_type

    def execute(self):
        if self.link_type != LinkType.directory:
            log.log(
                TRACE,
                "renaming %s => %s",
                self.target_short_path,
                self.holding_short_path,
            )
            backoff_rename(self.target_full_path, self.holding_full_path, force=True)

    def reverse(self):
        if self.link_type != LinkType.directory and lexists(self.holding_full_path):
            log.log(
                TRACE,
                "reversing rename %s => %s",
                self.holding_short_path,
                self.target_short_path,
            )
            backoff_rename(self.holding_full_path, self.target_full_path, force=True)

    def cleanup(self):
        if not isdir(self.holding_full_path):
            rm_rf(self.holding_full_path, clean_empty_parents=True)


class RemoveMenuAction(RemoveFromPrefixPathAction):
    @classmethod
    def create_actions(cls, transaction_context, linked_package_data, target_prefix):
        return tuple(
            cls(transaction_context, linked_package_data, target_prefix, trgt)
            for trgt in linked_package_data.files
            if bool(_MENU_RE.match(trgt))
        )

    def __init__(
        self, transaction_context, linked_package_data, target_prefix, target_short_path
    ):
        super().__init__(
            transaction_context, linked_package_data, target_prefix, target_short_path
        )

    def execute(self):
        log.log(TRACE, "removing menu for %s ", self.target_prefix)
        make_menu(self.target_prefix, self.target_short_path, remove=True)

    def reverse(self):
        log.log(TRACE, "re-creating menu for %s ", self.target_prefix)
        make_menu(self.target_prefix, self.target_short_path, remove=False)

    def cleanup(self):
        pass


class RemoveLinkedPackageRecordAction(UnlinkPathAction):
    def __init__(
        self, transaction_context, linked_package_data, target_prefix, target_short_path
    ):
        super().__init__(
            transaction_context, linked_package_data, target_prefix, target_short_path
        )

    def execute(self):
        super().execute()
        PrefixData(self.target_prefix).remove(self.linked_package_data.name)

    def reverse(self):
        super().reverse()
        PrefixData(self.target_prefix)._load_single_record(self.target_full_path)


class UnregisterEnvironmentLocationAction(PathAction):
    def __init__(self, transaction_context, target_prefix):
        self.transaction_context = transaction_context
        self.target_prefix = target_prefix

        self._execute_successful = False

    def verify(self):
        self._verified = True

    def execute(self):
        log.log(TRACE, "unregistering environment in catalog %s", self.target_prefix)

        unregister_env(self.target_prefix)
        self._execute_successful = True

    def reverse(self):
        pass

    def cleanup(self):
        pass

    @property
    def target_full_path(self):
        raise NotImplementedError()


# ######################################################
#  Fetch / Extract Actions
# ######################################################


class CacheUrlAction(PathAction):
    def __init__(
        self,
        url,
        target_pkgs_dir,
        target_package_basename,
        sha256=None,
        size=None,
        md5=None,
    ):
        self.url = url
        self.target_pkgs_dir = target_pkgs_dir
        self.target_package_basename = target_package_basename
        self.sha256 = sha256
        self.size = size
        self.md5 = md5
        self.hold_path = self.target_full_path + CONDA_TEMP_EXTENSION

    def verify(self):
        assert "::" not in self.url
        self._verified = True

    def execute(self, progress_update_callback=None):
        # I hate inline imports, but I guess it's ok since we're importing from the conda.core
        # The alternative is passing the PackageCache class to CacheUrlAction __init__
        from .package_cache_data import PackageCacheData

        target_package_cache = PackageCacheData(self.target_pkgs_dir)

        log.log(TRACE, "caching url %s => %s", self.url, self.target_full_path)

        if lexists(self.hold_path):
            rm_rf(self.hold_path)

        if lexists(self.target_full_path):
            if self.url.startswith("file:/") and self.url == path_to_url(
                self.target_full_path
            ):
                # the source and destination are the same file, so we're done
                return
            else:
                backoff_rename(self.target_full_path, self.hold_path, force=True)

        if self.url.startswith("file:/"):
            source_path = url_to_path(self.url)
            self._execute_local(
                source_path, target_package_cache, progress_update_callback
            )
        else:
            self._execute_channel(target_package_cache, progress_update_callback)

    def _execute_local(
        self, source_path, target_package_cache, progress_update_callback=None
    ):
        from .package_cache_data import PackageCacheData

        if dirname(source_path) in context.pkgs_dirs:
            # if url points to another package cache, link to the writable cache
            create_hard_link_or_copy(source_path, self.target_full_path)
            source_package_cache = PackageCacheData(dirname(source_path))

            # the package is already in a cache, so it came from a remote url somewhere;
            #   make sure that remote url is the most recent url in the
            #   writable cache urls.txt
            origin_url = source_package_cache._urls_data.get_url(
                self.target_package_basename
            )
            if origin_url and has_platform(origin_url, context.known_subdirs):
                target_package_cache._urls_data.add_url(origin_url)
        else:
            # so our tarball source isn't a package cache, but that doesn't mean it's not
            #   in another package cache somewhere
            # let's try to find the actual, remote source url by matching md5sums, and then
            #   record that url as the remote source url in urls.txt
            # we do the search part of this operation before the create_link so that we
            #   don't md5sum-match the file created by 'create_link'
            # there is no point in looking for the tarball in the cache that we are writing
            #   this file into because we have already removed the previous file if there was
            #   any. This also makes sure that we ignore the md5sum of a possible extracted
            #   directory that might exist in this cache because we are going to overwrite it
            #   anyway when we extract the tarball.
            source_md5sum = compute_sum(source_path, "md5")
            exclude_caches = (self.target_pkgs_dir,)
            pc_entry = PackageCacheData.tarball_file_in_cache(
                source_path, source_md5sum, exclude_caches=exclude_caches
            )

            if pc_entry:
                origin_url = target_package_cache._urls_data.get_url(
                    pc_entry.extracted_package_dir
                )
            else:
                origin_url = None

            # copy the tarball to the writable cache
            create_link(
                source_path,
                self.target_full_path,
                link_type=LinkType.copy,
                force=context.force,
            )

            if origin_url and has_platform(origin_url, context.known_subdirs):
                target_package_cache._urls_data.add_url(origin_url)
            else:
                target_package_cache._urls_data.add_url(self.url)

    def _execute_channel(self, target_package_cache, progress_update_callback=None):
        kwargs = {}
        if self.size is not None:
            kwargs["size"] = self.size
        if self.sha256:
            kwargs["sha256"] = self.sha256
        elif self.md5:
            kwargs["md5"] = self.md5
        download(
            self.url,
            self.target_full_path,
            progress_update_callback=progress_update_callback,
            **kwargs,
        )
        target_package_cache._urls_data.add_url(self.url)

    def reverse(self):
        if lexists(self.hold_path):
            log.log(TRACE, "moving %s => %s", self.hold_path, self.target_full_path)
            backoff_rename(self.hold_path, self.target_full_path, force=True)

    def cleanup(self):
        rm_rf(self.hold_path)

    @property
    def target_full_path(self):
        return join(self.target_pkgs_dir, self.target_package_basename)

    def __str__(self):
        return f"CacheUrlAction<url={self.url!r}, target_full_path={self.target_full_path!r}>"


class ExtractPackageAction(PathAction):
    def __init__(
        self,
        source_full_path,
        target_pkgs_dir,
        target_extracted_dirname,
        record_or_spec,
        sha256,
        size,
        md5,
    ):
        self.source_full_path = source_full_path
        self.target_pkgs_dir = target_pkgs_dir
        self.target_extracted_dirname = target_extracted_dirname
        self.hold_path = self.target_full_path + CONDA_TEMP_EXTENSION
        self.record_or_spec = record_or_spec
        self.sha256 = sha256
        self.size = size
        self.md5 = md5

    def verify(self):
        self._verified = True

    def execute(self, progress_update_callback=None):
        # I hate inline imports, but I guess it's ok since we're importing from the conda.core
        # The alternative is passing the the classes to ExtractPackageAction __init__
        from .package_cache_data import PackageCacheData

        log.log(
            TRACE, "extracting %s => %s", self.source_full_path, self.target_full_path
        )

        if lexists(self.target_full_path):
            rm_rf(self.target_full_path)

        extract_tarball(
            self.source_full_path,
            self.target_full_path,
            progress_update_callback=progress_update_callback,
        )

        try:
            raw_index_json = read_index_json(self.target_full_path)
        except (OSError, JSONDecodeError, FileNotFoundError):
            # At this point, we can assume the package tarball is bad.
            # Remove everything and move on.
            print(
                f"ERROR: Encountered corrupt package tarball at {self.source_full_path}. Conda has "
                "left it in place. Please report this to the maintainers "
                "of the package."
            )
            sys.exit(1)

        if isinstance(self.record_or_spec, MatchSpec):
            url = self.record_or_spec.get_raw_value("url")
            assert url
            channel = (
                Channel(url)
                if has_platform(url, context.known_subdirs)
                else Channel(None)
            )
            fn = basename(url)
            sha256 = self.sha256 or compute_sum(self.source_full_path, "sha256")
            size = getsize(self.source_full_path)
            if self.size is not None:
                assert size == self.size, (size, self.size)
            md5 = self.md5 or compute_sum(self.source_full_path, "md5")
            repodata_record = PackageRecord.from_objects(
                raw_index_json,
                url=url,
                channel=channel,
                fn=fn,
                sha256=sha256,
                size=size,
                md5=md5,
            )
        else:
            repodata_record = PackageRecord.from_objects(
                self.record_or_spec, raw_index_json
            )

        repodata_record_path = join(
            self.target_full_path, "info", "repodata_record.json"
        )
        write_as_json_to_file(repodata_record_path, repodata_record)

        target_package_cache = PackageCacheData(self.target_pkgs_dir)
        package_cache_record = PackageCacheRecord.from_objects(
            repodata_record,
            package_tarball_full_path=self.source_full_path,
            extracted_package_dir=self.target_full_path,
        )
        target_package_cache.insert(package_cache_record)

    def reverse(self):
        rm_rf(self.target_full_path)
        if lexists(self.hold_path):
            log.log(TRACE, "moving %s => %s", self.hold_path, self.target_full_path)
            rm_rf(self.target_full_path)
            backoff_rename(self.hold_path, self.target_full_path)

    def cleanup(self):
        rm_rf(self.hold_path)

    @property
    def target_full_path(self):
        return join(self.target_pkgs_dir, self.target_extracted_dirname)

    def __str__(self):
        return f"ExtractPackageAction<source_full_path={self.source_full_path!r}, target_full_path={self.target_full_path!r}>"


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Backport of conda.core.package_cache_data for conda-build."""

from ..deprecations import deprecated
from .package_cache_data import ProgressiveFetchExtract

deprecated.module(
    "24.3", "24.9", addendum="Use `conda.core.package_cache_data` instead."
)

ProgressiveFetchExtract = ProgressiveFetchExtract


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""
Code in ``conda.core`` is the core logic.  It is strictly forbidden from having side effects.
No printing to stdout or stderr, no disk manipulation, no http requests.
All side effects should be implemented through ``conda.gateways``.  Objects defined in
``conda.models`` should be heavily preferred for ``conda.core`` function/method arguments
and return values.

Conda modules importable from ``conda.core`` are

- ``conda._vendor``
- ``conda.common``
- ``conda.core``
- ``conda.models``
- ``conda.gateways``

Conda modules strictly off limits for import within ``conda.core`` are

- ``conda.api``
- ``conda.cli``
- ``conda.client``

"""


# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause
"""Tools for fetching the current index."""

from __future__ import annotations

from itertools import chain
from logging import getLogger
from typing import TYPE_CHECKING

from boltons.setutils import IndexedSet

from ..base.context import context
from ..common.io import ThreadLimitedThreadPoolExecutor, time_recorder
from ..exceptions import ChannelNotAllowed, InvalidSpec
from ..gateways.logging import initialize_logging
from ..models.channel import Channel, all_channel_urls
from ..models.enums import PackageType
from ..models.match_spec import MatchSpec
from ..models.records import EMPTY_LINK, PackageCacheRecord, PackageRecord, PrefixRecord
from .package_cache_data import PackageCacheData
from .prefix_data import PrefixData
from .subdir_data import SubdirData, make_feature_record

if TYPE_CHECKING:
    from typing import Any


log = getLogger(__name__)


def check_allowlist(channel_urls: list[str]) -> None:
    """
    Check if the given channel URLs are allowed by the context's allowlist.

    :param channel_urls: A list of channel URLs to check against the allowlist.
    :raises ChannelNotAllowed: If any URL is not in the allowlist.
    """
    if context.allowlist_channels:
        allowlist_channel_urls = tuple(
            chain.from_iterable(
                Channel(c).base_urls for c in context.allowlist_channels
            )
        )
        for url in channel_urls:
            these_urls = Channel(url).base_urls
            if not all(this_url in allowlist_channel_urls for this_url in these_urls):
                raise ChannelNotAllowed(Channel(url))


LAST_CHANNEL_URLS = []


@time_recorder("get_index")
def get_index(
    channel_urls: tuple[str] = (),
    prepend: bool = True,
    platform: str | None = None,
    use_local: bool = False,
    use_cache: bool = False,
    unknown: bool | None = None,
    prefix: str | None = None,
    repodata_fn: str = context.repodata_fns[-1],
) -> dict:
    """
    Return the index of packages available on the channels

    If prepend=False, only the channels passed in as arguments are used.
    If platform=None, then the current platform is used.
    If prefix is supplied, then the packages installed in that prefix are added.

    :param channel_urls: Channels to include in the index.
    :param prepend: If False, only the channels passed in are used.
    :param platform: Target platform for the index.
    :param use_local: Whether to use local channels.
    :param use_cache: Whether to use cached index information.
    :param unknown: Include unknown packages.
    :param prefix: Path to environment prefix to include in the index.
    :param repodata_fn: Filename of the repodata file.
    :return: A dictionary representing the package index.
    """
    initialize_logging()  # needed in case this function is called directly as a public API

    if context.offline and unknown is None:
        unknown = True

    channel_urls = calculate_channel_urls(channel_urls, prepend, platform, use_local)
    LAST_CHANNEL_URLS.clear()
    LAST_CHANNEL_URLS.extend(channel_urls)

    check_allowlist(channel_urls)

    index = fetch_index(channel_urls, use_cache=use_cache, repodata_fn=repodata_fn)

    if prefix:
        _supplement_index_with_prefix(index, prefix)
    if unknown:
        _supplement_index_with_cache(index)
    if context.track_features:
        _supplement_index_with_features(index)
    return index


def fetch_index(
    channel_urls: list[str],
    use_cache: bool = False,
    index: dict | None = None,
    repodata_fn: str = context.repodata_fns[-1],
) -> dict:
    """
    Fetch the package index from the specified channels.

    :param channel_urls: A list of channel URLs to fetch the index from.
    :param use_cache: Whether to use the cached index data.
    :param index: An optional pre-existing index to update.
    :param repodata_fn: The name of the repodata file.
    :return: A dictionary representing the fetched or updated package index.
    """
    log.debug("channel_urls=" + repr(channel_urls))
    index = {}
    with ThreadLimitedThreadPoolExecutor() as executor:
        subdir_instantiator = lambda url: SubdirData(
            Channel(url), repodata_fn=repodata_fn
        )
        for f in executor.map(subdir_instantiator, channel_urls):
            index.update((rec, rec) for rec in f.iter_records())
    return index


def dist_str_in_index(index: dict[Any, Any], dist_str: str) -> bool:
    """
    Check if a distribution string matches any package in the index.

    :param index: The package index.
    :param dist_str: The distribution string to match against the index.
    :return: True if there is a match; False otherwise.
    """
    match_spec = MatchSpec.from_dist_str(dist_str)
    return any(match_spec.match(prec) for prec in index.values())


def _supplement_index_with_prefix(index: dict[Any, Any], prefix: str) -> None:
    """
    Supplement the given index with information from the specified environment prefix.

    :param index: The package index to supplement.
    :param prefix: The path to the environment prefix.
    """
    # supplement index with information from prefix/conda-meta
    assert prefix
    for prefix_record in PrefixData(prefix).iter_records():
        if prefix_record in index:
            current_record = index[prefix_record]
            if current_record.channel == prefix_record.channel:
                # The downloaded repodata takes priority, so we do not overwrite.
                # We do, however, copy the link information so that the solver (i.e. resolve)
                # knows this package is installed.
                link = prefix_record.get("link") or EMPTY_LINK
                index[prefix_record] = PrefixRecord.from_objects(
                    current_record, prefix_record, link=link
                )
            else:
                # If the local packages channel information does not agree with
                # the channel information in the index then they are most
                # likely referring to different packages.  This can occur if a
                # multi-channel changes configuration, e.g. defaults with and
                # without the free channel. In this case we need to fake the
                # channel data for the existing package.
                prefix_channel = prefix_record.channel
                prefix_channel._Channel__canonical_name = prefix_channel.url()
                del prefix_record._PackageRecord__pkey
                index[prefix_record] = prefix_record
        else:
            # If the package is not in the repodata, use the local data.
            # If the channel is known but the package is not in the index, it
            # is because 1) the channel is unavailable offline, or 2) it no
            # longer contains this package. Either way, we should prefer any
            # other version of the package to this one. On the other hand, if
            # it is in a channel we don't know about, assign it a value just
            # above the priority of all known channels.
            index[prefix_record] = prefix_record


def _supplement_index_with_cache(index: dict[Any, Any]) -> None:
    """
    Supplement the given index with packages from the cache.

    :param index: The package index to supplement.
    """
    # supplement index with packages from the cache
    for pcrec in PackageCacheData.get_all_extracted_entries():
        if pcrec in index:
            # The downloaded repodata takes priority
            current_record = index[pcrec]
            index[pcrec] = PackageCacheRecord.from_objects(current_record, pcrec)
        else:
            index[pcrec] = pcrec


def _make_virtual_package(
    name: str, version: str | None = None, build_string: str | None = None
) -> PackageRecord:
    """
    Create a virtual package record.

    :param name: The name of the virtual package.
    :param version: The version of the virtual package, defaults to "0".
    :param build_string: The build string of the virtual package, defaults to "0".
    :return: A PackageRecord representing the virtual package.
    """
    return PackageRecord(
        package_type=PackageType.VIRTUAL_SYSTEM,
        name=name,
        version=version or "0",
        build_string=build_string or "0",
        channel="@",
        subdir=context.subdir,
        md5="12345678901234567890123456789012",
        build_number=0,
        fn=name,
    )


def _supplement_index_with_features(
    index: dict[PackageRecord, PackageRecord], features: list[str] = []
) -> None:
    """
    Supplement the given index with virtual feature records.

    :param index: The package index to supplement.
    :param features: A list of feature names to add to the index.
    """
    for feature in chain(context.track_features, features):
        rec = make_feature_record(feature)
        index[rec] = rec


def _supplement_index_with_system(index: dict[PackageRecord, PackageRecord]) -> None:
    """
    Loads and populates virtual package records from conda plugins
    and adds them to the provided index, unless there is a naming
    conflict.

    :param index: The package index to supplement.
    """
    for package in context.plugin_manager.get_virtual_packages():
        rec = _make_virtual_package(f"__{package.name}", package.version, package.build)
        index[rec] = rec


def get_archspec_name() -> str | None:
    """
    Determine the architecture specification name for the current environment.

    :return: The architecture name if available, otherwise None.
    """
    from ..base.context import _arch_names, non_x86_machines

    target_plat, target_arch = context.subdir.split("-")
    # This has to reverse what Context.subdir is doing
    if target_arch in non_x86_machines:
        machine = target_arch
    elif target_arch == "zos":
        return None
    elif target_arch.isdigit():
        machine = _arch_names[int(target_arch)]
    else:
        return None

    native_subdir = context._native_subdir()

    if native_subdir != context.subdir:
        return machine
    else:
        import archspec.cpu

        return str(archspec.cpu.host())


def calculate_channel_urls(
    channel_urls: tuple[str] = (),
    prepend: bool = True,
    platform: str | None = None,
    use_local: bool = False,
) -> list[str]:
    """
    Calculate the full list of channel URLs to use based on the given parameters.

    :param channel_urls: Initial list of channel URLs.
    :param prepend: Whether to prepend default channels to the list.
    :param platform: The target platform for the channels.
    :param use_local: Whether to include the local channel.
    :return: The calculated list of channel URLs.
    """
    if use_local:
        channel_urls = ["local"] + list(channel_urls)
    if prepend:
        channel_urls += context.channels

    subdirs = (platform, "noarch") if platform is not None else context.subdirs
    return all_channel_urls(channel_urls, subdirs=subdirs)


def get_reduced_index(
    prefix: str | None,
    channels: list[str],
    subdirs: list[str],
    specs: list[MatchSpec],
    repodata_fn: str,
) -> dict:
    """
    Generate a reduced package index based on the given specifications.

    This function is useful for optimizing the solver by reducing the amount
    of data it needs to consider.

    :param prefix: Path to an environment prefix to include installed packages.
    :param channels: A list of channel names to include in the index.
    :param subdirs: A list of subdirectories to consider for each channel.
    :param specs: A list of MatchSpec objects to filter the packages.
    :param repodata_fn: Filename of the repodata file to use.
    :return: A dictionary representing the reduced package index.
    """
    records = IndexedSet()
    collected_names = set()
    collected_track_features = set()
    pending_names = set()
    pending_track_features = set()

    def push_spec(spec: MatchSpec) -> None:
        """
        Add a package name or track feature from a MatchSpec to the pending set.

        :param spec: The MatchSpec to process.
        """
        name = spec.get_raw_value("name")
        if name and name not in collected_names:
            pending_names.add(name)
        track_features = spec.get_raw_value("track_features")
        if track_features:
            for ftr_name in track_features:
                if ftr_name not in collected_track_features:
                    pending_track_features.add(ftr_name)

    def push_record(record: PackageRecord) -> None:
        """
        Process a package record to collect its dependencies and features.

        :param record: The package record to process.
        """
        try:
            combined_depends = record.combined_depends
        except InvalidSpec as e:
            log.warning(
                "Skipping %s due to InvalidSpec: %s",
                record.record_id(),
                e._kwargs["invalid_spec"],
            )
            return
        push_spec(MatchSpec(record.name))
        for _spec in combined_depends:
            push_spec(_spec)
        if record.track_features:
            for ftr_name in record.track_features:
                push_spec(MatchSpec(track_features=ftr_name))

    if prefix:
        for prefix_rec in PrefixData(prefix).iter_records():
            push_record(prefix_rec)
    for spec in specs:
        push_spec(spec)

    while pending_names or pending_track_features:
        while pending_names:
            name = pending_names.pop()
            collected_names.add(name)
            spec = MatchSpec(name)
            new_records = SubdirData.query_all(
                spec, channels=channels, subdirs=subdirs, repodata_fn=repodata_fn
            )
            for record in new_records:
                push_record(record)
            records.update(new_records)

        while pending_track_features:
            feature_name = pending_track_features.pop()
            collected_track_features.add(feature_name)
            spec = MatchSpec(track_features=feature_name)
            new_records = SubdirData.query_all(
                spec, channels=channels, subdirs=subdirs, repodata_fn=repodata_fn
            )
            for record in new_records:
                push_record(record)
            records.update(new_records)

    reduced_index = {rec: rec for rec in records}

    if prefix is not None:
        _supplement_index_with_prefix(reduced_index, prefix)

    if context.offline or (
        "unknown" in context._argparse_args and context._argparse_args.unknown
    ):
        # This is really messed up right now.  Dates all the way back to
        # https://github.com/conda/conda/commit/f761f65a82b739562a0d997a2570e2b8a0bdc783
        # TODO: revisit this later
        _supplement_index_with_cache(reduced_index)

    # add feature records for the solver
    known_features = set()
    for rec in reduced_index.values():
        known_features.update((*rec.track_features, *rec.features))
    known_features.update(context.track_features)
    for ftr_str in known_features:
        rec = make_feature_record(ftr_str)
        reduced_index[rec] = rec

    _supplement_index_with_system(reduced_index)

    return reduced_index


"""Collection of functions to coerce conversion of types with an intelligent guess."""
from collections.abc import Mapping
from itertools import chain
from re import IGNORECASE, compile

from enum import Enum

from ..deprecations import deprecated
from .compat import isiterable
from .decorators import memoizedproperty
from .exceptions import AuxlibError

__all__ = ["boolify", "typify", "maybecall", "listify", "numberify"]

BOOLISH_TRUE = ("true", "yes", "on", "y")
BOOLISH_FALSE = ("false", "off", "n", "no", "non", "none", "")
NULL_STRINGS = ("none", "~", "null", "\0")
BOOL_COERCEABLE_TYPES = (int, bool, float, complex, list, set, dict, tuple)
NUMBER_TYPES = (int, float, complex)
NUMBER_TYPES_SET = {*NUMBER_TYPES}
STRING_TYPES_SET = {str}

NO_MATCH = object()


class TypeCoercionError(AuxlibError, ValueError):

    def __init__(self, value, msg, *args, **kwargs):
        self.value = value
        super().__init__(msg, *args, **kwargs)


class _Regex:

    @memoizedproperty
    def BOOLEAN_TRUE(self):
        return compile(r'^true$|^yes$|^on$', IGNORECASE), True

    @memoizedproperty
    def BOOLEAN_FALSE(self):
        return compile(r'^false$|^no$|^off$', IGNORECASE), False

    @memoizedproperty
    def NONE(self):
        return compile(r'^none$|^null$', IGNORECASE), None

    @memoizedproperty
    def INT(self):
        return compile(r'^[-+]?\d+$'), int

    @memoizedproperty
    def BIN(self):
        return compile(r'^[-+]?0[bB][01]+$'), bin

    @memoizedproperty
    def OCT(self):
        return compile(r'^[-+]?0[oO][0-7]+$'), oct

    @memoizedproperty
    def HEX(self):
        return compile(r'^[-+]?0[xX][0-9a-fA-F]+$'), hex

    @memoizedproperty
    def FLOAT(self):
        return compile(r'^[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?$'), float

    @memoizedproperty
    def COMPLEX(self):
        return (compile(r'^(?:[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?)?'  # maybe first float
                        r'[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?j$'),     # second float with j
                complex)

    @property
    def numbers(self):
        yield self.INT
        yield self.FLOAT
        yield self.BIN
        yield self.OCT
        yield self.HEX
        yield self.COMPLEX

    @property
    def boolean(self):
        yield self.BOOLEAN_TRUE
        yield self.BOOLEAN_FALSE

    @property
    def none(self):
        yield self.NONE

    def convert_number(self, value_string):
        return self._convert(value_string, (self.numbers, ))

    def convert(self, value_string):
        return self._convert(value_string, (self.boolean, self.none, self.numbers, ))

    def _convert(self, value_string, type_list):
        return next((typish(value_string) if callable(typish) else typish
                     for regex, typish in chain.from_iterable(type_list)
                     if regex.match(value_string)),
                    NO_MATCH)


_REGEX = _Regex()


def numberify(value):
    """

    Examples:
        >>> [numberify(x) for x in ('1234', 1234, '0755', 0o0755, False, 0, '0', True, 1, '1')]
          [1234, 1234, 755, 493, 0, 0, 0, 1, 1, 1]
        >>> [numberify(x) for x in ('12.34', 12.34, 1.2+3.5j, '1.2+3.5j')]
        [12.34, 12.34, (1.2+3.5j), (1.2+3.5j)]

    """
    if isinstance(value, bool):
        return int(value)
    if isinstance(value, NUMBER_TYPES):
        return value
    candidate = _REGEX.convert_number(value)
    if candidate is not NO_MATCH:
        return candidate
    raise TypeCoercionError(value, f"Cannot convert {value} to a number.")


def boolify(value, nullable=False, return_string=False):
    """Convert a number, string, or sequence type into a pure boolean.

    Args:
        value (number, string, sequence): pretty much anything

    Returns:
        bool: boolean representation of the given value

    Examples:
        >>> [boolify(x) for x in ('yes', 'no')]
        [True, False]
        >>> [boolify(x) for x in (0.1, 0+0j, True, '0', '0.0', '0.1', '2')]
        [True, False, True, False, False, True, True]
        >>> [boolify(x) for x in ("true", "yes", "on", "y")]
        [True, True, True, True]
        >>> [boolify(x) for x in ("no", "non", "none", "off", "")]
        [False, False, False, False, False]
        >>> [boolify(x) for x in ([], set(), dict(), tuple())]
        [False, False, False, False]
        >>> [boolify(x) for x in ([1], set([False]), dict({'a': 1}), tuple([2]))]
        [True, True, True, True]

    """
    # cast number types naturally
    if isinstance(value, BOOL_COERCEABLE_TYPES):
        return bool(value)
    # try to coerce string into number
    val = str(value).strip().lower().replace(".", "", 1)
    if val.isnumeric():
        return bool(float(val))
    elif val in BOOLISH_TRUE:
        return True
    elif nullable and val in NULL_STRINGS:
        return None
    elif val in BOOLISH_FALSE:
        return False
    else:  # must be False
        try:
            return bool(complex(val))
        except ValueError:
            if isinstance(value, str) and return_string:
                return value
            raise TypeCoercionError(value, "The value %r cannot be boolified." % value)


@deprecated("24.3", "24.9")
def boolify_truthy_string_ok(value):
    try:
        return boolify(value)
    except ValueError:
        assert isinstance(value, str), repr(value)
        return True


def typify_str_no_hint(value):
    candidate = _REGEX.convert(value)
    return candidate if candidate is not NO_MATCH else value


def typify(value, type_hint=None):
    """Take a primitive value, usually a string, and try to make a more relevant type out of it.
    An optional type_hint will try to coerce the value to that type.

    Args:
        value (Any): Usually a string, not a sequence
        type_hint (type or tuple[type]):

    Examples:
        >>> typify('32')
        32
        >>> typify('32', float)
        32.0
        >>> typify('32.0')
        32.0
        >>> typify('32.0.0')
        '32.0.0'
        >>> [typify(x) for x in ('true', 'yes', 'on')]
        [True, True, True]
        >>> [typify(x) for x in ('no', 'FALSe', 'off')]
        [False, False, False]
        >>> [typify(x) for x in ('none', 'None', None)]
        [None, None, None]

    """
    # value must be a string, or there at least needs to be a type hint
    if isinstance(value, str):
        value = value.strip()
    elif type_hint is None:
        # can't do anything because value isn't a string and there's no type hint
        return value

    # now we either have a stripped string, a type hint, or both
    # use the hint if it exists
    if isiterable(type_hint):
        if isinstance(type_hint, type) and issubclass(type_hint, Enum):
            try:
                return type_hint(value)
            except ValueError as e:
                try:
                    return type_hint[value]
                except KeyError:
                    raise TypeCoercionError(value, str(e))
        type_hint = set(type_hint)
        if not (type_hint - NUMBER_TYPES_SET):
            return numberify(value)
        elif not (type_hint - STRING_TYPES_SET):
            return str(value)
        elif not (type_hint - {bool, type(None)}):
            return boolify(value, nullable=True)
        elif not (type_hint - (STRING_TYPES_SET | {bool})):
            return boolify(value, return_string=True)
        elif not (type_hint - (STRING_TYPES_SET | {type(None)})):
            value = str(value)
            return None if value.lower() == 'none' else value
        elif not (type_hint - {bool, int}):
            return typify_str_no_hint(str(value))
        else:
            raise NotImplementedError()
    elif type_hint is not None:
        # coerce using the type hint, or use boolify for bool
        try:
            return boolify(value) if type_hint == bool else type_hint(value)
        except ValueError as e:
            # ValueError: invalid literal for int() with base 10: 'nope'
            raise TypeCoercionError(value, str(e))
    else:
        # no type hint, but we know value is a string, so try to match with the regex patterns
        #   if there's still no match, `typify_str_no_hint` will return `value`
        return typify_str_no_hint(value)


def typify_data_structure(value, type_hint=None):
    if isinstance(value, Mapping):
        return type(value)((k, typify(v, type_hint)) for k, v in value.items())
    elif isiterable(value):
        return type(value)(typify(v, type_hint) for v in value)
    elif isinstance(value, str) and isinstance(type_hint, type) and issubclass(type_hint, str):
        # This block is necessary because if we fall through to typify(), we end up calling
        # .strip() on the str, when sometimes we want to preserve preceding and trailing
        # whitespace.
        return type_hint(value)
    else:
        return typify(value, type_hint)


def maybecall(value):
    return value() if callable(value) else value


@deprecated("24.3", "24.9")
def listify(val, return_type=tuple):
    """
    Examples:
        >>> listify('abc', return_type=list)
        ['abc']
        >>> listify(None)
        ()
        >>> listify(False)
        (False,)
        >>> listify(('a', 'b', 'c'), return_type=list)
        ['a', 'b', 'c']
    """
    # TODO: flatlistify((1, 2, 3), 4, (5, 6, 7))
    if val is None:
        return return_type()
    elif isiterable(val):
        return return_type(val)
    else:
        return return_type((val, ))


from itertools import islice
from json import JSONEncoder, dumps, loads
from logging import getLogger, INFO, Formatter, StreamHandler, DEBUG
from sys import stderr

from . import NullHandler

log = getLogger(__name__)
root_log = getLogger()

NullHandler = NullHandler

DEBUG_FORMATTER = Formatter(
    "[%(levelname)s] [%(asctime)s.%(msecs)03d] %(process)d %(name)s:%(funcName)s(%(lineno)d):\n"
    "%(message)s\n",
    "%Y-%m-%d %H:%M:%S")

INFO_FORMATTER = Formatter(
    "[%(levelname)s] [%(asctime)s.%(msecs)03d] %(process)d %(name)s(%(lineno)d): %(message)s\n",
    "%Y-%m-%d %H:%M:%S")


def set_root_level(level=INFO):
    root_log.setLevel(level)


def attach_stderr(level=INFO):
    has_stderr_handler = any(handler.name == 'stderr' for handler in root_log.handlers)
    if not has_stderr_handler:
        handler = StreamHandler(stderr)
        handler.name = 'stderr'
        if level is not None:
            handler.setLevel(level)
        handler.setFormatter(DEBUG_FORMATTER if level == DEBUG else INFO_FORMATTER)
        root_log.addHandler(handler)
        return True
    else:
        return False


def detach_stderr():
    for handler in root_log.handlers:
        if handler.name == 'stderr':
            root_log.removeHandler(handler)
            return True
    return False


def initialize_logging(level=INFO):
    attach_stderr(level)


class DumpEncoder(JSONEncoder):
    def default(self, obj):
        if hasattr(obj, 'dump'):
            return obj.dump()
        # Let the base class default method raise the TypeError
        return super().default(obj)


_DUMPS = DumpEncoder(indent=2, ensure_ascii=False, sort_keys=True).encode


def jsondumps(obj):
    return _DUMPS(obj)


def fullname(obj):
    try:
        return obj.__module__ + "." + obj.__class__.__name__
    except AttributeError:
        return obj.__class__.__name__


request_header_sort_dict = {
    'Host': '\x00\x00',
    'User-Agent': '\x00\x01',
}
def request_header_sort_key(item):
    return request_header_sort_dict.get(item[0], item[0].lower())


response_header_sort_dict = {
    'Content-Length': '\x7e\x7e\x61',
    'Connection': '\x7e\x7e\x62',
}
def response_header_sort_key(item):
    return response_header_sort_dict.get(item[0], item[0].lower())


def stringify(obj, content_max_len=0):
    def bottle_builder(builder, bottle_object):
        builder.append(
            "{} {}{} {}".format(
                bottle_object.method,
                bottle_object.path,
                bottle_object.environ.get("QUERY_STRING", ""),
                bottle_object.get("SERVER_PROTOCOL"),
            )
        )
        builder += [f"{key}: {value}" for key, value in bottle_object.headers.items()]
        builder.append('')
        body = bottle_object.body.read().strip()
        if body:
            builder.append(body)

    def requests_models_PreparedRequest_builder(builder, request_object):
        builder.append(
            ">>{} {} {}".format(
                request_object.method,
                request_object.path_url,
                request_object.url.split(":", 1)[0].upper(),
            )
        )
        builder.extend(
            f"> {key}: {value}"
            for key, value in sorted(request_object.headers.items(), key=request_header_sort_key)
        )
        builder.append("")
        if request_object.body:
            builder.append(request_object.body)

    def requests_models_Response_builder(builder, response_object):
        builder.append(
            "<<{} {} {}".format(
                response_object.url.split(":", 1)[0].upper(),
                response_object.status_code,
                response_object.reason,
            )
        )
        builder.extend(
            f"< {key}: {value}"
            for key, value in sorted(response_object.headers.items(), key=response_header_sort_key)
        )
        elapsed = str(response_object.elapsed).split(":", 1)[-1]
        builder.append(f"< Elapsed: {elapsed}")
        if content_max_len:
            builder.append('')
            content_type = response_object.headers.get('Content-Type')
            if content_type == 'application/json':
                text = response_object.text
                if len(text) > content_max_len:
                    content = text
                else:
                    resp = loads(text)
                    resp = dict(islice(resp.items(), content_max_len))
                    content = dumps(resp, indent=2)
                content = content[:content_max_len] if len(content) > content_max_len else content
                builder.append(content)
                builder.append('')
            elif content_type is not None and (content_type.startswith('text/')
                                               or content_type == 'application/xml'):
                text = response_object.text
                content = text[:content_max_len] if len(text) > content_max_len else text
                builder.append(content)

    try:
        name = fullname(obj)
        builder = ['']  # start with new line
        if name.startswith('bottle.'):
            bottle_builder(builder, obj)
        elif name.endswith('requests.models.PreparedRequest'):
            requests_models_PreparedRequest_builder(builder, obj)
        elif name.endswith('requests.models.Response'):
            if getattr(obj, 'request'):
                requests_models_PreparedRequest_builder(builder, obj.request)
            else:
                log.info("request is 'None' for Response object with url %s", obj.url)
            requests_models_Response_builder(builder, obj)
        else:
            return None
        builder.append('')  # end with new line
        return "\n".join(builder)
    except Exception as e:
        log.exception(e)


"""Common collection classes."""
from functools import reduce
from collections.abc import Mapping, Set

from .compat import isiterable
from ..deprecations import deprecated

try:
    from frozendict import frozendict
except ImportError:
    from .._vendor.frozendict import frozendict


@deprecated("24.9", "25.3", addendum="Use `frozendict.deepfreeze` instead.")
def make_immutable(value):
    # this function is recursive, and if nested data structures fold back on themselves,
    #   there will likely be recursion errors
    if isinstance(value, Mapping):
        if isinstance(value, frozendict):
            return value
        return frozendict((k, make_immutable(v)) for k, v in value.items())
    elif isinstance(value, Set):
        if isinstance(value, frozenset):
            return value
        return frozenset(make_immutable(v) for v in value)
    elif isiterable(value):
        if isinstance(value, tuple):
            return value
        return tuple(make_immutable(v) for v in value)
    else:
        return value


# http://stackoverflow.com/a/14620633/2127762
class AttrDict(dict):
    """Sub-classes dict, and further allows attribute-like access to dictionary items.

    Examples:
        >>> d = AttrDict({'a': 1})
        >>> d.a, d['a'], d.get('a')
        (1, 1, 1)
        >>> d.b = 2
        >>> d.b, d['b']
        (2, 2)
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.__dict__ = self


def first(seq, key=bool, default=None, apply=lambda x: x):
    """Give the first value that satisfies the key test.

    Args:
        seq (iterable):
        key (callable): test for each element of iterable
        default: returned when all elements fail test
        apply (callable): applied to element before return, but not to default value

    Returns: first element in seq that passes key, mutated with optional apply

    Examples:
        >>> first([0, False, None, [], (), 42])
        42
        >>> first([0, False, None, [], ()]) is None
        True
        >>> first([0, False, None, [], ()], default='ohai')
        'ohai'
        >>> import re
        >>> m = first(re.match(regex, 'abc') for regex in ['b.*', 'a(.*)'])
        >>> m.group(1)
        'bc'

        The optional `key` argument specifies a one-argument predicate function
        like that used for `filter()`.  The `key` argument, if supplied, must be
        in keyword form.  For example:
        >>> first([1, 1, 3, 4, 5], key=lambda x: x % 2 == 0)
        4

    """
    return next((apply(x) for x in seq if key(x)), default() if callable(default) else default)


@deprecated("24.3", "24.9")
def firstitem(map, key=lambda k, v: bool(k), default=None, apply=lambda k, v: (k, v)):
    return next((apply(k, v) for k, v in map if key(k, v)), default)


def last(seq, key=bool, default=None, apply=lambda x: x):
    return next((apply(x) for x in reversed(seq) if key(x)), default)


@deprecated("24.3", "24.9")
def call_each(seq):
    """Calls each element of sequence to invoke the side effect.

    Args:
        seq:

    Returns: None

    """
    try:
        reduce(lambda _, y: y(), seq)
    except TypeError as e:
        if str(e) != "reduce() of empty sequence with no initial value":
            raise


from logging import getLogger
from textwrap import dedent

log = getLogger(__name__)


def dals(string):
    """dedent and left-strip"""
    return dedent(string).lstrip()


def _get_attr(obj, attr_name, aliases=()):
    try:
        return getattr(obj, attr_name)
    except AttributeError:
        for alias in aliases:
            try:
                return getattr(obj, alias)
            except AttributeError:
                continue
        else:
            raise


def find_or_none(key, search_maps, aliases=(), _map_index=0):
    """Return the value of the first key found in the list of search_maps,
    otherwise return None.

    Examples:
        >>> from .collection import AttrDict
        >>> d1 = AttrDict({'a': 1, 'b': 2, 'c': 3, 'e': None})
        >>> d2 = AttrDict({'b': 5, 'e': 6, 'f': 7})
        >>> find_or_none('c', (d1, d2))
        3
        >>> find_or_none('f', (d1, d2))
        7
        >>> find_or_none('b', (d1, d2))
        2
        >>> print(find_or_none('g', (d1, d2)))
        None
        >>> find_or_none('e', (d1, d2))
        6

    """
    try:
        attr = _get_attr(search_maps[_map_index], key, aliases)
        return attr if attr is not None else find_or_none(key, search_maps[1:], aliases)
    except AttributeError:
        # not found in current map object, so go to next
        return find_or_none(key, search_maps, aliases, _map_index+1)
    except IndexError:
        # ran out of map objects to search
        return None


def find_or_raise(key, search_maps, aliases=(), _map_index=0):
    try:
        attr = _get_attr(search_maps[_map_index], key, aliases)
        return attr if attr is not None else find_or_raise(key, search_maps[1:], aliases)
    except AttributeError:
        # not found in current map object, so go to next
        return find_or_raise(key, search_maps, aliases, _map_index+1)
    except IndexError:
        # ran out of map objects to search
        raise AttributeError()


from logging import getLogger
from ..deprecations import deprecated

log = getLogger(__name__)


def Raise(exception):  # NOQA
    raise exception


class AuxlibError:
    """Mixin to identify exceptions associated with the auxlib package."""


@deprecated("24.3", "24.9")
class AuthenticationError(AuxlibError, ValueError):
    pass


@deprecated("24.3", "24.9")
class NotFoundError(AuxlibError, KeyError):
    pass


@deprecated("24.3", "24.9")
class InitializationError(AuxlibError, EnvironmentError):
    pass


@deprecated("24.3", "24.9")
class SenderError(AuxlibError, IOError):
    pass


@deprecated("24.3", "24.9")
class AssignmentError(AuxlibError, AttributeError):
    pass


class ValidationError(AuxlibError, TypeError):

    def __init__(self, key, value=None, valid_types=None, msg=None):
        self.__cause__ = None  # in python3 don't chain ValidationError exceptions
        if msg is not None:
            super().__init__(msg)
        elif value is None:
            super().__init__(f"Value for {key} cannot be None.")
        elif valid_types is None:
            super().__init__(f"Invalid value {value} for {key}")
        else:
            super().__init__(
                f"{key} must be of type {valid_types}, not {value!r}"
            )


class ThisShouldNeverHappenError(AuxlibError, AttributeError):
    pass


from collections.abc import Hashable
from types import GeneratorType

from functools import wraps


# TODO: spend time filling out functionality and make these more robust


def memoizemethod(method):
    """
    Decorator to cause a method to cache it's results in self for each
    combination of inputs and return the cached result on subsequent calls.
    Does not support named arguments or arg values that are not hashable.

    >>> class Foo (object):
    ...   @memoizemethod
    ...   def foo(self, x, y=0):
    ...     print('running method with', x, y)
    ...     return x + y + 3
    ...
    >>> foo1 = Foo()
    >>> foo2 = Foo()
    >>> foo1.foo(10)
    running method with 10 0
    13
    >>> foo1.foo(10)
    13
    >>> foo2.foo(11, y=7)
    running method with 11 7
    21
    >>> foo2.foo(11)
    running method with 11 0
    14
    >>> foo2.foo(11, y=7)
    21
    >>> class Foo (object):
    ...   def __init__(self, lower):
    ...     self.lower = lower
    ...   @memoizemethod
    ...   def range_tuple(self, upper):
    ...     print('running function')
    ...     return tuple(i for i in range(self.lower, upper))
    ...   @memoizemethod
    ...   def range_iter(self, upper):
    ...     print('running function')
    ...     return (i for i in range(self.lower, upper))
    ...
    >>> foo = Foo(3)
    >>> foo.range_tuple(6)
    running function
    (3, 4, 5)
    >>> foo.range_tuple(7)
    running function
    (3, 4, 5, 6)
    >>> foo.range_tuple(6)
    (3, 4, 5)
    >>> foo.range_iter(6)
    Traceback (most recent call last):
    TypeError: Can't memoize a generator or non-hashable object!
    """

    @wraps(method)
    def _wrapper(self, *args, **kwargs):
        # NOTE:  a __dict__ check is performed here rather than using the
        # built-in hasattr function because hasattr will look up to an object's
        # class if the attr is not directly found in the object's dict.  That's
        # bad for this if the class itself has a memoized classmethod for
        # example that has been called before the memoized instance method,
        # then the instance method will use the class's result cache, causing
        # its results to be globally stored rather than on a per instance
        # basis.
        if '_memoized_results' not in self.__dict__:
            self._memoized_results = {}
        memoized_results = self._memoized_results

        key = (method.__name__, args, tuple(sorted(kwargs.items())))
        if key in memoized_results:
            return memoized_results[key]
        else:
            try:
                result = method(self, *args, **kwargs)
            except KeyError as e:
                if '__wrapped__' in str(e):
                    result = None  # is this the right thing to do?  happened during py3 conversion
                else:
                    raise
            if isinstance(result, GeneratorType) or not isinstance(result, Hashable):
                raise TypeError("Can't memoize a generator or non-hashable object!")
            return memoized_results.setdefault(key, result)

    return _wrapper


def clear_memoized_methods(obj, *method_names):
    """
    Clear the memoized method or @memoizedproperty results for the given
    method names from the given object.

    >>> v = [0]
    >>> def inc():
    ...     v[0] += 1
    ...     return v[0]
    ...
    >>> class Foo(object):
    ...    @memoizemethod
    ...    def foo(self):
    ...        return inc()
    ...    @memoizedproperty
    ...    def g(self):
    ...       return inc()
    ...
    >>> f = Foo()
    >>> f.foo(), f.foo()
    (1, 1)
    >>> clear_memoized_methods(f, 'foo')
    >>> (f.foo(), f.foo(), f.g, f.g)
    (2, 2, 3, 3)
    >>> (f.foo(), f.foo(), f.g, f.g)
    (2, 2, 3, 3)
    >>> clear_memoized_methods(f, 'g', 'no_problem_if_undefined')
    >>> f.g, f.foo(), f.g
    (4, 2, 4)
    >>> f.foo()
    2
    """
    for key in list(getattr(obj, '_memoized_results', {}).keys()):
        # key[0] is the method name
        if key[0] in method_names:
            del obj._memoized_results[key]

    property_dict = obj._cache_
    for prop in method_names:
        inner_attname = '__%s' % prop
        if inner_attname in property_dict:
            del property_dict[inner_attname]


def memoizedproperty(func):
    """
    Decorator to cause a method to cache it's results in self for each
    combination of inputs and return the cached result on subsequent calls.
    Does not support named arguments or arg values that are not hashable.

    >>> class Foo (object):
    ...   _x = 1
    ...   @memoizedproperty
    ...   def foo(self):
    ...     self._x += 1
    ...     print('updating and returning {0}'.format(self._x))
    ...     return self._x
    ...
    >>> foo1 = Foo()
    >>> foo2 = Foo()
    >>> foo1.foo
    updating and returning 2
    2
    >>> foo1.foo
    2
    >>> foo2.foo
    updating and returning 2
    2
    >>> foo1.foo
    2
    """
    inner_attname = '__%s' % func.__name__

    def new_fget(self):
        if not hasattr(self, '_cache_'):
            self._cache_ = {}
        cache = self._cache_
        if inner_attname not in cache:
            cache[inner_attname] = func(self)
        return cache[inner_attname]

    return property(new_fget)


class classproperty:  # pylint: disable=C0103
    # from celery.five

    def __init__(self, getter=None, setter=None):
        if getter is not None and not isinstance(getter, classmethod):
            getter = classmethod(getter)
        if setter is not None and not isinstance(setter, classmethod):
            setter = classmethod(setter)
        self.__get = getter
        self.__set = setter

        info = getter.__get__(object)  # just need the info attrs.
        self.__doc__ = info.__doc__
        self.__name__ = info.__name__
        self.__module__ = info.__module__

    def __get__(self, obj, type_=None):
        if obj and type_ is None:
            type_ = obj.__class__
        return self.__get.__get__(obj, type_)()

    def __set__(self, obj, value):
        if obj is None:
            return self
        return self.__set.__get__(obj)(value)

    def setter(self, setter):
        return self.__class__(self.__get, setter)

# memoize & clear:
#     class method
#     function
#     classproperty
#     property
#     staticproperty?
# memoizefunction
# memoizemethod
# memoizedproperty


"""Auxlib is an auxiliary library to the python standard library.

The aim is to provide core generic features for app development in python. Auxlib fills in some
python stdlib gaps much like `pytoolz <https://github.com/pytoolz/>`_ has for functional
programming, `pyrsistent <https://github.com/tobgu/pyrsistent/>`_ has for data structures, or
`boltons <https://github.com/mahmoud/boltons/>`_ has generally.

Major areas addressed include:
  - :ref:`packaging`: package versioning, with a clean and less invasive alternative to
    versioneer
  - :ref:`entity`: robust base class for type-enforced data models and transfer objects
  - :ref:`type_coercion`: intelligent type coercion utilities
  - :ref:`configuration`: a map implementation designed specifically to hold application
    configuration and context information
  - :ref:`factory`: factory pattern implementation
  - :ref:`path`: file path utilities especially helpful when working with various python
    package formats
  - :ref:`logz`: logging initialization routines to simplify python logging setup
  - :ref:`crypt`: simple, but correct, pycrypto wrapper

[2021-11-09] Our version of auxlib has deviated from the upstream project by a significant amount
(especially compared with the other vendored packages). Further, the upstream project has low
popularity and is no longer actively maintained. Consequently it was decided to absorb, refactor,
and replace auxlib. As a first step of this process we moved conda._vendor.auxlib to conda.auxlib.
"""

# don't mess up logging for library users
from logging import getLogger, Handler
class NullHandler(Handler):  # NOQA
    def emit(self, record):
        pass


getLogger('auxlib').addHandler(NullHandler())

__all__ = [
    "__version__", "__author__",
    "__email__", "__license__", "__copyright__",
    "__summary__", "__url__",
]

__version__ = "0.0.43"

__author__ = 'Kale Franz'
__email__ = 'kale@franz.io'
__url__ = 'https://github.com/kalefranz/auxlib'
__license__ = "ISC"
__copyright__ = "(c) 2015 Kale Franz. All rights reserved."
__summary__ = """auxiliary library to the python standard library"""


class _Null:
    """
    Examples:
        >>> len(_Null())
        0
        >>> bool(_Null())
        False
        >>> _Null().__nonzero__()
        False
    """
    def __nonzero__(self):
        return self.__bool__()

    def __bool__(self):
        return False

    def __len__(self):
        return 0

    def __eq__(self, other):
        return isinstance(other, _Null)

    def __hash__(self):
        return hash(_Null)

    def __str__(self):
        return 'Null'

    def __json__(self):
        return 'null'

    to_json = __json__


# Use this NULL object when needing to distinguish a value from None
# For example, when parsing json, you may need to determine if a json key was given and set
#   to null, or the key didn't exist at all.  There could be a bit of potential confusion here,
#   because in python null == None, while here I'm defining NULL to mean 'not defined'.
NULL = _Null()


"""
This module provides serializable, validatable, type-enforcing domain objects and data
transfer objects. It has many of the same motivations as the python
`Marshmallow <http://marshmallow.readthedocs.org/en/latest/why.html>`_ package. It is most
similar to `Schematics <http://schematics.readthedocs.io/>`_.

========
Tutorial
========

Chapter 1: Entity and Field Basics
----------------------------------

    >>> class Color(Enum):
    ...     blue = 0
    ...     black = 1
    ...     red = 2
    >>> class Car(Entity):
    ...     weight = NumberField(required=False)
    ...     wheels = IntField(default=4, validation=lambda x: 3 <= x <= 4)
    ...     color = EnumField(Color)

    >>> # create a new car object
    >>> car = Car(color=Color.blue, weight=4242.46)
    >>> car
    Car(weight=4242.46, color=0)

    >>> # it has 4 wheels, all by default
    >>> car.wheels
    4

    >>> # but a car can't have 5 wheels!
    >>> #  the `validation=` field is a simple callable that returns a
    >>> #  boolean based on validity
    >>> car.wheels = 5
    Traceback (most recent call last):
    ValidationError: Invalid value 5 for wheels

    >>> # we can call .dump() on car, and just get back a standard
    >>> #  python dict actually, it's an ordereddict to match attribute
    >>> #  declaration order
    >>> type(car.dump())
    <class '...OrderedDict'>
    >>> car.dump()
    OrderedDict([('weight', 4242.46), ('wheels', 4), ('color', 0)])

    >>> # and json too (note the order!)
    >>> car.json()
    '{"weight": 4242.46, "wheels": 4, "color": 0}'

    >>> # green cars aren't allowed
    >>> car.color = "green"
    Traceback (most recent call last):
    ValidationError: 'green' is not a valid Color

    >>> # but black cars are!
    >>> car.color = "black"
    >>> car.color
    <Color.black: 1>

    >>> # car.color really is an enum, promise
    >>> type(car.color)
    <enum 'Color'>

    >>> # enum assignment can be with any of (and preferentially)
    >>> #   (1) an enum literal,
    >>> #   (2) a valid enum value, or
    >>> #   (3) a valid enum name
    >>> car.color = Color.blue; car.color.value
    0
    >>> car.color = 1; car.color.name
    'black'

    >>> # let's do a round-trip marshalling of this thing
    >>> same_car = Car.from_json(car.json())  # or equally Car.from_json(json.dumps(car.dump()))
    >>> same_car == car
    True

    >>> # actually, they're two different instances
    >>> same_car is not car
    True

    >>> # this works too
    >>> cloned_car = Car(**car.dump())
    >>> cloned_car == car
    True

    >>> # while we're at it, these are all equivalent too
    >>> car == Car.from_objects(car)
    True
    >>> car == Car.from_objects({"weight": 4242.46, "wheels": 4, "color": 1})
    True
    >>> car == Car.from_json('{"weight": 4242.46, "color": 1}')
    True

    >>> # .from_objects() even lets you stack and combine objects
    >>> class DumbClass:
    ...     color = 0
    ...     wheels = 3
    >>> Car.from_objects(DumbClass(), dict(weight=2222, color=1))
    Car(weight=2222, wheels=3, color=0)
    >>> # and also pass kwargs that override properties pulled
    >>> #  off any objects
    >>> Car.from_objects(DumbClass(), {'weight': 2222, 'color': 1}, color=2, weight=33)
    Car(weight=33, wheels=3, color=2)


Chapter 2: Entity and Field Composition
---------------------------------------

    >>> # now let's get fancy
    >>> # a ComposableField "nests" another valid Entity
    >>> # a ListField's first argument is a "generic" type,
    >>> #   which can be a valid Entity, any python primitive
    >>> #   type, or a list of Entities/types
    >>> class Fleet(Entity):
    ...     boss_car = ComposableField(Car)
    ...     cars = ListField(Car)

    >>> # here's our fleet of company cars
    >>> company_fleet = Fleet(boss_car=Car(color='red'), cars=[car, same_car, cloned_car])
    >>> company_fleet.pretty_json()  #doctest: +SKIP
    {
      "boss_car": {
        "wheels": 4
        "color": 2,
      },
      "cars": [
        {
          "weight": 4242.46,
          "wheels": 4
          "color": 1,
        },
        {
          "weight": 4242.46,
          "wheels": 4
          "color": 1,
        },
        {
          "weight": 4242.46,
          "wheels": 4
          "color": 1,
        }
      ]
    }

    >>> # the boss' car is red of course (and it's still an Enum)
    >>> company_fleet.boss_car.color.name
    'red'

    >>> # and there are three cars left for the employees
    >>> len(company_fleet.cars)
    3


Chapter 3: Immutability
-----------------------

    >>> class ImmutableCar(ImmutableEntity):
    ...     wheels = IntField(default=4, validation=lambda x: 3 <= x <= 4)
    ...     color = EnumField(Color)
    >>> icar = ImmutableCar.from_objects({'wheels': 3, 'color': 'blue'})
    >>> icar
    ImmutableCar(wheels=3, color=0)

    >>> icar.wheels = 4
    Traceback (most recent call last):
    AttributeError: Assignment not allowed. ImmutableCar is immutable.

    >>> class FixedWheelCar(Entity):
    ...     wheels = IntField(default=4, immutable=True)
    ...     color = EnumField(Color)
    >>> fwcar = FixedWheelCar.from_objects(icar)
    >>> fwcar.json()
    '{"wheels": 3, "color": 0}'

    >>> # repainting the car is easy
    >>> fwcar.color = Color.red
    >>> fwcar.color.name
    'red'

    >>> # can't really change the number of wheels though
    >>> fwcar.wheels = 18
    Traceback (most recent call last):
    AttributeError: The wheels field is immutable.


Chapter X: The del and null Weeds
---------------------------------

    >>> old_date = lambda: isoparse('1982-02-17')
    >>> class CarBattery(Entity):
    ...     # NOTE: default value can be a callable!
    ...     first_charge = DateField(required=False)  # default=None, nullable=False
    ...     latest_charge = DateField(default=old_date, nullable=True)  # required=True
    ...     expiration = DateField(default=old_date, required=False, nullable=False)

    >>> # starting point
    >>> battery = CarBattery()
    >>> battery
    CarBattery()
    >>> battery.json()
    '{"latest_charge": "1982-02-17T00:00:00", "expiration": "1982-02-17T00:00:00"}'

    >>> # first_charge is not assigned a default value. Once one is assigned, it can be deleted,
    >>> #   but it can't be made null.
    >>> battery.first_charge = isoparse('2016-03-23')
    >>> battery
    CarBattery(first_charge=datetime.datetime(2016, 3, 23, 0, 0))
    >>> battery.first_charge = None
    Traceback (most recent call last):
    ValidationError: Value for first_charge not given or invalid.
    >>> del battery.first_charge
    >>> battery
    CarBattery()

    >>> # latest_charge can be null, but it can't be deleted. The default value is a callable.
    >>> del battery.latest_charge
    Traceback (most recent call last):
    AttributeError: The latest_charge field is required and cannot be deleted.
    >>> battery.latest_charge = None
    >>> battery.json()
    '{"latest_charge": null, "expiration": "1982-02-17T00:00:00"}'

    >>> # expiration is assigned by default, can't be made null, but can be deleted.
    >>> battery.expiration
    datetime.datetime(1982, 2, 17, 0, 0)
    >>> battery.expiration = None
    Traceback (most recent call last):
    ValidationError: Value for expiration not given or invalid.
    >>> del battery.expiration
    >>> battery.json()
    '{"latest_charge": null}'


"""

from collections.abc import Mapping, Sequence
from datetime import datetime
from enum import Enum
from functools import reduce
from json import JSONEncoder, dumps as json_dumps, loads as json_loads
from logging import getLogger
from pathlib import Path

from boltons.timeutils import isoparse

from . import NULL
from .compat import isiterable, odict
from .collection import AttrDict
from .exceptions import Raise, ValidationError
from .ish import find_or_raise
from .logz import DumpEncoder
from .type_coercion import maybecall

try:
    from frozendict import deepfreeze, frozendict
    from frozendict import getFreezeConversionMap as _getFreezeConversionMap
    from frozendict import register as _register

    if Enum not in _getFreezeConversionMap():
        # leave enums as is, deepfreeze will flatten it into a dict
        # see https://github.com/Marco-Sulla/python-frozendict/issues/98
        _register(Enum, lambda x : x)

    del _getFreezeConversionMap
    del _register
except ImportError:
    from .._vendor.frozendict import frozendict
    from ..auxlib.collection import make_immutable as deepfreeze

log = getLogger(__name__)

__all__ = [
    "Entity", "ImmutableEntity", "Field",
    "BooleanField", "BoolField", "IntegerField", "IntField",
    "NumberField", "StringField", "DateField",
    "EnumField", "ListField", "MapField", "ComposableField",
]

KEY_OVERRIDES_MAP = "__key_overrides__"


NOTES = """

Current deficiencies to schematics:
  - no get_mock_object method
  - no context-dependent serialization or MultilingualStringType
  - name = StringType(serialized_name='person_name', alternate_names=['human_name'])
  - name = StringType(serialize_when_none=False)
  - more flexible validation error messages
  - field validation can depend on other fields
  - 'roles' containing denylists for .dump() and .json()
    __roles__ = {
        EntityRole.registered_name: Denylist('field1', 'field2'),
        EntityRole.another_registered_name: Allowlist('field3', 'field4'),
    }


TODO:
  - alternate field names
  - add dump_if_null field option
  - add help/description parameter to Field
  - consider leveraging slots
  - collect all validation errors before raising
  - Allow returning string error message for validation instead of False
  - profile and optimize
  - use boltons instead of dateutil
  - correctly implement copy and deepcopy on fields and Entity, DictSafeMixin
    http://stackoverflow.com/questions/1500718/what-is-the-right-way-to-override-the-copy-deepcopy-operations-on-an-object-in-p


Optional Field Properties:
  - validation = None
  - default = None
  - required = True
  - in_dump = True
  - nullable = False

Behaviors:
  - Nullable is a "hard" setting, in that the value is either always or never allowed to be None.
  - What happens then if required=False and nullable=False?
      - The object can be init'd without a value (though not with a None value).
        getattr throws AttributeError
      - Any assignment must be not None.


  - Setting a value to None doesn't "unset" a value.  (That's what del is for.)  And you can't
    del a value if required=True, nullable=False, default=None.

  - If a field is not required, del does *not* "unmask" the default value.  Instead, del
    removes the value from the object entirely.  To get back the default value, need to recreate
    the object.  Entity.from_objects(old_object)


  - Disabling in_dump is a "hard" setting, in that with it disabled the field will never get
    dumped.  With it enabled, the field may or may not be dumped depending on its value and other
    settings.

  - Required is a "hard" setting, in that if True, a valid value or default must be provided. None
    is only a valid value or default if nullable is True.

  - In general, nullable means that None is a valid value.
    - getattr returns None instead of raising Attribute error
    - If in_dump, field is given with null value.
    - If default is not None, assigning None clears a previous assignment. Future getattrs return
      the default value.
    - What does nullable mean with default=None and required=True? Does instantiation raise
      an error if assignment not made on init? Can IntField(nullable=True) be init'd?

  - If required=False and nullable=False, field will only be in dump if field!=None.
    Also, getattr raises AttributeError.
  - If required=False and nullable=True, field will be in dump if field==None.

  - If in_dump is True, does default value get dumped:
    - if no assignment, default exists
    - if nullable, and assigned None
  - How does optional validation work with nullable and assigning None?
  - When does gettattr throw AttributeError, and when does it return None?



"""


class Field:
    """
    Fields are doing something very similar to boxing and unboxing
    of c#/java primitives.  __set__ should take a "primitive" or "raw" value and create a "boxed"
    or "programmatically usable" value of it.  While __get__ should return the boxed value,
    dump in turn should unbox the value into a primitive or raw value.

    Arguments:
        types_ (primitive literal or type or sequence of types):
        default (any, callable, optional):  If default is callable, it's guaranteed to return a
            valid value at the time of Entity creation.
        required (boolean, optional):
        validation (callable, optional):
        dump (boolean, optional):
    """

    # Used to track order of field declarations. Supporting python 2.7, so can't rely
    #   on __prepare__.  Strategy lifted from http://stackoverflow.com/a/4460034/2127762
    _order_helper = 0

    def __init__(self, default=NULL, required=True, validation=None,
                 in_dump=True, default_in_dump=True, nullable=False, immutable=False, aliases=()):
        self._required = required
        self._validation = validation
        self._in_dump = in_dump
        self._default_in_dump = default_in_dump
        self._nullable = nullable
        self._immutable = immutable
        self._aliases = aliases
        if default is NULL:
            self._default = NULL
        else:
            self._default = default if callable(default) else self.box(None, None, default)
            self.validate(None, self.box(None, None, maybecall(default)))

        self._order_helper = Field._order_helper
        Field._order_helper += 1

    @property
    def name(self):
        try:
            return self._name
        except AttributeError:
            log.error("The name attribute has not been set for this field. "
                      "Call set_name at class creation time.")
            raise

    def set_name(self, name):
        self._name = name
        return self

    def __get__(self, instance, instance_type):
        try:
            if instance is None:  # if calling from the class object
                val = getattr(instance_type, KEY_OVERRIDES_MAP)[self.name]
            else:
                val = instance.__dict__[self.name]
        except AttributeError:
            log.error("The name attribute has not been set for this field.")
            raise AttributeError("The name attribute has not been set for this field.")
        except KeyError:
            if self.default is NULL:
                raise AttributeError(f"A value for {self.name} has not been set")
            else:
                val = maybecall(self.default)  # default *can* be a callable
        if val is None and not self.nullable:
            # means the "tricky edge case" was activated in __delete__
            raise AttributeError(f"The {self.name} field has been deleted.")
        return self.unbox(instance, instance_type, val)

    def __set__(self, instance, val):
        if self.immutable and instance._initd:
            raise AttributeError(f"The {self.name} field is immutable.")
        # validate will raise an exception if invalid
        # validate will return False if the value should be removed
        instance.__dict__[self.name] = self.validate(
            instance,
            self.box(instance, instance.__class__, val),
        )

    def __delete__(self, instance):
        if self.immutable and instance._initd:
            raise AttributeError(f"The {self.name} field is immutable.")
        elif self.required:
            raise AttributeError(f"The {self.name} field is required and cannot be deleted.")
        elif not self.nullable:
            # tricky edge case
            # given a field Field(default='some value', required=False, nullable=False)
            # works together with Entity.dump() logic for selecting fields to include in dump
            # `if value is not None or field.nullable`
            instance.__dict__[self.name] = None
        else:
            instance.__dict__.pop(self.name, None)

    def box(self, instance, instance_type, val):
        return val

    def unbox(self, instance, instance_type, val):
        return val

    def dump(self, instance, instance_type, val):
        return val

    def validate(self, instance, val):
        """

        Returns:
            True: if val is valid

        Raises:
            ValidationError
        """
        # note here calling, but not assigning; could lead to unexpected behavior
        if isinstance(val, self._type) and (self._validation is None or self._validation(val)):
            return val
        elif val is NULL and not self.required:
            return val
        elif val is None and self.nullable:
            return val
        else:
            raise ValidationError(getattr(self, 'name', 'undefined name'), val)

    @property
    def required(self):
        return self._required

    @property
    def type(self):
        return self._type

    @property
    def default(self):
        return self._default

    @property
    def in_dump(self):
        return self._in_dump

    @property
    def default_in_dump(self):
        return self._default_in_dump

    @property
    def nullable(self):
        return self.is_nullable

    @property
    def is_nullable(self):
        return self._nullable

    @property
    def immutable(self):
        return self._immutable


class BooleanField(Field):
    _type = bool

    def box(self, instance, instance_type, val):
        return None if val is None else bool(val)


BoolField = BooleanField


class IntegerField(Field):
    _type = int


IntField = IntegerField


class NumberField(Field):
    _type = (int, float, complex)


class StringField(Field):
    _type = str

    def box(self, instance, instance_type, val):
        return str(val) if isinstance(val, NumberField._type) else val


class DateField(Field):
    _type = datetime

    def box(self, instance, instance_type, val):
        try:
            return isoparse(val) if isinstance(val, str) else val
        except ValueError as e:
            raise ValidationError(val, msg=e)

    def dump(self, instance, instance_type, val):
        return None if val is None else val.isoformat()


class EnumField(Field):

    def __init__(self, enum_class, default=NULL, required=True, validation=None,
                 in_dump=True, default_in_dump=True, nullable=False, immutable=False, aliases=()):
        if not issubclass(enum_class, Enum):
            raise ValidationError(None, msg="enum_class must be an instance of Enum")
        self._type = enum_class
        super().__init__(
            default, required, validation, in_dump, default_in_dump, nullable, immutable, aliases
        )

    def box(self, instance, instance_type, val):
        if val is None:
            # let the required/nullable logic handle validation for this case
            return None
        try:
            # try to box using val as an Enum name
            return self._type(val)
        except ValueError as e1:
            try:
                # try to box using val as an Enum value
                return self._type[val]
            except KeyError:
                raise ValidationError(val, msg=e1)

    def dump(self, instance, instance_type, val):
        return None if val in (None, NULL) else val.value


class ListField(Field):
    _type = tuple

    def __init__(self, element_type, default=NULL, required=True, validation=None,
                 in_dump=True, default_in_dump=True, nullable=False, immutable=False, aliases=()):
        self._element_type = element_type
        super().__init__(
            default, required, validation, in_dump, default_in_dump, nullable, immutable, aliases
        )

    def box(self, instance, instance_type, val):
        if val is None:
            return None
        elif isinstance(val, str):
            raise ValidationError(
                f"Attempted to assign a string to ListField {self.name}"
            )
        elif isiterable(val):
            et = self._element_type
            if isinstance(et, type) and issubclass(et, Entity):
                return self._type(v if isinstance(v, et) else et(**v) for v in val)
            else:
                return deepfreeze(val) if self.immutable else self._type(val)
        else:
            raise ValidationError(
                val, msg=f"Cannot assign a non-iterable value to {self.name}"
            )

    def unbox(self, instance, instance_type, val):
        return self._type() if val is None and not self.nullable else val

    def dump(self, instance, instance_type, val):
        if isinstance(self._element_type, type) and issubclass(self._element_type, Entity):
            return self._type(v.dump() for v in val)
        else:
            return val

    def validate(self, instance, val):
        val = super().validate(instance, val)
        if val:
            et = self._element_type
            self._type(Raise(ValidationError(self.name, el, et)) for el in val
                       if not isinstance(el, et))
        return val


class MutableListField(ListField):
    _type = list


class MapField(Field):
    _type = frozendict

    def __init__(
        self,
        default=NULL,
        required=True,
        validation=None,
        in_dump=True,
        default_in_dump=True,
        nullable=False,
        immutable=True,
        aliases=(),
    ):
        super().__init__(
            default, required, validation, in_dump, default_in_dump, nullable, immutable, aliases
        )

    def box(self, instance, instance_type, val):
        # TODO: really need to make this recursive to make any lists or maps immutable
        if val is None:
            return self._type()
        elif isiterable(val):
            val = deepfreeze(val)
            if not isinstance(val, Mapping):
                raise ValidationError(
                    val, msg=f"Cannot assign a non-iterable value to {self.name}"
                )
            return val
        else:
            raise ValidationError(
                val, msg=f"Cannot assign a non-iterable value to {self.name}"
            )


class ComposableField(Field):

    def __init__(self, field_class, default=NULL, required=True, validation=None,
                 in_dump=True, default_in_dump=True, nullable=False, immutable=False, aliases=()):
        self._type = field_class
        super().__init__(
            default, required, validation, in_dump, default_in_dump, nullable, immutable, aliases
        )

    def box(self, instance, instance_type, val):
        if val is None:
            return None
        if isinstance(val, self._type):
            return val
        else:
            # assuming val is a dict now
            try:
                # if there is a key named 'self', have to rename it
                if hasattr(val, 'pop'):
                    val['slf'] = val.pop('self')
            except KeyError:
                pass  # no key of 'self', so no worries
            if isinstance(val, self._type):
                return val if isinstance(val, self._type) else self._type(**val)
            elif isinstance(val, Mapping):
                return self._type(**val)
            elif isinstance(val, Sequence) and not isinstance(val, str):
                return self._type(*val)
            else:
                return self._type(val)

    def dump(self, instance, instance_type, val):
        return None if val is None else val.dump()


class EntityType(type):

    @staticmethod
    def __get_entity_subclasses(bases):
        try:
            return [base for base in bases if issubclass(base, Entity) and base is not Entity]
        except NameError:
            # NameError: global name 'Entity' is not defined
            return ()

    def __new__(mcs, name, bases, dct):
        # if we're about to mask a field that's already been created with something that's
        #  not a field, then assign it to an alternate variable name
        non_field_keys = (
            key
            for key, value in dct.items()
            if not isinstance(value, Field) and not key.startswith("__")
        )
        entity_subclasses = EntityType.__get_entity_subclasses(bases)
        if entity_subclasses:
            keys_to_override = [key for key in non_field_keys
                                if any(isinstance(base.__dict__.get(key), Field)
                                       for base in entity_subclasses)]
            dct[KEY_OVERRIDES_MAP] = {key: dct.pop(key) for key in keys_to_override}
        else:
            dct[KEY_OVERRIDES_MAP] = {}

        return super().__new__(mcs, name, bases, dct)

    def __init__(cls, name, bases, attr):
        super().__init__(name, bases, attr)

        fields = odict()
        _field_sort_key = lambda x: x[1]._order_helper
        for clz in reversed(type.mro(cls)):
            clz_fields = (
                (name, field.set_name(name))
                for name, field in clz.__dict__.items()
                if isinstance(field, Field)
            )
            fields.update(sorted(clz_fields, key=_field_sort_key))

        cls.__fields__ = frozendict(fields)
        if hasattr(cls, '__register__'):
            cls.__register__()

    def __call__(cls, *args, **kwargs):
        instance = super().__call__(*args, **kwargs)
        setattr(instance, f"_{cls.__name__}__initd", True)
        return instance

    @property
    def fields(cls):
        return cls.__fields__.keys()


class Entity(metaclass=EntityType):
    __fields__ = odict()
    _lazy_validate = False

    def __init__(self, **kwargs):
        for key, field in self.__fields__.items():
            try:
                setattr(self, key, kwargs[key])
            except KeyError:
                alias = next((ls for ls in field._aliases if ls in kwargs), None)
                if alias is not None:
                    setattr(self, key, kwargs[alias])
                elif key in getattr(self, KEY_OVERRIDES_MAP):
                    # handle case of fields inherited from subclass but overrode on class object
                    setattr(self, key, getattr(self, KEY_OVERRIDES_MAP)[key])
                elif field.required and field.default is NULL:
                    raise ValidationError(
                        key,
                        msg="{} requires a {} field. Instantiated with "
                        "{}".format(self.__class__.__name__, key, kwargs),
                    )
            except ValidationError:
                if kwargs[key] is not None or field.required:
                    raise
        if not self._lazy_validate:
            self.validate()

    @classmethod
    def from_objects(cls, *objects, **override_fields):
        init_vars = {}
        search_maps = tuple(AttrDict(o) if isinstance(o, dict) else o
                            for o in ((override_fields,) + objects))
        for key, field in cls.__fields__.items():
            try:
                init_vars[key] = find_or_raise(key, search_maps, field._aliases)
            except AttributeError:
                pass

        return cls(**init_vars)

    @classmethod
    def from_json(cls, json_str):
        return cls(**json_loads(json_str))

    @classmethod
    def load(cls, data_dict):
        return cls(**data_dict)

    def validate(self):
        # TODO: here, validate should only have to determine if the required keys are set
        try:
            reduce(
                lambda _, name: getattr(self, name),
                (name for name, field in self.__fields__.items() if field.required),
            )
        except TypeError as e:
            if str(e) == "reduce() of empty sequence with no initial value":
                pass
        except AttributeError as e:
            raise ValidationError(None, msg=e)

    def __repr__(self):
        def _valid(key):
            # TODO: re-enable once aliases are implemented
            # if key.startswith('_'):
            #     return False
            if '__' in key:
                return False
            try:
                getattr(self, key)
                return True
            except AttributeError:
                return False

        def _val(key):
            val = getattr(self, key)
            return repr(val.value) if isinstance(val, Enum) else repr(val)

        def _sort_helper(key):
            field = self.__fields__.get(key)
            return field._order_helper if field is not None else -1

        kwarg_str = ", ".join(
            f"{key}={_val(key)}" for key in sorted(self.__dict__, key=_sort_helper) if _valid(key)
        )
        return f"{self.__class__.__name__}({kwarg_str})"

    @classmethod
    def __register__(cls):
        pass

    def json(self, indent=None, separators=None, **kwargs):
        return json_dumps(self, indent=indent, separators=separators, cls=DumpEncoder, **kwargs)

    def pretty_json(self, indent=2, separators=(',', ': '), **kwargs):
        return self.json(indent=indent, separators=separators, **kwargs)

    def dump(self):
        return odict((field.name, field.dump(self, self.__class__, value))
                     for field, value in ((field, getattr(self, field.name, NULL))
                                          for field in self.__dump_fields())
                     if value is not NULL and not (value is field.default
                                                   and not field.default_in_dump))

    @classmethod
    def __dump_fields(cls):
        if "__dump_fields_cache" not in cls.__dict__:
            cls.__dump_fields_cache = tuple(
                field for field in cls.__fields__.values() if field.in_dump
            )
        return cls.__dump_fields_cache

    def __eq__(self, other):
        if self.__class__ != other.__class__:
            return False
        rando_default = 19274656290  # need an arbitrary but definite value if field does not exist
        return all(getattr(self, field, rando_default) == getattr(other, field, rando_default)
                   for field in self.__fields__)

    def __hash__(self):
        return sum(hash(getattr(self, field, None)) for field in self.__fields__)

    @property
    def _initd(self):
        return getattr(self, f"_{self.__class__.__name__}__initd", None)


class ImmutableEntity(Entity):

    def __setattr__(self, attribute, value):
        if self._initd:
            raise AttributeError(
                f"Assignment not allowed. {self.__class__.__name__} is immutable."
            )
        super().__setattr__(attribute, value)

    def __delattr__(self, item):
        if self._initd:
            raise AttributeError(f"Deletion not allowed. {self.__class__.__name__} is immutable.")
        super().__delattr__(item)


class DictSafeMixin:

    def __getitem__(self, item):
        return getattr(self, item)

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __delitem__(self, key):
        delattr(self, key)

    def get(self, item, default=None):
        return getattr(self, item, default)

    def __contains__(self, item):
        value = getattr(self, item, None)
        if value is None:
            return False
        field = self.__fields__[item]
        if isinstance(field, (MapField, ListField)):
            return len(value) > 0
        return True

    def __iter__(self):
        for key in self.__fields__:
            if key in self:
                yield key

    def items(self):
        for key in self.__fields__:
            if key in self:
                yield key, getattr(self, key)

    def copy(self):
        return self.__class__(**self.dump())

    def setdefault(self, key, default_value):
        if key not in self:
            setattr(self, key, default_value)

    def update(self, E=None, **F):
        # D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
        # If E present and has a .keys() method, does:     for k in E: D[k] = E[k]
        # If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v
        # In either case, this is followed by: for k in F: D[k] = F[k]
        if E is not None:
            try:
                for k, v in E.items():
                    self[k] = v
            except AttributeError:
                for k, v in E:
                    self[k] = v
        for k in F:
            self[k] = F[k]


class EntityEncoder(JSONEncoder):
    # json.dumps(obj, cls=SetEncoder)
    def default(self, obj):
        if hasattr(obj, 'dump'):
            return obj.dump()
        elif hasattr(obj, '__json__'):
            return obj.__json__()
        elif hasattr(obj, 'to_json'):
            return obj.to_json()
        elif hasattr(obj, 'as_json'):
            return obj.as_json()
        elif isinstance(obj, Enum):
            return obj.value
        elif isinstance(obj, Path):
            return str(obj)
        return JSONEncoder.default(self, obj)


from collections import OrderedDict as odict  # noqa: F401
import os
from shlex import split

from ..deprecations import deprecated


deprecated.constant("24.3", "24.9", "NoneType", type(None))
deprecated.constant("24.3", "24.9", "primitive_types", (str, int, float, complex, bool, type(None)))


def isiterable(obj):
    # and not a string
    from collections.abc import Iterable
    return not isinstance(obj, str) and isinstance(obj, Iterable)


# shlex.split() is a poor function to use for anything general purpose (like calling subprocess).
# It mishandles Unicode in Python 3 but all is not lost. We can escape it, then escape the escapes
# then call shlex.split() then un-escape that.
def shlex_split_unicode(to_split, posix=True):
    # shlex.split does its own un-escaping that we must counter.
    e_to_split = to_split.replace("\\", "\\\\")
    return split(e_to_split, posix=posix)


@deprecated("24.3", "24.9")
def utf8_writer(fp):
    return fp


def Utf8NamedTemporaryFile(
    mode="w+b", buffering=-1, newline=None, suffix=None, prefix=None, dir=None, delete=True
):
    from tempfile import NamedTemporaryFile

    if "CONDA_TEST_SAVE_TEMPS" in os.environ:
        delete = False
    encoding = None
    if "b" not in mode:
        encoding = "utf-8"
    return NamedTemporaryFile(
        mode=mode,
        buffering=buffering,
        encoding=encoding,
        newline=newline,
        suffix=suffix,
        prefix=prefix,
        dir=dir,
        delete=delete,
    )


#!/usr/bin/env python
# Copyright (c) 2005-2010 ActiveState Software Inc.

"""Utilities for determining application-specific dirs.

See <http://github.com/ActiveState/appdirs> for details and usage.
"""
# Dev Notes:
# - MSDN on where to store app data files:
#   http://support.microsoft.com/default.aspx?scid=kb;en-us;310294#XSLTH3194121123120121120120
# - Mac OS X: http://developer.apple.com/documentation/MacOSX/Conceptual/BPFileSystem/index.html
# - XDG spec for Un*x: http://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html
from ..deprecations import deprecated
deprecated.module("24.3", "24.9", addendum="Use `platformdirs` instead.")


__version_info__ = (1, 2, 0)
__version__ = '.'.join(map(str, __version_info__))


import sys
import os

PY3 = sys.version_info[0] == 3

if PY3:
    unicode = str

class AppDirsError(Exception):
    pass



def user_data_dir(appname, appauthor=None, version=None, roaming=False):
    r"""Return full path to the user-specific data dir for this application.

        "appname" is the name of application.
        "appauthor" (only required and used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".
        "roaming" (boolean, default False) can be set True to use the Windows
            roaming appdata directory. That means that for users on a Windows
            network setup for roaming profiles, this user data will be
            sync'd on login. See
            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>
            for a discussion of issues.

    Typical user data directories are:
        Mac OS X:               ~/Library/Application Support/<AppName>
        Unix:                   ~/.config/<appname>    # or in $XDG_CONFIG_HOME if defined
        Win XP (not roaming):   C:\Documents and Settings\<username>\Application Data\<AppAuthor>\<AppName>
        Win XP (roaming):       C:\Documents and Settings\<username>\Local Settings\Application Data\<AppAuthor>\<AppName>
        Win 7  (not roaming):   C:\Users\<username>\AppData\Local\<AppAuthor>\<AppName>
        Win 7  (roaming):       C:\Users\<username>\AppData\Roaming\<AppAuthor>\<AppName>

    For Unix, we follow the XDG spec and support $XDG_CONFIG_HOME. We don't
    use $XDG_DATA_HOME as that data dir is mostly used at the time of
    installation, instead of the application adding data during runtime.
    Also, in practice, Linux apps tend to store their data in
    "~/.config/<appname>" instead of "~/.local/share/<appname>".
    """
    if sys.platform.startswith("win"):
        if appauthor is None:
            raise AppDirsError("must specify 'appauthor' on Windows")
        const = roaming and "CSIDL_APPDATA" or "CSIDL_LOCAL_APPDATA"
        path = os.path.join(_get_win_folder(const), appauthor, appname)
    elif sys.platform == 'darwin':
        path = os.path.join(
            os.path.expanduser('~/Library/Application Support/'),
            appname)
    else:
        path = os.path.join(
            os.getenv('XDG_CONFIG_HOME', os.path.expanduser("~/.config")),
            appname.lower())
    if version:
        path = os.path.join(path, version)
    return path


def site_data_dir(appname, appauthor=None, version=None):
    """Return full path to the user-shared data dir for this application.

        "appname" is the name of application.
        "appauthor" (only required and used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".

    Typical user data directories are:
        Mac OS X:   /Library/Application Support/<AppName>
        Unix:       /etc/xdg/<appname>
        Win XP:     C:\Documents and Settings\All Users\Application Data\<AppAuthor>\<AppName>
        Vista:      (Fail! "C:\ProgramData" is a hidden *system* directory on Vista.)
        Win 7:      C:\ProgramData\<AppAuthor>\<AppName>   # Hidden, but writeable on Win 7.

    For Unix, this is using the $XDG_CONFIG_DIRS[0] default.

    WARNING: Do not use this on Windows. See the Vista-Fail note above for why.
    """
    if sys.platform.startswith("win"):
        if appauthor is None:
            raise AppDirsError("must specify 'appauthor' on Windows")
        path = os.path.join(_get_win_folder("CSIDL_COMMON_APPDATA"),
                            appauthor, appname)
    elif sys.platform == 'darwin':
        path = os.path.join(
            os.path.expanduser('/Library/Application Support'),
            appname)
    else:
        # XDG default for $XDG_CONFIG_DIRS[0]. Perhaps should actually
        # *use* that envvar, if defined.
        path = "/etc/xdg/"+appname.lower()
    if version:
        path = os.path.join(path, version)
    return path


def user_cache_dir(appname, appauthor=None, version=None, opinion=True):
    r"""Return full path to the user-specific cache dir for this application.

        "appname" is the name of application.
        "appauthor" (only required and used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".
        "opinion" (boolean) can be False to disable the appending of
            "Cache" to the base app data dir for Windows. See
            discussion below.

    Typical user cache directories are:
        Mac OS X:   ~/Library/Caches/<AppName>
        Unix:       ~/.cache/<appname> (XDG default)
        Win XP:     C:\Documents and Settings\<username>\Local Settings\Application Data\<AppAuthor>\<AppName>\Cache
        Vista:      C:\Users\<username>\AppData\Local\<AppAuthor>\<AppName>\Cache

    On Windows the only suggestion in the MSDN docs is that local settings go in
    the `CSIDL_LOCAL_APPDATA` directory. This is identical to the non-roaming
    app data dir (the default returned by `user_data_dir` above). Apps typically
    put cache data somewhere *under* the given dir here. Some examples:
        ...\Mozilla\Firefox\Profiles\<ProfileName>\Cache
        ...\Acme\SuperApp\Cache\1.0
    OPINION: This function appends "Cache" to the `CSIDL_LOCAL_APPDATA` value.
    This can be disabled with the `opinion=False` option.
    """
    if sys.platform.startswith("win"):
        if appauthor is None:
            raise AppDirsError("must specify 'appauthor' on Windows")
        path = os.path.join(_get_win_folder("CSIDL_LOCAL_APPDATA"),
                            appauthor, appname)
        if opinion:
            path = os.path.join(path, "Cache")
    elif sys.platform == 'darwin':
        path = os.path.join(
            os.path.expanduser('~/Library/Caches'),
            appname)
    else:
        path = os.path.join(
            os.getenv('XDG_CACHE_HOME', os.path.expanduser('~/.cache')),
            appname.lower())
    if version:
        path = os.path.join(path, version)
    return path

def user_log_dir(appname, appauthor=None, version=None, opinion=True):
    r"""Return full path to the user-specific log dir for this application.

        "appname" is the name of application.
        "appauthor" (only required and used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".
        "opinion" (boolean) can be False to disable the appending of
            "Logs" to the base app data dir for Windows, and "log" to the
            base cache dir for Unix. See discussion below.

    Typical user cache directories are:
        Mac OS X:   ~/Library/Logs/<AppName>
        Unix:       ~/.cache/<appname>/log  # or under $XDG_CACHE_HOME if defined
        Win XP:     C:\Documents and Settings\<username>\Local Settings\Application Data\<AppAuthor>\<AppName>\Logs
        Vista:      C:\Users\<username>\AppData\Local\<AppAuthor>\<AppName>\Logs

    On Windows the only suggestion in the MSDN docs is that local settings
    go in the `CSIDL_LOCAL_APPDATA` directory. (Note: I'm interested in
    examples of what some windows apps use for a logs dir.)

    OPINION: This function appends "Logs" to the `CSIDL_LOCAL_APPDATA`
    value for Windows and appends "log" to the user cache dir for Unix.
    This can be disabled with the `opinion=False` option.
    """
    if sys.platform == "darwin":
        path = os.path.join(
            os.path.expanduser('~/Library/Logs'),
            appname)
    elif sys.platform == "win32":
        path = user_data_dir(appname, appauthor, version); version=False
        if opinion:
            path = os.path.join(path, "Logs")
    else:
        path = user_cache_dir(appname, appauthor, version); version=False
        if opinion:
            path = os.path.join(path, "log")
    if version:
        path = os.path.join(path, version)
    return path


class AppDirs(object):
    """Convenience wrapper for getting application dirs."""
    def __init__(self, appname, appauthor, version=None, roaming=False):
        self.appname = appname
        self.appauthor = appauthor
        self.version = version
        self.roaming = roaming
    @property
    def user_data_dir(self):
        return user_data_dir(self.appname, self.appauthor,
            version=self.version, roaming=self.roaming)
    @property
    def site_data_dir(self):
        return site_data_dir(self.appname, self.appauthor,
            version=self.version)
    @property
    def user_cache_dir(self):
        return user_cache_dir(self.appname, self.appauthor,
            version=self.version)
    @property
    def user_log_dir(self):
        return user_log_dir(self.appname, self.appauthor,
            version=self.version)




#---- internal support stuff

def _get_win_folder_from_registry(csidl_name):
    """This is a fallback technique at best. I'm not sure if using the
    registry for this guarantees us the correct answer for all CSIDL_*
    names.
    """
    import _winreg

    shell_folder_name = {
        "CSIDL_APPDATA": "AppData",
        "CSIDL_COMMON_APPDATA": "Common AppData",
        "CSIDL_LOCAL_APPDATA": "Local AppData",
    }[csidl_name]

    key = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER,
        r"Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders")
    dir, type = _winreg.QueryValueEx(key, shell_folder_name)
    return dir

def _get_win_folder_with_pywin32(csidl_name):
    from win32com.shell import shellcon, shell
    dir = shell.SHGetFolderPath(0, getattr(shellcon, csidl_name), 0, 0)
    # Try to make this a unicode path because SHGetFolderPath does
    # not return unicode strings when there is unicode data in the
    # path.
    try:
        dir = unicode(dir)

        # Downgrade to short path name if have highbit chars. See
        # <http://bugs.activestate.com/show_bug.cgi?id=85099>.
        has_high_char = False
        for c in dir:
            if ord(c) > 255:
                has_high_char = True
                break
        if has_high_char:
            try:
                import win32api
                dir = win32api.GetShortPathName(dir)
            except ImportError:
                pass
    except UnicodeError:
        pass
    return dir

def _get_win_folder_with_ctypes(csidl_name):
    import ctypes

    csidl_const = {
        "CSIDL_APPDATA": 26,
        "CSIDL_COMMON_APPDATA": 35,
        "CSIDL_LOCAL_APPDATA": 28,
    }[csidl_name]

    buf = ctypes.create_unicode_buffer(1024)
    ctypes.windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)

    # Downgrade to short path name if have highbit chars. See
    # <http://bugs.activestate.com/show_bug.cgi?id=85099>.
    has_high_char = False
    for c in buf:
        if ord(c) > 255:
            has_high_char = True
            break
    if has_high_char:
        buf2 = ctypes.create_unicode_buffer(1024)
        if ctypes.windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):
            buf = buf2

    return buf.value

if sys.platform == "win32":
    try:
        import win32com.shell
        _get_win_folder = _get_win_folder_with_pywin32
    except ImportError:
        try:
            import ctypes
            _get_win_folder = _get_win_folder_with_ctypes
        except ImportError:
            _get_win_folder = _get_win_folder_from_registry



#---- self test code

if __name__ == "__main__":
    appname = "MyApp"
    appauthor = "MyCompany"

    props = ("user_data_dir", "site_data_dir", "user_cache_dir",
        "user_log_dir")

    print("-- app dirs (without optional 'version')")
    dirs = AppDirs(appname, appauthor, version="1.0")
    for prop in props:
        print("%s: %s" % (prop, getattr(dirs, prop)))

    print("\n-- app dirs (with optional 'version')")
    dirs = AppDirs(appname, appauthor)
    for prop in props:
        print("%s: %s" % (prop, getattr(dirs, prop)))


Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright {yyyy} {name of copyright owner}

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



# -*- coding: utf-8 -*-
"""
Conda's pure-python dependencies will be
`vendored <http://stackoverflow.com/questions/26217488/what-is-vendoring>`_
until conda 5.0 when conda will be isolated in its own private environment.

Introduction of dependencies for the 4.x series is discussed in
https://github.com/conda/conda/issues/2825.
"""


appdirs==1.2.0
py-cpuinfo==9.0.0
distro==1.0.4
frozendict==1.2


# Copyright 2015,2016 Nir Cohen
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
The ``distro`` package (``distro`` stands for Linux Distribution) provides
information about the Linux distribution it runs on, such as a reliable
machine-readable distro ID, or version information.

It is a renewed alternative implementation for Python's original
:py:func:`platform.linux_distribution` function, but it provides much more
functionality. An alternative implementation became necessary because Python
3.5 deprecated this function, and Python 3.7 is expected to remove it
altogether. Its predecessor function :py:func:`platform.dist` was already
deprecated since Python 2.6 and is also expected to be removed in Python 3.7.
Still, there are many cases in which access to Linux distribution information
is needed. See `Python issue 1322 <https://bugs.python.org/issue1322>`_ for
more information.
"""
from ..deprecations import deprecated
deprecated.module("24.3", "24.9", addendum="Use `distro` instead.")

import os
import re
import sys
import json
import shlex
import logging
import argparse
import subprocess


if not sys.platform.startswith('linux'):
    raise ImportError('Unsupported platform: {0}'.format(sys.platform))

_UNIXCONFDIR = os.environ.get('UNIXCONFDIR', '/etc')
_OS_RELEASE_BASENAME = 'os-release'

#: Translation table for normalizing the "ID" attribute defined in os-release
#: files, for use by the :func:`distro.id` method.
#:
#: * Key: Value as defined in the os-release file, translated to lower case,
#:   with blanks translated to underscores.
#:
#: * Value: Normalized value.
NORMALIZED_OS_ID = {}

#: Translation table for normalizing the "Distributor ID" attribute returned by
#: the lsb_release command, for use by the :func:`distro.id` method.
#:
#: * Key: Value as returned by the lsb_release command, translated to lower
#:   case, with blanks translated to underscores.
#:
#: * Value: Normalized value.
NORMALIZED_LSB_ID = {
    'enterpriseenterprise': 'oracle',  # Oracle Enterprise Linux
    'redhatenterpriseworkstation': 'rhel',  # RHEL 6, 7 Workstation
    'redhatenterpriseserver': 'rhel',  # RHEL 6, 7 Server
}

#: Translation table for normalizing the distro ID derived from the file name
#: of distro release files, for use by the :func:`distro.id` method.
#:
#: * Key: Value as derived from the file name of a distro release file,
#:   translated to lower case, with blanks translated to underscores.
#:
#: * Value: Normalized value.
NORMALIZED_DISTRO_ID = {
    'redhat': 'rhel',  # RHEL 6.x, 7.x
}

# Pattern for content of distro release file (reversed)
_DISTRO_RELEASE_CONTENT_REVERSED_PATTERN = re.compile(
    r'(?:[^)]*\)(.*)\()? *(?:STL )?([\d.+\-a-z]*\d) *(?:esaeler *)?(.+)')

# Pattern for base file name of distro release file
_DISTRO_RELEASE_BASENAME_PATTERN = re.compile(
    r'(\w+)[-_](release|version)$')

# Base file names to be ignored when searching for distro release file
_DISTRO_RELEASE_IGNORE_BASENAMES = (
    'debian_version',
    'lsb-release',
    'oem-release',
    _OS_RELEASE_BASENAME,
    'system-release'
)


def linux_distribution(full_distribution_name=True):
    """
    Return information about the current Linux distribution as a tuple
    ``(id_name, version, codename)`` with items as follows:

    * ``id_name``:  If *full_distribution_name* is false, the result of
      :func:`distro.id`. Otherwise, the result of :func:`distro.name`.

    * ``version``:  The result of :func:`distro.version`.

    * ``codename``:  The result of :func:`distro.codename`.

    The interface of this function is compatible with the original
    :py:func:`platform.linux_distribution` function, supporting a subset of
    its parameters.

    The data it returns may not exactly be the same, because it uses more data
    sources than the original function, and that may lead to different data if
    the Linux distribution is not consistent across multiple data sources it
    provides (there are indeed such distributions ...).

    Another reason for differences is the fact that the :func:`distro.id`
    method normalizes the distro ID string to a reliable machine-readable value
    for a number of popular Linux distributions.
    """
    return _distro.linux_distribution(full_distribution_name)


def id():
    """
    Return the distro ID of the current Linux distribution, as a
    machine-readable string.

    For a number of Linux distributions, the returned distro ID value is
    *reliable*, in the sense that it is documented and that it does not change
    across releases of the distribution.

    This package maintains the following reliable distro ID values:

    ==============  =========================================
    Distro ID       Distribution
    ==============  =========================================
    "ubuntu"        Ubuntu
    "debian"        Debian
    "rhel"          RedHat Enterprise Linux
    "centos"        CentOS
    "fedora"        Fedora
    "sles"          SUSE Linux Enterprise Server
    "opensuse"      openSUSE
    "amazon"        Amazon Linux
    "arch"          Arch Linux
    "cloudlinux"    CloudLinux OS
    "exherbo"       Exherbo Linux
    "gentoo"        GenToo Linux
    "ibm_powerkvm"  IBM PowerKVM
    "kvmibm"        KVM for IBM z Systems
    "linuxmint"     Linux Mint
    "mageia"        Mageia
    "mandriva"      Mandriva Linux
    "parallels"     Parallels
    "pidora"        Pidora
    "raspbian"      Raspbian
    "oracle"        Oracle Linux (and Oracle Enterprise Linux)
    "scientific"    Scientific Linux
    "slackware"     Slackware
    "xenserver"     XenServer
    ==============  =========================================

    If you have a need to get distros for reliable IDs added into this set,
    or if you find that the :func:`distro.id` function returns a different
    distro ID for one of the listed distros, please create an issue in the
    `distro issue tracker`_.

    **Lookup hierarchy and transformations:**

    First, the ID is obtained from the following sources, in the specified
    order. The first available and non-empty value is used:

    * the value of the "ID" attribute of the os-release file,

    * the value of the "Distributor ID" attribute returned by the lsb_release
      command,

    * the first part of the file name of the distro release file,

    The so determined ID value then passes the following transformations,
    before it is returned by this method:

    * it is translated to lower case,

    * blanks (which should not be there anyway) are translated to underscores,

    * a normalization of the ID is performed, based upon
      `normalization tables`_. The purpose of this normalization is to ensure
      that the ID is as reliable as possible, even across incompatible changes
      in the Linux distributions. A common reason for an incompatible change is
      the addition of an os-release file, or the addition of the lsb_release
      command, with ID values that differ from what was previously determined
      from the distro release file name.
    """
    return _distro.id()


def name(pretty=False):
    """
    Return the name of the current Linux distribution, as a human-readable
    string.

    If *pretty* is false, the name is returned without version or codename.
    (e.g. "CentOS Linux")

    If *pretty* is true, the version and codename are appended.
    (e.g. "CentOS Linux 7.1.1503 (Core)")

    **Lookup hierarchy:**

    The name is obtained from the following sources, in the specified order.
    The first available and non-empty value is used:

    * If *pretty* is false:

      - the value of the "NAME" attribute of the os-release file,

      - the value of the "Distributor ID" attribute returned by the lsb_release
        command,

      - the value of the "<name>" field of the distro release file.

    * If *pretty* is true:

      - the value of the "PRETTY_NAME" attribute of the os-release file,

      - the value of the "Description" attribute returned by the lsb_release
        command,

      - the value of the "<name>" field of the distro release file, appended
        with the value of the pretty version ("<version_id>" and "<codename>"
        fields) of the distro release file, if available.
    """
    return _distro.name(pretty)


def version(pretty=False, best=False):
    """
    Return the version of the current Linux distribution, as a human-readable
    string.

    If *pretty* is false, the version is returned without codename (e.g.
    "7.0").

    If *pretty* is true, the codename in parenthesis is appended, if the
    codename is non-empty (e.g. "7.0 (Maipo)").

    Some distributions provide version numbers with different precisions in
    the different sources of distribution information. Examining the different
    sources in a fixed priority order does not always yield the most precise
    version (e.g. for Debian 8.2, or CentOS 7.1).

    The *best* parameter can be used to control the approach for the returned
    version:

    If *best* is false, the first non-empty version number in priority order of
    the examined sources is returned.

    If *best* is true, the most precise version number out of all examined
    sources is returned.

    **Lookup hierarchy:**

    In all cases, the version number is obtained from the following sources.
    If *best* is false, this order represents the priority order:

    * the value of the "VERSION_ID" attribute of the os-release file,
    * the value of the "Release" attribute returned by the lsb_release
      command,
    * the version number parsed from the "<version_id>" field of the first line
      of the distro release file,
    * the version number parsed from the "PRETTY_NAME" attribute of the
      os-release file, if it follows the format of the distro release files.
    * the version number parsed from the "Description" attribute returned by
      the lsb_release command, if it follows the format of the distro release
      files.
    """
    return _distro.version(pretty, best)


def version_parts(best=False):
    """
    Return the version of the current Linux distribution as a tuple
    ``(major, minor, build_number)`` with items as follows:

    * ``major``:  The result of :func:`distro.major_version`.

    * ``minor``:  The result of :func:`distro.minor_version`.

    * ``build_number``:  The result of :func:`distro.build_number`.

    For a description of the *best* parameter, see the :func:`distro.version`
    method.
    """
    return _distro.version_parts(best)


def major_version(best=False):
    """
    Return the major version of the current Linux distribution, as a string,
    if provided.
    Otherwise, the empty string is returned. The major version is the first
    part of the dot-separated version string.

    For a description of the *best* parameter, see the :func:`distro.version`
    method.
    """
    return _distro.major_version(best)


def minor_version(best=False):
    """
    Return the minor version of the current Linux distribution, as a string,
    if provided.
    Otherwise, the empty string is returned. The minor version is the second
    part of the dot-separated version string.

    For a description of the *best* parameter, see the :func:`distro.version`
    method.
    """
    return _distro.minor_version(best)


def build_number(best=False):
    """
    Return the build number of the current Linux distribution, as a string,
    if provided.
    Otherwise, the empty string is returned. The build number is the third part
    of the dot-separated version string.

    For a description of the *best* parameter, see the :func:`distro.version`
    method.
    """
    return _distro.build_number(best)


def like():
    """
    Return a space-separated list of distro IDs of distributions that are
    closely related to the current Linux distribution in regards to packaging
    and programming interfaces, for example distributions the current
    distribution is a derivative from.

    **Lookup hierarchy:**

    This information item is only provided by the os-release file.
    For details, see the description of the "ID_LIKE" attribute in the
    `os-release man page
    <http://www.freedesktop.org/software/systemd/man/os-release.html>`_.
    """
    return _distro.like()


def codename():
    """
    Return the codename for the release of the current Linux distribution,
    as a string.

    If the distribution does not have a codename, an empty string is returned.

    Note that the returned codename is not always really a codename. For
    example, openSUSE returns "x86_64". This function does not handle such
    cases in any special way and just returns the string it finds, if any.

    **Lookup hierarchy:**

    * the codename within the "VERSION" attribute of the os-release file, if
      provided,

    * the value of the "Codename" attribute returned by the lsb_release
      command,

    * the value of the "<codename>" field of the distro release file.
    """
    return _distro.codename()


def info(pretty=False, best=False):
    """
    Return certain machine-readable information items about the current Linux
    distribution in a dictionary, as shown in the following example:

    .. sourcecode:: python

        {
            'id': 'rhel',
            'version': '7.0',
            'version_parts': {
                'major': '7',
                'minor': '0',
                'build_number': ''
            },
            'like': 'fedora',
            'codename': 'Maipo'
        }

    The dictionary structure and keys are always the same, regardless of which
    information items are available in the underlying data sources. The values
    for the various keys are as follows:

    * ``id``:  The result of :func:`distro.id`.

    * ``version``:  The result of :func:`distro.version`.

    * ``version_parts -> major``:  The result of :func:`distro.major_version`.

    * ``version_parts -> minor``:  The result of :func:`distro.minor_version`.

    * ``version_parts -> build_number``:  The result of
      :func:`distro.build_number`.

    * ``like``:  The result of :func:`distro.like`.

    * ``codename``:  The result of :func:`distro.codename`.

    For a description of the *pretty* and *best* parameters, see the
    :func:`distro.version` method.
    """
    return _distro.info(pretty, best)


def os_release_info():
    """
    Return a dictionary containing key-value pairs for the information items
    from the os-release file data source of the current Linux distribution.

    See `os-release file`_ for details about these information items.
    """
    return _distro.os_release_info()


def lsb_release_info():
    """
    Return a dictionary containing key-value pairs for the information items
    from the lsb_release command data source of the current Linux distribution.

    See `lsb_release command output`_ for details about these information
    items.
    """
    return _distro.lsb_release_info()


def distro_release_info():
    """
    Return a dictionary containing key-value pairs for the information items
    from the distro release file data source of the current Linux distribution.

    See `distro release file`_ for details about these information items.
    """
    return _distro.distro_release_info()


def os_release_attr(attribute):
    """
    Return a single named information item from the os-release file data source
    of the current Linux distribution.

    Parameters:

    * ``attribute`` (string): Key of the information item.

    Returns:

    * (string): Value of the information item, if the item exists.
      The empty string, if the item does not exist.

    See `os-release file`_ for details about these information items.
    """
    return _distro.os_release_attr(attribute)


def lsb_release_attr(attribute):
    """
    Return a single named information item from the lsb_release command output
    data source of the current Linux distribution.

    Parameters:

    * ``attribute`` (string): Key of the information item.

    Returns:

    * (string): Value of the information item, if the item exists.
      The empty string, if the item does not exist.

    See `lsb_release command output`_ for details about these information
    items.
    """
    return _distro.lsb_release_attr(attribute)


def distro_release_attr(attribute):
    """
    Return a single named information item from the distro release file
    data source of the current Linux distribution.

    Parameters:

    * ``attribute`` (string): Key of the information item.

    Returns:

    * (string): Value of the information item, if the item exists.
      The empty string, if the item does not exist.

    See `distro release file`_ for details about these information items.
    """
    return _distro.distro_release_attr(attribute)


class LinuxDistribution(object):
    """
    Provides information about a Linux distribution.

    This package creates a private module-global instance of this class with
    default initialization arguments, that is used by the
    `consolidated accessor functions`_ and `single source accessor functions`_.
    By using default initialization arguments, that module-global instance
    returns data about the current Linux distribution (i.e. the distro this
    package runs on).

    Normally, it is not necessary to create additional instances of this class.
    However, in situations where control is needed over the exact data sources
    that are used, instances of this class can be created with a specific
    distro release file, or a specific os-release file, or without invoking the
    lsb_release command.
    """

    def __init__(self,
                 include_lsb=True,
                 os_release_file='',
                 distro_release_file=''):
        """
        The initialization method of this class gathers information from the
        available data sources, and stores that in private instance attributes.
        Subsequent access to the information items uses these private instance
        attributes, so that the data sources are read only once.

        Parameters:

        * ``include_lsb`` (bool): Controls whether the
          `lsb_release command output`_ is included as a data source.

          If the lsb_release command is not available in the program execution
          path, the data source for the lsb_release command will be empty.

        * ``os_release_file`` (string): The path name of the
          `os-release file`_ that is to be used as a data source.

          An empty string (the default) will cause the default path name to
          be used (see `os-release file`_ for details).

          If the specified or defaulted os-release file does not exist, the
          data source for the os-release file will be empty.

        * ``distro_release_file`` (string): The path name of the
          `distro release file`_ that is to be used as a data source.

          An empty string (the default) will cause a default search algorithm
          to be used (see `distro release file`_ for details).

          If the specified distro release file does not exist, or if no default
          distro release file can be found, the data source for the distro
          release file will be empty.

        Public instance attributes:

        * ``os_release_file`` (string): The path name of the
          `os-release file`_ that is actually used as a data source. The
          empty string if no distro release file is used as a data source.

        * ``distro_release_file`` (string): The path name of the
          `distro release file`_ that is actually used as a data source. The
          empty string if no distro release file is used as a data source.

        Raises:

        * :py:exc:`IOError`: Some I/O issue with an os-release file or distro
          release file.

        * :py:exc:`subprocess.CalledProcessError`: The lsb_release command had
          some issue (other than not being available in the program execution
          path).

        * :py:exc:`UnicodeError`: A data source has unexpected characters or
          uses an unexpected encoding.
        """
        self.os_release_file = os_release_file or \
            os.path.join(_UNIXCONFDIR, _OS_RELEASE_BASENAME)
        self.distro_release_file = distro_release_file or ''  # updated later
        self._os_release_info = self._get_os_release_info()
        self._lsb_release_info = self._get_lsb_release_info() \
            if include_lsb else {}
        self._distro_release_info = self._get_distro_release_info()

    def __repr__(self):
        """Return repr of all info
        """
        return \
            "LinuxDistribution(" \
            "os_release_file={0!r}, " \
            "distro_release_file={1!r}, " \
            "_os_release_info={2!r}, " \
            "_lsb_release_info={3!r}, " \
            "_distro_release_info={4!r})".format(
                self.os_release_file,
                self.distro_release_file,
                self._os_release_info,
                self._lsb_release_info,
                self._distro_release_info)

    def linux_distribution(self, full_distribution_name=True):
        """
        Return information about the Linux distribution that is compatible
        with Python's :func:`platform.linux_distribution`, supporting a subset
        of its parameters.

        For details, see :func:`distro.linux_distribution`.
        """
        return (
            self.name() if full_distribution_name else self.id(),
            self.version(),
            self.codename()
        )

    def id(self):
        """Return the distro ID of the Linux distribution, as a string.

        For details, see :func:`distro.id`.
        """
        def normalize(distro_id, table):
            distro_id = distro_id.lower().replace(' ', '_')
            return table.get(distro_id, distro_id)

        distro_id = self.os_release_attr('id')
        if distro_id:
            return normalize(distro_id, NORMALIZED_OS_ID)

        distro_id = self.lsb_release_attr('distributor_id')
        if distro_id:
            return normalize(distro_id, NORMALIZED_LSB_ID)

        distro_id = self.distro_release_attr('id')
        if distro_id:
            return normalize(distro_id, NORMALIZED_DISTRO_ID)

        return ''

    def name(self, pretty=False):
        """
        Return the name of the Linux distribution, as a string.

        For details, see :func:`distro.name`.
        """
        name = self.os_release_attr('name') \
            or self.lsb_release_attr('distributor_id') \
            or self.distro_release_attr('name')
        if pretty:
            name = self.os_release_attr('pretty_name') \
                or self.lsb_release_attr('description')
            if not name:
                name = self.distro_release_attr('name')
                version = self.version(pretty=True)
                if version:
                    name = name + ' ' + version
        return name or ''

    def version(self, pretty=False, best=False):
        """
        Return the version of the Linux distribution, as a string.

        For details, see :func:`distro.version`.
        """
        versions = [
            self.os_release_attr('version_id'),
            self.lsb_release_attr('release'),
            self.distro_release_attr('version_id'),
            self._parse_distro_release_content(
                self.os_release_attr('pretty_name')).get('version_id', ''),
            self._parse_distro_release_content(
                self.lsb_release_attr('description')).get('version_id', '')
        ]
        version = ''
        if best:
            # This algorithm uses the last version in priority order that has
            # the best precision. If the versions are not in conflict, that
            # does not matter; otherwise, using the last one instead of the
            # first one might be considered a surprise.
            for v in versions:
                if v.count(".") > version.count(".") or version == '':
                    version = v
        else:
            for v in versions:
                if v != '':
                    version = v
                    break
        if pretty and version and self.codename():
            version = u'{0} ({1})'.format(version, self.codename())
        return version

    def version_parts(self, best=False):
        """
        Return the version of the Linux distribution, as a tuple of version
        numbers.

        For details, see :func:`distro.version_parts`.
        """
        version_str = self.version(best=best)
        if version_str:
            version_regex = re.compile(r'(\d+)\.?(\d+)?\.?(\d+)?')
            matches = version_regex.match(version_str)
            if matches:
                major, minor, build_number = matches.groups()
                return major, minor or '', build_number or ''
        return '', '', ''

    def major_version(self, best=False):
        """
        Return the major version number of the current distribution.

        For details, see :func:`distro.major_version`.
        """
        return self.version_parts(best)[0]

    def minor_version(self, best=False):
        """
        Return the minor version number of the Linux distribution.

        For details, see :func:`distro.minor_version`.
        """
        return self.version_parts(best)[1]

    def build_number(self, best=False):
        """
        Return the build number of the Linux distribution.

        For details, see :func:`distro.build_number`.
        """
        return self.version_parts(best)[2]

    def like(self):
        """
        Return the IDs of distributions that are like the Linux distribution.

        For details, see :func:`distro.like`.
        """
        return self.os_release_attr('id_like') or ''

    def codename(self):
        """
        Return the codename of the Linux distribution.

        For details, see :func:`distro.codename`.
        """
        return self.os_release_attr('codename') \
            or self.lsb_release_attr('codename') \
            or self.distro_release_attr('codename') \
            or ''

    def info(self, pretty=False, best=False):
        """
        Return certain machine-readable information about the Linux
        distribution.

        For details, see :func:`distro.info`.
        """
        return dict(
            id=self.id(),
            version=self.version(pretty, best),
            version_parts=dict(
                major=self.major_version(best),
                minor=self.minor_version(best),
                build_number=self.build_number(best)
            ),
            like=self.like(),
            codename=self.codename(),
        )

    def os_release_info(self):
        """
        Return a dictionary containing key-value pairs for the information
        items from the os-release file data source of the Linux distribution.

        For details, see :func:`distro.os_release_info`.
        """
        return self._os_release_info

    def lsb_release_info(self):
        """
        Return a dictionary containing key-value pairs for the information
        items from the lsb_release command data source of the Linux
        distribution.

        For details, see :func:`distro.lsb_release_info`.
        """
        return self._lsb_release_info

    def distro_release_info(self):
        """
        Return a dictionary containing key-value pairs for the information
        items from the distro release file data source of the Linux
        distribution.

        For details, see :func:`distro.distro_release_info`.
        """
        return self._distro_release_info

    def os_release_attr(self, attribute):
        """
        Return a single named information item from the os-release file data
        source of the Linux distribution.

        For details, see :func:`distro.os_release_attr`.
        """
        return self._os_release_info.get(attribute, '')

    def lsb_release_attr(self, attribute):
        """
        Return a single named information item from the lsb_release command
        output data source of the Linux distribution.

        For details, see :func:`distro.lsb_release_attr`.
        """
        return self._lsb_release_info.get(attribute, '')

    def distro_release_attr(self, attribute):
        """
        Return a single named information item from the distro release file
        data source of the Linux distribution.

        For details, see :func:`distro.distro_release_attr`.
        """
        return self._distro_release_info.get(attribute, '')

    def _get_os_release_info(self):
        """
        Get the information items from the specified os-release file.

        Returns:
            A dictionary containing all information items.
        """
        if os.path.isfile(self.os_release_file):
            with open(self.os_release_file) as release_file:
                return self._parse_os_release_content(release_file)
        return {}

    @staticmethod
    def _parse_os_release_content(lines):
        """
        Parse the lines of an os-release file.

        Parameters:

        * lines: Iterable through the lines in the os-release file.
                 Each line must be a unicode string or a UTF-8 encoded byte
                 string.

        Returns:
            A dictionary containing all information items.
        """
        props = {}
        lexer = shlex.shlex(lines, posix=True)
        lexer.whitespace_split = True

        # The shlex module defines its `wordchars` variable using literals,
        # making it dependent on the encoding of the Python source file.
        # In Python 2.6 and 2.7, the shlex source file is encoded in
        # 'iso-8859-1', and the `wordchars` variable is defined as a byte
        # string. This causes a UnicodeDecodeError to be raised when the
        # parsed content is a unicode object. The following fix resolves that
        # (... but it should be fixed in shlex...):
        if sys.version_info[0] == 2 and isinstance(lexer.wordchars, bytes):
            lexer.wordchars = lexer.wordchars.decode('iso-8859-1')

        tokens = list(lexer)
        for token in tokens:
            # At this point, all shell-like parsing has been done (i.e.
            # comments processed, quotes and backslash escape sequences
            # processed, multi-line values assembled, trailing newlines
            # stripped, etc.), so the tokens are now either:
            # * variable assignments: var=value
            # * commands or their arguments (not allowed in os-release)
            if '=' in token:
                k, v = token.split('=', 1)
                if isinstance(v, bytes):
                    v = v.decode('utf-8')
                props[k.lower()] = v
                if k == 'VERSION':
                    # this handles cases in which the codename is in
                    # the `(CODENAME)` (rhel, centos, fedora) format
                    # or in the `, CODENAME` format (Ubuntu).
                    codename = re.search(r'(\(\D+\))|,(\s+)?\D+', v)
                    if codename:
                        codename = codename.group()
                        codename = codename.strip('()')
                        codename = codename.strip(',')
                        codename = codename.strip()
                        # codename appears within paranthese.
                        props['codename'] = codename
                    else:
                        props['codename'] = ''
            else:
                # Ignore any tokens that are not variable assignments
                pass
        return props

    def _get_lsb_release_info(self):
        """
        Get the information items from the lsb_release command output.

        Returns:
            A dictionary containing all information items.
        """
        cmd = 'lsb_release -a'
        # conda customization: On Ubuntu 17.10, lsb_release calls the
        # system Python and it will not find our custom sysconfigdata
        env = os.environ.copy()
        if '_PYTHON_SYSCONFIGDATA_NAME' in env:
            del env['_PYTHON_SYSCONFIGDATA_NAME']
        process = subprocess.Popen(
            cmd,
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env)
        stdout, stderr = process.communicate()
        stdout, stderr = stdout.decode('utf-8'), stderr.decode('utf-8')
        code = process.returncode
        if code == 0:
            content = stdout.splitlines()
            return self._parse_lsb_release_content(content)
        elif code == 127:  # Command not found
            return {}
        else:
            if sys.version_info[:2] >= (3, 5):
                raise subprocess.CalledProcessError(code, cmd, stdout, stderr)
            elif sys.version_info[:2] >= (2, 7):
                raise subprocess.CalledProcessError(code, cmd, stdout)
            elif sys.version_info[:2] == (2, 6):
                raise subprocess.CalledProcessError(code, cmd)

    @staticmethod
    def _parse_lsb_release_content(lines):
        """
        Parse the output of the lsb_release command.

        Parameters:

        * lines: Iterable through the lines of the lsb_release output.
                 Each line must be a unicode string or a UTF-8 encoded byte
                 string.

        Returns:
            A dictionary containing all information items.
        """
        props = {}
        for line in lines:
            line = line.decode('utf-8') if isinstance(line, bytes) else line
            kv = line.strip('\n').split(':', 1)
            if len(kv) != 2:
                # Ignore lines without colon.
                continue
            k, v = kv
            props.update({k.replace(' ', '_').lower(): v.strip()})
        return props

    def _get_distro_release_info(self):
        """
        Get the information items from the specified distro release file.

        Returns:
            A dictionary containing all information items.
        """
        if self.distro_release_file:
            # If it was specified, we use it and parse what we can, even if
            # its file name or content does not match the expected pattern.
            distro_info = self._parse_distro_release_file(
                self.distro_release_file)
            basename = os.path.basename(self.distro_release_file)
            # The file name pattern for user-specified distro release files
            # is somewhat more tolerant (compared to when searching for the
            # file), because we want to use what was specified as best as
            # possible.
            match = _DISTRO_RELEASE_BASENAME_PATTERN.match(basename)
            if match:
                distro_info['id'] = match.group(1)
            return distro_info
        else:
            try:
                basenames = os.listdir(_UNIXCONFDIR)
                # We sort for repeatability in cases where there are multiple
                # distro specific files; e.g. CentOS, Oracle, Enterprise all
                # containing `redhat-release` on top of their own.
                basenames.sort()
            except OSError:
                # This may occur when /etc is not readable but we can't be
                # sure about the *-release files. Check common entries of
                # /etc for information. If they turn out to not be there the
                # error is handled in `_parse_distro_release_file()`.
                basenames = ['SuSE-release',
                             'arch-release',
                             'base-release',
                             'centos-release',
                             'fedora-release',
                             'gentoo-release',
                             'mageia-release',
                             'manjaro-release',
                             'oracle-release',
                             'redhat-release',
                             'sl-release',
                             'slackware-version']
            for basename in basenames:
                if basename in _DISTRO_RELEASE_IGNORE_BASENAMES:
                    continue
                match = _DISTRO_RELEASE_BASENAME_PATTERN.match(basename)
                if match:
                    filepath = os.path.join(_UNIXCONFDIR, basename)
                    distro_info = self._parse_distro_release_file(filepath)
                    if 'name' in distro_info:
                        # The name is always present if the pattern matches
                        self.distro_release_file = filepath
                        distro_info['id'] = match.group(1)
                        return distro_info
            return {}

    def _parse_distro_release_file(self, filepath):
        """
        Parse a distro release file.

        Parameters:

        * filepath: Path name of the distro release file.

        Returns:
            A dictionary containing all information items.
        """
        try:
            with open(filepath) as fp:
                # Only parse the first line. For instance, on SLES there
                # are multiple lines. We don't want them...
                return self._parse_distro_release_content(fp.readline())
        except (OSError, IOError):
            # Ignore not being able to read a specific, seemingly version
            # related file.
            # See https://github.com/nir0s/distro/issues/162
            return {}

    @staticmethod
    def _parse_distro_release_content(line):
        """
        Parse a line from a distro release file.

        Parameters:
        * line: Line from the distro release file. Must be a unicode string
                or a UTF-8 encoded byte string.

        Returns:
            A dictionary containing all information items.
        """
        if isinstance(line, bytes):
            line = line.decode('utf-8')
        matches = _DISTRO_RELEASE_CONTENT_REVERSED_PATTERN.match(
            line.strip()[::-1])
        distro_info = {}
        if matches:
            # regexp ensures non-None
            distro_info['name'] = matches.group(3)[::-1]
            if matches.group(2):
                distro_info['version_id'] = matches.group(2)[::-1]
            if matches.group(1):
                distro_info['codename'] = matches.group(1)[::-1]
        elif line:
            distro_info['name'] = line.strip()
        return distro_info


_distro = LinuxDistribution()


def main():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    logger.addHandler(logging.StreamHandler(sys.stdout))

    parser = argparse.ArgumentParser(description="Linux distro info tool")
    parser.add_argument(
        '--json',
        '-j',
        help="Output in machine readable format",
        action="store_true")
    args = parser.parse_args()

    if args.json:
        logger.info(json.dumps(info(), indent=4, sort_keys=True))
    else:
        logger.info('Name: %s', name(pretty=True))
        distribution_version = version(pretty=True)
        logger.info('Version: %s', distribution_version)
        distribution_codename = codename()
        logger.info('Codename: %s', distribution_codename)


if __name__ == '__main__':
    main()


# This is the MIT license

Copyright (c) 2010 ActiveState Software Inc.

Permission is hereby granted, free of charge, to any person obtaining a
copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



#!/usr/bin/env python
# -*- coding: UTF-8 -*-

# Copyright (c) 2014-2022 Matthew Brennan Jones <matthew.brennan.jones@gmail.com>
# Py-cpuinfo gets CPU info with pure Python
# It uses the MIT License
# It is hosted at: https://github.com/workhorsy/py-cpuinfo
#
# Permission is hereby granted, free of charge, to any person obtaining
# a copy of this software and associated documentation files (the
# "Software"), to deal in the Software without restriction, including
# without limitation the rights to use, copy, modify, merge, publish,
# distribute, sublicense, and/or sell copies of the Software, and to
# permit persons to whom the Software is furnished to do so, subject to
# the following conditions:
#
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

CPUINFO_VERSION = (9, 0, 0)
CPUINFO_VERSION_STRING = '.'.join([str(n) for n in CPUINFO_VERSION])

import os, sys
import platform
import multiprocessing
import ctypes


CAN_CALL_CPUID_IN_SUBPROCESS = True

g_trace = None


class Trace(object):
	def __init__(self, is_active, is_stored_in_string):
		self._is_active = is_active
		if not self._is_active:
			return

		from datetime import datetime
		from io import StringIO

		if is_stored_in_string:
			self._output = StringIO()
		else:
			date = datetime.now().strftime("%Y-%m-%d_%H-%M-%S-%f")
			self._output = open('cpuinfo_trace_{0}.trace'.format(date), 'w')

		self._stdout = StringIO()
		self._stderr = StringIO()
		self._err = None

	def header(self, msg):
		if not self._is_active: return

		from inspect import stack
		frame = stack()[1]
		file = frame[1]
		line = frame[2]
		self._output.write("{0} ({1} {2})\n".format(msg, file, line))
		self._output.flush()

	def success(self):
		if not self._is_active: return

		from inspect import stack
		frame = stack()[1]
		file = frame[1]
		line = frame[2]

		self._output.write("Success ... ({0} {1})\n\n".format(file, line))
		self._output.flush()

	def fail(self, msg):
		if not self._is_active: return

		from inspect import stack
		frame = stack()[1]
		file = frame[1]
		line = frame[2]

		if isinstance(msg, str):
			msg = ''.join(['\t' + line for line in msg.split('\n')]) + '\n'

			self._output.write(msg)
			self._output.write("Failed ... ({0} {1})\n\n".format(file, line))
			self._output.flush()
		elif isinstance(msg, Exception):
			from traceback import format_exc
			err_string = format_exc()
			self._output.write("\tFailed ... ({0} {1})\n".format(file, line))
			self._output.write(''.join(['\t\t{0}\n'.format(n) for n in err_string.split('\n')]) + '\n')
			self._output.flush()

	def command_header(self, msg):
		if not self._is_active: return

		from inspect import stack
		frame = stack()[3]
		file = frame[1]
		line = frame[2]
		self._output.write("\t{0} ({1} {2})\n".format(msg, file, line))
		self._output.flush()

	def command_output(self, msg, output):
		if not self._is_active: return

		self._output.write("\t\t{0}\n".format(msg))
		self._output.write(''.join(['\t\t\t{0}\n'.format(n) for n in output.split('\n')]) + '\n')
		self._output.flush()

	def keys(self, keys, info, new_info):
		if not self._is_active: return

		from inspect import stack
		frame = stack()[2]
		file = frame[1]
		line = frame[2]

		# List updated keys
		self._output.write("\tChanged keys ({0} {1})\n".format(file, line))
		changed_keys = [key for key in keys if key in info and key in new_info and info[key] != new_info[key]]
		if changed_keys:
			for key in changed_keys:
				self._output.write('\t\t{0}: {1} to {2}\n'.format(key, info[key], new_info[key]))
		else:
			self._output.write('\t\tNone\n')

		# List new keys
		self._output.write("\tNew keys ({0} {1})\n".format(file, line))
		new_keys = [key for key in keys if key in new_info and key not in info]
		if new_keys:
			for key in new_keys:
				self._output.write('\t\t{0}: {1}\n'.format(key, new_info[key]))
		else:
			self._output.write('\t\tNone\n')

		self._output.write('\n')
		self._output.flush()

	def write(self, msg):
		if not self._is_active: return

		self._output.write(msg + '\n')
		self._output.flush()

	def to_dict(self, info, is_fail):
		return {
		'output' : self._output.getvalue(),
		'stdout' : self._stdout.getvalue(),
		'stderr' : self._stderr.getvalue(),
		'info' : info,
		'err' : self._err,
		'is_fail' : is_fail
		}

class DataSource(object):
	bits = platform.architecture()[0]
	cpu_count = multiprocessing.cpu_count()
	is_windows = platform.system().lower() == 'windows'
	arch_string_raw = platform.machine()
	uname_string_raw = platform.uname()[5]
	can_cpuid = True

	@staticmethod
	def has_proc_cpuinfo():
		return os.path.exists('/proc/cpuinfo')

	@staticmethod
	def has_dmesg():
		return len(_program_paths('dmesg')) > 0

	@staticmethod
	def has_var_run_dmesg_boot():
		uname = platform.system().strip().strip('"').strip("'").strip().lower()
		return 'linux' in uname and os.path.exists('/var/run/dmesg.boot')

	@staticmethod
	def has_cpufreq_info():
		return len(_program_paths('cpufreq-info')) > 0

	@staticmethod
	def has_sestatus():
		return len(_program_paths('sestatus')) > 0

	@staticmethod
	def has_sysctl():
		return len(_program_paths('sysctl')) > 0

	@staticmethod
	def has_isainfo():
		return len(_program_paths('isainfo')) > 0

	@staticmethod
	def has_kstat():
		return len(_program_paths('kstat')) > 0

	@staticmethod
	def has_sysinfo():
		uname = platform.system().strip().strip('"').strip("'").strip().lower()
		is_beos = 'beos' in uname or 'haiku' in uname
		return is_beos and len(_program_paths('sysinfo')) > 0

	@staticmethod
	def has_lscpu():
		return len(_program_paths('lscpu')) > 0

	@staticmethod
	def has_ibm_pa_features():
		return len(_program_paths('lsprop')) > 0

	@staticmethod
	def has_wmic():
		returncode, output = _run_and_get_stdout(['wmic', 'os', 'get', 'Version'])
		return returncode == 0 and len(output) > 0

	@staticmethod
	def cat_proc_cpuinfo():
		return _run_and_get_stdout(['cat', '/proc/cpuinfo'])

	@staticmethod
	def cpufreq_info():
		return _run_and_get_stdout(['cpufreq-info'])

	@staticmethod
	def sestatus_b():
		return _run_and_get_stdout(['sestatus', '-b'])

	@staticmethod
	def dmesg_a():
		return _run_and_get_stdout(['dmesg', '-a'])

	@staticmethod
	def cat_var_run_dmesg_boot():
		return _run_and_get_stdout(['cat', '/var/run/dmesg.boot'])

	@staticmethod
	def sysctl_machdep_cpu_hw_cpufrequency():
		return _run_and_get_stdout(['sysctl', 'machdep.cpu', 'hw.cpufrequency'])

	@staticmethod
	def isainfo_vb():
		return _run_and_get_stdout(['isainfo', '-vb'])

	@staticmethod
	def kstat_m_cpu_info():
		return _run_and_get_stdout(['kstat', '-m', 'cpu_info'])

	@staticmethod
	def sysinfo_cpu():
		return _run_and_get_stdout(['sysinfo', '-cpu'])

	@staticmethod
	def lscpu():
		return _run_and_get_stdout(['lscpu'])

	@staticmethod
	def ibm_pa_features():
		import glob

		ibm_features = glob.glob('/proc/device-tree/cpus/*/ibm,pa-features')
		if ibm_features:
			return _run_and_get_stdout(['lsprop', ibm_features[0]])

	@staticmethod
	def wmic_cpu():
		return _run_and_get_stdout(['wmic', 'cpu', 'get', 'Name,CurrentClockSpeed,L2CacheSize,L3CacheSize,Description,Caption,Manufacturer', '/format:list'])

	@staticmethod
	def winreg_processor_brand():
		processor_brand = _read_windows_registry_key(r"Hardware\Description\System\CentralProcessor\0", "ProcessorNameString")
		return processor_brand.strip()

	@staticmethod
	def winreg_vendor_id_raw():
		vendor_id_raw = _read_windows_registry_key(r"Hardware\Description\System\CentralProcessor\0", "VendorIdentifier")
		return vendor_id_raw

	@staticmethod
	def winreg_arch_string_raw():
		arch_string_raw = _read_windows_registry_key(r"SYSTEM\CurrentControlSet\Control\Session Manager\Environment", "PROCESSOR_ARCHITECTURE")
		return arch_string_raw

	@staticmethod
	def winreg_hz_actual():
		hz_actual = _read_windows_registry_key(r"Hardware\Description\System\CentralProcessor\0", "~Mhz")
		hz_actual = _to_decimal_string(hz_actual)
		return hz_actual

	@staticmethod
	def winreg_feature_bits():
		feature_bits = _read_windows_registry_key(r"Hardware\Description\System\CentralProcessor\0", "FeatureSet")
		return feature_bits


def _program_paths(program_name):
	paths = []
	exts = filter(None, os.environ.get('PATHEXT', '').split(os.pathsep))
	for p in os.environ['PATH'].split(os.pathsep):
		p = os.path.join(p, program_name)
		if os.access(p, os.X_OK):
			paths.append(p)
		for e in exts:
			pext = p + e
			if os.access(pext, os.X_OK):
				paths.append(pext)
	return paths

def _run_and_get_stdout(command, pipe_command=None):
	from subprocess import Popen, PIPE

	g_trace.command_header('Running command "' + ' '.join(command) + '" ...')

	# Run the command normally
	if not pipe_command:
		p1 = Popen(command, stdout=PIPE, stderr=PIPE, stdin=PIPE)
	# Run the command and pipe it into another command
	else:
		p2 = Popen(command, stdout=PIPE, stderr=PIPE, stdin=PIPE)
		p1 = Popen(pipe_command, stdin=p2.stdout, stdout=PIPE, stderr=PIPE)
		p2.stdout.close()

	# Get the stdout and stderr
	stdout_output, stderr_output = p1.communicate()
	stdout_output = stdout_output.decode(encoding='UTF-8')
	stderr_output = stderr_output.decode(encoding='UTF-8')

	# Send the result to the logger
	g_trace.command_output('return code:', str(p1.returncode))
	g_trace.command_output('stdout:', stdout_output)

	# Return the return code and stdout
	return p1.returncode, stdout_output

def _read_windows_registry_key(key_name, field_name):
	g_trace.command_header('Reading Registry key "{0}" field "{1}" ...'.format(key_name, field_name))

	try:
		import _winreg as winreg
	except ImportError as err:
		try:
			import winreg
		except ImportError as err:
			pass

	key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, key_name)
	value = winreg.QueryValueEx(key, field_name)[0]
	winreg.CloseKey(key)
	g_trace.command_output('value:', str(value))
	return value

# Make sure we are running on a supported system
def _check_arch():
	arch, bits = _parse_arch(DataSource.arch_string_raw)
	if not arch in ['X86_32', 'X86_64', 'ARM_7', 'ARM_8',
	               'PPC_64', 'S390X', 'MIPS_32', 'MIPS_64',
				   "RISCV_32", "RISCV_64"]:
		raise Exception("py-cpuinfo currently only works on X86 "
		                "and some ARM/PPC/S390X/MIPS/RISCV CPUs.")

def _obj_to_b64(thing):
	import pickle
	import base64

	a = thing
	b = pickle.dumps(a)
	c = base64.b64encode(b)
	d = c.decode('utf8')
	return d

def _b64_to_obj(thing):
	import pickle
	import base64

	try:
		a = base64.b64decode(thing)
		b = pickle.loads(a)
		return b
	except Exception:
		return {}

def _utf_to_str(input):
	if isinstance(input, list):
		return [_utf_to_str(element) for element in input]
	elif isinstance(input, dict):
		return {_utf_to_str(key): _utf_to_str(value)
			for key, value in input.items()}
	else:
		return input

def _copy_new_fields(info, new_info):
	keys = [
		'vendor_id_raw', 'hardware_raw', 'brand_raw', 'hz_advertised_friendly', 'hz_actual_friendly',
		'hz_advertised', 'hz_actual', 'arch', 'bits', 'count',
		'arch_string_raw', 'uname_string_raw',
		'l2_cache_size', 'l2_cache_line_size', 'l2_cache_associativity',
		'stepping', 'model', 'family',
		'processor_type', 'flags',
		'l3_cache_size', 'l1_data_cache_size', 'l1_instruction_cache_size'
	]

	g_trace.keys(keys, info, new_info)

	# Update the keys with new values
	for key in keys:
		if new_info.get(key, None) and not info.get(key, None):
			info[key] = new_info[key]
		elif key == 'flags' and new_info.get('flags'):
			for f in new_info['flags']:
				if f not in info['flags']: info['flags'].append(f)
			info['flags'].sort()

def _get_field_actual(cant_be_number, raw_string, field_names):
	for line in raw_string.splitlines():
		for field_name in field_names:
			field_name = field_name.lower()
			if ':' in line:
				left, right = line.split(':', 1)
				left = left.strip().lower()
				right = right.strip()
				if left == field_name and len(right) > 0:
					if cant_be_number:
						if not right.isdigit():
							return right
					else:
						return right

	return None

def _get_field(cant_be_number, raw_string, convert_to, default_value, *field_names):
	retval = _get_field_actual(cant_be_number, raw_string, field_names)

	# Convert the return value
	if retval and convert_to:
		try:
			retval = convert_to(retval)
		except Exception:
			retval = default_value

	# Return the default if there is no return value
	if retval is None:
		retval = default_value

	return retval

def _to_decimal_string(ticks):
	try:
		# Convert to string
		ticks = '{0}'.format(ticks)
		# Sometimes ',' is used as a decimal separator
		ticks = ticks.replace(',', '.')

		# Strip off non numbers and decimal places
		ticks = "".join(n for n in ticks if n.isdigit() or n=='.').strip()
		if ticks == '':
			ticks = '0'

		# Add decimal if missing
		if '.' not in ticks:
			ticks = '{0}.0'.format(ticks)

		# Remove trailing zeros
		ticks = ticks.rstrip('0')

		# Add one trailing zero for empty right side
		if ticks.endswith('.'):
			ticks = '{0}0'.format(ticks)

		# Make sure the number can be converted to a float
		ticks = float(ticks)
		ticks = '{0}'.format(ticks)
		return ticks
	except Exception:
		return '0.0'

def _hz_short_to_full(ticks, scale):
	try:
		# Make sure the number can be converted to a float
		ticks = float(ticks)
		ticks = '{0}'.format(ticks)

		# Scale the numbers
		hz = ticks.lstrip('0')
		old_index = hz.index('.')
		hz = hz.replace('.', '')
		hz = hz.ljust(scale + old_index+1, '0')
		new_index = old_index + scale
		hz = '{0}.{1}'.format(hz[:new_index], hz[new_index:])
		left, right = hz.split('.')
		left, right = int(left), int(right)
		return (left, right)
	except Exception:
		return (0, 0)

def _hz_friendly_to_full(hz_string):
	try:
		hz_string = hz_string.strip().lower()
		hz, scale = (None, None)

		if hz_string.endswith('ghz'):
			scale = 9
		elif hz_string.endswith('mhz'):
			scale = 6
		elif hz_string.endswith('hz'):
			scale = 0

		hz = "".join(n for n in hz_string if n.isdigit() or n=='.').strip()
		if not '.' in hz:
			hz += '.0'

		hz, scale = _hz_short_to_full(hz, scale)

		return (hz, scale)
	except Exception:
		return (0, 0)

def _hz_short_to_friendly(ticks, scale):
	try:
		# Get the raw Hz as a string
		left, right = _hz_short_to_full(ticks, scale)
		result = '{0}.{1}'.format(left, right)

		# Get the location of the dot, and remove said dot
		dot_index = result.index('.')
		result = result.replace('.', '')

		# Get the Hz symbol and scale
		symbol = "Hz"
		scale = 0
		if dot_index > 9:
			symbol = "GHz"
			scale = 9
		elif dot_index > 6:
			symbol = "MHz"
			scale = 6
		elif dot_index > 3:
			symbol = "KHz"
			scale = 3

		# Get the Hz with the dot at the new scaled point
		result = '{0}.{1}'.format(result[:-scale-1], result[-scale-1:])

		# Format the ticks to have 4 numbers after the decimal
		# and remove any superfluous zeroes.
		result = '{0:.4f} {1}'.format(float(result), symbol)
		result = result.rstrip('0')
		return result
	except Exception:
		return '0.0000 Hz'

def _to_friendly_bytes(input):
	import re

	if not input:
		return input
	input = "{0}".format(input)

	formats = {
		r"^[0-9]+B$" : 'B',
		r"^[0-9]+K$" : 'KB',
		r"^[0-9]+M$" : 'MB',
		r"^[0-9]+G$" : 'GB'
	}

	for pattern, friendly_size in formats.items():
		if re.match(pattern, input):
			return "{0} {1}".format(input[ : -1].strip(), friendly_size)

	return input

def _friendly_bytes_to_int(friendly_bytes):
	input = friendly_bytes.lower()

	formats = [
		{'gib' : 1024 * 1024 * 1024},
		{'mib' : 1024 * 1024},
		{'kib' : 1024},

		{'gb' : 1024 * 1024 * 1024},
		{'mb' : 1024 * 1024},
		{'kb' : 1024},

		{'g' : 1024 * 1024 * 1024},
		{'m' : 1024 * 1024},
		{'k' : 1024},
		{'b' : 1},
	]

	try:
		for entry in formats:
			pattern = list(entry.keys())[0]
			multiplier = list(entry.values())[0]
			if input.endswith(pattern):
				return int(input.split(pattern)[0].strip()) * multiplier

	except Exception as err:
		pass

	return friendly_bytes

def _parse_cpu_brand_string(cpu_string):
	# Just return 0 if the processor brand does not have the Hz
	if not 'hz' in cpu_string.lower():
		return ('0.0', 0)

	hz = cpu_string.lower()
	scale = 0

	if hz.endswith('mhz'):
		scale = 6
	elif hz.endswith('ghz'):
		scale = 9
	if '@' in hz:
		hz = hz.split('@')[1]
	else:
		hz = hz.rsplit(None, 1)[1]

	hz = hz.rstrip('mhz').rstrip('ghz').strip()
	hz = _to_decimal_string(hz)

	return (hz, scale)

def _parse_cpu_brand_string_dx(cpu_string):
	import re

	# Find all the strings inside brackets ()
	starts = [m.start() for m in re.finditer(r"\(", cpu_string)]
	ends = [m.start() for m in re.finditer(r"\)", cpu_string)]
	insides = {k: v for k, v in zip(starts, ends)}
	insides = [cpu_string[start+1 : end] for start, end in insides.items()]

	# Find all the fields
	vendor_id, stepping, model, family = (None, None, None, None)
	for inside in insides:
		for pair in inside.split(','):
			pair = [n.strip() for n in pair.split(':')]
			if len(pair) > 1:
				name, value = pair[0], pair[1]
				if name == 'origin':
					vendor_id = value.strip('"')
				elif name == 'stepping':
					stepping = int(value.lstrip('0x'), 16)
				elif name == 'model':
					model = int(value.lstrip('0x'), 16)
				elif name in ['fam', 'family']:
					family = int(value.lstrip('0x'), 16)

	# Find the Processor Brand
	# Strip off extra strings in brackets at end
	brand = cpu_string.strip()
	is_working = True
	while is_working:
		is_working = False
		for inside in insides:
			full = "({0})".format(inside)
			if brand.endswith(full):
				brand = brand[ :-len(full)].strip()
				is_working = True

	# Find the Hz in the brand string
	hz_brand, scale = _parse_cpu_brand_string(brand)

	# Find Hz inside brackets () after the brand string
	if hz_brand == '0.0':
		for inside in insides:
			hz = inside
			for entry in ['GHz', 'MHz', 'Hz']:
				if entry in hz:
					hz = "CPU @ " + hz[ : hz.find(entry) + len(entry)]
					hz_brand, scale = _parse_cpu_brand_string(hz)
					break

	return (hz_brand, scale, brand, vendor_id, stepping, model, family)

def _parse_dmesg_output(output):
	try:
		# Get all the dmesg lines that might contain a CPU string
		lines = output.split(' CPU0:')[1:] + \
				output.split(' CPU1:')[1:] + \
				output.split(' CPU:')[1:] + \
				output.split('\nCPU0:')[1:] + \
				output.split('\nCPU1:')[1:] + \
				output.split('\nCPU:')[1:]
		lines = [l.split('\n')[0].strip() for l in lines]

		# Convert the lines to CPU strings
		cpu_strings = [_parse_cpu_brand_string_dx(l) for l in lines]

		# Find the CPU string that has the most fields
		best_string = None
		highest_count = 0
		for cpu_string in cpu_strings:
			count = sum([n is not None for n in cpu_string])
			if count > highest_count:
				highest_count = count
				best_string = cpu_string

		# If no CPU string was found, return {}
		if not best_string:
			return {}

		hz_actual, scale, processor_brand, vendor_id, stepping, model, family = best_string

		# Origin
		if '  Origin=' in output:
			fields = output[output.find('  Origin=') : ].split('\n')[0]
			fields = fields.strip().split()
			fields = [n.strip().split('=') for n in fields]
			fields = [{n[0].strip().lower() : n[1].strip()} for n in fields]

			for field in fields:
				name = list(field.keys())[0]
				value = list(field.values())[0]

				if name == 'origin':
					vendor_id = value.strip('"')
				elif name == 'stepping':
					stepping = int(value.lstrip('0x'), 16)
				elif name == 'model':
					model = int(value.lstrip('0x'), 16)
				elif name in ['fam', 'family']:
					family = int(value.lstrip('0x'), 16)

		# Features
		flag_lines = []
		for category in ['  Features=', '  Features2=', '  AMD Features=', '  AMD Features2=']:
			if category in output:
				flag_lines.append(output.split(category)[1].split('\n')[0])

		flags = []
		for line in flag_lines:
			line = line.split('<')[1].split('>')[0].lower()
			for flag in line.split(','):
				flags.append(flag)
		flags.sort()

		# Convert from GHz/MHz string to Hz
		hz_advertised, scale = _parse_cpu_brand_string(processor_brand)

		# If advertised hz not found, use the actual hz
		if hz_advertised == '0.0':
			scale = 6
			hz_advertised = _to_decimal_string(hz_actual)

		info = {
		'vendor_id_raw' : vendor_id,
		'brand_raw' : processor_brand,

		'stepping' : stepping,
		'model' : model,
		'family' : family,
		'flags' : flags
		}

		if hz_advertised and hz_advertised != '0.0':
			info['hz_advertised_friendly'] = _hz_short_to_friendly(hz_advertised, scale)
			info['hz_actual_friendly'] = _hz_short_to_friendly(hz_actual, scale)

		if hz_advertised and hz_advertised != '0.0':
			info['hz_advertised'] = _hz_short_to_full(hz_advertised, scale)
			info['hz_actual'] = _hz_short_to_full(hz_actual, scale)

		return {k: v for k, v in info.items() if v}
	except Exception as err:
		g_trace.fail(err)
		#raise

	return {}

def _parse_arch(arch_string_raw):
	import re

	arch, bits = None, None
	arch_string_raw = arch_string_raw.lower()

	# X86
	if re.match(r'^i\d86$|^x86$|^x86_32$|^i86pc$|^ia32$|^ia-32$|^bepc$', arch_string_raw):
		arch = 'X86_32'
		bits = 32
	elif re.match(r'^x64$|^x86_64$|^x86_64t$|^i686-64$|^amd64$|^ia64$|^ia-64$', arch_string_raw):
		arch = 'X86_64'
		bits = 64
	# ARM
	elif re.match(r'^armv8-a|aarch64|arm64$', arch_string_raw):
		arch = 'ARM_8'
		bits = 64
	elif re.match(r'^armv7$|^armv7[a-z]$|^armv7-[a-z]$|^armv6[a-z]$', arch_string_raw):
		arch = 'ARM_7'
		bits = 32
	elif re.match(r'^armv8$|^armv8[a-z]$|^armv8-[a-z]$', arch_string_raw):
		arch = 'ARM_8'
		bits = 32
	# PPC
	elif re.match(r'^ppc32$|^prep$|^pmac$|^powermac$', arch_string_raw):
		arch = 'PPC_32'
		bits = 32
	elif re.match(r'^powerpc$|^ppc64$|^ppc64le$', arch_string_raw):
		arch = 'PPC_64'
		bits = 64
	# SPARC
	elif re.match(r'^sparc32$|^sparc$', arch_string_raw):
		arch = 'SPARC_32'
		bits = 32
	elif re.match(r'^sparc64$|^sun4u$|^sun4v$', arch_string_raw):
		arch = 'SPARC_64'
		bits = 64
	# S390X
	elif re.match(r'^s390x$', arch_string_raw):
		arch = 'S390X'
		bits = 64
	elif arch_string_raw == 'mips':
		arch = 'MIPS_32'
		bits = 32
	elif arch_string_raw == 'mips64':
		arch = 'MIPS_64'
		bits = 64
	# RISCV
	elif re.match(r'^riscv$|^riscv32$|^riscv32be$', arch_string_raw):
		arch = 'RISCV_32'
		bits = 32
	elif re.match(r'^riscv64$|^riscv64be$', arch_string_raw):
		arch = 'RISCV_64'
		bits = 64

	return (arch, bits)

def _is_bit_set(reg, bit):
	mask = 1 << bit
	is_set = reg & mask > 0
	return is_set


def _is_selinux_enforcing(trace):
	# Just return if the SE Linux Status Tool is not installed
	if not DataSource.has_sestatus():
		trace.fail('Failed to find sestatus.')
		return False

	# Run the sestatus, and just return if it failed to run
	returncode, output = DataSource.sestatus_b()
	if returncode != 0:
		trace.fail('Failed to run sestatus. Skipping ...')
		return False

	# Figure out if explicitly in enforcing mode
	for line in output.splitlines():
		line = line.strip().lower()
		if line.startswith("current mode:"):
			if line.endswith("enforcing"):
				return True
			else:
				return False

	# Figure out if we can execute heap and execute memory
	can_selinux_exec_heap = False
	can_selinux_exec_memory = False
	for line in output.splitlines():
		line = line.strip().lower()
		if line.startswith("allow_execheap") and line.endswith("on"):
			can_selinux_exec_heap = True
		elif line.startswith("allow_execmem") and line.endswith("on"):
			can_selinux_exec_memory = True

	trace.command_output('can_selinux_exec_heap:', can_selinux_exec_heap)
	trace.command_output('can_selinux_exec_memory:', can_selinux_exec_memory)

	return (not can_selinux_exec_heap or not can_selinux_exec_memory)

def _filter_dict_keys_with_empty_values(info, acceptable_values = {}):
	filtered_info = {}
	for key in info:
		value = info[key]

		# Keep if value is acceptable
		if key in acceptable_values:
			if acceptable_values[key] == value:
				filtered_info[key] = value
				continue

		# Filter out None, 0, "", (), {}, []
		if not value:
			continue

		# Filter out (0, 0)
		if value == (0, 0):
			continue

		# Filter out -1
		if value == -1:
			continue

		# Filter out strings that start with "0.0"
		if type(value) == str and value.startswith('0.0'):
			continue

		filtered_info[key] = value

	return filtered_info

class ASM(object):
	def __init__(self, restype=None, argtypes=(), machine_code=[]):
		self.restype = restype
		self.argtypes = argtypes
		self.machine_code = machine_code
		self.prochandle = None
		self.mm = None
		self.func = None
		self.address = None
		self.size = 0

	def compile(self):
		machine_code = bytes.join(b'', self.machine_code)
		self.size = ctypes.c_size_t(len(machine_code))

		if DataSource.is_windows:
			# Allocate a memory segment the size of the machine code, and make it executable
			size = len(machine_code)
			# Alloc at least 1 page to ensure we own all pages that we want to change protection on
			if size < 0x1000: size = 0x1000
			MEM_COMMIT = ctypes.c_ulong(0x1000)
			PAGE_READWRITE = ctypes.c_ulong(0x4)
			pfnVirtualAlloc = ctypes.windll.kernel32.VirtualAlloc
			pfnVirtualAlloc.restype = ctypes.c_void_p
			self.address = pfnVirtualAlloc(None, ctypes.c_size_t(size), MEM_COMMIT, PAGE_READWRITE)
			if not self.address:
				raise Exception("Failed to VirtualAlloc")

			# Copy the machine code into the memory segment
			memmove = ctypes.CFUNCTYPE(ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_size_t)(ctypes._memmove_addr)
			if memmove(self.address, machine_code, size) < 0:
				raise Exception("Failed to memmove")

			# Enable execute permissions
			PAGE_EXECUTE = ctypes.c_ulong(0x10)
			old_protect = ctypes.c_ulong(0)
			pfnVirtualProtect = ctypes.windll.kernel32.VirtualProtect
			res = pfnVirtualProtect(ctypes.c_void_p(self.address), ctypes.c_size_t(size), PAGE_EXECUTE, ctypes.byref(old_protect))
			if not res:
				raise Exception("Failed VirtualProtect")

			# Flush Instruction Cache
			# First, get process Handle
			if not self.prochandle:
				pfnGetCurrentProcess = ctypes.windll.kernel32.GetCurrentProcess
				pfnGetCurrentProcess.restype = ctypes.c_void_p
				self.prochandle = ctypes.c_void_p(pfnGetCurrentProcess())
			# Actually flush cache
			res = ctypes.windll.kernel32.FlushInstructionCache(self.prochandle, ctypes.c_void_p(self.address), ctypes.c_size_t(size))
			if not res:
				raise Exception("Failed FlushInstructionCache")
		else:
			from mmap import mmap, MAP_PRIVATE, MAP_ANONYMOUS, PROT_WRITE, PROT_READ, PROT_EXEC

			# Allocate a private and executable memory segment the size of the machine code
			machine_code = bytes.join(b'', self.machine_code)
			self.size = len(machine_code)
			self.mm = mmap(-1, self.size, flags=MAP_PRIVATE | MAP_ANONYMOUS, prot=PROT_WRITE | PROT_READ | PROT_EXEC)

			# Copy the machine code into the memory segment
			self.mm.write(machine_code)
			self.address = ctypes.addressof(ctypes.c_int.from_buffer(self.mm))

		# Cast the memory segment into a function
		functype = ctypes.CFUNCTYPE(self.restype, *self.argtypes)
		self.func = functype(self.address)

	def run(self):
		# Call the machine code like a function
		retval = self.func()

		return retval

	def free(self):
		# Free the function memory segment
		if DataSource.is_windows:
			MEM_RELEASE = ctypes.c_ulong(0x8000)
			ctypes.windll.kernel32.VirtualFree(ctypes.c_void_p(self.address), ctypes.c_size_t(0), MEM_RELEASE)
		else:
			self.mm.close()

		self.prochandle = None
		self.mm = None
		self.func = None
		self.address = None
		self.size = 0


class CPUID(object):
	def __init__(self, trace=None):
		if trace is None:
			trace = Trace(False, False)

		# Figure out if SE Linux is on and in enforcing mode
		self.is_selinux_enforcing = _is_selinux_enforcing(trace)

	def _asm_func(self, restype=None, argtypes=(), machine_code=[]):
		asm = ASM(restype, argtypes, machine_code)
		asm.compile()
		return asm

	def _run_asm(self, *machine_code):
		asm = ASM(ctypes.c_uint32, (), machine_code)
		asm.compile()
		retval = asm.run()
		asm.free()
		return retval

	# http://en.wikipedia.org/wiki/CPUID#EAX.3D0:_Get_vendor_ID
	def get_vendor_id(self):
		# EBX
		ebx = self._run_asm(
			b"\x31\xC0",        # xor eax,eax
			b"\x0F\xA2"         # cpuid
			b"\x89\xD8"         # mov ax,bx
			b"\xC3"             # ret
		)

		# ECX
		ecx = self._run_asm(
			b"\x31\xC0",        # xor eax,eax
			b"\x0f\xa2"         # cpuid
			b"\x89\xC8"         # mov ax,cx
			b"\xC3"             # ret
		)

		# EDX
		edx = self._run_asm(
			b"\x31\xC0",        # xor eax,eax
			b"\x0f\xa2"         # cpuid
			b"\x89\xD0"         # mov ax,dx
			b"\xC3"             # ret
		)

		# Each 4bits is a ascii letter in the name
		vendor_id = []
		for reg in [ebx, edx, ecx]:
			for n in [0, 8, 16, 24]:
				vendor_id.append(chr((reg >> n) & 0xFF))
		vendor_id = ''.join(vendor_id)

		return vendor_id

	# http://en.wikipedia.org/wiki/CPUID#EAX.3D1:_Processor_Info_and_Feature_Bits
	def get_info(self):
		# EAX
		eax = self._run_asm(
			b"\xB8\x01\x00\x00\x00",   # mov eax,0x1"
			b"\x0f\xa2"                # cpuid
			b"\xC3"                    # ret
		)

		# Get the CPU info
		stepping_id = (eax >> 0) & 0xF # 4 bits
		model = (eax >> 4) & 0xF # 4 bits
		family_id = (eax >> 8) & 0xF # 4 bits
		processor_type = (eax >> 12) & 0x3 # 2 bits
		extended_model_id = (eax >> 16) & 0xF # 4 bits
		extended_family_id = (eax >> 20) & 0xFF # 8 bits
		family = 0

		if family_id in [15]:
			family = extended_family_id + family_id
		else:
			family = family_id

		if family_id in [6, 15]:
			model = (extended_model_id << 4) + model

		return {
			'stepping' : stepping_id,
			'model' : model,
			'family' : family,
			'processor_type' : processor_type
		}

	# http://en.wikipedia.org/wiki/CPUID#EAX.3D80000000h:_Get_Highest_Extended_Function_Supported
	def get_max_extension_support(self):
		# Check for extension support
		max_extension_support = self._run_asm(
			b"\xB8\x00\x00\x00\x80" # mov ax,0x80000000
			b"\x0f\xa2"             # cpuid
			b"\xC3"                 # ret
		)

		return max_extension_support

	# http://en.wikipedia.org/wiki/CPUID#EAX.3D1:_Processor_Info_and_Feature_Bits
	def get_flags(self, max_extension_support):
		# EDX
		edx = self._run_asm(
			b"\xB8\x01\x00\x00\x00",   # mov eax,0x1"
			b"\x0f\xa2"                # cpuid
			b"\x89\xD0"                # mov ax,dx
			b"\xC3"                    # ret
		)

		# ECX
		ecx = self._run_asm(
			b"\xB8\x01\x00\x00\x00",   # mov eax,0x1"
			b"\x0f\xa2"                # cpuid
			b"\x89\xC8"                # mov ax,cx
			b"\xC3"                    # ret
		)

		# Get the CPU flags
		flags = {
			'fpu' : _is_bit_set(edx, 0),
			'vme' : _is_bit_set(edx, 1),
			'de' : _is_bit_set(edx, 2),
			'pse' : _is_bit_set(edx, 3),
			'tsc' : _is_bit_set(edx, 4),
			'msr' : _is_bit_set(edx, 5),
			'pae' : _is_bit_set(edx, 6),
			'mce' : _is_bit_set(edx, 7),
			'cx8' : _is_bit_set(edx, 8),
			'apic' : _is_bit_set(edx, 9),
			#'reserved1' : _is_bit_set(edx, 10),
			'sep' : _is_bit_set(edx, 11),
			'mtrr' : _is_bit_set(edx, 12),
			'pge' : _is_bit_set(edx, 13),
			'mca' : _is_bit_set(edx, 14),
			'cmov' : _is_bit_set(edx, 15),
			'pat' : _is_bit_set(edx, 16),
			'pse36' : _is_bit_set(edx, 17),
			'pn' : _is_bit_set(edx, 18),
			'clflush' : _is_bit_set(edx, 19),
			#'reserved2' : _is_bit_set(edx, 20),
			'dts' : _is_bit_set(edx, 21),
			'acpi' : _is_bit_set(edx, 22),
			'mmx' : _is_bit_set(edx, 23),
			'fxsr' : _is_bit_set(edx, 24),
			'sse' : _is_bit_set(edx, 25),
			'sse2' : _is_bit_set(edx, 26),
			'ss' : _is_bit_set(edx, 27),
			'ht' : _is_bit_set(edx, 28),
			'tm' : _is_bit_set(edx, 29),
			'ia64' : _is_bit_set(edx, 30),
			'pbe' : _is_bit_set(edx, 31),

			'pni' : _is_bit_set(ecx, 0),
			'pclmulqdq' : _is_bit_set(ecx, 1),
			'dtes64' : _is_bit_set(ecx, 2),
			'monitor' : _is_bit_set(ecx, 3),
			'ds_cpl' : _is_bit_set(ecx, 4),
			'vmx' : _is_bit_set(ecx, 5),
			'smx' : _is_bit_set(ecx, 6),
			'est' : _is_bit_set(ecx, 7),
			'tm2' : _is_bit_set(ecx, 8),
			'ssse3' : _is_bit_set(ecx, 9),
			'cid' : _is_bit_set(ecx, 10),
			#'reserved3' : _is_bit_set(ecx, 11),
			'fma' : _is_bit_set(ecx, 12),
			'cx16' : _is_bit_set(ecx, 13),
			'xtpr' : _is_bit_set(ecx, 14),
			'pdcm' : _is_bit_set(ecx, 15),
			#'reserved4' : _is_bit_set(ecx, 16),
			'pcid' : _is_bit_set(ecx, 17),
			'dca' : _is_bit_set(ecx, 18),
			'sse4_1' : _is_bit_set(ecx, 19),
			'sse4_2' : _is_bit_set(ecx, 20),
			'x2apic' : _is_bit_set(ecx, 21),
			'movbe' : _is_bit_set(ecx, 22),
			'popcnt' : _is_bit_set(ecx, 23),
			'tscdeadline' : _is_bit_set(ecx, 24),
			'aes' : _is_bit_set(ecx, 25),
			'xsave' : _is_bit_set(ecx, 26),
			'osxsave' : _is_bit_set(ecx, 27),
			'avx' : _is_bit_set(ecx, 28),
			'f16c' : _is_bit_set(ecx, 29),
			'rdrnd' : _is_bit_set(ecx, 30),
			'hypervisor' : _is_bit_set(ecx, 31)
		}

		# Get a list of only the flags that are true
		flags = [k for k, v in flags.items() if v]

		# http://en.wikipedia.org/wiki/CPUID#EAX.3D7.2C_ECX.3D0:_Extended_Features
		if max_extension_support >= 7:
			# EBX
			ebx = self._run_asm(
				b"\x31\xC9",            # xor ecx,ecx
				b"\xB8\x07\x00\x00\x00" # mov eax,7
				b"\x0f\xa2"         # cpuid
				b"\x89\xD8"         # mov ax,bx
				b"\xC3"             # ret
			)

			# ECX
			ecx = self._run_asm(
				b"\x31\xC9",            # xor ecx,ecx
				b"\xB8\x07\x00\x00\x00" # mov eax,7
				b"\x0f\xa2"         # cpuid
				b"\x89\xC8"         # mov ax,cx
				b"\xC3"             # ret
			)

			# Get the extended CPU flags
			extended_flags = {
				#'fsgsbase' : _is_bit_set(ebx, 0),
				#'IA32_TSC_ADJUST' : _is_bit_set(ebx, 1),
				'sgx' : _is_bit_set(ebx, 2),
				'bmi1' : _is_bit_set(ebx, 3),
				'hle' : _is_bit_set(ebx, 4),
				'avx2' : _is_bit_set(ebx, 5),
				#'reserved' : _is_bit_set(ebx, 6),
				'smep' : _is_bit_set(ebx, 7),
				'bmi2' : _is_bit_set(ebx, 8),
				'erms' : _is_bit_set(ebx, 9),
				'invpcid' : _is_bit_set(ebx, 10),
				'rtm' : _is_bit_set(ebx, 11),
				'pqm' : _is_bit_set(ebx, 12),
				#'FPU CS and FPU DS deprecated' : _is_bit_set(ebx, 13),
				'mpx' : _is_bit_set(ebx, 14),
				'pqe' : _is_bit_set(ebx, 15),
				'avx512f' : _is_bit_set(ebx, 16),
				'avx512dq' : _is_bit_set(ebx, 17),
				'rdseed' : _is_bit_set(ebx, 18),
				'adx' : _is_bit_set(ebx, 19),
				'smap' : _is_bit_set(ebx, 20),
				'avx512ifma' : _is_bit_set(ebx, 21),
				'pcommit' : _is_bit_set(ebx, 22),
				'clflushopt' : _is_bit_set(ebx, 23),
				'clwb' : _is_bit_set(ebx, 24),
				'intel_pt' : _is_bit_set(ebx, 25),
				'avx512pf' : _is_bit_set(ebx, 26),
				'avx512er' : _is_bit_set(ebx, 27),
				'avx512cd' : _is_bit_set(ebx, 28),
				'sha' : _is_bit_set(ebx, 29),
				'avx512bw' : _is_bit_set(ebx, 30),
				'avx512vl' : _is_bit_set(ebx, 31),

				'prefetchwt1' : _is_bit_set(ecx, 0),
				'avx512vbmi' : _is_bit_set(ecx, 1),
				'umip' : _is_bit_set(ecx, 2),
				'pku' : _is_bit_set(ecx, 3),
				'ospke' : _is_bit_set(ecx, 4),
				#'reserved' : _is_bit_set(ecx, 5),
				'avx512vbmi2' : _is_bit_set(ecx, 6),
				#'reserved' : _is_bit_set(ecx, 7),
				'gfni' : _is_bit_set(ecx, 8),
				'vaes' : _is_bit_set(ecx, 9),
				'vpclmulqdq' : _is_bit_set(ecx, 10),
				'avx512vnni' : _is_bit_set(ecx, 11),
				'avx512bitalg' : _is_bit_set(ecx, 12),
				#'reserved' : _is_bit_set(ecx, 13),
				'avx512vpopcntdq' : _is_bit_set(ecx, 14),
				#'reserved' : _is_bit_set(ecx, 15),
				#'reserved' : _is_bit_set(ecx, 16),
				#'mpx0' : _is_bit_set(ecx, 17),
				#'mpx1' : _is_bit_set(ecx, 18),
				#'mpx2' : _is_bit_set(ecx, 19),
				#'mpx3' : _is_bit_set(ecx, 20),
				#'mpx4' : _is_bit_set(ecx, 21),
				'rdpid' : _is_bit_set(ecx, 22),
				#'reserved' : _is_bit_set(ecx, 23),
				#'reserved' : _is_bit_set(ecx, 24),
				#'reserved' : _is_bit_set(ecx, 25),
				#'reserved' : _is_bit_set(ecx, 26),
				#'reserved' : _is_bit_set(ecx, 27),
				#'reserved' : _is_bit_set(ecx, 28),
				#'reserved' : _is_bit_set(ecx, 29),
				'sgx_lc' : _is_bit_set(ecx, 30),
				#'reserved' : _is_bit_set(ecx, 31)
			}

			# Get a list of only the flags that are true
			extended_flags = [k for k, v in extended_flags.items() if v]
			flags += extended_flags

		# http://en.wikipedia.org/wiki/CPUID#EAX.3D80000001h:_Extended_Processor_Info_and_Feature_Bits
		if max_extension_support >= 0x80000001:
			# EBX
			ebx = self._run_asm(
				b"\xB8\x01\x00\x00\x80" # mov ax,0x80000001
				b"\x0f\xa2"         # cpuid
				b"\x89\xD8"         # mov ax,bx
				b"\xC3"             # ret
			)

			# ECX
			ecx = self._run_asm(
				b"\xB8\x01\x00\x00\x80" # mov ax,0x80000001
				b"\x0f\xa2"         # cpuid
				b"\x89\xC8"         # mov ax,cx
				b"\xC3"             # ret
			)

			# Get the extended CPU flags
			extended_flags = {
				'fpu' : _is_bit_set(ebx, 0),
				'vme' : _is_bit_set(ebx, 1),
				'de' : _is_bit_set(ebx, 2),
				'pse' : _is_bit_set(ebx, 3),
				'tsc' : _is_bit_set(ebx, 4),
				'msr' : _is_bit_set(ebx, 5),
				'pae' : _is_bit_set(ebx, 6),
				'mce' : _is_bit_set(ebx, 7),
				'cx8' : _is_bit_set(ebx, 8),
				'apic' : _is_bit_set(ebx, 9),
				#'reserved' : _is_bit_set(ebx, 10),
				'syscall' : _is_bit_set(ebx, 11),
				'mtrr' : _is_bit_set(ebx, 12),
				'pge' : _is_bit_set(ebx, 13),
				'mca' : _is_bit_set(ebx, 14),
				'cmov' : _is_bit_set(ebx, 15),
				'pat' : _is_bit_set(ebx, 16),
				'pse36' : _is_bit_set(ebx, 17),
				#'reserved' : _is_bit_set(ebx, 18),
				'mp' : _is_bit_set(ebx, 19),
				'nx' : _is_bit_set(ebx, 20),
				#'reserved' : _is_bit_set(ebx, 21),
				'mmxext' : _is_bit_set(ebx, 22),
				'mmx' : _is_bit_set(ebx, 23),
				'fxsr' : _is_bit_set(ebx, 24),
				'fxsr_opt' : _is_bit_set(ebx, 25),
				'pdpe1gp' : _is_bit_set(ebx, 26),
				'rdtscp' : _is_bit_set(ebx, 27),
				#'reserved' : _is_bit_set(ebx, 28),
				'lm' : _is_bit_set(ebx, 29),
				'3dnowext' : _is_bit_set(ebx, 30),
				'3dnow' : _is_bit_set(ebx, 31),

				'lahf_lm' : _is_bit_set(ecx, 0),
				'cmp_legacy' : _is_bit_set(ecx, 1),
				'svm' : _is_bit_set(ecx, 2),
				'extapic' : _is_bit_set(ecx, 3),
				'cr8_legacy' : _is_bit_set(ecx, 4),
				'abm' : _is_bit_set(ecx, 5),
				'sse4a' : _is_bit_set(ecx, 6),
				'misalignsse' : _is_bit_set(ecx, 7),
				'3dnowprefetch' : _is_bit_set(ecx, 8),
				'osvw' : _is_bit_set(ecx, 9),
				'ibs' : _is_bit_set(ecx, 10),
				'xop' : _is_bit_set(ecx, 11),
				'skinit' : _is_bit_set(ecx, 12),
				'wdt' : _is_bit_set(ecx, 13),
				#'reserved' : _is_bit_set(ecx, 14),
				'lwp' : _is_bit_set(ecx, 15),
				'fma4' : _is_bit_set(ecx, 16),
				'tce' : _is_bit_set(ecx, 17),
				#'reserved' : _is_bit_set(ecx, 18),
				'nodeid_msr' : _is_bit_set(ecx, 19),
				#'reserved' : _is_bit_set(ecx, 20),
				'tbm' : _is_bit_set(ecx, 21),
				'topoext' : _is_bit_set(ecx, 22),
				'perfctr_core' : _is_bit_set(ecx, 23),
				'perfctr_nb' : _is_bit_set(ecx, 24),
				#'reserved' : _is_bit_set(ecx, 25),
				'dbx' : _is_bit_set(ecx, 26),
				'perftsc' : _is_bit_set(ecx, 27),
				'pci_l2i' : _is_bit_set(ecx, 28),
				#'reserved' : _is_bit_set(ecx, 29),
				#'reserved' : _is_bit_set(ecx, 30),
				#'reserved' : _is_bit_set(ecx, 31)
			}

			# Get a list of only the flags that are true
			extended_flags = [k for k, v in extended_flags.items() if v]
			flags += extended_flags

		flags.sort()
		return flags

	# http://en.wikipedia.org/wiki/CPUID#EAX.3D80000002h.2C80000003h.2C80000004h:_Processor_Brand_String
	def get_processor_brand(self, max_extension_support):
		processor_brand = ""

		# Processor brand string
		if max_extension_support >= 0x80000004:
			instructions = [
				b"\xB8\x02\x00\x00\x80", # mov ax,0x80000002
				b"\xB8\x03\x00\x00\x80", # mov ax,0x80000003
				b"\xB8\x04\x00\x00\x80"  # mov ax,0x80000004
			]
			for instruction in instructions:
				# EAX
				eax = self._run_asm(
					instruction,  # mov ax,0x8000000?
					b"\x0f\xa2"   # cpuid
					b"\x89\xC0"   # mov ax,ax
					b"\xC3"       # ret
				)

				# EBX
				ebx = self._run_asm(
					instruction,  # mov ax,0x8000000?
					b"\x0f\xa2"   # cpuid
					b"\x89\xD8"   # mov ax,bx
					b"\xC3"       # ret
				)

				# ECX
				ecx = self._run_asm(
					instruction,  # mov ax,0x8000000?
					b"\x0f\xa2"   # cpuid
					b"\x89\xC8"   # mov ax,cx
					b"\xC3"       # ret
				)

				# EDX
				edx = self._run_asm(
					instruction,  # mov ax,0x8000000?
					b"\x0f\xa2"   # cpuid
					b"\x89\xD0"   # mov ax,dx
					b"\xC3"       # ret
				)

				# Combine each of the 4 bytes in each register into the string
				for reg in [eax, ebx, ecx, edx]:
					for n in [0, 8, 16, 24]:
						processor_brand += chr((reg >> n) & 0xFF)

		# Strip off any trailing NULL terminators and white space
		processor_brand = processor_brand.strip("\0").strip()

		return processor_brand

	# http://en.wikipedia.org/wiki/CPUID#EAX.3D80000006h:_Extended_L2_Cache_Features
	def get_cache(self, max_extension_support):
		cache_info = {}

		# Just return if the cache feature is not supported
		if max_extension_support < 0x80000006:
			return cache_info

		# ECX
		ecx = self._run_asm(
			b"\xB8\x06\x00\x00\x80"  # mov ax,0x80000006
			b"\x0f\xa2"              # cpuid
			b"\x89\xC8"              # mov ax,cx
			b"\xC3"                   # ret
		)

		cache_info = {
			'size_b' : (ecx & 0xFF) * 1024,
			'associativity' : (ecx >> 12) & 0xF,
			'line_size_b' : (ecx >> 16) & 0xFFFF
		}

		return cache_info

	def get_ticks_func(self):
		retval = None

		if DataSource.bits == '32bit':
			# Works on x86_32
			restype = None
			argtypes = (ctypes.POINTER(ctypes.c_uint), ctypes.POINTER(ctypes.c_uint))
			get_ticks_x86_32 = self._asm_func(restype, argtypes,
				[
				b"\x55",         # push bp
				b"\x89\xE5",     # mov bp,sp
				b"\x31\xC0",     # xor ax,ax
				b"\x0F\xA2",     # cpuid
				b"\x0F\x31",     # rdtsc
				b"\x8B\x5D\x08", # mov bx,[di+0x8]
				b"\x8B\x4D\x0C", # mov cx,[di+0xc]
				b"\x89\x13",     # mov [bp+di],dx
				b"\x89\x01",     # mov [bx+di],ax
				b"\x5D",         # pop bp
				b"\xC3"          # ret
				]
			)

			# Monkey patch func to combine high and low args into one return
			old_func = get_ticks_x86_32.func
			def new_func():
				# Pass two uint32s into function
				high = ctypes.c_uint32(0)
				low = ctypes.c_uint32(0)
				old_func(ctypes.byref(high), ctypes.byref(low))

				# Shift the two uint32s into one uint64
				retval = ((high.value << 32) & 0xFFFFFFFF00000000) | low.value
				return retval
			get_ticks_x86_32.func = new_func

			retval = get_ticks_x86_32
		elif DataSource.bits == '64bit':
			# Works on x86_64
			restype = ctypes.c_uint64
			argtypes = ()
			get_ticks_x86_64 = self._asm_func(restype, argtypes,
				[
				b"\x48",         # dec ax
				b"\x31\xC0",     # xor ax,ax
				b"\x0F\xA2",     # cpuid
				b"\x0F\x31",     # rdtsc
				b"\x48",         # dec ax
				b"\xC1\xE2\x20", # shl dx,byte 0x20
				b"\x48",         # dec ax
				b"\x09\xD0",     # or ax,dx
				b"\xC3",         # ret
				]
			)

			retval = get_ticks_x86_64
		return retval

	def get_raw_hz(self):
		from time import sleep

		ticks_fn = self.get_ticks_func()

		start = ticks_fn.func()
		sleep(1)
		end = ticks_fn.func()

		ticks = (end - start)
		ticks_fn.free()

		return ticks

def _get_cpu_info_from_cpuid_actual():
	'''
	Warning! This function has the potential to crash the Python runtime.
	Do not call it directly. Use the _get_cpu_info_from_cpuid function instead.
	It will safely call this function in another process.
	'''

	from io import StringIO

	trace = Trace(True, True)
	info = {}

	# Pipe stdout and stderr to strings
	sys.stdout = trace._stdout
	sys.stderr = trace._stderr

	try:
		# Get the CPU arch and bits
		arch, bits = _parse_arch(DataSource.arch_string_raw)

		# Return none if this is not an X86 CPU
		if not arch in ['X86_32', 'X86_64']:
			trace.fail('Not running on X86_32 or X86_64. Skipping ...')
			return trace.to_dict(info, True)

		# Return none if SE Linux is in enforcing mode
		cpuid = CPUID(trace)
		if cpuid.is_selinux_enforcing:
			trace.fail('SELinux is enforcing. Skipping ...')
			return trace.to_dict(info, True)

		# Get the cpu info from the CPUID register
		max_extension_support = cpuid.get_max_extension_support()
		cache_info = cpuid.get_cache(max_extension_support)
		info = cpuid.get_info()

		processor_brand = cpuid.get_processor_brand(max_extension_support)

		# Get the Hz and scale
		hz_actual = cpuid.get_raw_hz()
		hz_actual = _to_decimal_string(hz_actual)

		# Get the Hz and scale
		hz_advertised, scale = _parse_cpu_brand_string(processor_brand)
		info = {
		'vendor_id_raw' : cpuid.get_vendor_id(),
		'hardware_raw' : '',
		'brand_raw' : processor_brand,

		'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),
		'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, 0),
		'hz_advertised' : _hz_short_to_full(hz_advertised, scale),
		'hz_actual' : _hz_short_to_full(hz_actual, 0),

		'l2_cache_size' : cache_info['size_b'],
		'l2_cache_line_size' : cache_info['line_size_b'],
		'l2_cache_associativity' : cache_info['associativity'],

		'stepping' : info['stepping'],
		'model' : info['model'],
		'family' : info['family'],
		'processor_type' : info['processor_type'],
		'flags' : cpuid.get_flags(max_extension_support)
		}

		info = _filter_dict_keys_with_empty_values(info)
		trace.success()
	except Exception as err:
		from traceback import format_exc
		err_string = format_exc()
		trace._err = ''.join(['\t\t{0}\n'.format(n) for n in err_string.split('\n')]) + '\n'
		return trace.to_dict(info, True)

	return trace.to_dict(info, False)

def _get_cpu_info_from_cpuid_subprocess_wrapper(queue):
	orig_stdout = sys.stdout
	orig_stderr = sys.stderr

	output = _get_cpu_info_from_cpuid_actual()

	sys.stdout = orig_stdout
	sys.stderr = orig_stderr

	queue.put(_obj_to_b64(output))

def _get_cpu_info_from_cpuid():
	'''
	Returns the CPU info gathered by querying the X86 cpuid register in a new process.
	Returns {} on non X86 cpus.
	Returns {} if SELinux is in enforcing mode.
	'''

	g_trace.header('Tying to get info from CPUID ...')

	from multiprocessing import Process, Queue

	# Return {} if can't cpuid
	if not DataSource.can_cpuid:
		g_trace.fail('Can\'t CPUID. Skipping ...')
		return {}

	# Get the CPU arch and bits
	arch, bits = _parse_arch(DataSource.arch_string_raw)

	# Return {} if this is not an X86 CPU
	if not arch in ['X86_32', 'X86_64']:
		g_trace.fail('Not running on X86_32 or X86_64. Skipping ...')
		return {}

	try:
		if CAN_CALL_CPUID_IN_SUBPROCESS:
			# Start running the function in a subprocess
			queue = Queue()
			p = Process(target=_get_cpu_info_from_cpuid_subprocess_wrapper, args=(queue,))
			p.start()

			# Wait for the process to end, while it is still alive
			while p.is_alive():
				p.join(0)

			# Return {} if it failed
			if p.exitcode != 0:
				g_trace.fail('Failed to run CPUID in process. Skipping ...')
				return {}

			# Return {} if no results
			if queue.empty():
				g_trace.fail('Failed to get anything from CPUID process. Skipping ...')
				return {}
			# Return the result, only if there is something to read
			else:
				output = _b64_to_obj(queue.get())
				import pprint
				pp = pprint.PrettyPrinter(indent=4)
				#pp.pprint(output)

				if 'output' in output and output['output']:
					g_trace.write(output['output'])

				if 'stdout' in output and output['stdout']:
					sys.stdout.write('{0}\n'.format(output['stdout']))
					sys.stdout.flush()

				if 'stderr' in output and output['stderr']:
					sys.stderr.write('{0}\n'.format(output['stderr']))
					sys.stderr.flush()

				if 'is_fail' not in output:
					g_trace.fail('Failed to get is_fail from CPUID process. Skipping ...')
					return {}

				# Fail if there was an exception
				if 'err' in output and output['err']:
					g_trace.fail('Failed to run CPUID in process. Skipping ...')
					g_trace.write(output['err'])
					g_trace.write('Failed ...')
					return {}

				if 'is_fail' in output and output['is_fail']:
					g_trace.write('Failed ...')
					return {}

				if 'info' not in output or not output['info']:
					g_trace.fail('Failed to get return info from CPUID process. Skipping ...')
					return {}

				return output['info']
		else:
			# FIXME: This should write the values like in the above call to actual
			orig_stdout = sys.stdout
			orig_stderr = sys.stderr

			output = _get_cpu_info_from_cpuid_actual()

			sys.stdout = orig_stdout
			sys.stderr = orig_stderr

			g_trace.success()
			return output['info']
	except Exception as err:
		g_trace.fail(err)

	# Return {} if everything failed
	return {}

def _get_cpu_info_from_proc_cpuinfo():
	'''
	Returns the CPU info gathered from /proc/cpuinfo.
	Returns {} if /proc/cpuinfo is not found.
	'''

	g_trace.header('Tying to get info from /proc/cpuinfo ...')

	try:
		# Just return {} if there is no cpuinfo
		if not DataSource.has_proc_cpuinfo():
			g_trace.fail('Failed to find /proc/cpuinfo. Skipping ...')
			return {}

		returncode, output = DataSource.cat_proc_cpuinfo()
		if returncode != 0:
			g_trace.fail('Failed to run cat /proc/cpuinfo. Skipping ...')
			return {}

		# Various fields
		vendor_id = _get_field(False, output, None, '', 'vendor_id', 'vendor id', 'vendor')
		processor_brand = _get_field(True, output, None, None, 'model name', 'cpu', 'processor', 'uarch')
		cache_size = _get_field(False, output, None, '', 'cache size')
		stepping = _get_field(False, output, int, -1, 'stepping')
		model = _get_field(False, output, int, -1, 'model')
		family = _get_field(False, output, int, -1, 'cpu family')
		hardware = _get_field(False, output, None, '', 'Hardware')

		# Flags
		flags = _get_field(False, output, None, None, 'flags', 'Features', 'ASEs implemented')
		if flags:
			flags = flags.split()
			flags.sort()

		# Check for other cache format
		if not cache_size:
			try:
				for i in range(0, 10):
					name = "cache{0}".format(i)
					value = _get_field(False, output, None, None, name)
					if value:
						value = [entry.split('=') for entry in value.split(' ')]
						value = dict(value)
						if 'level' in value and value['level'] == '3' and 'size' in value:
							cache_size = value['size']
							break
			except Exception:
				pass

		# Convert from MHz string to Hz
		hz_actual = _get_field(False, output, None, '', 'cpu MHz', 'cpu speed', 'clock', 'cpu MHz dynamic', 'cpu MHz static')
		hz_actual = hz_actual.lower().rstrip('mhz').strip()
		hz_actual = _to_decimal_string(hz_actual)

		# Convert from GHz/MHz string to Hz
		hz_advertised, scale = (None, 0)
		try:
			hz_advertised, scale = _parse_cpu_brand_string(processor_brand)
		except Exception:
			pass

		info = {
		'hardware_raw' : hardware,
		'brand_raw' : processor_brand,

		'l3_cache_size' : _friendly_bytes_to_int(cache_size),
		'flags' : flags,
		'vendor_id_raw' : vendor_id,
		'stepping' : stepping,
		'model' : model,
		'family' : family,
		}

		# Make the Hz the same for actual and advertised if missing any
		if not hz_advertised or hz_advertised == '0.0':
			hz_advertised = hz_actual
			scale = 6
		elif not hz_actual or hz_actual == '0.0':
			hz_actual = hz_advertised

		# Add the Hz if there is one
		if _hz_short_to_full(hz_advertised, scale) > (0, 0):
			info['hz_advertised_friendly'] = _hz_short_to_friendly(hz_advertised, scale)
			info['hz_advertised'] = _hz_short_to_full(hz_advertised, scale)
		if _hz_short_to_full(hz_actual, scale) > (0, 0):
			info['hz_actual_friendly'] = _hz_short_to_friendly(hz_actual, 6)
			info['hz_actual'] = _hz_short_to_full(hz_actual, 6)

		info = _filter_dict_keys_with_empty_values(info, {'stepping':0, 'model':0, 'family':0})
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		#raise # NOTE: To have this throw on error, uncomment this line
		return {}

def _get_cpu_info_from_cpufreq_info():
	'''
	Returns the CPU info gathered from cpufreq-info.
	Returns {} if cpufreq-info is not found.
	'''

	g_trace.header('Tying to get info from cpufreq-info ...')

	try:
		hz_brand, scale = '0.0', 0

		if not DataSource.has_cpufreq_info():
			g_trace.fail('Failed to find cpufreq-info. Skipping ...')
			return {}

		returncode, output = DataSource.cpufreq_info()
		if returncode != 0:
			g_trace.fail('Failed to run cpufreq-info. Skipping ...')
			return {}

		hz_brand = output.split('current CPU frequency is')[1].split('\n')[0]
		i = hz_brand.find('Hz')
		assert(i != -1)
		hz_brand = hz_brand[0 : i+2].strip().lower()

		if hz_brand.endswith('mhz'):
			scale = 6
		elif hz_brand.endswith('ghz'):
			scale = 9
		hz_brand = hz_brand.rstrip('mhz').rstrip('ghz').strip()
		hz_brand = _to_decimal_string(hz_brand)

		info = {
			'hz_advertised_friendly' : _hz_short_to_friendly(hz_brand, scale),
			'hz_actual_friendly' : _hz_short_to_friendly(hz_brand, scale),
			'hz_advertised' : _hz_short_to_full(hz_brand, scale),
			'hz_actual' : _hz_short_to_full(hz_brand, scale),
		}

		info = _filter_dict_keys_with_empty_values(info)
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		#raise # NOTE: To have this throw on error, uncomment this line
		return {}

def _get_cpu_info_from_lscpu():
	'''
	Returns the CPU info gathered from lscpu.
	Returns {} if lscpu is not found.
	'''

	g_trace.header('Tying to get info from lscpu ...')

	try:
		if not DataSource.has_lscpu():
			g_trace.fail('Failed to find lscpu. Skipping ...')
			return {}

		returncode, output = DataSource.lscpu()
		if returncode != 0:
			g_trace.fail('Failed to run lscpu. Skipping ...')
			return {}

		info = {}

		new_hz = _get_field(False, output, None, None, 'CPU max MHz', 'CPU MHz')
		if new_hz:
			new_hz = _to_decimal_string(new_hz)
			scale = 6
			info['hz_advertised_friendly'] = _hz_short_to_friendly(new_hz, scale)
			info['hz_actual_friendly'] = _hz_short_to_friendly(new_hz, scale)
			info['hz_advertised'] = _hz_short_to_full(new_hz, scale)
			info['hz_actual'] = _hz_short_to_full(new_hz, scale)

		new_hz = _get_field(False, output, None, None, 'CPU dynamic MHz', 'CPU static MHz')
		if new_hz:
			new_hz = _to_decimal_string(new_hz)
			scale = 6
			info['hz_advertised_friendly'] = _hz_short_to_friendly(new_hz, scale)
			info['hz_actual_friendly'] = _hz_short_to_friendly(new_hz, scale)
			info['hz_advertised'] = _hz_short_to_full(new_hz, scale)
			info['hz_actual'] = _hz_short_to_full(new_hz, scale)

		vendor_id = _get_field(False, output, None, None, 'Vendor ID')
		if vendor_id:
			info['vendor_id_raw'] = vendor_id

		brand = _get_field(False, output, None, None, 'Model name')
		if brand:
			info['brand_raw'] = brand
		else:
			brand = _get_field(False, output, None, None, 'Model')
			if brand and not brand.isdigit():
				info['brand_raw'] = brand

		family = _get_field(False, output, None, None, 'CPU family')
		if family and family.isdigit():
			info['family'] = int(family)

		stepping = _get_field(False, output, None, None, 'Stepping')
		if stepping and stepping.isdigit():
			info['stepping'] = int(stepping)

		model = _get_field(False, output, None, None, 'Model')
		if model and model.isdigit():
			info['model'] = int(model)

		l1_data_cache_size = _get_field(False, output, None, None, 'L1d cache')
		if l1_data_cache_size:
			l1_data_cache_size = l1_data_cache_size.split('(')[0].strip()
			info['l1_data_cache_size'] = _friendly_bytes_to_int(l1_data_cache_size)

		l1_instruction_cache_size = _get_field(False, output, None, None, 'L1i cache')
		if l1_instruction_cache_size:
			l1_instruction_cache_size = l1_instruction_cache_size.split('(')[0].strip()
			info['l1_instruction_cache_size'] = _friendly_bytes_to_int(l1_instruction_cache_size)

		l2_cache_size = _get_field(False, output, None, None, 'L2 cache', 'L2d cache')
		if l2_cache_size:
			l2_cache_size = l2_cache_size.split('(')[0].strip()
			info['l2_cache_size'] = _friendly_bytes_to_int(l2_cache_size)

		l3_cache_size = _get_field(False, output, None, None, 'L3 cache')
		if l3_cache_size:
			l3_cache_size = l3_cache_size.split('(')[0].strip()
			info['l3_cache_size'] = _friendly_bytes_to_int(l3_cache_size)

		# Flags
		flags = _get_field(False, output, None, None, 'flags', 'Features', 'ASEs implemented')
		if flags:
			flags = flags.split()
			flags.sort()
			info['flags'] = flags

		info = _filter_dict_keys_with_empty_values(info, {'stepping':0, 'model':0, 'family':0})
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		#raise # NOTE: To have this throw on error, uncomment this line
		return {}

def _get_cpu_info_from_dmesg():
	'''
	Returns the CPU info gathered from dmesg.
	Returns {} if dmesg is not found or does not have the desired info.
	'''

	g_trace.header('Tying to get info from the dmesg ...')

	# Just return {} if this arch has an unreliable dmesg log
	arch, bits = _parse_arch(DataSource.arch_string_raw)
	if arch in ['S390X']:
		g_trace.fail('Running on S390X. Skipping ...')
		return {}

	# Just return {} if there is no dmesg
	if not DataSource.has_dmesg():
		g_trace.fail('Failed to find dmesg. Skipping ...')
		return {}

	# If dmesg fails return {}
	returncode, output = DataSource.dmesg_a()
	if output is None or returncode != 0:
		g_trace.fail('Failed to run \"dmesg -a\". Skipping ...')
		return {}

	info = _parse_dmesg_output(output)
	g_trace.success()
	return info


# https://openpowerfoundation.org/wp-content/uploads/2016/05/LoPAPR_DRAFT_v11_24March2016_cmt1.pdf
# page 767
def _get_cpu_info_from_ibm_pa_features():
	'''
	Returns the CPU info gathered from lsprop /proc/device-tree/cpus/*/ibm,pa-features
	Returns {} if lsprop is not found or ibm,pa-features does not have the desired info.
	'''

	g_trace.header('Tying to get info from lsprop ...')

	try:
		# Just return {} if there is no lsprop
		if not DataSource.has_ibm_pa_features():
			g_trace.fail('Failed to find lsprop. Skipping ...')
			return {}

		# If ibm,pa-features fails return {}
		returncode, output = DataSource.ibm_pa_features()
		if output is None or returncode != 0:
			g_trace.fail('Failed to glob /proc/device-tree/cpus/*/ibm,pa-features. Skipping ...')
			return {}

		# Filter out invalid characters from output
		value = output.split("ibm,pa-features")[1].lower()
		value = [s for s in value if s in list('0123456789abcfed')]
		value = ''.join(value)

		# Get data converted to Uint32 chunks
		left = int(value[0 : 8], 16)
		right = int(value[8 : 16], 16)

		# Get the CPU flags
		flags = {
			# Byte 0
			'mmu' : _is_bit_set(left, 0),
			'fpu' : _is_bit_set(left, 1),
			'slb' : _is_bit_set(left, 2),
			'run' : _is_bit_set(left, 3),
			#'reserved' : _is_bit_set(left, 4),
			'dabr' : _is_bit_set(left, 5),
			'ne' : _is_bit_set(left, 6),
			'wtr' : _is_bit_set(left, 7),

			# Byte 1
			'mcr' : _is_bit_set(left, 8),
			'dsisr' : _is_bit_set(left, 9),
			'lp' : _is_bit_set(left, 10),
			'ri' : _is_bit_set(left, 11),
			'dabrx' : _is_bit_set(left, 12),
			'sprg3' : _is_bit_set(left, 13),
			'rislb' : _is_bit_set(left, 14),
			'pp' : _is_bit_set(left, 15),

			# Byte 2
			'vpm' : _is_bit_set(left, 16),
			'dss_2.05' : _is_bit_set(left, 17),
			#'reserved' : _is_bit_set(left, 18),
			'dar' : _is_bit_set(left, 19),
			#'reserved' : _is_bit_set(left, 20),
			'ppr' : _is_bit_set(left, 21),
			'dss_2.02' : _is_bit_set(left, 22),
			'dss_2.06' : _is_bit_set(left, 23),

			# Byte 3
			'lsd_in_dscr' : _is_bit_set(left, 24),
			'ugr_in_dscr' : _is_bit_set(left, 25),
			#'reserved' : _is_bit_set(left, 26),
			#'reserved' : _is_bit_set(left, 27),
			#'reserved' : _is_bit_set(left, 28),
			#'reserved' : _is_bit_set(left, 29),
			#'reserved' : _is_bit_set(left, 30),
			#'reserved' : _is_bit_set(left, 31),

			# Byte 4
			'sso_2.06' : _is_bit_set(right, 0),
			#'reserved' : _is_bit_set(right, 1),
			#'reserved' : _is_bit_set(right, 2),
			#'reserved' : _is_bit_set(right, 3),
			#'reserved' : _is_bit_set(right, 4),
			#'reserved' : _is_bit_set(right, 5),
			#'reserved' : _is_bit_set(right, 6),
			#'reserved' : _is_bit_set(right, 7),

			# Byte 5
			'le' : _is_bit_set(right, 8),
			'cfar' : _is_bit_set(right, 9),
			'eb' : _is_bit_set(right, 10),
			'lsq_2.07' : _is_bit_set(right, 11),
			#'reserved' : _is_bit_set(right, 12),
			#'reserved' : _is_bit_set(right, 13),
			#'reserved' : _is_bit_set(right, 14),
			#'reserved' : _is_bit_set(right, 15),

			# Byte 6
			'dss_2.07' : _is_bit_set(right, 16),
			#'reserved' : _is_bit_set(right, 17),
			#'reserved' : _is_bit_set(right, 18),
			#'reserved' : _is_bit_set(right, 19),
			#'reserved' : _is_bit_set(right, 20),
			#'reserved' : _is_bit_set(right, 21),
			#'reserved' : _is_bit_set(right, 22),
			#'reserved' : _is_bit_set(right, 23),

			# Byte 7
			#'reserved' : _is_bit_set(right, 24),
			#'reserved' : _is_bit_set(right, 25),
			#'reserved' : _is_bit_set(right, 26),
			#'reserved' : _is_bit_set(right, 27),
			#'reserved' : _is_bit_set(right, 28),
			#'reserved' : _is_bit_set(right, 29),
			#'reserved' : _is_bit_set(right, 30),
			#'reserved' : _is_bit_set(right, 31),
		}

		# Get a list of only the flags that are true
		flags = [k for k, v in flags.items() if v]
		flags.sort()

		info = {
			'flags' : flags
		}
		info = _filter_dict_keys_with_empty_values(info)
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		return {}


def _get_cpu_info_from_cat_var_run_dmesg_boot():
	'''
	Returns the CPU info gathered from /var/run/dmesg.boot.
	Returns {} if dmesg is not found or does not have the desired info.
	'''

	g_trace.header('Tying to get info from the /var/run/dmesg.boot log ...')

	# Just return {} if there is no /var/run/dmesg.boot
	if not DataSource.has_var_run_dmesg_boot():
		g_trace.fail('Failed to find /var/run/dmesg.boot file. Skipping ...')
		return {}

	# If dmesg.boot fails return {}
	returncode, output = DataSource.cat_var_run_dmesg_boot()
	if output is None or returncode != 0:
		g_trace.fail('Failed to run \"cat /var/run/dmesg.boot\". Skipping ...')
		return {}

	info = _parse_dmesg_output(output)
	g_trace.success()
	return info


def _get_cpu_info_from_sysctl():
	'''
	Returns the CPU info gathered from sysctl.
	Returns {} if sysctl is not found.
	'''

	g_trace.header('Tying to get info from sysctl ...')

	try:
		# Just return {} if there is no sysctl
		if not DataSource.has_sysctl():
			g_trace.fail('Failed to find sysctl. Skipping ...')
			return {}

		# If sysctl fails return {}
		returncode, output = DataSource.sysctl_machdep_cpu_hw_cpufrequency()
		if output is None or returncode != 0:
			g_trace.fail('Failed to run \"sysctl machdep.cpu hw.cpufrequency\". Skipping ...')
			return {}

		# Various fields
		vendor_id = _get_field(False, output, None, None, 'machdep.cpu.vendor')
		processor_brand = _get_field(True, output, None, None, 'machdep.cpu.brand_string')
		cache_size = _get_field(False, output, int, 0, 'machdep.cpu.cache.size')
		stepping = _get_field(False, output, int, 0, 'machdep.cpu.stepping')
		model = _get_field(False, output, int, 0, 'machdep.cpu.model')
		family = _get_field(False, output, int, 0, 'machdep.cpu.family')

		# Flags
		flags = _get_field(False, output, None, '', 'machdep.cpu.features').lower().split()
		flags.extend(_get_field(False, output, None, '', 'machdep.cpu.leaf7_features').lower().split())
		flags.extend(_get_field(False, output, None, '', 'machdep.cpu.extfeatures').lower().split())
		flags.sort()

		# Convert from GHz/MHz string to Hz
		hz_advertised, scale = _parse_cpu_brand_string(processor_brand)
		hz_actual = _get_field(False, output, None, None, 'hw.cpufrequency')
		hz_actual = _to_decimal_string(hz_actual)

		info = {
		'vendor_id_raw' : vendor_id,
		'brand_raw' : processor_brand,

		'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),
		'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, 0),
		'hz_advertised' : _hz_short_to_full(hz_advertised, scale),
		'hz_actual' : _hz_short_to_full(hz_actual, 0),

		'l2_cache_size' : int(cache_size) * 1024,

		'stepping' : stepping,
		'model' : model,
		'family' : family,
		'flags' : flags
		}

		info = _filter_dict_keys_with_empty_values(info)
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		return {}


def _get_cpu_info_from_sysinfo():
	'''
	Returns the CPU info gathered from sysinfo.
	Returns {} if sysinfo is not found.
	'''

	info = _get_cpu_info_from_sysinfo_v1()
	info.update(_get_cpu_info_from_sysinfo_v2())
	return info

def _get_cpu_info_from_sysinfo_v1():
	'''
	Returns the CPU info gathered from sysinfo.
	Returns {} if sysinfo is not found.
	'''

	g_trace.header('Tying to get info from sysinfo version 1 ...')

	try:
		# Just return {} if there is no sysinfo
		if not DataSource.has_sysinfo():
			g_trace.fail('Failed to find sysinfo. Skipping ...')
			return {}

		# If sysinfo fails return {}
		returncode, output = DataSource.sysinfo_cpu()
		if output is None or returncode != 0:
			g_trace.fail('Failed to run \"sysinfo -cpu\". Skipping ...')
			return {}

		# Various fields
		vendor_id = '' #_get_field(False, output, None, None, 'CPU #0: ')
		processor_brand = output.split('CPU #0: "')[1].split('"\n')[0].strip()
		cache_size = '' #_get_field(False, output, None, None, 'machdep.cpu.cache.size')
		stepping = int(output.split(', stepping ')[1].split(',')[0].strip())
		model = int(output.split(', model ')[1].split(',')[0].strip())
		family = int(output.split(', family ')[1].split(',')[0].strip())

		# Flags
		flags = []
		for line in output.split('\n'):
			if line.startswith('\t\t'):
				for flag in line.strip().lower().split():
					flags.append(flag)
		flags.sort()

		# Convert from GHz/MHz string to Hz
		hz_advertised, scale = _parse_cpu_brand_string(processor_brand)
		hz_actual = hz_advertised

		info = {
		'vendor_id_raw' : vendor_id,
		'brand_raw' : processor_brand,

		'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),
		'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, scale),
		'hz_advertised' : _hz_short_to_full(hz_advertised, scale),
		'hz_actual' : _hz_short_to_full(hz_actual, scale),

		'l2_cache_size' : _to_friendly_bytes(cache_size),

		'stepping' : stepping,
		'model' : model,
		'family' : family,
		'flags' : flags
		}

		info = _filter_dict_keys_with_empty_values(info)
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		#raise # NOTE: To have this throw on error, uncomment this line
		return {}

def _get_cpu_info_from_sysinfo_v2():
	'''
	Returns the CPU info gathered from sysinfo.
	Returns {} if sysinfo is not found.
	'''

	g_trace.header('Tying to get info from sysinfo version 2 ...')

	try:
		# Just return {} if there is no sysinfo
		if not DataSource.has_sysinfo():
			g_trace.fail('Failed to find sysinfo. Skipping ...')
			return {}

		# If sysinfo fails return {}
		returncode, output = DataSource.sysinfo_cpu()
		if output is None or returncode != 0:
			g_trace.fail('Failed to run \"sysinfo -cpu\". Skipping ...')
			return {}

		# Various fields
		vendor_id = '' #_get_field(False, output, None, None, 'CPU #0: ')
		processor_brand = output.split('CPU #0: "')[1].split('"\n')[0].strip()
		cache_size = '' #_get_field(False, output, None, None, 'machdep.cpu.cache.size')
		signature = output.split('Signature:')[1].split('\n')[0].strip()
		#
		stepping = int(signature.split('stepping ')[1].split(',')[0].strip())
		model = int(signature.split('model ')[1].split(',')[0].strip())
		family = int(signature.split('family ')[1].split(',')[0].strip())

		# Flags
		def get_subsection_flags(output):
			retval = []
			for line in output.split('\n')[1:]:
				if not line.startswith('                ') and not line.startswith('		'): break
				for entry in line.strip().lower().split(' '):
					retval.append(entry)
			return retval

		flags = get_subsection_flags(output.split('Features: ')[1]) + \
				get_subsection_flags(output.split('Extended Features (0x00000001): ')[1]) + \
				get_subsection_flags(output.split('Extended Features (0x80000001): ')[1])
		flags.sort()

		# Convert from GHz/MHz string to Hz
		lines = [n for n in output.split('\n') if n]
		raw_hz = lines[0].split('running at ')[1].strip().lower()
		hz_advertised = raw_hz.rstrip('mhz').rstrip('ghz').strip()
		hz_advertised = _to_decimal_string(hz_advertised)
		hz_actual = hz_advertised

		scale = 0
		if raw_hz.endswith('mhz'):
			scale = 6
		elif raw_hz.endswith('ghz'):
			scale = 9

		info = {
		'vendor_id_raw' : vendor_id,
		'brand_raw' : processor_brand,

		'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),
		'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, scale),
		'hz_advertised' : _hz_short_to_full(hz_advertised, scale),
		'hz_actual' : _hz_short_to_full(hz_actual, scale),

		'l2_cache_size' : _to_friendly_bytes(cache_size),

		'stepping' : stepping,
		'model' : model,
		'family' : family,
		'flags' : flags
		}

		info = _filter_dict_keys_with_empty_values(info)
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		#raise # NOTE: To have this throw on error, uncomment this line
		return {}

def _get_cpu_info_from_wmic():
	'''
	Returns the CPU info gathered from WMI.
	Returns {} if not on Windows, or wmic is not installed.
	'''
	g_trace.header('Tying to get info from wmic ...')

	try:
		# Just return {} if not Windows or there is no wmic
		if not DataSource.is_windows or not DataSource.has_wmic():
			g_trace.fail('Failed to find WMIC, or not on Windows. Skipping ...')
			return {}

		returncode, output = DataSource.wmic_cpu()
		if output is None or returncode != 0:
			g_trace.fail('Failed to run wmic. Skipping ...')
			return {}

		# Break the list into key values pairs
		value = output.split("\n")
		value = [s.rstrip().split('=') for s in value if '=' in s]
		value = {k: v for k, v in value if v}

		# Get the advertised MHz
		processor_brand = value.get('Name')
		hz_advertised, scale_advertised = _parse_cpu_brand_string(processor_brand)

		# Get the actual MHz
		hz_actual = value.get('CurrentClockSpeed')
		scale_actual = 6
		if hz_actual:
			hz_actual = _to_decimal_string(hz_actual)

		# Get cache sizes
		l2_cache_size = value.get('L2CacheSize') # NOTE: L2CacheSize is in kilobytes
		if l2_cache_size:
			l2_cache_size = int(l2_cache_size) * 1024

		l3_cache_size = value.get('L3CacheSize') # NOTE: L3CacheSize is in kilobytes
		if l3_cache_size:
			l3_cache_size = int(l3_cache_size) * 1024

		# Get family, model, and stepping
		family, model, stepping = '', '', ''
		description = value.get('Description') or value.get('Caption')
		entries = description.split(' ')

		if 'Family' in entries and entries.index('Family') < len(entries)-1:
			i = entries.index('Family')
			family = int(entries[i + 1])

		if 'Model' in entries and entries.index('Model') < len(entries)-1:
			i = entries.index('Model')
			model = int(entries[i + 1])

		if 'Stepping' in entries and entries.index('Stepping') < len(entries)-1:
			i = entries.index('Stepping')
			stepping = int(entries[i + 1])

		info = {
			'vendor_id_raw' : value.get('Manufacturer'),
			'brand_raw' : processor_brand,

			'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale_advertised),
			'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, scale_actual),
			'hz_advertised' : _hz_short_to_full(hz_advertised, scale_advertised),
			'hz_actual' : _hz_short_to_full(hz_actual, scale_actual),

			'l2_cache_size' : l2_cache_size,
			'l3_cache_size' : l3_cache_size,

			'stepping' : stepping,
			'model' : model,
			'family' : family,
		}

		info = _filter_dict_keys_with_empty_values(info)
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		#raise # NOTE: To have this throw on error, uncomment this line
		return {}

def _get_cpu_info_from_registry():
	'''
	Returns the CPU info gathered from the Windows Registry.
	Returns {} if not on Windows.
	'''

	g_trace.header('Tying to get info from Windows registry ...')

	try:
		# Just return {} if not on Windows
		if not DataSource.is_windows:
			g_trace.fail('Not running on Windows. Skipping ...')
			return {}

		# Get the CPU name
		processor_brand = DataSource.winreg_processor_brand().strip()

		# Get the CPU vendor id
		vendor_id = DataSource.winreg_vendor_id_raw()

		# Get the CPU arch and bits
		arch_string_raw = DataSource.winreg_arch_string_raw()
		arch, bits = _parse_arch(arch_string_raw)

		# Get the actual CPU Hz
		hz_actual = DataSource.winreg_hz_actual()
		hz_actual = _to_decimal_string(hz_actual)

		# Get the advertised CPU Hz
		hz_advertised, scale = _parse_cpu_brand_string(processor_brand)

		# If advertised hz not found, use the actual hz
		if hz_advertised == '0.0':
			scale = 6
			hz_advertised = _to_decimal_string(hz_actual)

		# Get the CPU features
		feature_bits = DataSource.winreg_feature_bits()

		def is_set(bit):
			mask = 0x80000000 >> bit
			retval = mask & feature_bits > 0
			return retval

		# http://en.wikipedia.org/wiki/CPUID
		# http://unix.stackexchange.com/questions/43539/what-do-the-flags-in-proc-cpuinfo-mean
		# http://www.lohninger.com/helpcsuite/public_constants_cpuid.htm
		flags = {
			'fpu' : is_set(0), # Floating Point Unit
			'vme' : is_set(1), # V86 Mode Extensions
			'de' : is_set(2), # Debug Extensions - I/O breakpoints supported
			'pse' : is_set(3), # Page Size Extensions (4 MB pages supported)
			'tsc' : is_set(4), # Time Stamp Counter and RDTSC instruction are available
			'msr' : is_set(5), # Model Specific Registers
			'pae' : is_set(6), # Physical Address Extensions (36 bit address, 2MB pages)
			'mce' : is_set(7), # Machine Check Exception supported
			'cx8' : is_set(8), # Compare Exchange Eight Byte instruction available
			'apic' : is_set(9), # Local APIC present (multiprocessor operation support)
			'sepamd' : is_set(10), # Fast system calls (AMD only)
			'sep' : is_set(11), # Fast system calls
			'mtrr' : is_set(12), # Memory Type Range Registers
			'pge' : is_set(13), # Page Global Enable
			'mca' : is_set(14), # Machine Check Architecture
			'cmov' : is_set(15), # Conditional MOVe instructions
			'pat' : is_set(16), # Page Attribute Table
			'pse36' : is_set(17), # 36 bit Page Size Extensions
			'serial' : is_set(18), # Processor Serial Number
			'clflush' : is_set(19), # Cache Flush
			#'reserved1' : is_set(20), # reserved
			'dts' : is_set(21), # Debug Trace Store
			'acpi' : is_set(22), # ACPI support
			'mmx' : is_set(23), # MultiMedia Extensions
			'fxsr' : is_set(24), # FXSAVE and FXRSTOR instructions
			'sse' : is_set(25), # SSE instructions
			'sse2' : is_set(26), # SSE2 (WNI) instructions
			'ss' : is_set(27), # self snoop
			#'reserved2' : is_set(28), # reserved
			'tm' : is_set(29), # Automatic clock control
			'ia64' : is_set(30), # IA64 instructions
			'3dnow' : is_set(31) # 3DNow! instructions available
		}

		# Get a list of only the flags that are true
		flags = [k for k, v in flags.items() if v]
		flags.sort()

		info = {
		'vendor_id_raw' : vendor_id,
		'brand_raw' : processor_brand,

		'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),
		'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, 6),
		'hz_advertised' : _hz_short_to_full(hz_advertised, scale),
		'hz_actual' : _hz_short_to_full(hz_actual, 6),

		'flags' : flags
		}

		info = _filter_dict_keys_with_empty_values(info)
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		return {}

def _get_cpu_info_from_kstat():
	'''
	Returns the CPU info gathered from isainfo and kstat.
	Returns {} if isainfo or kstat are not found.
	'''

	g_trace.header('Tying to get info from kstat ...')

	try:
		# Just return {} if there is no isainfo or kstat
		if not DataSource.has_isainfo() or not DataSource.has_kstat():
			g_trace.fail('Failed to find isinfo or kstat. Skipping ...')
			return {}

		# If isainfo fails return {}
		returncode, flag_output = DataSource.isainfo_vb()
		if flag_output is None or returncode != 0:
			g_trace.fail('Failed to run \"isainfo -vb\". Skipping ...')
			return {}

		# If kstat fails return {}
		returncode, kstat = DataSource.kstat_m_cpu_info()
		if kstat is None or returncode != 0:
			g_trace.fail('Failed to run \"kstat -m cpu_info\". Skipping ...')
			return {}

		# Various fields
		vendor_id = kstat.split('\tvendor_id ')[1].split('\n')[0].strip()
		processor_brand = kstat.split('\tbrand ')[1].split('\n')[0].strip()
		stepping = int(kstat.split('\tstepping ')[1].split('\n')[0].strip())
		model = int(kstat.split('\tmodel ')[1].split('\n')[0].strip())
		family = int(kstat.split('\tfamily ')[1].split('\n')[0].strip())

		# Flags
		flags = flag_output.strip().split('\n')[-1].strip().lower().split()
		flags.sort()

		# Convert from GHz/MHz string to Hz
		scale = 6
		hz_advertised = kstat.split('\tclock_MHz ')[1].split('\n')[0].strip()
		hz_advertised = _to_decimal_string(hz_advertised)

		# Convert from GHz/MHz string to Hz
		hz_actual = kstat.split('\tcurrent_clock_Hz ')[1].split('\n')[0].strip()
		hz_actual = _to_decimal_string(hz_actual)

		info = {
		'vendor_id_raw' : vendor_id,
		'brand_raw' : processor_brand,

		'hz_advertised_friendly' : _hz_short_to_friendly(hz_advertised, scale),
		'hz_actual_friendly' : _hz_short_to_friendly(hz_actual, 0),
		'hz_advertised' : _hz_short_to_full(hz_advertised, scale),
		'hz_actual' : _hz_short_to_full(hz_actual, 0),

		'stepping' : stepping,
		'model' : model,
		'family' : family,
		'flags' : flags
		}

		info = _filter_dict_keys_with_empty_values(info)
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		return {}

def _get_cpu_info_from_platform_uname():

	g_trace.header('Tying to get info from platform.uname ...')

	try:
		uname = DataSource.uname_string_raw.split(',')[0]

		family, model, stepping = (None, None, None)
		entries = uname.split(' ')

		if 'Family' in entries and entries.index('Family') < len(entries)-1:
			i = entries.index('Family')
			family = int(entries[i + 1])

		if 'Model' in entries and entries.index('Model') < len(entries)-1:
			i = entries.index('Model')
			model = int(entries[i + 1])

		if 'Stepping' in entries and entries.index('Stepping') < len(entries)-1:
			i = entries.index('Stepping')
			stepping = int(entries[i + 1])

		info = {
			'family' : family,
			'model' : model,
			'stepping' : stepping
		}
		info = _filter_dict_keys_with_empty_values(info)
		g_trace.success()
		return info
	except Exception as err:
		g_trace.fail(err)
		return {}

def _get_cpu_info_internal():
	'''
	Returns the CPU info by using the best sources of information for your OS.
	Returns {} if nothing is found.
	'''

	g_trace.write('!' * 80)

	# Get the CPU arch and bits
	arch, bits = _parse_arch(DataSource.arch_string_raw)

	friendly_maxsize = { 2**31-1: '32 bit', 2**63-1: '64 bit' }.get(sys.maxsize) or 'unknown bits'
	friendly_version = "{0}.{1}.{2}.{3}.{4}".format(*sys.version_info)
	PYTHON_VERSION = "{0} ({1})".format(friendly_version, friendly_maxsize)

	info = {
		'python_version' : PYTHON_VERSION,
		'cpuinfo_version' : CPUINFO_VERSION,
		'cpuinfo_version_string' : CPUINFO_VERSION_STRING,
		'arch' : arch,
		'bits' : bits,
		'count' : DataSource.cpu_count,
		'arch_string_raw' : DataSource.arch_string_raw,
	}

	g_trace.write("python_version: {0}".format(info['python_version']))
	g_trace.write("cpuinfo_version: {0}".format(info['cpuinfo_version']))
	g_trace.write("arch: {0}".format(info['arch']))
	g_trace.write("bits: {0}".format(info['bits']))
	g_trace.write("count: {0}".format(info['count']))
	g_trace.write("arch_string_raw: {0}".format(info['arch_string_raw']))

	# Try the Windows wmic
	_copy_new_fields(info, _get_cpu_info_from_wmic())

	# Try the Windows registry
	_copy_new_fields(info, _get_cpu_info_from_registry())

	# Try /proc/cpuinfo
	_copy_new_fields(info, _get_cpu_info_from_proc_cpuinfo())

	# Try cpufreq-info
	_copy_new_fields(info, _get_cpu_info_from_cpufreq_info())

	# Try LSCPU
	_copy_new_fields(info, _get_cpu_info_from_lscpu())

	# Try sysctl
	_copy_new_fields(info, _get_cpu_info_from_sysctl())

	# Try kstat
	_copy_new_fields(info, _get_cpu_info_from_kstat())

	# Try dmesg
	_copy_new_fields(info, _get_cpu_info_from_dmesg())

	# Try /var/run/dmesg.boot
	_copy_new_fields(info, _get_cpu_info_from_cat_var_run_dmesg_boot())

	# Try lsprop ibm,pa-features
	_copy_new_fields(info, _get_cpu_info_from_ibm_pa_features())

	# Try sysinfo
	_copy_new_fields(info, _get_cpu_info_from_sysinfo())

	# Try querying the CPU cpuid register
	# FIXME: This should print stdout and stderr to trace log
	_copy_new_fields(info, _get_cpu_info_from_cpuid())

	# Try platform.uname
	_copy_new_fields(info, _get_cpu_info_from_platform_uname())

	g_trace.write('!' * 80)

	return info

def get_cpu_info_json():
	'''
	Returns the CPU info by using the best sources of information for your OS.
	Returns the result in a json string
	'''

	import json

	output = None

	# If running under pyinstaller, run normally
	if getattr(sys, 'frozen', False):
		info = _get_cpu_info_internal()
		output = json.dumps(info)
		output = "{0}".format(output)
	# if not running under pyinstaller, run in another process.
	# This is done because multiprocesing has a design flaw that
	# causes non main programs to run multiple times on Windows.
	else:
		from subprocess import Popen, PIPE

		command = [sys.executable, __file__, '--json']
		p1 = Popen(command, stdout=PIPE, stderr=PIPE, stdin=PIPE)
		output = p1.communicate()[0]

		if p1.returncode != 0:
			return "{}"

		output = output.decode(encoding='UTF-8')

	return output

def get_cpu_info():
	'''
	Returns the CPU info by using the best sources of information for your OS.
	Returns the result in a dict
	'''

	import json

	output = get_cpu_info_json()

	# Convert JSON to Python with non unicode strings
	output = json.loads(output, object_hook = _utf_to_str)

	return output

def main():
	from argparse import ArgumentParser
	import json

	# Parse args
	parser = ArgumentParser(description='Gets CPU info with pure Python')
	parser.add_argument('--json', action='store_true', help='Return the info in JSON format')
	parser.add_argument('--version', action='store_true', help='Return the version of py-cpuinfo')
	parser.add_argument('--trace', action='store_true', help='Traces code paths used to find CPU info to file')
	args = parser.parse_args()

	global g_trace
	g_trace = Trace(args.trace, False)

	try:
		_check_arch()
	except Exception as err:
		sys.stderr.write(str(err) + "\n")
		sys.exit(1)

	info = _get_cpu_info_internal()

	if not info:
		sys.stderr.write("Failed to find cpu info\n")
		sys.exit(1)

	if args.json:
		print(json.dumps(info))
	elif args.version:
		print(CPUINFO_VERSION_STRING)
	else:
		print('Python Version: {0}'.format(info.get('python_version', '')))
		print('Cpuinfo Version: {0}'.format(info.get('cpuinfo_version_string', '')))
		print('Vendor ID Raw: {0}'.format(info.get('vendor_id_raw', '')))
		print('Hardware Raw: {0}'.format(info.get('hardware_raw', '')))
		print('Brand Raw: {0}'.format(info.get('brand_raw', '')))
		print('Hz Advertised Friendly: {0}'.format(info.get('hz_advertised_friendly', '')))
		print('Hz Actual Friendly: {0}'.format(info.get('hz_actual_friendly', '')))
		print('Hz Advertised: {0}'.format(info.get('hz_advertised', '')))
		print('Hz Actual: {0}'.format(info.get('hz_actual', '')))
		print('Arch: {0}'.format(info.get('arch', '')))
		print('Bits: {0}'.format(info.get('bits', '')))
		print('Count: {0}'.format(info.get('count', '')))
		print('Arch String Raw: {0}'.format(info.get('arch_string_raw', '')))
		print('L1 Data Cache Size: {0}'.format(info.get('l1_data_cache_size', '')))
		print('L1 Instruction Cache Size: {0}'.format(info.get('l1_instruction_cache_size', '')))
		print('L2 Cache Size: {0}'.format(info.get('l2_cache_size', '')))
		print('L2 Cache Line Size: {0}'.format(info.get('l2_cache_line_size', '')))
		print('L2 Cache Associativity: {0}'.format(info.get('l2_cache_associativity', '')))
		print('L3 Cache Size: {0}'.format(info.get('l3_cache_size', '')))
		print('Stepping: {0}'.format(info.get('stepping', '')))
		print('Model: {0}'.format(info.get('model', '')))
		print('Family: {0}'.format(info.get('family', '')))
		print('Processor Type: {0}'.format(info.get('processor_type', '')))
		print('Flags: {0}'.format(', '.join(info.get('flags', ''))))


if __name__ == '__main__':
	main()
else:
	g_trace = Trace(False, False)
	_check_arch()



import sys
from .cpuinfo import *

from ...deprecations import deprecated

deprecated.module("24.3", "24.9")



Copyright (c) 2012 Santiago Lezica

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


from ...deprecations import deprecated
deprecated.module("24.9", "25.3", addendum="Use `frozendict` instead.")

from collections.abc import Mapping

try:
    from collections import OrderedDict
except ImportError:  # python < 2.7
    OrderedDict = NotImplemented


iteritems = getattr(dict, 'iteritems', dict.items) # py2-3 compatibility


class frozendict(Mapping):
    """
    An immutable wrapper around dictionaries that implements the complete :py:class:`collections.Mapping`
    interface. It can be used as a drop-in replacement for dictionaries where immutability is desired.
    """

    dict_cls = dict

    def __init__(self, *args, **kwargs):
        self._dict = self.dict_cls(*args, **kwargs)
        self._hash = None

    def __getitem__(self, key):
        return self._dict[key]

    def __contains__(self, key):
        return key in self._dict

    def copy(self, **add_or_replace):
        return self.__class__(self, **add_or_replace)

    def __iter__(self):
        return iter(self._dict)

    def __len__(self):
        return len(self._dict)

    def __repr__(self):
        return '<%s %r>' % (self.__class__.__name__, self._dict)

    def __hash__(self):
        if self._hash is None:
            h = 0
            for key, value in iteritems(self._dict):
                h ^= hash((key, value))
            self._hash = h
        return self._hash

    def __json__(self):
        # Works with auxlib's EntityEncoder.
        return self._dict

    def to_json(self):
        return self.__json__()


class FrozenOrderedDict(frozendict):
    """
    A frozendict subclass that maintains key order
    """

    dict_cls = OrderedDict


if OrderedDict is NotImplemented:
    del FrozenOrderedDict