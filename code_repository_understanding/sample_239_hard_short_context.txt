# Multi Robot Scene Completion: Towards Task-agnostic Collaborative Perception

## Abstract:

Collaborative perception learns how to share information among multiple robots to perceive the environment better than individually done. Past research on this has been task-specific, such as detection or segmentation. Yet this leads to different information sharing for different tasks, hindering the large-scale deployment of collaborative perception. We propose the first task-agnostic collaborative perception paradigm that learns a single collaboration module in a self-supervised manner for different downstream tasks. This is done by a novel task termed multi-robot scene completion, where each robot learns to effectively share information for reconstructing a complete scene viewed by all robots. Moreover, we propose a spatiotemporal autoencoder (STAR) that amortizes over time the communication cost by spatial sub-sampling and temporal mixing. Extensive experiments validate our method's effectiveness on scene completion and collaborative perception in autonomous driving scenarios.

## Installation
The work is tested with:

* python 3.7
* pytorch 1.8.1
* torchvision 0.9.1
* timm 0.3.2

Download the GitHub repository:
```bash
git clone https://github.com/coperception/star.git
cd star
```

Create a conda environment with the dependencies:
```bash
conda env create -f environment.yml
conda activate star
```

If conda installation failed, install the dependencies through pip:  
(Make sure your Python version is `3.7`)
```bash
pip install -r requirements.txt
```

## Usage:

To train, run:

```bash
cd completion/
make train_completion
```

To test the trained model on scene completion:

```bash
cd completion/
make test_completion
```

More commands and experiment settings are included in the [Makefile](https://github.com/coperception/star/raw/main/completion/Makefile).

You can find the training and test scripts at: [completion](https://github.com/coperception/star/raw/main/completion/).

## Dataset:

Our experiments are conducted on the V2X-Sim[1] simulated dataset. Find more about the dataset on the [website](https://ai4ce.github.io/V2X-Sim/).

*[1] Li, Yiming, et al. "V2X-Sim: Multi-agent collaborative perception dataset and benchmark for autonomous driving." IEEE Robotics and Automation Letters 7.4 (2022): 10914-10921.*

## Citation:

```
@inproceedings{li2022multi,
  title={Multi-Robot Scene Completion: Towards Task-Agnostic Collaborative Perception},
  author={Li, Yiming and Zhang, Juexiao and Ma, Dekun and Wang, Yue and Feng, Chen},
  booktitle={6th Annual Conference on Robot Learning}
}
```


numpy
torch==1.8.1
opencv-python
torchvision==0.9.1
typing
nuscenes-devkit==1.0.9
pyquaternion
numba
matplotlib
mmcv
terminaltables
shapely
coperception==0.0.10
timm==0.3.2

# Copyright (c) 2020 Mitsubishi Electric Research Laboratories (MERL). All rights reserved. The software, documentation and/or data in this file is provided on an "as is" basis, and MERL has no obligations to provide maintenance, support, updates, enhancements or modifications. MERL specifically disclaims any warranties, including, but not limited to, the implied warranties of merchantability and fitness for any particular purpose. In no event shall MERL be liable to any party for direct, indirect, special, incidental, or consequential damages, including lost profits, arising out of the use of this software and its documentation, even if MERL has been advised of the possibility of such damages. As more fully described in the license agreement that was required in order to download this software, documentation and/or data, permission to use, copy and modify this software without fee is granted, but only for educational, research and non-commercial purposes.
import argparse
import os
import sys
sys.path.append("..")

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import cv2
import torch.optim as optim
from tqdm import tqdm

from star.datasets.MultiTempSeg import MultiTempV2XSimSeg
from coperception.configs import Config
from coperception.utils.SegMetrics import ComputeIoU
from coperception.utils.SegModule import *
from coperception.utils.loss import *
from coperception.models.seg import UNet
from torch.utils.data import DataLoader
from coperception.utils.data_util import apply_pose_noise

from star.models import multiagent_mae, VQVAENet
import timm
assert timm.__version__ == "0.3.2"  # version check
import timm.optim.optim_factory as optim_factory


def check_folder(folder_path):
    if not os.path.exists(folder_path):
        os.mkdir(folder_path)
    return folder_path


@torch.no_grad()
def main(config, args):
    config.nepoch = args.nepoch
    batch_size = args.batch
    num_workers = args.nworker
    logpath = args.logpath
    pose_noise = args.pose_noise
    compress_level = args.compress_level
    only_v2i = args.only_v2i

    # Specify gpu device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    device_num = torch.cuda.device_count()
    print("device number", device_num)

    if args.bound == "upperbound":
        flag = "upperbound"
    else:
        if args.com == "when2com":
            flag = "when2com"
            if args.warp_flag:
                flag += "_warp"
            if args.inference == "argmax_test":
                flag = flag.replace("when2com", "who2com")
        elif args.com == "v2v":
            flag = "v2v"
        elif args.com == "mean":
            flag = "mean"
        elif args.com == "max":
            flag = "max"
        elif args.com == "sum":
            flag = "sum"
        elif args.com == "cat":
            flag = "cat"
        elif args.com == "agent":
            flag = "agent"
        elif args.com == "disco":
            flag = "disco"
        elif args.com == "ind_mae" or args.com == "joint_mae":
            flag = "mae"    
        elif args.com == "late":
            flag = "late"
        elif args.com == "vqvae":
            flag = "vqvae"
        else:
            flag = "lowerbound"

    num_agent = args.num_agent
    agent_idx_range = range(1, num_agent) if args.no_cross_road else range(num_agent)
    # TODO: kd_flag
    kd_flag = False
    valset = MultiTempV2XSimSeg(
        dataset_roots=[args.data + "/agent%d" % i for i in agent_idx_range],
        config=config,
        split="val",
        val=True,
        com=args.com,
        bound=args.bound,
        kd_flag=kd_flag,
        no_cross_road=args.no_cross_road,
        time_stamp = args.time_stamp,
    )
    valloader = DataLoader(
        valset, batch_size=batch_size, shuffle=False, num_workers=num_workers
    )
    print("Validation dataset size:", len(valset))

    config.flag = flag
    config.com = args.com
    config.inference = args.inference
    config.split = "test"
    # build model
    if args.no_cross_road:
        num_agent -= 1

    if args.com == "sum":
        model_completion = SumFusion(
            config,
            layer=args.layer,
            kd_flag=args.kd_flag,
            num_agent=num_agent,
            train_completion=True,
        )
    elif args.com == "mean":
        model_completion = MeanFusion(
            config,
            layer=args.layer,
            kd_flag=args.kd_flag,
            num_agent=num_agent,
            train_completion=True,
        )
    elif args.com == "max":
        model_completion = MaxFusion(
            config,
            layer=args.layer,
            kd_flag=args.kd_flag,
            num_agent=num_agent,
            train_completion=True,
        )
    elif args.com == "cat":
        model_completion = CatFusion(
            config,
            layer=args.layer,
            kd_flag=args.kd_flag,
            num_agent=num_agent,
            train_completion=True,
        )
    elif args.com == "v2v":
        model_completion = V2VNet(
            config,
            gnn_iter_times=args.gnn_iter_times,
            layer=args.layer,
            layer_channel=256,
            num_agent=num_agent,
        )
    elif args.com == "joint_mae" or  args.com == "ind_mae":
        # Juexiao added for mae
        model_completion = multiagent_mae.__dict__[args.mae_model](norm_pix_loss=args.norm_pix_loss, time_stamp=args.time_stamp, mask_method=args.mask)
        # also include individual reconstruction: reconstruct then aggregate
    elif args.com == "vqvae":
        model_completion = VQVAENet(
            num_hiddens = 256,
            num_residual_hiddens = 32,
            num_residual_layers = 2,
            num_embeddings = 512,
            embedding_dim = 64,
            commitment_cost = 0.25,
            decay = 0., # for VQ
        )
    elif args.com == "late":
        model_completion = FaFNet(
            config,
            kd_flag=0,
            num_agent=num_agent,
            train_completion=True,
        )
    else:
        raise NotImplementedError("Fusion type: {args.com} not implemented")

    model = UNet(
        config.in_channels,
        config.num_class,
        num_agent=num_agent,
        compress_level=compress_level,
    )
    # model = nn.DataParallel(model)
    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    checkpoint_completion = torch.load(args.resume_completion, map_location='cpu')
    completion_start_epoch = checkpoint_completion["epoch"] + 1
    model_completion.load_state_dict(checkpoint_completion["model_state_dict"])
    model_completion = model_completion.to(device)
    model_completion.eval() # but will not actually update weights
    print("completion model loaded from {} at epoch {}".format(args.resume_completion, completion_start_epoch))

    config.com = "" # Seg module do not communicate

    checkpoint = torch.load(args.resume)
    segmodule = SegModule(model, model, config, optimizer, False)
    segmodule.model.load_state_dict(checkpoint["model_state_dict"])
    start_epoch = checkpoint["epoch"] + 1
    print("Load model from {}, at epoch {}".format(args.resume, start_epoch - 1))
    # ==== eval ====
    segmodule.model.eval()
    compute_iou = ComputeIoU(num_class=config.num_class)  # num_class
    os.makedirs(logpath, exist_ok=True)
    logpath = os.path.join(logpath, f"{flag}_seg_eval")
    os.makedirs(logpath, exist_ok=True)
    logpath = os.path.join(logpath, "no_cross" if args.no_cross_road else "with_cross")
    os.makedirs(logpath, exist_ok=True)
    print("log path:", logpath)

    for idx, sample in enumerate(tqdm(valloader)):

        if args.com:
            (
                padded_voxel_points_list,
                padded_voxel_points_next_list,
                padded_voxel_points_teacher_list,
                label_one_hot_list,
                trans_matrices,
                target_agent,
                num_sensor,
            ) = list(zip(*sample))
        else:
            (
                padded_voxel_points_list,
                padded_voxel_points_next_list,
                padded_voxel_points_teacher_list,
                label_one_hot_list,
            ) = list(zip(*sample))

        if flag == "upperbound":
            padded_voxel_points = torch.cat(tuple(padded_voxel_points_teacher_list), 0)
        else:
            padded_voxel_points = torch.cat(tuple(padded_voxel_points_list), 0)

        label_one_hot = torch.cat(tuple(label_one_hot_list), 0)
        # print('voxel', padded_voxel_points.size())  # batch*agent seq h w z
        # print('label', label_one_hot.size())
        num_all_agents = torch.stack(tuple(num_sensor), 1)
        if args.no_cross_road:
            num_all_agents -= 1
        trans_matrices = torch.stack(trans_matrices, 1)

        padded_voxel_points_teacher = torch.cat(tuple(padded_voxel_points_teacher_list), 0)
        # print(padded_voxel_points_teacher.size())
        padded_voxel_points_next = torch.cat(tuple(padded_voxel_points_next_list), 0)
        # print(padded_voxel_points_next.size())
        if padded_voxel_points_next.size(1) >0:
            padded_voxel_points_next = padded_voxel_points_next.permute(0, 1, 4, 2, 3)

        data = {
            'bev_teacher': padded_voxel_points_teacher.squeeze(1).permute(0,3,1,2).to(device),
        }
        
        if flag != "upperbound":
            completion_data = {
                'bev_seq': padded_voxel_points.squeeze(1).permute(0,3,1,2).to(device).float(),
                'bev_seq_next': padded_voxel_points_next.to(device).float(),
                'bev_teacher': padded_voxel_points_teacher.squeeze(1).permute(0,3,1,2).to(device),
                'num_agent_tensor': num_all_agents.to(device),
                'trans_matrices': trans_matrices,
            }
            if args.com == "joint_mae" or  args.com == "ind_mae":
                _, completed_point_cloud, _, _ = model_completion.inference(completion_data['bev_seq'], 
                                                            completion_data['bev_seq_next'], 
                                                            completion_data['bev_teacher'], 
                                                            completion_data['trans_matrices'], 
                                                            completion_data['num_agent_tensor'],
                                                            batch_size,
                                                            mask_ratio=args.mask_ratio)
            elif args.com == "vqvae":
                _, completed_point_cloud, _ = model_completion(padded_voxel_points.unsqueeze(1).to(device).float(), 
                                                completion_data['trans_matrices'], 
                                                completion_data['num_agent_tensor'],
                                                batch_size=batch_size)
            else:
                # print(padded_voxel_points.size())
                completed_point_cloud, _ = model_completion(padded_voxel_points.unsqueeze(1).to(device).float(), 
                                                completion_data['trans_matrices'], 
                                                completion_data['num_agent_tensor'],
                                                batch_size=batch_size)

            completed_point_cloud = completed_point_cloud.permute(0, 2, 3, 1)
            # torch.save(completion_data['bev_seq'], "seg_bev.pt")
            # torch.save(completion_data['bev_teacher'], "seg_bev_teacher.pt")
            # torch.save(completed_point_cloud, "seg_recon.pt")
            # exit(1)
            data["bev_seq"] = completed_point_cloud.to(device)
        else:
            print("test on the upperbound")
            data["bev_seq"] = padded_voxel_points.to(device).float()

        # print("test on the lower bound")
        # data["bev_seq"] = padded_voxel_points.to(device).float()
        data["labels"] = label_one_hot.to(device)
        if args.com:
            # trans_matrices = torch.stack(trans_matrices, 1)

            # add pose noise
            if pose_noise > 0:
                apply_pose_noise(pose_noise, trans_matrices)

            target_agent = torch.stack(target_agent, 1)
            num_sensor = torch.stack(num_sensor, 1)

            if args.no_cross_road:
                num_sensor -= 1

            data["trans_matrices"] = trans_matrices.to(device)
            data["target_agent"] = target_agent.to(device)
            data["num_sensor"] = num_sensor.to(device)

        pred, labels = segmodule.step(data, num_agent, batch_size, loss=False)

        # If has RSU, do not count RSU's output into evaluation
        if not args.no_cross_road:
            pred = pred[1:, :, :, :]
            labels = labels[1:, :, :]

        labels = labels.detach().cpu().numpy().astype(np.int32)

        # late fusion
        if args.apply_late_fusion:
            pred = torch.flip(pred, (2,))
            size = (1, *pred[0].shape)

            for ii in range(num_sensor[0, 0]):
                for jj in range(num_sensor[0, 0]):
                    if ii == jj:
                        continue

                    nb_agent = torch.unsqueeze(pred[jj], 0)
                    tfm_ji = trans_matrices[0, jj, ii]
                    M = (
                        torch.hstack((tfm_ji[:2, :2], -tfm_ji[:2, 3:4]))
                        .float()
                        .unsqueeze(0)
                    )  # [1,2,3]

                    mask = torch.tensor(
                        [[[1, 1, 4 / 128], [1, 1, 4 / 128]]], device=M.device
                    )

                    M *= mask
                    grid = F.affine_grid(M, size=torch.Size(size)).to(device)
                    warp_feat = F.grid_sample(nb_agent, grid).squeeze()
                    pred[ii] += warp_feat

            pred = torch.flip(pred, (2,))
        # ============

        pred = torch.argmax(F.softmax(pred, dim=1), dim=1)
        # print('pred size', pred.size())
        compute_iou(pred, labels)

        if args.vis and idx % 50 == 0:  # render segmatic map
            plt.clf()
            pred_map = np.zeros((256, 256, 3))
            gt_map = np.zeros((256, 256, 3))

            for k, v in config.class_to_rgb.items():
                pred_map[np.where(pred.cpu().numpy()[0] == k)] = v
                gt_map[np.where(label_one_hot.numpy()[0] == k)] = v

            plt.imsave(
                f"{logpath}/{idx}_voxel_points.png",
                np.asarray(
                    np.max(padded_voxel_points.cpu().numpy()[0], axis=2), dtype=np.uint8
                ),
            )
            cv2.imwrite(f"{logpath}/{idx}_pred.png", pred_map[:, :, ::-1])
            cv2.imwrite(f"{logpath}/{idx}_gt.png", gt_map[:, :, ::-1])

            # save completion results also
            if idx==0 or idx==150 or idx==200 or idx == 500 or idx == 600 or idx==700:
                print("save tensor for idx", idx)
                torch.save(data['bev_teacher'].cpu(), os.path.join(logpath, "{}-target.pt").format(idx))
                torch.save(data['bev_seq'].permute(0,3,1,2).cpu(), os.path.join(logpath, "{}-recon.pt").format(idx))

            # target_img = torch.max(data['bev_teacher'].cpu(), dim=1, keepdim=True)[0]
            # recon_img = torch.max(data['bev_seq'].permute(0,3,1,2).cpu(), dim=1, keepdim=True)[0]
            # start_agent_idx = 0 if args.no_cross_road else 1
            # for kid in range(start_agent_idx, num_agent):
            #     recon_save = "{}_reconstruction-epc{}-agent{}.png".format(idx, completion_start_epoch-1, kid)
            #     target_save = "{}_target-epc{}-agent{}.png".format(idx, completion_start_epoch-1, kid)
            #     plt.imshow(target_img[kid,:,:,:].permute(1,2,0).squeeze(-1).numpy(), alpha=1.0, zorder=12, cmap="Purples")
            #     plt.axis('off')
            #     plt.savefig(os.path.join(logpath, target_save))
            #     plt.imshow(recon_img[kid,:,:,:].permute(1,2,0).squeeze(-1).numpy(), alpha=1.0, zorder=12, cmap="Purples")
            #     plt.axis('off')
            #     plt.savefig(os.path.join(logpath, recon_save))



    print("iou:", compute_iou.get_ious())
    print("miou:", compute_iou.get_miou(ignore=0))
    log_file = open(f"{logpath}/log.txt", "w")
    log_file.write(f"iou: {compute_iou.get_ious()}\n")
    log_file.write(f"miou: {compute_iou.get_miou(ignore=0)}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-d",
        "--data",
        default="./dataset/train",
        type=str,
        help="The path to the preprocessed sparse BEV training data",
    )
    parser.add_argument(
        "--resume",
        type=str,
        help="The path to the saved model that is loaded to resume training",
    )
    parser.add_argument("--model_only", action="store_true", help="only load model")
    parser.add_argument("--batch", default=1, type=int, help="Batch size")
    parser.add_argument("--nepoch", default=10, type=int, help="Number of epochs")
    parser.add_argument("--nworker", default=2, type=int, help="Number of workers")
    parser.add_argument("--lr", default=0.001, type=float, help="Initial learning rate")
    parser.add_argument("--com", default="", type=str, help="Whether to communicate")
    parser.add_argument("--inference", default="activated")
    parser.add_argument("--warp_flag", action="store_true")
    parser.add_argument("--vis", action="store_true")
    parser.add_argument("--log", action="store_true", help="Whether to log")
    parser.add_argument("--logpath", default="", help="The path to the output log file")
    parser.add_argument(
        "--no_cross_road", action="store_true", help="Do not load data of cross roads"
    )
    parser.add_argument(
        "--num_agent", default=6, type=int, help="The total number of agents"
    )
    parser.add_argument(
        "--pose_noise",
        default=0,
        type=float,
        help="draw noise from normal distribution with given mean (in meters), apply to transformation matrix.",
    )
    parser.add_argument(
        "--apply_late_fusion",
        default=0,
        type=int,
        help="1: apply late fusion. 0: no late fusion",
    )
    parser.add_argument(
        "--compress_level",
        default=0,
        type=int,
        help="Compress the communication layer channels by 2**x times in encoder",
    )
    parser.add_argument(
        "--only_v2i",
        default=0,
        type=int,
        help="1: only v2i, 0: v2v and v2i",
    )

    ## ----- mae args added by Juexiao -----
    parser.add_argument(
        "--mask_ratio", default=0.50, type=float, help="The input mask ratio"
    )
    parser.add_argument(
        "--mask", default="random", type=str, choices=["random", "complement"], help="Used in MAE training"
    )
    parser.add_argument(
        "--time_stamp", default=2, type=int, help="The total number of time stamp to use"
    )
    parser.add_argument(
        "--mae_model", default="fusion_bev_mae_vit_base_patch8", type=str, 
        help="The mae model to use"
    )
    parser.add_argument(
        "--norm_pix_loss", default=False, type=bool, help="Whether normalize target pixel value for loss"
    )
    ## ----------------------
    parser.add_argument(
        "--resume_completion",
        default="",
        type=str,
        help="The path to the saved completion model that is loaded to resume training",
    )

    parser.add_argument("--bound", default="lowerbound", type=str)
    torch.multiprocessing.set_sharing_strategy("file_system")

    args = parser.parse_args()
    print(args)
    config = Config("train")
    main(config, args)

import argparse
import os
import sys
sys.path.append("..")
from copy import deepcopy

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import torch.optim as optim
from torch.utils.data import DataLoader

from star.datasets.MultiTempDet import MultiTempV2XSimDet
from coperception.configs import Config, ConfigGlobal
from coperception.utils.CoDetModule import *
from coperception.utils.loss import *
from coperception.utils.mean_ap import eval_map
from coperception.models.det import *
from coperception.utils.detection_util import late_fusion
from coperception.utils.data_util import apply_pose_noise

import glob
from star.utils.CoModule import CoModule
from star.models import multiagent_mae, VQVAENet
import timm
assert timm.__version__ == "0.3.2"  # version check
import timm.optim.optim_factory as optim_factory

def check_folder(folder_path):
    if not os.path.exists(folder_path):
        os.mkdir(folder_path)
    return folder_path


@torch.no_grad()
def main(args):
    config = Config("train", binary=True, only_det=True)
    config_global = ConfigGlobal("train", binary=True, only_det=True)

    need_log = args.log
    num_workers = args.nworker
    apply_late_fusion = args.apply_late_fusion
    pose_noise = args.pose_noise
    compress_level = args.compress_level
    only_v2i = args.only_v2i

    # Specify gpu device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    device_num = torch.cuda.device_count()
    print("device number", device_num)

    config.inference = args.inference
    if args.bound == "upperbound":
        flag = "upperbound"
    else:
        if args.com == "when2com":
            flag = "when2com"
            if args.inference == "argmax_test":
                flag = "who2com"
            if args.warp_flag:
                flag = flag + "_warp"
        elif args.com in {"v2v", "disco", "sum", "mean", "max", "cat", "agent", "ind_mae", "joint_mae", "late", "vqvae"}:
            flag = args.com
        else:
            flag = "lowerbound"
            if args.box_com:
                flag += "_box_com"

    print("flag", flag)
    config.flag = flag
    config.split = "test"

    num_agent = args.num_agent
    # agent0 is the cross road
    agent_idx_range = range(1, num_agent) if args.no_cross_road else range(num_agent)
    validation_dataset = MultiTempV2XSimDet(
        dataset_roots=[f"{args.data}/agent{i}" for i in agent_idx_range],
        config=config,
        config_global=config_global,
        split="val",
        val=True,
        bound="both",
        kd_flag=args.kd_flag,
        no_cross_road=args.no_cross_road,
        time_stamp= args.time_stamp
    )
    validation_data_loader = DataLoader(
        validation_dataset, batch_size=1, shuffle=False, num_workers=num_workers
    )
    print("Validation dataset size:", len(validation_dataset))

    if args.no_cross_road:
        num_agent -= 1

    if args.com == "joint_mae" or  args.com == "ind_mae":
        # Juexiao added for mae
        model_completion = multiagent_mae.__dict__[args.mae_model](norm_pix_loss=args.norm_pix_loss, time_stamp=args.time_stamp, mask_method=args.mask, encode_partial=args.encode_partial)
        # also include individual reconstruction: reconstruct then aggregate
    elif args.com == "late":
        model_completion = FaFNet(
            config,
            layer=args.layer,
            kd_flag=args.kd_flag,
            num_agent=num_agent,
            train_completion=True,
        )
    elif args.com == "vqvae":
        model_completion = VQVAENet(
            num_hiddens = 256,
            num_residual_hiddens = 32,
            num_residual_layers = 2,
            num_embeddings = 512,
            embedding_dim = 64,
            commitment_cost = 0.25,
            decay = 0., # for VQ
        )
    else:
        raise NotImplementedError("Fusion type: {args.com} not implemented")

    
    model = FaFNet(
        config, layer=args.layer, kd_flag=args.kd_flag, num_agent=num_agent, train_completion=False,
    )
    
    # model_completion = nn.DataParallel(model_completion)
    model = nn.DataParallel(model)
    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = {
        "cls": SoftmaxFocalClassificationLoss(),
        "loc": WeightedSmoothL1LocalizationLoss(),
    }

    if args.com == "ind_mae" or args.com == "joint_mae":
        completion_param_groups = optim_factory.add_weight_decay(model_completion, 0.05)
        optimizer_completion = optim.Adam(completion_param_groups, lr=args.lr, betas=(0.9, 0.95))
    else:
        optimizer_completion = optim.Adam(model_completion.parameters(), lr=args.lr)

    # detection model
    fafmodule = FaFModule(model, model, config, optimizer, criterion, args.kd_flag)
    # completion model
    module_completion = CoModule(model_completion, optimizer_completion, args.com)
    checkpoint_completion = torch.load(args.resume_completion, map_location='cpu')
    completion_start_epoch = checkpoint_completion["epoch"] + 1
    module_completion.resume_from_cpu(checkpoint=checkpoint_completion, device=device, trainable=False)
    print("completion model loaded from {} at epoch {}".format(args.resume_completion, completion_start_epoch-1))

    # model_save_path = args.resume[: args.resume.rfind("/")]
    model_save_path = os.path.join(args.logpath, "agnostic-det")

    if args.inference == "argmax_test":
        model_save_path = model_save_path.replace("when2com", "who2com")

    os.makedirs(model_save_path, exist_ok=True)
    log_file_name = os.path.join(model_save_path, "log_det.txt")
    saver = open(log_file_name, "a")
    saver.write("GPU number: {}\n".format(torch.cuda.device_count()))
    saver.flush()

    # Logging the details for this experiment
    saver.write("command line: {}\n".format(" ".join(sys.argv[1:])))
    saver.write(args.__repr__() + "\n\n")
    saver.flush()

    checkpoint = torch.load(
        args.resume, map_location="cpu"
    )  # We have low GPU utilization for testing
    start_epoch = checkpoint["epoch"] + 1
    fafmodule.model.load_state_dict(checkpoint["model_state_dict"])
    fafmodule.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    fafmodule.scheduler.load_state_dict(checkpoint["scheduler_state_dict"])

    print("Load detection model from {}, at epoch {}".format(args.resume, start_epoch - 1))

    #  ===== eval =====
    fafmodule.model.eval()
    module_completion.model.eval()
    # eval_save_path = check_folder(os.path.join(model_save_path, flag))
    eval_save_path = check_folder(model_save_path)
    print("save evalutaion results at", eval_save_path)
    save_fig_path = [
        check_folder(os.path.join(eval_save_path, f"vis{i}")) for i in agent_idx_range
    ]
    tracking_path = [
        check_folder(os.path.join(eval_save_path, f"tracking{i}"))
        for i in agent_idx_range
    ]

    # for local and global mAP evaluation
    det_results_local = [[] for i in agent_idx_range]
    annotations_local = [[] for i in agent_idx_range]

    tracking_file = [set()] * num_agent
    for cnt, sample in enumerate(validation_data_loader):
        t = time.time()
        (
            padded_voxel_point_list,
            padded_voxel_point_next_list,
            padded_voxel_points_teacher_list,
            label_one_hot_list,
            reg_target_list,
            reg_loss_mask_list,
            anchors_map_list,
            vis_maps_list,
            gt_max_iou,
            filenames,
            target_agent_id_list,
            num_agent_list,
            trans_matrices_list,
        ) = zip(*sample)

        print(filenames)

        filename0 = filenames[0]
        trans_matrices = torch.stack(tuple(trans_matrices_list), 1)
        target_agent_ids = torch.stack(tuple(target_agent_id_list), 1)
        num_all_agents = torch.stack(tuple(num_agent_list), 1)

        # add pose noise
        if pose_noise > 0:
            apply_pose_noise(pose_noise, trans_matrices)

        if args.no_cross_road:
            num_all_agents -= 1
        # print("bev", type(padded_voxel_point_list), len(padded_voxel_point_list), len(padded_voxel_point_list[0]))
        # print(padded_voxel_point_list)
        if flag == "upperbound":
            padded_voxel_points = torch.cat(tuple(padded_voxel_points_teacher_list), 0)
        else:
            padded_voxel_points = torch.cat(tuple(padded_voxel_point_list), 0)

        label_one_hot = torch.cat(tuple(label_one_hot_list), 0)
        reg_target = torch.cat(tuple(reg_target_list), 0)
        reg_loss_mask = torch.cat(tuple(reg_loss_mask_list), 0)
        anchors_map = torch.cat(tuple(anchors_map_list), 0)
        vis_maps = torch.cat(tuple(vis_maps_list), 0)

        padded_voxel_points_teacher = torch.cat(
                tuple(padded_voxel_points_teacher_list), 0
        )

        data = {
            "bev_seq": padded_voxel_points.to(device),
            "bev_seq_teacher": padded_voxel_points_teacher.to(device),
            "labels": label_one_hot.to(device),
            "reg_targets": reg_target.to(device),
            "anchors": anchors_map.to(device),
            "vis_maps": vis_maps.to(device),
            "reg_loss_mask": reg_loss_mask.to(device).type(dtype=torch.bool),
            "target_agent_ids": target_agent_ids.to(device),
            "num_agent": num_all_agents.to(device),
            "trans_matrices": trans_matrices.to(device),
        }

        if flag != "upperbound":
            print("predict detection over reconstructed.")
            if args.com == "ind_mae" or args.com == "joint_mae":
                # multi timestamp input bev supported in the data
                data["bev_seq_next"] = torch.cat(tuple(padded_voxel_point_next_list), 0).to(device)
                # loss, reconstruction, _ = faf_module.step_mae_completion(data, batch_size, args.mask_ratio, trainable=False)
                loss, completed_point_cloud, _, _ = module_completion.infer_mae_completion(data, batch_size=1, mask_ratio=args.mask_ratio)
            elif args.com == "vqvae":
                _, completed_point_cloud, _, _ = module_completion.step_vae_completion(data, batch_size=1, trainable=False)
            else:
                _, completed_point_cloud = module_completion.step_completion(
                    data, batch_size=1, trainable=False
                )

            completed_point_cloud = completed_point_cloud.unsqueeze(1)
            completed_point_cloud = completed_point_cloud.permute(0, 1, 3, 4, 2)

            # substitute with the completed data
            data["bev_seq"] = completed_point_cloud
            # print("check lowerbound")
            if args.visualization:
                # visualize completion and ground truth (teacher)
                reconstructed = data["bev_seq"].squeeze(1).permute(0,3,1,2)
                recon_img = torch.max(reconstructed, dim=1, keepdim=True)[0]
                target = data['bev_seq_teacher'].squeeze(1).permute(0,3,1,2)
                target_img = torch.max(target, dim=1, keepdim=True)[0]
                filename = str(filename0[0][0])
                eval_start_idx = 0 if args.no_cross_road else 1
                # local qualitative evaluation
                for kid in range(eval_start_idx, num_agent):
                    recon_name = "recon-epc{}-agent{}.png".format(completion_start_epoch-1, kid)
                    target_name = "target-epc{}-agent{}.png".format(completion_start_epoch-1, kid)
                    cut = filename[filename.rfind("agent") + 7 :]
                    seq_name = cut[: cut.rfind("_")]
                    idx = cut[cut.rfind("_") + 1 : cut.rfind("/")]
                    seq_save = os.path.join(save_fig_path[kid], seq_name)
                    check_folder(seq_save)
                    recon_save = str(idx) + recon_name
                    target_save = str(idx) + target_name
                    plt.imshow(target_img[kid,:,:,:].permute(1,2,0).squeeze(-1).cpu().numpy(), alpha=1.0, zorder=12, cmap="Purples")
                    plt.axis('off')
                    print("save", os.path.join(seq_save, target_save))
                    plt.savefig(os.path.join(seq_save, target_save))
                    plt.imshow(recon_img[kid,:,:,:].permute(1,2,0).squeeze(-1).cpu().numpy(), alpha=1.0, zorder=12, cmap="Purples")
                    plt.axis('off')
                    plt.savefig(os.path.join(seq_save, recon_save))
                    # imshow(target_img[kid,:,:,:].permute(1,2,0).squeeze(-1).numpy(), alpha=1.0, zorder=12)
                    # imshow(recon_img[kid,:,:,:].permute(1,2,0).squeeze(-1).numpy(), alpha=1.0, zorder=12)


        if flag == "lowerbound_box_com":
            loss, cls_loss, loc_loss, result = fafmodule.predict_all_with_box_com(
                data, data["trans_matrices"]
            )
        elif flag == "disco":
            (
                loss,
                cls_loss,
                loc_loss,
                result,
                save_agent_weight_list,
            ) = fafmodule.predict_all(data, 1, num_agent=num_agent)
        else:
            loss, cls_loss, loc_loss, result = fafmodule.predict_all(
                data, 1, num_agent=num_agent
            )

        box_color_map = ["red", "yellow", "blue", "purple", "black", "orange"]

        # If has RSU, do not count RSU's output into evaluation
        eval_start_idx = 0 if args.no_cross_road else 1
        
        # local qualitative evaluation
        for k in range(eval_start_idx, num_agent):
            box_colors = None
            if apply_late_fusion == 1 and len(result[k]) != 0:
                pred_restore = result[k][0][0][0]["pred"]
                score_restore = result[k][0][0][0]["score"]
                selected_idx_restore = result[k][0][0][0]["selected_idx"]

            data_agents = {
                # "bev_seq": torch.unsqueeze(padded_voxel_points[k, :, :, :, :], 1), #NOTE: what is this for? only visualization, not involved in mAP calc
                "bev_seq": torch.unsqueeze(padded_voxel_points_teacher[k, :, :, :, :], 1),
                "reg_targets": torch.unsqueeze(reg_target[k, :, :, :, :, :], 0),
                "anchors": torch.unsqueeze(anchors_map[k, :, :, :, :], 0),
            }
            temp = gt_max_iou[k]

            if len(temp[0]["gt_box"]) == 0:
                data_agents["gt_max_iou"] = []
            else:
                data_agents["gt_max_iou"] = temp[0]["gt_box"][0, :, :]

            # late fusion
            if apply_late_fusion == 1 and len(result[k]) != 0:
                box_colors = late_fusion(
                    k, num_agent, result, trans_matrices, box_color_map
                )

            result_temp = result[k]

            temp = {
                "bev_seq": data_agents["bev_seq"][0, -1].cpu().numpy(), 
                "result": [] if len(result_temp) == 0 else result_temp[0][0],
                "reg_targets": data_agents["reg_targets"].cpu().numpy()[0],
                "anchors_map": data_agents["anchors"].cpu().numpy()[0],
                "gt_max_iou": data_agents["gt_max_iou"],
            }
            det_results_local[k], annotations_local[k] = cal_local_mAP(
                config, temp, det_results_local[k], annotations_local[k]
            )

            filename = str(filename0[0][0])
            cut = filename[filename.rfind("agent") + 7 :]
            seq_name = cut[: cut.rfind("_")]
            idx = cut[cut.rfind("_") + 1 : cut.rfind("/")]
            seq_save = os.path.join(save_fig_path[k], seq_name)
            check_folder(seq_save)
            idx_save = str(idx) + ".png"
            temp_ = deepcopy(temp)
            if args.visualization:
                visualization(
                    config,
                    temp,
                    box_colors,
                    box_color_map,
                    apply_late_fusion,
                    os.path.join(seq_save, idx_save),
                )

            # # plot the cell-wise edge
            # if flag == "disco" and k < len(save_agent_weight_list):
            #     one_agent_edge = save_agent_weight_list[k]
            #     for kk in range(len(one_agent_edge)):
            #         idx_edge_save = (
            #             str(idx) + "_edge_" + str(kk) + "_to_" + str(k) + ".png"
            #         )
            #         savename_edge = os.path.join(seq_save, idx_edge_save)
            #         sns.set()
            #         plt.savefig(savename_edge, dpi=500)
            #         plt.close(0)

            # == tracking ==
            if args.tracking:
                scene, frame = filename.split("/")[-2].split("_")
                det_file = os.path.join(tracking_path[k], f"det_{scene}.txt")
                if scene not in tracking_file[k]:
                    det_file = open(det_file, "w")
                    tracking_file[k].add(scene)
                else:
                    det_file = open(det_file, "a")
                det_corners = get_det_corners(config, temp_)
                for ic, c in enumerate(det_corners):
                    det_file.write(
                        ",".join(
                            [
                                str(
                                    int(frame) + 1
                                ),  # frame idx is 1-based for tracking
                                "-1",
                                "{:.2f}".format(c[0]),
                                "{:.2f}".format(c[1]),
                                "{:.2f}".format(c[2]),
                                "{:.2f}".format(c[3]),
                                str(result_temp[0][0][0]["score"][ic]),
                                "-1",
                                "-1",
                                "-1",
                            ]
                        )
                        + "\n"
                    )
                    det_file.flush()

                det_file.close()

            # restore data before late-fusion
            if apply_late_fusion == 1 and len(result[k]) != 0:
                result[k][0][0][0]["pred"] = pred_restore
                result[k][0][0][0]["score"] = score_restore
                result[k][0][0][0]["selected_idx"] = selected_idx_restore

        print("Validation scene {}, at frame {}".format(seq_name, idx))
        print("Takes {} s\n".format(str(time.time() - t)))

    logger_root = args.logpath if args.logpath != "" else "logs"
    logger_root = os.path.join(
        logger_root, f"{flag}_eval", "no_cross" if args.no_cross_road else "with_cross"
    )
    os.makedirs(logger_root, exist_ok=True)
    log_file_path = os.path.join(logger_root, "log_test.txt")
    log_file = open(log_file_path, "w")

    def print_and_write_log(log_str):
        print(log_str)
        log_file.write(log_str + "\n")

    mean_ap_local = []
    # local mAP evaluation
    det_results_all_local = []
    annotations_all_local = []
    for k in range(eval_start_idx, num_agent):
        if type(det_results_local[k]) != list or len(det_results_local[k]) == 0:
            continue

        print_and_write_log("Local mAP@0.5 from agent {}".format(k))
        mean_ap, _ = eval_map(
            det_results_local[k],
            annotations_local[k],
            scale_ranges=None,
            iou_thr=0.5,
            dataset=None,
            logger=None,
        )
        mean_ap_local.append(mean_ap)
        print_and_write_log("Local mAP@0.7 from agent {}".format(k))

        ean_ap, _ = eval_map(
            det_results_local[k],
            annotations_local[k],
            scale_ranges=None,
            iou_thr=0.7,
            dataset=None,
            logger=None,
        )
        mean_ap_local.append(mean_ap)

        det_results_all_local += det_results_local[k]
        annotations_all_local += annotations_local[k]

    # average local mAP evaluation
    print_and_write_log("Average Local mAP@0.5")

    mean_ap_local_average, _ = eval_map(
        det_results_all_local,
        annotations_all_local,
        scale_ranges=None,
        iou_thr=0.5,
        dataset=None,
        logger=None,
    )
    mean_ap_local.append(mean_ap_local_average)

    print_and_write_log("Average Local mAP@0.7")

    mean_ap_local_average, _ = eval_map(
        det_results_all_local,
        annotations_all_local,
        scale_ranges=None,
        iou_thr=0.7,
        dataset=None,
        logger=None,
    )
    mean_ap_local.append(mean_ap_local_average)

    print_and_write_log(
        "Quantitative evaluation results of model from {}, at epoch {}".format(
            args.resume, start_epoch - 1
        )
    )
    print_and_write_log(
        "Quantitative evaluation results of completion model from {}".format(
            args.resume_completion,
        )
    )

    for k in range(eval_start_idx, num_agent):
        print_and_write_log(
            "agent{} mAP@0.5 is {} and mAP@0.7 is {}".format(
                k, mean_ap_local[k * 2], mean_ap_local[(k * 2) + 1]
            )
        )

    print_and_write_log(
        "average local mAP@0.5 is {} and average local mAP@0.7 is {}".format(
            mean_ap_local[-2], mean_ap_local[-1]
        )
    )

    if need_log:
        saver.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-d",
        "--data",
        default=None,
        type=str,
        help="The path to the preprocessed sparse BEV training data",
    )
    parser.add_argument("--batch", default=4, type=int, help="The number of scene")
    parser.add_argument("--nepoch", default=100, type=int, help="Number of epochs")
    parser.add_argument("--nworker", default=1, type=int, help="Number of workers")
    parser.add_argument("--lr", default=0.001, type=float, help="Initial learning rate")
    parser.add_argument("--log", action="store_true", help="Whether to log")
    parser.add_argument("--logpath", default="", help="The path to the output log file")
    parser.add_argument(
        "--resume",
        default="",
        type=str,
        help="The path to the saved det model that is loaded to test",
    )
    parser.add_argument(
        "--resume_teacher",
        default="",
        type=str,
        help="The path to the saved teacher model that is loaded to resume training",
    )
    parser.add_argument(
        "--layer",
        default=3,
        type=int,
        help="Communicate which layer in the single layer com mode",
    )
    parser.add_argument(
        "--warp_flag", action="store_true", help="Whether to use pose info for When2com"
    )
    parser.add_argument(
        "--kd_flag",
        default=0,
        type=int,
        help="Whether to enable distillation (only DiscNet is 1 )",
    )
    parser.add_argument("--kd_weight", default=100000, type=int, help="KD loss weight")
    parser.add_argument(
        "--gnn_iter_times",
        default=3,
        type=int,
        help="Number of message passing for V2VNet",
    )
    parser.add_argument(
        "--visualization", type=int, default=0, help="Visualize validation result"
    )
    parser.add_argument(
        "--com", default="", type=str, help="disco/when2com/v2v/sum/mean/max/cat/agent"
    )
    parser.add_argument(
        "--bound",
        type=str,
        help="The input setting: lowerbound -> single-view or upperbound -> multi-view",
    )
    parser.add_argument("--inference", type=str)
    parser.add_argument("--tracking", action="store_true")
    parser.add_argument("--box_com", action="store_true")
    parser.add_argument(
        "--no_cross_road", action="store_true", help="Do not load data of cross roads"
    )
    # scene_batch => batch size in each scene
    parser.add_argument(
        "--num_agent", default=6, type=int, help="The total number of agents"
    )
    parser.add_argument(
        "--apply_late_fusion",
        default=0,
        type=int,
        help="1: apply late fusion. 0: no late fusion",
    )
    parser.add_argument(
        "--compress_level",
        default=0,
        type=int,
        help="Compress the communication layer channels by 2**x times in encoder",
    )
    parser.add_argument(
        "--pose_noise",
        default=0,
        type=float,
        help="draw noise from normal distribution with given mean (in meters), apply to transformation matrix.",
    )
    parser.add_argument(
        "--only_v2i",
        default=0,
        type=int,
        help="1: only v2i, 0: v2v and v2i",
    )

    ## ----- args for completion model load ----
    parser.add_argument(
        "--resume_completion",
        default="",
        type=str,
        help="The path to the saved completion model that is loaded to resume training",
    )

    ## ----- mae args added by Juexiao -----
    parser.add_argument(
        "--mask_ratio", default=0.50, type=float, help="The input mask ratio"
    )
    parser.add_argument(
        "--mask", default="random", type=str, choices=["random", "complement"], help="Used in MAE training"
    )
    parser.add_argument(
        "--time_stamp", default=2, type=int, help="The total number of time stamp to use"
    )
    parser.add_argument(
        "--mae_model", default="fusion_bev_mae_vit_base_patch8", type=str, 
        help="The mae model to use"
    )
    parser.add_argument(
        "--norm_pix_loss", default=False, type=bool, help="Whether normalize target pixel value for loss"
    )
    ## ----------------------
    parser.add_argument(
        "--encode_partial", action="store_true", help="encode partial information before masking"
    )

    torch.multiprocessing.set_sharing_strategy("file_system")
    args = parser.parse_args()
    print(args)
    main(args)

import argparse
import matplotlib
matplotlib.use('Agg')
from multiprocessing.connection import deliver_challenge
import torch.multiprocessing
torch.multiprocessing.set_sharing_strategy('file_system')
import os
import sys
sys.path.append("..")
import glob

import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

from star.datasets.MultiTempDet import MultiTempV2XSimDet
from star.utils.CoModule import CoModule

from coperception.configs import Config, ConfigGlobal

from coperception.utils.loss import *
from coperception.models.det import *
from star.models import multiagent_mae, VQVAENet, VQSTAR, CNNNet
from coperception.utils import AverageMeter


# from mae script -----------
import timm
assert timm.__version__ == "0.3.2"  # version check
import timm.optim.optim_factory as optim_factory
import star.utils.lr_sched as lr_sched
import matplotlib.pyplot as plt
# ----------------------------

def check_folder(folder_path):
    if not os.path.exists(folder_path):
        os.mkdir(folder_path)
    return folder_path


def main(args):
    config = Config("train", binary=True, only_det=True)
    config_global = ConfigGlobal("train", binary=True, only_det=True)

    num_epochs = args.nepoch
    need_log = args.log
    num_workers = args.nworker
    start_epoch = 1
    batch_size = args.batch
    num_agent = args.num_agent
    auto_resume_path = args.auto_resume_path


    # Specify gpu device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    device_num = torch.cuda.device_count()
    print("device number", device_num)

    if args.com in {"mean", "max", "cat", "sum", "v2v", "ind_mae", "joint_mae", "late", "vqvae", "vqstar"}:
        flag = args.com
    else:
        raise ValueError(f"com: {args.com} is not supported")

    config.flag = flag

    agent_idx_range = range(1, num_agent) if args.no_cross_road else range(num_agent)

    training_dataset = MultiTempV2XSimDet(
        dataset_roots=[f"{args.data}/agent{i}" for i in agent_idx_range],
        config=config,
        config_global=config_global,
        split="train",
        bound="both",
        kd_flag=args.kd_flag,
        no_cross_road=args.no_cross_road,
        time_stamp = args.time_stamp
    )
    training_data_loader = DataLoader(
        training_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers
    )
    print("Training dataset size:", len(training_dataset))

    logger_root = args.logpath if args.logpath != "" else "logs"

    if args.no_cross_road:
        num_agent -= 1

    if args.com == "joint_mae" or  args.com == "ind_mae":
        # Juexiao added for mae
        model = multiagent_mae.__dict__[args.mae_model](norm_pix_loss=args.norm_pix_loss, time_stamp=args.time_stamp, mask_method=args.mask,
                                                        encode_partial=args.encode_partial, no_temp_emb=args.no_temp_emb, decode_singletemp=args.decode_singletemp)
        # also include individual reconstruction: reconstruct then aggregate
    elif args.com == "vqstar":
        model = VQSTAR.vqstar(
            norm_pix_loss=args.norm_pix_loss, time_stamp=args.time_stamp, mask_method=args.mask,
            decay=args.decay, commitment_cost=args.commitment_cost, 
            num_vq_embeddings=args.num_vq_embeddings, vq_embedding_dim=args.vq_embedding_dim
            )
    elif args.com == "late":
        model = CNNNet(
            config,
            layer=args.layer,
            kd_flag=args.kd_flag,
            num_agent=num_agent,
            train_completion=True,
        )
    elif args.com == "vqvae":
        model = VQVAENet(
            num_hiddens = 256,
            num_residual_hiddens = 32,
            num_residual_layers = 2,
            num_embeddings = 512,
            embedding_dim = 64,
            commitment_cost = 0.25,
            decay = 0., # for VQ
        )
    else:
        raise NotImplementedError("Invalid argument com:" + args.com)

    # model = nn.DataParallel(model)
    model = model.to(device)
    # Juexiao added for mae
    if args.com == "ind_mae" or args.com == "joint_mae":
        param_groups = optim_factory.add_weight_decay(model, args.weight_decay)
        optimizer = optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))
    elif args.com == "vqstar":
        # param_groups = optim_factory.add_weight_decay(model, args.weight_decay)
        optimizer = optim.Adam(model.parameters(), lr=args.lr) #, betas=(0.9, 0.99))
    else:
        optimizer = optim.Adam(model.parameters(), lr=args.lr)
    criterion = {
        "cls": SoftmaxFocalClassificationLoss(),
        "loc": WeightedSmoothL1LocalizationLoss(),
    }

    co_module = CoModule(model, optimizer, args.com)

    model_save_path = check_folder(logger_root)
    model_save_path = check_folder(os.path.join(model_save_path, flag))

    if args.no_cross_road:
        model_save_path = check_folder(os.path.join(model_save_path, "no_cross"))
    else:
        model_save_path = check_folder(os.path.join(model_save_path, "with_cross"))

    # auto_resume
    # check if there is valid check point file
    cross_path = "no_cross" if args.no_cross_road else "with_cross"
    if auto_resume_path:
        has_valid_pth = False
        for pth_file in os.listdir(os.path.join(auto_resume_path, f"{flag}/{cross_path}")):
            if pth_file.startswith("completion_epoch_") and pth_file.endswith(".pth"):
                has_valid_pth = True
                break

        if not has_valid_pth:
            print(
                f"No valid check point file in {auto_resume_path} dir, weights not loaded."
            )
            auto_resume_path = ""

    if auto_resume_path or args.resume:
        if auto_resume_path:
            list_of_files = glob.glob(f"{model_save_path}/*.pth")
            latest_pth = max(list_of_files, key=os.path.getctime)
            checkpoint = torch.load(latest_pth, map_location='cpu')
        else:
            model_save_path = args.resume[: args.resume.rfind("/")]
            checkpoint = torch.load(args.resume, map_location='cpu')

        log_file_name = os.path.join(model_save_path, "log_completion.txt")
        saver = open(log_file_name, "a")
        saver.write("GPU number: {}\n".format(torch.cuda.device_count()))
        saver.flush()

        # Logging the details for this experiment
        saver.write("command line: {}\n".format(" ".join(sys.argv[1:])))
        saver.write(args.__repr__() + "\n\n")
        saver.flush()

        start_epoch = checkpoint["epoch"] + 1
        co_module.resume_from_cpu(checkpoint=checkpoint, device=device)

        print("Load model from {}, at epoch {}".format(auto_resume_path or args.resume, start_epoch - 1))

    else:
        if need_log:
            log_file_name = os.path.join(model_save_path, "log_completion.txt")
            saver = open(log_file_name, "w")
            saver.write("GPU number: {}\n".format(torch.cuda.device_count()))
            saver.flush()

            # Logging the details for this experiment
            saver.write("command line: {}\n".format(" ".join(sys.argv[0:])))
            saver.write(args.__repr__() + "\n\n")
            saver.flush() 


    if args.wandb:
        print("visualize using wandb")
        import wandb
        wandb.init(project="mae_bev_train", name="coperception+"+str(args.com)+"+dim"+str(args.num_vq_embeddings))
        wandb.config.update(args)
    
    for epoch in range(start_epoch, num_epochs + 1):
        lr = co_module.optimizer.param_groups[0]["lr"]
        print("Epoch {}, learning rate {}".format(epoch, lr))

        if need_log:
            saver.write("epoch: {}, lr: {}\t".format(epoch, lr))
            saver.flush()

        running_loss_disp = AverageMeter("Total loss", ":.6f")
        running_loss_class = AverageMeter(
            "classification Loss", ":.6f"
        )  # for cell classification error
        running_loss_loc = AverageMeter(
            "Localization Loss", ":.6f"
        )  # for state estimation error

        co_module.model.train()

        t = tqdm(training_data_loader)
        for data_iter_step, sample in enumerate(t):
            (
                padded_voxel_point_list,
                padded_voxel_point_next_list, # time_stamp=1: tuple(num_agent x [batchsize, 0]), >1: tuple(num_agent x [batchsize, time_stamp-1, H,W,C])
                padded_voxel_points_teacher_list,
                label_one_hot_list,
                reg_target_list,
                reg_loss_mask_list,
                anchors_map_list,
                vis_maps_list,
                target_agent_id_list,
                num_agent_list,
                trans_matrices_list,
            ) = zip(*sample)
            # accomodate multitemp dataset
            # timestamp 1, 2, 3 and etc

            trans_matrices = torch.stack(tuple(trans_matrices_list), 1)
            target_agent_id = torch.stack(tuple(target_agent_id_list), 1)
            num_all_agents = torch.stack(tuple(num_agent_list), 1) # num_agent

            if args.no_cross_road:
                num_all_agents -= 1

            padded_voxel_point = torch.cat(tuple(padded_voxel_point_list), 0) #[num_agent x batch_size, H, W, C]

            label_one_hot = torch.cat(tuple(label_one_hot_list), 0)
            reg_target = torch.cat(tuple(reg_target_list), 0)
            reg_loss_mask = torch.cat(tuple(reg_loss_mask_list), 0)
            anchors_map = torch.cat(tuple(anchors_map_list), 0)
            vis_maps = torch.cat(tuple(vis_maps_list), 0)

            data = {
                "bev_seq": padded_voxel_point.to(device),
                "labels": label_one_hot.to(device),
                "reg_targets": reg_target.to(device),
                "anchors": anchors_map.to(device),
                "reg_loss_mask": reg_loss_mask.to(device).type(dtype=torch.bool),
                "vis_maps": vis_maps.to(device),
                "target_agent_ids": target_agent_id.to(device),
                "num_agent": num_all_agents.to(device),
                "trans_matrices": trans_matrices,
            }
            padded_voxel_points_teacher = torch.cat(
                tuple(padded_voxel_points_teacher_list), 0
            )
            data["bev_seq_teacher"] = padded_voxel_points_teacher.to(device)


            if args.com == "ind_mae" or args.com == "joint_mae":
                # multi timestamp input bev supported in the data
                data["bev_seq_next"] = torch.cat(tuple(padded_voxel_point_next_list), 0).to(device)
                # adjust learning rate for mae
                lr_sched.adjust_learning_rate(co_module.optimizer, data_iter_step / len(training_data_loader) + epoch, args)
                loss, reconstruction, _ = co_module.step_mae_completion(data, batch_size, args.mask_ratio, trainable=True)
            elif args.com == "vqstar":
                data["bev_seq_next"] = torch.cat(tuple(padded_voxel_point_next_list), 0).to(device)
                # adjust learning rate for mae
                lr_sched.adjust_learning_rate(co_module.optimizer, data_iter_step / len(training_data_loader) + epoch, args)
                loss, reconstruction, _, perplexity = co_module.step_vqstar_completion(data, batch_size, args.mask_ratio, trainable=True)
            elif args.com == "vqvae":
                loss, _, _, perplexity = co_module.step_vae_completion(data, batch_size, trainable=True)
            else:
                loss, _ = co_module.step_completion(data, batch_size, trainable=True)
            running_loss_disp.update(loss)
            curr_lr = co_module.optimizer.param_groups[0]['lr']
            t.set_description("Epoch {},     lr {}".format(epoch, curr_lr))
            t.set_postfix(loss=running_loss_disp.avg)
            
            ## if visualize
            if data_iter_step % args.logstep == 0:
                if args.wandb:
                    # teacher_bev = torch.max(padded_voxel_points_teacher.squeeze(1).permute(0,3,1,2), dim=1, keepdim=True)[0]
                    # pred_bev = torch.max(reconstruction, dim=1, keepdim=True)[0]
                    # teacher_img = wandb.Image(plt.imshow(teacher_bev[0,:,:,:].detach().cpu().permute(1,2,0).squeeze(-1).numpy(), alpha=1.0, zorder=12))
                    # pred_img = wandb.Image(plt.imshow(pred_bev[0,:,:,:].detach().cpu().permute(1,2,0).squeeze(-1).numpy(), alpha=1.0, zorder=12)) 
                    # wandb.log({"overfit visualization": [teacher_img, pred_img]})
                    wandb.log({"imme loss": loss})
                    wandb.log({"runnning loss": running_loss_disp.avg})
                    wandb.log({"lr": curr_lr})
                    if args.com == "vqvae" or args.com == "vqstar":
                        wandb.log({"perplexity": perplexity})
                else:
                    # plain output record
                    print("**Epoch {} -- iterstep {}** : imme loss {:.6f}, running loss {:.6f}, current lr {:.6f}".format(epoch, data_iter_step, loss, running_loss_disp.avg, curr_lr))
                    if args.com == "vqvae" or args.com == "vqstar":
                        print("perplexity", perplexity)
            

                

        if args.com=="late" or args.com=="vqvae":
            co_module.scheduler.step() ## avoid this affects mae training

        # save model
        if need_log:
            saver.write(
                "{}\t{}\t{}\n".format(
                    running_loss_disp, running_loss_class, running_loss_loc
                )
            )
            saver.flush()
            if config.MGDA:
                save_dict = {
                    "epoch": epoch,
                    "encoder_state_dict": co_module.encoder.state_dict(),
                    "optimizer_encoder_state_dict": co_module.optimizer_encoder.state_dict(),
                    "scheduler_encoder_state_dict": co_module.scheduler_encoder.state_dict(),
                    "head_state_dict": co_module.head.state_dict(),
                    "optimizer_head_state_dict": co_module.optimizer_head.state_dict(),
                    "scheduler_head_state_dict": co_module.scheduler_head.state_dict(),
                    "loss": running_loss_disp.avg,
                }
            else:
                save_dict = {
                    "epoch": epoch,
                    "model_state_dict": co_module.model.state_dict(),
                    "optimizer_state_dict": co_module.optimizer.state_dict(),
                    "scheduler_state_dict": co_module.scheduler.state_dict(),
                    "mae_scaler_state_dict": co_module.mae_loss_scaler.state_dict(),
                    "loss": running_loss_disp.avg,
                }
            torch.save(
                save_dict,
                os.path.join(
                    model_save_path, "completion_epoch_" + str(epoch) + ".pth"
                ),
            )

    if need_log:
        saver.close()
    
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-d",
        "--data",
        default=None,
        type=str,
        help="The path to the preprocessed sparse BEV training data",
    )
    parser.add_argument("--batch", default=4, type=int, help="Batch size")
    parser.add_argument("--nepoch", default=100, type=int, help="Number of epochs")
    parser.add_argument("--nworker", default=2, type=int, help="Number of workers")
    parser.add_argument("--lr", default=0.001, type=float, help="Initial learning rate")
    parser.add_argument("--log", action="store_true", help="Whether to log")
    parser.add_argument("--logpath", default="", help="The path to the output log file")
    parser.add_argument(
        "--resume",
        default="",
        type=str,
        help="The path to the saved model that is loaded to resume training",
    )
    parser.add_argument(
        "--layer",
        default=3,
        type=int,
        help="Communicate which layer in the single layer com mode",
    )
    parser.add_argument(
        "--kd_flag",
        default=0,
        type=int,
        help="Whether to enable distillation (only DiscNet is 1 )",
    )
    parser.add_argument("--kd_weight", default=100000, type=int, help="KD loss weight")
    parser.add_argument(
        "--gnn_iter_times",
        default=3,
        type=int,
        help="Number of message passing for V2VNet",
    )
    parser.add_argument(
        "--com", default="", type=str, help="disco/when2com/v2v/sum/mean/max/cat/agent"
    )
    parser.add_argument(
        "--no_cross_road", action="store_true", help="Do not load data of cross roads"
    )
    parser.add_argument(
        "--num_agent", default=6, type=int, help="The total number of agents"
    )

    ## ----- mae args added by Juexiao -----
    parser.add_argument(
        "--mask_ratio", default=0.50, type=float, help="The input mask ratio"
    )
    parser.add_argument(
        "--mask", default="random", type=str, choices=["random", "complement"], help="Used in MAE training"
    )
    parser.add_argument(
        "--warmup_epochs", default=4, type=int, help="The number of warm up epochs"
    )
    parser.add_argument("--min_lr", default=0., type=float, help="minimum learning rate")
    parser.add_argument(
        "--time_stamp", default=2, type=int, help="The total number of time stamp to use"
    )
    parser.add_argument(
        "--mae_model", default="fusion_bev_mae_vit_base_patch8", type=str, 
        help="The mae model to use"
    )
    parser.add_argument(
        "--norm_pix_loss", default=False, type=bool, help="Whether normalize target pixel value for loss"
    )
    parser.add_argument(
        "--weight_decay", default=0.05, type=float, help="Used in MAE training"
    )
    ## ----------------------
    # logger
    parser.add_argument(
        "--wandb", action="store_true", help="Whether use wandb to visualize"
    )
    parser.add_argument(
        "--logstep", default=200, type=int, help="Step interval for making a log."
    )

    # auto_resume
    parser.add_argument(
        "--auto_resume_path",
        default="",
        type=str,
        help="The path to automatically reload the latest pth",
    )

    ## args for vector quantization
    parser.add_argument(
        "--decay", default=0., type=float, help="used in vector quantization"
    )
    parser.add_argument(
        "--commitment_cost", default=0.25, type=float, help="Used in vector quantization"
    )
    parser.add_argument(
        "--num_vq_embeddings", default=512, type=int, help="Used in vector quantization"
    )
    parser.add_argument(
        "--vq_embedding_dim", default=64, type=int, help="Used in vector quantization"
    )

    ## agrs for ablation study:
    parser.add_argument(
        "--encode_partial", action="store_true", help="encode partial information before masking"
    )
    parser.add_argument(
        "--no_temp_emb", action="store_true", help="Do not use temp embedding"
    )
    parser.add_argument(
        "--decode_singletemp", action="store_true", help="decode single stemp"
    )

    torch.multiprocessing.set_sharing_strategy("file_system")
    args = parser.parse_args()
    print(args)
    main(args)


# test completion using occupancy IoU as the metric
import argparse
import os
import sys
sys.path.append("..")
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

from yaml import load

import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

from star.datasets.MultiTempDet import MultiTempV2XSimDet
from star.utils.CoModule import CoModule

from coperception.configs import Config, ConfigGlobal
from coperception.utils.loss import *
from coperception.models.det import *
from star.models import multiagent_mae, VQVAENet, CNNNet, VQSTAR
from coperception.utils import AverageMeter

# from mae script -----------
import timm
assert timm.__version__ == "0.3.2"  # version check
import timm.optim.optim_factory as optim_factory
import matplotlib.pyplot as plt
# ----------------------------

# for IoU test
from star.utils.metrics import Metrics

def check_folder(folder_path):
    if not os.path.exists(folder_path):
        os.mkdir(folder_path)
    return folder_path


def main(args):
    config = Config("train", binary=True, only_det=True) # need to change
    config_global = ConfigGlobal("train", binary=True, only_det=True)

    num_epochs = args.nepoch
    need_log = args.log
    num_workers = args.nworker
    start_epoch = 1
    batch_size = args.batch
    num_agent = args.num_agent

    # Specify gpu device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    device_num = torch.cuda.device_count()
    print("device number", device_num)

    if args.com in {"mean", "max", "cat", "sum", "v2v", "ind_mae", "joint_mae", "late", "vqvae", "vqstar"}:
        flag = args.com
    else:
        raise ValueError(f"com: {args.com} is not supported")

    config.flag = flag

    agent_idx_range = range(1, num_agent) if args.no_cross_road else range(num_agent)

    test_dataset = MultiTempV2XSimDet(
        dataset_roots=[f"{args.data}/agent{i}" for i in agent_idx_range],
        config=config,
        config_global=config_global,
        split="train",
        bound="both",
        kd_flag=args.kd_flag,
        no_cross_road=args.no_cross_road,
        time_stamp = args.time_stamp
    )
    test_data_loader = DataLoader(
        test_dataset, shuffle=False, batch_size=batch_size, num_workers=num_workers
    )
    print("Testing dataset size:", len(test_dataset))

    # logger_root = args.logpath if args.logpath != "" else "logs"

    if args.no_cross_road:
        num_agent -= 1

    if args.com == "joint_mae" or  args.com == "ind_mae":
        # Juexiao added for mae
        model = multiagent_mae.__dict__[args.mae_model](norm_pix_loss=args.norm_pix_loss, time_stamp=args.time_stamp, mask_method=args.mask)
        # also include individual reconstruction: reconstruct then aggregate
    elif args.com == "late":
        model = CNNNet(
            config,
            layer=args.layer,
            kd_flag=args.kd_flag,
            num_agent=num_agent,
            train_completion=True,
        )
    elif args.com == "vqvae":
        model = VQVAENet(
            num_hiddens = 256,
            num_residual_hiddens = 32,
            num_residual_layers = 2,
            num_embeddings = 512,
            embedding_dim = 64,
            commitment_cost = 0.25,
            decay = 0., # for VQ
        )
    elif args.com == "vqstar":
        model = VQSTAR.vqstar(
            norm_pix_loss=args.norm_pix_loss, time_stamp=args.time_stamp, mask_method=args.mask,
            decay=args.decay, commitment_cost=args.commitment_cost, 
            num_vq_embeddings=args.num_vq_embeddings, vq_embedding_dim=args.vq_embedding_dim
            )
    else:
        raise NotImplementedError("Invalid argument com:" + args.com)

    # model = nn.DataParallel(model)

    model_load_path = args.load_path[: args.load_path.rfind("/")]

    checkpoint = torch.load(args.load_path, map_location='cpu')
    load_epoch = checkpoint["epoch"] + 1
    model.load_state_dict(checkpoint["model_state_dict"])
    print("Load model from {}, at epoch {}".format(args.load_path, load_epoch - 1))
    model = model.to(device)

    log_file_name = os.path.join(model_load_path, "log_test_completion_local_{}.txt".format(load_epoch-1))
    saver = open(log_file_name, "a")
    saver.write("GPU number: {}\n".format(torch.cuda.device_count()))
    saver.flush()

    # Logging the details for this experiment
    saver.write("command line: {}\n".format(" ".join(sys.argv[1:])))
    saver.write(args.__repr__() + "\n\n")
    saver.flush()

    # Juexiao added for mae
    if args.com == "ind_mae" or args.com == "joint_mae":
        param_groups = optim_factory.add_weight_decay(model, args.weight_decay)
        optimizer = optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))
    else:
        optimizer = optim.Adam(model.parameters(), lr=args.lr)
    criterion = {
        "cls": SoftmaxFocalClassificationLoss(),
        "loc": WeightedSmoothL1LocalizationLoss(),
    }

    co_module = CoModule(model, optimizer, args.com)

    running_loss_test = AverageMeter("Total Test loss", ":.6f")
    IoUEvaluator = Metrics(nbr_classes=2, num_iterations_epoch=load_epoch)
    co_module.model.eval()
    
    # sampled_latents = []
    et = tqdm(test_data_loader)
    for data_iter_step, sample in enumerate(et):
        (
            padded_voxel_point_list,
            padded_voxel_point_next_list, # time_stamp=1: tuple(num_agent x [batchsize, 0]), >1: tuple(num_agent x [batchsize, time_stamp-1, H,W,C])
            padded_voxel_points_teacher_list,
            label_one_hot_list,
            reg_target_list,
            reg_loss_mask_list,
            anchors_map_list,
            vis_maps_list,
            target_agent_id_list,
            num_agent_list,
            trans_matrices_list,
        ) = zip(*sample)

        trans_matrices = torch.stack(tuple(trans_matrices_list), 1)
        target_agent_id = torch.stack(tuple(target_agent_id_list), 1)
        num_all_agents = torch.stack(tuple(num_agent_list), 1) # num_agent

        if args.no_cross_road:
            num_all_agents -= 1

        padded_voxel_point = torch.cat(tuple(padded_voxel_point_list), 0) #[num_agent x batch_size, H, W, C]

        label_one_hot = torch.cat(tuple(label_one_hot_list), 0)
        reg_target = torch.cat(tuple(reg_target_list), 0)
        reg_loss_mask = torch.cat(tuple(reg_loss_mask_list), 0)
        anchors_map = torch.cat(tuple(anchors_map_list), 0)
        vis_maps = torch.cat(tuple(vis_maps_list), 0)

        data = {
            "bev_seq": padded_voxel_point.to(device),
            "labels": label_one_hot.to(device),
            "reg_targets": reg_target.to(device),
            "anchors": anchors_map.to(device),
            "reg_loss_mask": reg_loss_mask.to(device).type(dtype=torch.bool),
            "vis_maps": vis_maps.to(device),
            "target_agent_ids": target_agent_id.to(device),
            "num_agent": num_all_agents.to(device),
            "trans_matrices": trans_matrices,
        }
        padded_voxel_points_teacher = torch.cat(
            tuple(padded_voxel_points_teacher_list), 0
        )
        data["bev_seq_teacher"] = padded_voxel_points_teacher.to(device)

        if args.com == "ind_mae" or args.com == "joint_mae":
            # multi timestamp input bev supported in the data
            data["bev_seq_next"] = torch.cat(tuple(padded_voxel_point_next_list), 0).to(device)
            # adjust learning rate for mae
            with torch.no_grad():
                loss, reconstruction, ind_rec, latent = co_module.infer_mae_completion(data, batch_size, args.mask_ratio)
        elif args.com == "vqstar":
            data["bev_seq_next"] = torch.cat(tuple(padded_voxel_point_next_list), 0).to(device)
            with torch.no_grad():
                loss, reconstruction, ind_rec, perplexity, encodings = co_module.infer_vqstar_completion(data, batch_size, args.mask_ratio)
        elif args.com == "vqvae":
            with torch.no_grad():
                loss, reconstruction, ind_rec, perplexity = co_module.step_vae_completion(data, batch_size, trainable=False)
                ind_rec = torch.argmax(torch.softmax(ind_rec, dim=1), dim=1)
        else:
            with torch.no_grad():
                loss, reconstruction = co_module.step_completion(data, batch_size, trainable=False)

        target = data["bev_seq_teacher"].squeeze(1).permute(0,3,1,2) #[B, C, H, W]
        bev_seq = data['bev_seq'].squeeze(1).permute(0,3,1,2)
        IoUEvaluator.add_batch(reconstruction, target)
        IoUEvaluator.update_IoU()
        # print(IoUEvaluator.every_batch_IoU)
        # save every 100
        if args.save_vis and data_iter_step%200==0:
            print(IoUEvaluator.every_batch_IoU[-1])
            target_img = torch.max(target.cpu(), dim=1, keepdim=True)[0]
            recon_img = torch.max(reconstruction.detach().cpu(), dim=1, keepdim=True)[0]
            indi_img = torch.max(ind_rec.cpu(), dim=1, keepdim=True)[0]
            start_agent_idx = 0 if args.no_cross_road else 1
            for kid in range(start_agent_idx, num_agent):
                recon_save = "pure-reconstruction-epc{}-id{}-agent{}.png".format(load_epoch-1, data_iter_step, kid)
                target_save = "target-epc{}-id{}-agent{}.png".format(load_epoch-1, data_iter_step, kid)
                indi_save = "individual-epc{}-id{}-agent{}.png".format(load_epoch-1, data_iter_step, kid)
                plt.imshow(target_img[kid,:,:,:].permute(1,2,0).squeeze(-1).numpy(), alpha=1.0, zorder=12, cmap="Purples")
                plt.axis('off')
                plt.savefig(os.path.join(model_load_path, target_save))
                plt.imshow(recon_img[kid,:,:,:].permute(1,2,0).squeeze(-1).numpy(), alpha=1.0, zorder=12, cmap="Purples")
                plt.axis('off')
                plt.savefig(os.path.join(model_load_path, recon_save))
                plt.imshow(indi_img[kid,:,:,:].permute(1,2,0).squeeze(-1).numpy(), alpha=1.0, zorder=12, cmap="Purples")
                plt.axis('off')
                plt.savefig(os.path.join(model_load_path,indi_save))
                # print("codebook idx:", encodings[kid, :].detach().cpu().nonzero().squeeze())
            torch.save(encodings.detach().cpu(), os.path.join(model_load_path, "encodings-epc{}-id{}.pt".format(load_epoch-1, data_iter_step)))
            # torch.save(reconstruction, os.path.join(model_load_path, "reconstruction-epc{}-id{}.pt".format(load_epoch-1, data_iter_step)))
            # torch.save(target, os.path.join(model_load_path, "target-epc{}-id{}.pt".format(load_epoch-1, data_iter_step)))
            # torch.save(bev_seq, os.path.join(model_load_path, "bev-epc{}-id{}.pt".format(load_epoch-1, data_iter_step)))
            # torch.save(ind_rec, os.path.join(model_load_path, "individual-epc{}-id{}.pt".format(load_epoch-1, data_iter_step)))
            # exit(1)

        running_loss_test.update(loss)
        et.set_postfix(loss=running_loss_test.avg)

    # show result
    total_IoU = IoUEvaluator.get_average_IoU()
    print("Occupancy IoU for test set:", total_IoU)
    # print(IoUEvaluator.evaluator.get_confusion())
    saver.write("Occupancy IoU for test set: {}\n".format(total_IoU))
    saver.flush()

    saver.close()



if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-d",
        "--data",
        default=None,
        type=str,
        help="The path to the preprocessed sparse BEV training data",
    )
    parser.add_argument("--batch", default=4, type=int, help="Batch size")
    parser.add_argument("--nepoch", default=100, type=int, help="Number of epochs")
    parser.add_argument("--nworker", default=2, type=int, help="Number of workers")
    parser.add_argument("--lr", default=0.001, type=float, help="Initial learning rate")
    parser.add_argument("--log", action="store_true", help="Whether to log")
    parser.add_argument("--logpath", default="", help="The path to the output log file")
    parser.add_argument(
        "--resume",
        default="",
        type=str,
        help="The path to the saved model that is loaded to resume training",
    )
    parser.add_argument(
        "--layer",
        default=3,
        type=int,
        help="Communicate which layer in the single layer com mode",
    )
    parser.add_argument(
        "--kd_flag",
        default=0,
        type=int,
        help="Whether to enable distillation (only DiscNet is 1 )",
    )
    parser.add_argument("--kd_weight", default=100000, type=int, help="KD loss weight")
    parser.add_argument(
        "--gnn_iter_times",
        default=3,
        type=int,
        help="Number of message passing for V2VNet",
    )
    parser.add_argument(
        "--com", default="", type=str, help="disco/when2com/v2v/sum/mean/max/cat/agent"
    )
    parser.add_argument(
        "--no_cross_road", action="store_true", help="Do not load data of cross roads"
    )
    parser.add_argument(
        "--num_agent", default=6, type=int, help="The total number of agents"
    )

    ## ----- mae args added by Juexiao -----
    parser.add_argument(
        "--mask_ratio", default=0.50, type=float, help="The input mask ratio"
    )
    parser.add_argument(
        "--mask", default="complement", type=str, choices=["random", "complement"], help="Used in MAE training"
    )
    parser.add_argument(
        "--warmup_epochs", default=4, type=int, help="The number of warm up epochs"
    )
    parser.add_argument("--min_lr", default=0., type=float, help="minimum learning rate")
    parser.add_argument(
        "--time_stamp", default=1, type=int, help="The total number of time stamp to use"
    )
    parser.add_argument(
        "--mae_model", default="fusion_bev_mae_vit_base_patch8", type=str, 
        help="The mae model to use"
    )
    parser.add_argument(
        "--norm_pix_loss", default=False, type=bool, help="Whether normalize target pixel value for loss"
    )
    parser.add_argument(
        "--weight_decay", default=0.05, type=float, help="Used in MAE training"
    )
    ## args for vector quantization
    parser.add_argument(
        "--decay", default=0., type=float, help="used in vector quantization"
    )
    parser.add_argument(
        "--commitment_cost", default=0.25, type=float, help="Used in vector quantization"
    )
    parser.add_argument(
        "--num_vq_embeddings", default=512, type=int, help="Used in vector quantization"
    )
    parser.add_argument(
        "--vq_embedding_dim", default=64, type=int, help="Used in vector quantization"
    )
    ## ----------------------
    parser.add_argument(
        "--wandb", action="store_true", help="Whether use wandb to visualize"
    )
    ## for test
    parser.add_argument(
        "--load_path", default=None, type=str, help="the path to the save model"
    )
    parser.add_argument(
        "--save_vis", action="store_true", help="Whether save output for visualization"
    )

    torch.multiprocessing.set_sharing_strategy("file_system")
    args = parser.parse_args()
    print(args)
    main(args)


from .utils import *
from .models import *
from .datasets import *


# from:
# https://nbviewer.org/github/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb
import torch
import torch.nn as nn
import torch.nn.functional as F

class Residual(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):
        super(Residual, self).__init__()
        self._block = nn.Sequential(
            nn.ReLU(True),
            nn.Conv2d(in_channels=in_channels,
                      out_channels=num_residual_hiddens,
                      kernel_size=3, stride=1, padding=1, bias=False),
            nn.ReLU(True),
            nn.Conv2d(in_channels=num_residual_hiddens,
                      out_channels=num_hiddens,
                      kernel_size=1, stride=1, bias=False)
        )
    
    def forward(self, x):
        return x + self._block(x)


class ResidualStack(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):
        super(ResidualStack, self).__init__()
        self._num_residual_layers = num_residual_layers
        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)
                             for _ in range(self._num_residual_layers)])

    def forward(self, x):
        for i in range(self._num_residual_layers):
            x = self._layers[i](x)
        return F.relu(x)


class VectorQuantizer(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, commitment_cost):
        super(VectorQuantizer, self).__init__()
        
        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings
        
        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)
        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)
        self._commitment_cost = commitment_cost

    def forward(self, inputs):
        # convert inputs from BCHW -> BHWC
        inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape
        
        # Flatten input
        flat_input = inputs.view(-1, self._embedding_dim)
        
        # Calculate distances
        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) 
                    + torch.sum(self._embedding.weight**2, dim=1)
                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))
            
        # Encoding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)
        
        # Quantize and unflatten
        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)
        
        # Loss
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        q_latent_loss = F.mse_loss(quantized, inputs.detach())
        loss = q_latent_loss + self._commitment_cost * e_latent_loss
        
        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))
        
        # convert quantized from BHWC -> BCHW
        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings


class VectorQuantizerEMA(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):
        super(VectorQuantizerEMA, self).__init__()
        
        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings
        
        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)
        self._embedding.weight.data.normal_()
        self._commitment_cost = commitment_cost
        
        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))
        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))
        self._ema_w.data.normal_()
        
        self._decay = decay
        self._epsilon = epsilon

    def forward(self, inputs):
        # convert inputs from BCHW -> BHWC
        inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape
        
        # Flatten input
        flat_input = inputs.view(-1, self._embedding_dim)
        
        # Calculate distances
        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) 
                    + torch.sum(self._embedding.weight**2, dim=1)
                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))
            
        # Encoding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)
        
        # Quantize and unflatten
        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)
        
        # Use EMA to update the embedding vectors
        if self.training:
            self._ema_cluster_size = self._ema_cluster_size * self._decay + \
                                     (1 - self._decay) * torch.sum(encodings, 0)
            
            # Laplace smoothing of the cluster size
            n = torch.sum(self._ema_cluster_size.data)
            self._ema_cluster_size = (
                (self._ema_cluster_size + self._epsilon)
                / (n + self._num_embeddings * self._epsilon) * n)
            
            dw = torch.matmul(encodings.t(), flat_input)
            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)
            
            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))
        
        # Loss
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        loss = self._commitment_cost * e_latent_loss
        
        # Straight Through Estimator
        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))
        
        # convert quantized from BHWC -> BCHW
        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings


class VQVAEEncoder(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):
        super(VQVAEEncoder, self).__init__()

        self._conv_1 = nn.Conv2d(in_channels=in_channels,
                                 out_channels=num_hiddens//2,
                                 kernel_size=4,
                                 stride=2, padding=1)
        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//2,
                                 out_channels=num_hiddens,
                                 kernel_size=4,
                                 stride=2, padding=1)
        self._conv_3 = nn.Conv2d(in_channels=num_hiddens,
                                 out_channels=num_hiddens,
                                 kernel_size=3,
                                 stride=1, padding=1)
        self._residual_stack = ResidualStack(in_channels=num_hiddens,
                                             num_hiddens=num_hiddens,
                                             num_residual_layers=num_residual_layers,
                                             num_residual_hiddens=num_residual_hiddens)

    def forward(self, inputs):
        x = self._conv_1(inputs)
        x = F.relu(x)
        
        x = self._conv_2(x)
        x = F.relu(x)
        
        x = self._conv_3(x)
        return self._residual_stack(x)


class VQVAEDecoder(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):
        super(VQVAEDecoder, self).__init__()
        
        self._conv_1 = nn.Conv2d(in_channels=in_channels,
                                 out_channels=num_hiddens,
                                 kernel_size=3, 
                                 stride=1, padding=1)
        
        self._residual_stack = ResidualStack(in_channels=num_hiddens,
                                             num_hiddens=num_hiddens,
                                             num_residual_layers=num_residual_layers,
                                             num_residual_hiddens=num_residual_hiddens)
        
        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, 
                                                out_channels=num_hiddens//2,
                                                kernel_size=4, 
                                                stride=2, padding=1)
        
        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2, 
                                                out_channels=13,
                                                kernel_size=4, 
                                                stride=2, padding=1)

        self._conv_trans_2_free = nn.ConvTranspose2d(in_channels=num_hiddens//2, 
                                                out_channels=13,
                                                kernel_size=4, 
                                                stride=2, padding=1)

    def forward(self, inputs):
        x = self._conv_1(inputs)
        
        x = self._residual_stack(x)
        
        x = self._conv_trans_1(x)
        x = F.relu(x)
        
        occ_x = self._conv_trans_2(x)
        free_x = self._conv_trans_2_free(x)
        bin_x = torch.stack((free_x, occ_x), dim=1)
        # return self._conv_trans_2(x)
        return bin_x


class VQVAEModel(nn.Module):
    """Use vqvae to reconstruct the scene"""
    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, 
                 num_embeddings, embedding_dim, commitment_cost, decay=0):
        super(VQVAEModel, self).__init__()
        
        self._encoder = VQVAEEncoder(13, num_hiddens,
                                num_residual_layers, 
                                num_residual_hiddens)
        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens, 
                                      out_channels=embedding_dim,
                                      kernel_size=1, 
                                      stride=1) # map model from original hidden dim to embedding dim
        if decay > 0.0:
            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, 
                                              commitment_cost, decay)
        else:
            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,
                                           commitment_cost)
        self._decoder = VQVAEDecoder(embedding_dim,
                                num_hiddens, 
                                num_residual_layers, 
                                num_residual_hiddens)

    def forward(self, x):
        z = self._encoder(x)
        # print("after encoder z", z.size())
        z = self._pre_vq_conv(z)
        # print("pre quantized z", z.size())
        # exit(1)
        loss, quantized, perplexity, _ = self._vq_vae(z)
        x_recon = self._decoder(quantized)
        # x_recon = self._decoder(z)
        return loss, x_recon, perplexity


class VQVAENet(nn.Module):
    """wrapper of vqvae, handles fusion"""
    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, 
                 num_embeddings, embedding_dim, commitment_cost, decay=0):
        super().__init__()
        self.vqvae = VQVAEModel(
            num_hiddens = num_hiddens,
            num_residual_layers = num_residual_layers,
            num_residual_hiddens = num_residual_hiddens,
            num_embeddings = num_embeddings,
            embedding_dim = embedding_dim,
            commitment_cost = commitment_cost,
            decay = decay,
        )

    def get_feature_maps_size(self, feature_maps: tuple):
        size = list(feature_maps.shape)
        # NOTE: batch size will change the shape[0]. We need to manually set it to 1.
        size[0] = 1
        size = tuple(size)
        return size

    # get feat maps for each agent [10 512 32 32] -> [2 5 512 32 32]
    def build_feature_list(self, batch_size: int, feat_maps: dict) -> list:
        feature_map = {}
        # [5,256,32,32]
        feature_list = []

        for i in range(self.num_agent):
            feature_map[i] = torch.unsqueeze(feat_maps[batch_size * i:batch_size * (i + 1)], 1)
            # feature_map[i]: [B,1,256,32,32]
            feature_list.append(feature_map[i])

        return feature_list

    # [2 5 512 16 16] [batch, agent, channel, height, width]
    @staticmethod
    def build_local_communication_matrix(feature_list: list):
        return torch.cat(tuple(feature_list), 1)

    @staticmethod
    # FIXME: rename 'j'
    def feature_transformation(b, nb_agent_idx, local_com_mat, all_warp, device, size):
        nb_agent = torch.unsqueeze(local_com_mat[b, nb_agent_idx], 0)  # [1 512 16 16]
        nb_warp = all_warp[nb_agent_idx]  # [4 4]
        # normalize the translation vector
        x_trans = (4 * nb_warp[0, 3]) / 128
        y_trans = -(4 * nb_warp[1, 3]) / 128

        theta_rot = torch.tensor(
            [[nb_warp[0, 0], nb_warp[0, 1], 0.0], [nb_warp[1, 0], nb_warp[1, 1], 0.0]]).type(
            dtype=torch.float).to(device)
        theta_rot = torch.unsqueeze(theta_rot, 0)
        grid_rot = F.affine_grid(theta_rot, size=torch.Size(size))  # get grid for grid sample

        theta_trans = torch.tensor([[1.0, 0.0, x_trans], [0.0, 1.0, y_trans]]).type(dtype=torch.float).to(
            device)
        theta_trans = torch.unsqueeze(theta_trans, 0)
        grid_trans = F.affine_grid(theta_trans, size=torch.Size(size))  # get grid for grid sample

        # first rotate the feature map, then translate it
        warp_feat_rot = F.grid_sample(nb_agent, grid_rot, mode='nearest')
        warp_feat_trans = F.grid_sample(warp_feat_rot, grid_trans, mode='nearest')
        return torch.squeeze(warp_feat_trans, dim=0) # [512, 16, 16]


    def build_neighbors_feature_list(self, b, agent_idx, all_warp, num_agent, local_com_mat, device,
                                     size) -> None:
        for j in range(num_agent):
            if j != agent_idx:
                warp_feat = self.feature_transformation(b, j, local_com_mat, all_warp, device, size)
                self.neighbor_feat_list.append(warp_feat)
        
    def fusion(self, mode="sum"):
        fused = torch.sum(torch.stack(self.neighbor_feat_list), dim=0)
        if mode == "union":
            fused[fused>0.] = 1.0
        if mode == "sum":
            fused[fused>=0.5] = 1.0
            fused[fused<0.5] = 0.0
        return fused 

    @staticmethod
    def agents_to_batch(feats):
        agent_num = feats.shape[1]
        feat_list = []
        for i in range(agent_num):
            feat_list.append(feats[:, i, :, :, :])
        feat_mat = torch.cat(tuple(feat_list), 0)
        return feat_mat
    
    def ego_late_fusion(self, pred, gt, trans_matrices, num_agent_tensor, batch_size):
        """ aggregation, ego use ground truth input, used specifically in inference time"""
        device = pred.device
        # indiv_imgs = self.unpatchify(pred) # [B C H W]
        indiv_imgs = pred.type(torch.FloatTensor).to(device)
        ## --- do fusion ---
        size = self.get_feature_maps_size(indiv_imgs)
        # print(size)
        assert indiv_imgs.size(0) % batch_size == 0, (indiv_imgs.size(), batch_size)
        self.num_agent = indiv_imgs.size(0) // batch_size
        feat_list = self.build_feature_list(batch_size, indiv_imgs)
        gt_list = self.build_feature_list(batch_size, gt)
        # print(feat_list)
        local_com_mat = self.build_local_communication_matrix(feat_list)  # [2 5 13 256 256] [batch, agent, channel, height, width]
        local_com_mat_update = self.build_local_communication_matrix(feat_list)  # to avoid the inplace operation
        local_com_mat_gt = self.build_local_communication_matrix(gt_list)

        for b in range(batch_size):
            num_agent = num_agent_tensor[b, 0]
            for i in range(num_agent):
                # use gt for the ego
                self.tg_agent = local_com_mat_gt[b, i]
                # print("tg agent shape", self.tg_agent.shape) #[13,256,256] 
                self.neighbor_feat_list = []
                self.neighbor_feat_list.append(self.tg_agent)
                all_warp = trans_matrices[b, i]  # transformation [2 5 5 4 4]
                # print(all_warp.shape)[5,4,4]
                self.build_neighbors_feature_list(b, i, all_warp, num_agent, local_com_mat,
                                                    device, size)

                # feature update
                # torch.save(torch.stack(self.neighbor_feat_list).detach().cpu(), "/mnt/NAS/home/zjx/Masked-Multiagent-Autoencoder/debug/nbf-{}-{}.pt".format(b, i))
                local_com_mat_update[b, i] = self.fusion(mode="union")
                # local_com_mat_update[b, i] = self.fusion(mode="sum") 
        
        # weighted feature maps is passed to decoder
        fused_images = self.agents_to_batch(local_com_mat_update)
        # print('feat fuse mat size', feat_fuse_mat.size())
        return fused_images

    def forward(self, bevs, trans_matrices=None, num_agent_tensor=None, batch_size=None):
        # print("bev", bevs.size())
        bevs = bevs.permute(0,1,4,2,3)
        loss, ind_recon, perplexity = self.vqvae(bevs.squeeze(1))
        # print("ind_recon", ind_recon.size())
        # fuse reconstruction
        # binary_ind_recon = torch.zeros_like(ind_recon)
        # binary_ind_recon[ind_recon>0.5] = 1.0
        # # binary_ind_recon = ind_recon
        # result = self.ego_late_fusion(binary_ind_recon, bevs.squeeze(1), trans_matrices, num_agent_tensor, batch_size)
        #### if use classfication loss ####
        ind_result = torch.argmax(torch.softmax(ind_recon, dim=1), dim=1)
        result = self.ego_late_fusion(ind_result, bevs.squeeze(1), trans_matrices, num_agent_tensor, batch_size)
        # print("result", result.size())
        return loss, result, ind_recon, perplexity


from difflib import restore
from functools import partial

from requests import PreparedRequest

import torch
import torch.nn as nn
import torch.nn.functional as F

from star.models.mae_base import *
from star.utils.pos_embed import get_2d_sincos_pos_embed
import math

class VQSTARViT(MultiAgentMaskedAutoencoderViT):
    """
    STAR + Vector Quantizer
    """
    def __init__(self, img_size=224, patch_size=16, in_chans=3,
                 embed_dim=1024, depth=24, num_heads=16,
                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
                 mlp_ratio=4., norm_layer=nn.LayerNorm, decoder_head="mlp", norm_pix_loss=False, time_stamp=1, 
                 mask_method="complement", encode_partial=False, no_temp_emb=False, decode_singletemp=False, 
                 decay=0., commitment_cost=0.25, num_vq_embeddings=512, vq_embedding_dim=64):
        super(VQSTARViT, self).__init__(
                img_size=img_size, 
                patch_size=patch_size, 
                in_chans=in_chans,
                embed_dim=embed_dim, 
                depth=depth, 
                num_heads=num_heads,
                decoder_embed_dim=decoder_embed_dim, 
                decoder_depth=decoder_depth, 
                decoder_num_heads=decoder_num_heads,
                mlp_ratio=mlp_ratio, 
                norm_layer=norm_layer, 
                decoder_head = decoder_head,
                norm_pix_loss=norm_pix_loss,
                time_stamp = time_stamp,
                mask_method = mask_method,
                inter_emb_dim = vq_embedding_dim)
        # for neighbor agents' features
        self.patch_h = 0
        self.patch_w = 0
        # class_weights = torch.FloatTensor([1.0, 20.0]) # [free, occ]
        # print("Using weighted cross entropy loss with weights", class_weights)
        # self.cls_loss = nn.CrossEntropyLoss(weight = class_weights)
        self.cls_loss = nn.CrossEntropyLoss()
        # self.focal_loss = SoftmaxFocalLoss(alpha=0.75, gamma=3)

        # quantizer part
        if decay > 0.0:
            self._vq_star = STARVectorQuantizerEMA(num_vq_embeddings, vq_embedding_dim, 
                                              commitment_cost, decay)
        else:
            self._vq_star = STARVectorQuantizer(num_vq_embeddings, vq_embedding_dim,
                                           commitment_cost)

        
        if mask_method == "random":
            print("do random masking")
            self.masking_handle = self.amortized_random_masking
            self.unmasking_handle = self.amortized_random_unmasking
        elif mask_method == "complement":
            print("do complement masking")
            self.masking_handle = self.amortized_complement_masking
            self.unmasking_handle = self.amortized_complement_unmasking
        else:
            raise NotImplementedError(mask_method)

        # ---- ablation study for encoder ------
        if encode_partial:
            print("encode after masking")
            self.forward_encoder = self.forward_encoder_partial
        else:
            print("encoder BEFORE masking")
            self.forward_encoder = self.forward_encoder_all

        # ---- ablation study for decoder ------
        self.decode_singletemp = decode_singletemp
        self.no_temp_emb = no_temp_emb

        

    def amortized_random_masking(self, x_ts, mask_ratio):
        """
        Random masking for each data of each timestamp.
        Maintain indices for decoder to fuse and possibly fill in mask tokens
        x_ts: [bxa x time_stamp, L, D]
        """
        xts_masked, mask, ids_restore = self.random_masking(x_ts, mask_ratio)
        return xts_masked, mask, ids_restore

    def amortized_random_unmasking(self, x_ts, mask, ids_restore):
        """
        Reverse masking with fillings from other timestamp, using mask and torch where
        x_ts: [BxAxts, L, D]
        mask: [BxAxts, L] -> full length L
        ids_restore: [BxAxts, L] -> full length L
        """
        # print(x_ts.size(), mask.size(), ids_restore.size())
        # fill with mask tokens
        mask_tokens = self.mask_token.repeat(x_ts.shape[0], ids_restore.shape[1] + 1 - x_ts.shape[1], 1)
        x_ts_ = torch.cat([x_ts, mask_tokens], dim=1)  # no cls token
        x_ts_ = torch.gather(x_ts_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_ts.shape[2]))  # unshuffle
        BAT, L, D = x_ts_.size()
        assert BAT%self.time_stamp == 0, (BAT, self.time_stamp)
        BA = BAT // self.time_stamp
        # add temporal embedding
        x_ts_ = x_ts_.reshape(BA, self.time_stamp, L, D)

        # ---- ablation: use time emb -----
        if not self.no_temp_emb:
            # print("use temporal embedding")
            x_ts_ = x_ts_ + self.decoder_temp_embed #check dim
        # ----------------------------------

        # figure out how to fill in the other timestamp
        x_ts_unp = x_ts_.reshape(BA, self.time_stamp, self.patch_h, self.patch_w, x_ts_.shape[-1])
        # print("x_ts_unp size", x_ts_unp.size())
        mask_unp = mask.reshape(mask.shape[0], self.patch_h, self.patch_w) #self.unpatchify(mask.unsqueeze(-1))
        # assert mask_unp.size(1) == 1, (mask_unp.size())
        mask_unp = mask_unp.reshape(BA, self.time_stamp, self.patch_h, self.patch_w) # [BxA, ts, H, W], 0 is kept, 1 is removed
        reverse_mask = 1. - mask_unp # 0 is removed, >0 is kept
        mask_filled = reverse_mask[:,0,:,:] # current timestamp, 
        x_curr = x_ts_unp[:,0,:,:,:] #[BxA, h, w, D]
        # print("x_curr size", x_curr.size())
        if self.decode_singletemp:
            print("decode single time stamp")
            x_curr = x_curr.permute(0,3,1,2)
            return x_curr
        else:
            for ti in range(self.time_stamp-1):
                # not filled previously but can be filled by ti+1
                mask_curr = reverse_mask[:,ti+1, :, :]
                mask_select = ((mask_filled==0.) & (mask_curr>0.)).unsqueeze(-1)
                x_curr = torch.where(mask_select, x_ts_unp[:, ti+1, :, :, :], x_curr)
                mask_filled = mask_filled + mask_curr # update the mask
            x_curr = x_curr.permute(0,3,1,2) #[BxA, C, H, W]
            return x_curr

    def amortized_complement_masking(self, x_ts, mask_ratio):
        """
        x_ts: [bxa x time_stamp, L, D]
        NOTE: the returned mask and ids_restore has a different dimensionality from the random masking
        """
        mask_ratio = 1.0 - 1.0 / self.time_stamp # e.g. times_stamp = 1, mask ratio ==0, 2, 0.5
        BAT, L, D = x_ts.shape
        assert BAT % self.time_stamp == 0
        BA = BAT // self.time_stamp
        # len_keep_each = int(L * (1 - mask_ratio))
        len_keep_each = math.ceil(L * (1 - mask_ratio)) # make sure every patch is covered
        noise = torch.rand(BA, L, device=x_ts.device)
        # sort noise for each sample
        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove
        ids_restore = torch.argsort(ids_shuffle, dim=1)

        x_ts = x_ts.reshape(BA, self.time_stamp, L, D)

        mask = torch.ones([BA, self.time_stamp, L], device=x_ts.device)
        x_masked = []
        # keep the subset for each timestamp
        for ti in range(self.time_stamp):
            ids_keep = ids_shuffle[:, len_keep_each*ti : min(len_keep_each*(ti+1), L)]
            x_ts_masked = torch.gather(x_ts[:,ti,:,:], dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))
            if ids_keep.size(1) < len_keep_each:
                # padding
                x_padded = torch.zeros(ids_keep.size(0), len_keep_each - ids_keep.size(1), D, device=x_ts.device)
                x_ts_masked = torch.cat((x_ts_masked, x_padded), dim=1)
            x_masked.append(x_ts_masked)
            mask[:, ti, len_keep_each*ti : min(len_keep_each*(ti+1), L)] = 0
            mask[:, ti, :] = torch.gather(mask[:, ti, :], dim=1, index=ids_restore)
        x_masked = torch.stack(x_masked, dim=0).permute(1,0,2,3) #[BA, timestamp, L, D]
        x_masked = x_masked.reshape(BAT, -1, D) # because the seq len is not L after masking

        return x_masked, mask, ids_restore

    def amortized_complement_unmasking(self, x_ts, mask, ids_restore):
        """
        x_ts: [BxAxts, L, D]
        mask: [BxA, ts, L] -> this is real L
        ids_restore: [BxA, L] -> this is real L
        restore, add temporal embedding
        NOTE: subtlety: because of complementary masking, this L can be padded at the end thus >= real L
        """
        x_ts_ = x_ts
        BAT, L, D = x_ts_.shape
        realL = ids_restore.size(1)
        BA = BAT // self.time_stamp
        x_ts_ = x_ts_.reshape(BA, self.time_stamp, L, D)
        if not self.no_temp_emb:
            # print("use temporal embedding")
            x_ts_ = x_ts_ + self.decoder_temp_embed #check dim
        if self.decode_singletemp:
            # print("decode single timestamp")
            x_curr = x_ts_[:,0, :realL, :] # [BA, realL, D]
            mask_tokens = self.mask_token.repeat(x_curr.shape[0], ids_restore.shape[1] + 1 - x_curr.shape[1], 1)
            x_curr = torch.cat([x_curr, mask_tokens], dim=1)  # no cls token
            x_restore = torch.gather(x_curr, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_curr.shape[2]))  # unshuffle
        else:
            x_restore = []
            for ti in range(self.time_stamp):
                x_restore.append(x_ts_[:,ti, :, :]) #[BA, L, D]
            x_restore = torch.cat(x_restore, dim=1)
            x_restore = x_restore[:, :realL, :]
            x_restore = torch.gather(x_restore, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_ts.shape[2]))

        feature_maps = x_restore.reshape(BA, self.patch_h, self.patch_w, D)
        return feature_maps


    def late_fusion(self, pred, trans_matrices, num_agent_tensor, batch_size):
        """ reshape the model's predictions back to 256x256x13, do aggregation on this"""
        device = pred.device
        # indiv_imgs = self.unpatchify(pred) # [B C H W]
        indiv_imgs = pred.type(torch.FloatTensor).to(device)
        ## --- do fusion ---
        size = self.get_feature_maps_size(indiv_imgs)
        # print(size)
        assert indiv_imgs.size(0) % batch_size == 0, (indiv_imgs.size(), batch_size)
        self.num_agent = indiv_imgs.size(0) // batch_size
        feat_list = self.build_feature_list(batch_size, indiv_imgs)
        # [[1,1,256,32,32]x5] NOTE should it be [[B, 1, 256, 32, 32]x5]?
        # print(feat_list)
        local_com_mat = self.build_local_communication_matrix(feat_list)  # [2 5 13 256 256] [batch, agent, channel, height, width]
        local_com_mat_update = self.build_local_communication_matrix(feat_list)  # to avoid the inplace operation

        for b in range(batch_size):
            num_agent = num_agent_tensor[b, 0]
            for i in range(num_agent):
                self.tg_agent = local_com_mat[b, i]
                # print("tg agent shape", self.tg_agent.shape) #[13,256,256] 
                self.neighbor_feat_list = []
                self.neighbor_feat_list.append(self.tg_agent)
                all_warp = trans_matrices[b, i]  # transformation [2 5 5 4 4]
                # print(all_warp.shape)[5,4,4]
                self.build_neighbors_feature_list(b, i, all_warp, num_agent, local_com_mat,
                                                    device, size)

                # feature update
                # torch.save(torch.stack(self.neighbor_feat_list).detach().cpu(), "/mnt/NAS/home/zjx/Masked-Multiagent-Autoencoder/debug/nbf-{}-{}.pt".format(b, i))
                local_com_mat_update[b, i] = self.fusion(mode="union") 
        
        # weighted feature maps is passed to decoder
        fused_images = self.agents_to_batch(local_com_mat_update)
        # print('feat fuse mat size', feat_fuse_mat.size())
        return fused_images

    def ego_late_fusion(self, pred, gt, trans_matrices, num_agent_tensor, batch_size):
        """ aggregation, ego use ground truth input, used specifically in inference time"""
        device = pred.device
        # indiv_imgs = self.unpatchify(pred) # [B C H W]
        indiv_imgs = pred.type(torch.FloatTensor).to(device)
        ## --- do fusion ---
        size = self.get_feature_maps_size(indiv_imgs)
        # print(size)
        assert indiv_imgs.size(0) % batch_size == 0, (indiv_imgs.size(), batch_size)
        self.num_agent = indiv_imgs.size(0) // batch_size
        feat_list = self.build_feature_list(batch_size, indiv_imgs)
        gt_list = self.build_feature_list(batch_size, gt)
        # print(feat_list)
        local_com_mat = self.build_local_communication_matrix(feat_list)  # [2 5 13 256 256] [batch, agent, channel, height, width]
        local_com_mat_update = self.build_local_communication_matrix(feat_list)  # to avoid the inplace operation
        local_com_mat_gt = self.build_local_communication_matrix(gt_list)

        for b in range(batch_size):
            num_agent = num_agent_tensor[b, 0]
            for i in range(num_agent):
                # use gt for the ego
                self.tg_agent = local_com_mat_gt[b, i]
                # print("tg agent shape", self.tg_agent.shape) #[13,256,256] 
                self.neighbor_feat_list = []
                self.neighbor_feat_list.append(self.tg_agent)
                all_warp = trans_matrices[b, i]  # transformation [2 5 5 4 4]
                # print(all_warp.shape)[5,4,4]
                self.build_neighbors_feature_list(b, i, all_warp, num_agent, local_com_mat,
                                                    device, size)

                # feature update
                # torch.save(torch.stack(self.neighbor_feat_list).detach().cpu(), "/mnt/NAS/home/zjx/Masked-Multiagent-Autoencoder/debug/nbf-{}-{}.pt".format(b, i))
                local_com_mat_update[b, i] = self.fusion(mode="union") 
        
        # weighted feature maps is passed to decoder
        fused_images = self.agents_to_batch(local_com_mat_update)
        # print('feat fuse mat size', feat_fuse_mat.size())
        return fused_images

    def forward_encoder_all(self, x1, x_next, mask_ratio):
        """
        x1: [bxa, C, H, W]
        x_next: [bxa, ts-1, C, H, W] beq_next_frames
        """
        # cat x1 and x_next to encoder independently
        BA, C, H, W = x1.size()
        x1 = x1.unsqueeze(1)
        if self.time_stamp>1:
            x_ind = torch.cat((x1, x_next), dim=1) # [bxa, ts, C, H, W]
        else:
            x_ind = x1
        # print(x_ind.size())
        assert x_ind.size(1) == self.time_stamp
        x_ind = x_ind.reshape(BA*self.time_stamp, C, H, W)
        # embed patches
        x_ind = self.patch_embed(x_ind)
        x_ind = x_ind + self.pos_embed[:, 1:, :]

        # apply Transformer blocks
        for blk in self.blocks:
            x_ind = blk(x_ind)
        x = self.norm(x_ind)
        # compress for communication
        # TODO: mask ONLY for transmission, encode the complete sequence
        x_masked, mask, ids_restore = self.masking_handle(x, mask_ratio)
        x = self.compressor(x_masked)

        return x, mask, ids_restore

    def forward_encoder_partial(self, x1, x_next, mask_ratio):
        """
        x1: [bxa, C, H, W]
        x_next: [bxa, ts-1, C, H, W] beq_next_frames
        """
        # cat x1 and x_next to encoder independently
        BA, C, H, W = x1.size()
        x1 = x1.unsqueeze(1)
        if self.time_stamp>1:
            x_ind = torch.cat((x1, x_next), dim=1) # [bxa, ts, C, H, W]
        else:
            x_ind = x1
        # print(x_ind.size())
        assert x_ind.size(1) == self.time_stamp
        x_ind = x_ind.reshape(BA*self.time_stamp, C, H, W)
        # embed patches
        x_ind = self.patch_embed(x_ind)
        x_ind = x_ind + self.pos_embed[:, 1:, :]

        # mask before transformer encoding
        # amortized masking, complement and random
        x_masked, mask, ids_restore = self.masking_handle(x_ind, mask_ratio)
        # print(x_masked.size())
        # print(mask.size())
        for blk in self.blocks:
            x_masked = blk(x_masked)
        x = self.norm(x_masked)
        x = self.compressor(x)

        # # -------- encoder then mask ----------
        # # apply Transformer blocks
        # for blk in self.blocks:
        #     x_ind = blk(x_ind)
        # x = self.norm(x_ind)
        # # compress for communication
        # # mask ONLY for transmission, encode the complete sequence
        # x_masked, mask, ids_restore = self.masking_handle(x, mask_ratio)
        # x = self.compressor(x_masked)
        # # --------------------------------------

        return x, mask, ids_restore

    def forward_decoder(self, latent, mask, ids_restore):
        """
        overwrite the original forward_decoder, now input latent are fused
        """
        # print("to decoder, latent", latent.size())
        latent = self.decompressor(latent)
        # # embed tokens
        latent = self.decoder_embed(latent)

        restored_latent = self.unmasking_handle(latent, mask, ids_restore)
        restored_latent = restored_latent.reshape(restored_latent.size(0), self.patch_h*self.patch_w, -1)
        # x = torch.cat([latent_cls, restored_latent], dim=1)

        # add pos embed
        x = restored_latent + self.decoder_pos_embed[:, 1:, :]

        # apply Transformer blocks
        for blk in self.decoder_blocks:
            x = blk(x)
        x = self.decoder_norm(x)

        # VERSION mlp -----
        x_occ = self.decoder_pred_occ(x) 
        x_free = self.decoder_pred_free(x)
        x_occ = self.unpatchify(x_occ)
        x_free = self.unpatchify(x_free)
        # remove cls token
        # x = x[:, 1:, :]
        # print("pred size", x.size())
        # --------------------
        # x_pred = x_occ
        # print("x_pred", x_pred.size())
        x_pred = torch.stack((x_free, x_occ), dim=1) # [B, class, C, H, W]
        return x_pred

    def forward_loss(self, teacher, pred):
        """
        overwrite the original forward_loss, now calculate loss on the entire image

        """
        # print("teacher", teacher.size())
        # target = self.patchify(teacher)
        target = teacher
        # print("target", target.size())
        if self.norm_pix_loss:
            mean = target.mean(dim=-1, keepdim=True)
            var = target.var(dim=-1, keepdim=True)
            target = (target - mean) / (var + 1.e-6)**.5

        loss = (pred - target) ** 2 ## L2 loss
        loss = loss.sum(dim=-1) # [N, L], sum loss per patch
        loss = loss.mean()
        return loss

    def forward_bce_loss(self, target, pred):
        target = target.type(torch.LongTensor).to(pred.device)
        loss = self.cls_loss(pred, target)
        return loss

    def forward_focal_loss(self, target, pred):
        return self.focal_loss(pred, target)

    def forward(self, imgs1, imgs_next, teacher, trans_matrices, num_agent_tensor, batch_size, mask_ratio=0.75):
        """
        Encoder encodes each timestamp alone
        Decoder fuses multi timestamp together
        """
        p = self.patch_embed.patch_size[0]
        self.patch_h = self.patch_w = imgs1.shape[2]//p
        # print("image, patch h, w", imgs1.shape, self.patch_h, self.patch_w)
        latent, mask, ids_restore = self.forward_encoder(imgs1, imgs_next, mask_ratio)
        # latent: [BAT, L, D]
        # print("to vq, latent", latent.size())
        vq_loss, quantized, perplexity, encodings = self._vq_star(latent)
        pred = self.forward_decoder(quantized, mask, ids_restore)
        # recon_loss = self.forward_loss(imgs1, pred)
        recon_loss = self.forward_bce_loss(imgs1, pred)
        # recon_loss = self.forward_focal_loss(imgs1, pred)
        # result = self.unpatchify(pred)
        ind_result = torch.argmax(torch.softmax(pred, dim=1), dim=1)
        # ind_result = pred
        result = self.late_fusion(ind_result, trans_matrices, num_agent_tensor, batch_size)
        # print(result.size())
        # ind_result = self.unpatchify(pred)
        loss = vq_loss + recon_loss
        return loss, result, mask, ind_result, perplexity

    def inference(self, imgs1, imgs_next, teacher, trans_matrices, num_agent_tensor, batch_size, mask_ratio=0.75):
        """
        Encoder encodes each timestamp alone
        Decoder fuses multi timestamp together
        """
        p = self.patch_embed.patch_size[0]
        self.patch_h = self.patch_w = imgs1.shape[2]//p
        # print("image, patch h, w", imgs1.shape, self.patch_h, self.patch_w)
        latent, mask, ids_restore = self.forward_encoder(imgs1, imgs_next, mask_ratio)
        # latent: [BAT, L, D]
        vq_loss, quantized, perplexity, encodings = self._vq_star(latent)
        # torch.save(quantized.detach().cpu(), "debug-epc15-id0-quantized.pt")
        pred = self.forward_decoder(quantized, mask, ids_restore)
        # recon_loss = self.forward_loss(imgs1, pred)
        recon_loss = self.forward_bce_loss(imgs1, pred)
        # recon_loss = self.forward_focal_loss(imgs1, pred)
        # print(imgs1.size(), imgs1.type())
        ind_result = torch.argmax(torch.softmax(pred, dim=1), dim=1)
        # ind_result = pred
        result = self.ego_late_fusion(ind_result, imgs1, trans_matrices, num_agent_tensor, batch_size)
        # result = self.ego_late_fusion(imgs1, imgs1, trans_matrices, num_agent_tensor, batch_size)
        # result = self.late_fusion(ind_result, trans_matrices, num_agent_tensor, batch_size)
        # ind_result = self.unpatchify(pred)
        loss = recon_loss + vq_loss
        return loss, result, ind_result, perplexity, encodings


def vq_amo_individual_star_patch8_dec256d4b(**kwargs):
    model = VQSTARViT(
        img_size=256, patch_size=8, in_chans=13, embed_dim=384, depth=6, num_heads=12,
        decoder_embed_dim=256, decoder_depth=4, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="mlp", **kwargs)
    return model

vqstar = vq_amo_individual_star_patch8_dec256d4b


import torch
import torch.nn.functional as F
import torch.nn as nn

class Backbone(nn.Module):
    """The backbone class that contains encode and decode function"""

    def __init__(self, height_feat_size, compress_level=0, train_completion=False):
        super().__init__()
        self.conv_pre_1 = nn.Conv2d(
            height_feat_size, 32, kernel_size=3, stride=1, padding=1
        )
        self.conv_pre_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)
        self.bn_pre_1 = nn.BatchNorm2d(32)
        self.bn_pre_2 = nn.BatchNorm2d(32)

        self.conv3d_1 = Conv3D(
            64, 64, kernel_size=(1, 1, 1), stride=1, padding=(0, 0, 0)
        )
        self.conv3d_2 = Conv3D(
            128, 128, kernel_size=(1, 1, 1), stride=1, padding=(0, 0, 0)
        )

        self.conv1_1 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)
        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)

        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)
        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)

        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)
        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)

        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)
        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)

        self.conv5_1 = nn.Conv2d(512 + 256, 256, kernel_size=3, stride=1, padding=1)
        self.conv5_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)

        self.conv6_1 = nn.Conv2d(256 + 128, 128, kernel_size=3, stride=1, padding=1)
        self.conv6_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)

        self.conv7_1 = nn.Conv2d(128 + 64, 64, kernel_size=3, stride=1, padding=1)
        self.conv7_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)

        self.conv8_1 = nn.Conv2d(64 + 32, 32, kernel_size=3, stride=1, padding=1)
        self.conv8_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)

        self.bn1_1 = nn.BatchNorm2d(64)
        self.bn1_2 = nn.BatchNorm2d(64)

        self.bn2_1 = nn.BatchNorm2d(128)
        self.bn2_2 = nn.BatchNorm2d(128)

        self.bn3_1 = nn.BatchNorm2d(256)
        self.bn3_2 = nn.BatchNorm2d(256)

        self.bn4_1 = nn.BatchNorm2d(512)
        self.bn4_2 = nn.BatchNorm2d(512)

        self.bn5_1 = nn.BatchNorm2d(256)
        self.bn5_2 = nn.BatchNorm2d(256)

        self.bn6_1 = nn.BatchNorm2d(128)
        self.bn6_2 = nn.BatchNorm2d(128)

        self.bn7_1 = nn.BatchNorm2d(64)
        self.bn7_2 = nn.BatchNorm2d(64)

        self.bn8_1 = nn.BatchNorm2d(32)
        self.bn8_2 = nn.BatchNorm2d(32)

        self.train_completion = train_completion
        if train_completion:
            self.conv9_1 = nn.Conv2d(32, 13, kernel_size=1, stride=1, padding=0)
            self.bn9_1 = nn.BatchNorm2d(13)

            self.conv9_2 = nn.Conv2d(13, 13, kernel_size=1, stride=1, padding=0)
            self.bn9_2 = nn.BatchNorm2d(13)

            self.conv9_3 = nn.Conv2d(32, 13, kernel_size=1, stride=1, padding=0)
            self.bn9_3 = nn.BatchNorm2d(13)

            self.conv9_4 = nn.Conv2d(13, 13, kernel_size=1, stride=1, padding=0)
            self.bn9_4 = nn.BatchNorm2d(13)

        self.compress_level = compress_level
        if compress_level > 0:
            assert compress_level <= 8
            compress_channel_num = 256 // (2**compress_level)

            # currently only support compress/decompress at layer x_3
            self.com_compresser = nn.Conv2d(
                256, compress_channel_num, kernel_size=1, stride=1
            )
            self.bn_compress = nn.BatchNorm2d(compress_channel_num)

            self.com_decompresser = nn.Conv2d(
                compress_channel_num, 256, kernel_size=1, stride=1
            )
            self.bn_decompress = nn.BatchNorm2d(256)

    def encode(self, x):
        """Encode the input BEV features.

        Args:
            x (tensor): the input BEV features.

        Returns:
            A list that contains all the encoded layers.
        """
        batch, seq, z, h, w = x.size()

        x = x.view(-1, x.size(-3), x.size(-2), x.size(-1))
        x = x.to(torch.float)
        x = F.relu(self.bn_pre_1(self.conv_pre_1(x)))
        x = F.relu(self.bn_pre_2(self.conv_pre_2(x)))

        # -------------------------------- Encoder Path --------------------------------
        # -- STC block 1
        x_1 = F.relu(self.bn1_1(self.conv1_1(x)))
        x_1 = F.relu(self.bn1_2(self.conv1_2(x_1)))

        x_1 = x_1.view(
            batch, -1, x_1.size(1), x_1.size(2), x_1.size(3)
        ).contiguous()  # (batch, seq, c, h, w)
        x_1 = self.conv3d_1(x_1)
        x_1 = x_1.view(
            -1, x_1.size(2), x_1.size(3), x_1.size(4)
        ).contiguous()  # (batch * seq, c, h, w)

        # -- STC block 2
        x_2 = F.relu(self.bn2_1(self.conv2_1(x_1)))
        x_2 = F.relu(self.bn2_2(self.conv2_2(x_2)))

        x_2 = x_2.view(
            batch, -1, x_2.size(1), x_2.size(2), x_2.size(3)
        ).contiguous()  # (batch, seq, c, h, w)
        x_2 = self.conv3d_2(x_2)
        x_2 = x_2.view(
            -1, x_2.size(2), x_2.size(3), x_2.size(4)
        ).contiguous()  # (batch * seq, c, h, w), seq = 1

        # -- STC block 3
        x_3 = F.relu(self.bn3_1(self.conv3_1(x_2)))
        x_3 = F.relu(self.bn3_2(self.conv3_2(x_3)))

        # -- STC block 4
        x_4 = F.relu(self.bn4_1(self.conv4_1(x_3)))
        x_4 = F.relu(self.bn4_2(self.conv4_2(x_4)))

        # compress x_3 (the layer that agents communicates on)
        if self.compress_level > 0:
            x_3 = F.relu(self.bn_compress(self.com_compresser(x_3)))
            x_3 = F.relu(self.bn_decompress(self.com_decompresser(x_3)))

        return [x, x_1, x_2, x_3, x_4]

    def decode(
        self,
        x,
        x_1,
        x_2,
        x_3,
        x_4,
        batch,
        kd_flag=False,
        requires_adaptive_max_pool3d=False,
    ):
        """Decode the input features.

        Args:
            x (tensor): layer-0 features.
            x_1 (tensor): layer-1 features.
            x_2 (tensor): layer-2 features.
            x_3 (tensor): layer-3 features.
            x_4 (tensor): layer-4 featuers.
            batch (int): The batch size.
            kd_flag (bool, optional): Required to be true for DiscoNet. Defaults to False.
            requires_adaptive_max_pool3d (bool, optional): If set to true, use adaptive max pooling 3d. Defaults to False.

        Returns:
            if kd_flag is true, return a list of output from layer-8 to layer-5
            else return a list of a single element: the output after passing through the decoder
        """
        # -------------------------------- Decoder Path --------------------------------
        x_5 = F.relu(
            self.bn5_1(
                self.conv5_1(
                    torch.cat((F.interpolate(x_4, scale_factor=(2, 2)), x_3), dim=1)
                )
            )
        )
        x_5 = F.relu(self.bn5_2(self.conv5_2(x_5)))

        x_2 = x_2.view(batch, -1, x_2.size(1), x_2.size(2), x_2.size(3))
        x_2 = x_2.permute(0, 2, 1, 3, 4).contiguous()
        x_2 = (
            F.adaptive_max_pool3d(x_2, (1, None, None))
            if requires_adaptive_max_pool3d
            else x_2
        )
        x_2 = x_2.permute(0, 2, 1, 3, 4).contiguous()
        x_2 = x_2.view(-1, x_2.size(2), x_2.size(3), x_2.size(4)).contiguous()

        x_6 = F.relu(
            self.bn6_1(
                self.conv6_1(
                    torch.cat((F.interpolate(x_5, scale_factor=(2, 2)), x_2), dim=1)
                )
            )
        )
        x_6 = F.relu(self.bn6_2(self.conv6_2(x_6)))

        x_1 = x_1.view(batch, -1, x_1.size(1), x_1.size(2), x_1.size(3))
        x_1 = x_1.permute(0, 2, 1, 3, 4).contiguous()
        x_1 = (
            F.adaptive_max_pool3d(x_1, (1, None, None))
            if requires_adaptive_max_pool3d
            else x_1
        )
        x_1 = x_1.permute(0, 2, 1, 3, 4).contiguous()
        x_1 = x_1.view(-1, x_1.size(2), x_1.size(3), x_1.size(4)).contiguous()

        x_7 = F.relu(
            self.bn7_1(
                self.conv7_1(
                    torch.cat((F.interpolate(x_6, scale_factor=(2, 2)), x_1), dim=1)
                )
            )
        )
        x_7 = F.relu(self.bn7_2(self.conv7_2(x_7)))

        x = x.view(batch, -1, x.size(1), x.size(2), x.size(3))
        x = x.permute(0, 2, 1, 3, 4).contiguous()
        x = (
            F.adaptive_max_pool3d(x, (1, None, None))
            if requires_adaptive_max_pool3d
            else x
        )
        x = x.permute(0, 2, 1, 3, 4).contiguous()
        x = x.view(-1, x.size(2), x.size(3), x.size(4)).contiguous()

        x_8 = F.relu(
            self.bn8_1(
                self.conv8_1(
                    torch.cat((F.interpolate(x_7, scale_factor=(2, 2)), x), dim=1)
                )
            )
        )
        res_x = F.relu(self.bn8_2(self.conv8_2(x_8)))

        if self.train_completion:
            occ_x = F.relu(self.bn9_1(self.conv9_1(res_x)))
            occ_x = F.relu(self.bn9_2(self.conv9_2(occ_x)))
            # get binary classification logits
            free_x = F.relu(self.bn9_3(self.conv9_3(res_x)))
            free_x = F.relu(self.bn9_4(self.conv9_4(free_x)))

            res_x = torch.stack((free_x, occ_x), dim=1)

        if kd_flag:
            return [res_x, x_7, x_6, x_5]
        else:
            return [res_x]


class STPN_KD(Backbone):
    """Used by non-intermediate models. Pass the output from encoder directly to decoder."""

    def __init__(self, height_feat_size=13, compress_level=0, train_completion=False):
        super().__init__(height_feat_size, compress_level, train_completion)

    def forward(self, x):
        batch, seq, z, h, w = x.size()
        encoded_layers = super().encode(x)
        decoded_layers = super().decode(
            *encoded_layers, batch, kd_flag=True, requires_adaptive_max_pool3d=True
        )
        return (*decoded_layers, encoded_layers[3], encoded_layers[4])



class CNNNet(nn.Module):
    """
    Modified from:
    https://arxiv.org/pdf/2012.12395.pdf

    Args:
        config (object): The Config object.
        layer (int, optional): Collaborate on which layer. Defaults to 3.
        in_channels (int, optional): The input channels. Defaults to 13.
        kd_flag (bool, optional): Whether to use knowledge distillation (for DiscoNet to ues). Defaults to True.
        num_agent (int, optional): The number of agents (including RSU). Defaults to 5.
    """

    def __init__(
        self,
        config,
        layer=3,
        in_channels=13,
        kd_flag=True,
        num_agent=5,
        compress_level=0,
        train_completion=True,
    ):
        super().__init__()
        self.config = config
        self.kd_flag = kd_flag
        self.in_channels = in_channels
        self.num_agent = num_agent
        self.train_completion = train_completion
        self.stpn = STPN_KD(config.map_dims[2], compress_level, train_completion)

    def get_feature_maps_size(self, feature_maps: tuple):
        size = list(feature_maps.shape)
        # NOTE: batch size will change the shape[0]. We need to manually set it to 1.
        size[0] = 1
        size = tuple(size)
        return size

    # get feat maps for each agent [10 512 32 32] -> [2 5 512 32 32]
    def build_feature_list(self, batch_size: int, feat_maps: dict) -> list:
        feature_map = {}
        # [5,256,32,32]
        feature_list = []

        for i in range(self.num_agent):
            feature_map[i] = torch.unsqueeze(feat_maps[batch_size * i:batch_size * (i + 1)], 1)
            # feature_map[i]: [B,1,256,32,32]
            feature_list.append(feature_map[i])

        return feature_list

    # [2 5 512 16 16] [batch, agent, channel, height, width]
    @staticmethod
    def build_local_communication_matrix(feature_list: list):
        return torch.cat(tuple(feature_list), 1)

    @staticmethod
    # FIXME: rename 'j'
    def feature_transformation(b, nb_agent_idx, local_com_mat, all_warp, device, size):
        nb_agent = torch.unsqueeze(local_com_mat[b, nb_agent_idx], 0)  # [1 512 16 16]
        nb_warp = all_warp[nb_agent_idx]  # [4 4]
        # normalize the translation vector
        x_trans = (4 * nb_warp[0, 3]) / 128
        y_trans = -(4 * nb_warp[1, 3]) / 128

        theta_rot = torch.tensor(
            [[nb_warp[0, 0], nb_warp[0, 1], 0.0], [nb_warp[1, 0], nb_warp[1, 1], 0.0]]).type(
            dtype=torch.float).to(device)
        theta_rot = torch.unsqueeze(theta_rot, 0)
        grid_rot = F.affine_grid(theta_rot, size=torch.Size(size))  # get grid for grid sample

        theta_trans = torch.tensor([[1.0, 0.0, x_trans], [0.0, 1.0, y_trans]]).type(dtype=torch.float).to(
            device)
        theta_trans = torch.unsqueeze(theta_trans, 0)
        grid_trans = F.affine_grid(theta_trans, size=torch.Size(size))  # get grid for grid sample

        # first rotate the feature map, then translate it
        warp_feat_rot = F.grid_sample(nb_agent, grid_rot, mode='nearest')
        warp_feat_trans = F.grid_sample(warp_feat_rot, grid_trans, mode='nearest')
        return torch.squeeze(warp_feat_trans, dim=0) # [512, 16, 16]


    def build_neighbors_feature_list(self, b, agent_idx, all_warp, num_agent, local_com_mat, device,
                                     size) -> None:
        for j in range(num_agent):
            if j != agent_idx:
                warp_feat = self.feature_transformation(b, j, local_com_mat, all_warp, device, size)
                self.neighbor_feat_list.append(warp_feat)
        
    def fusion(self, mode="sum"):
        fused = torch.sum(torch.stack(self.neighbor_feat_list), dim=0)
        if mode == "union":
            fused[fused>0.] = 1.0 
        return fused 

    @staticmethod
    def agents_to_batch(feats):
        agent_num = feats.shape[1]
        feat_list = []
        for i in range(agent_num):
            feat_list.append(feats[:, i, :, :, :])
        feat_mat = torch.cat(tuple(feat_list), 0)
        return feat_mat

    def ego_late_fusion(self, pred, gt, trans_matrices, num_agent_tensor, batch_size):
        """ aggregation, ego use ground truth input, used specifically in inference time"""
        device = pred.device
        # indiv_imgs = self.unpatchify(pred) # [B C H W]
        indiv_imgs = pred.type(torch.FloatTensor).to(device)
        ## --- do fusion ---
        size = self.get_feature_maps_size(indiv_imgs)
        # print(size)
        assert indiv_imgs.size(0) % batch_size == 0, (indiv_imgs.size(), batch_size)
        self.num_agent = indiv_imgs.size(0) // batch_size
        feat_list = self.build_feature_list(batch_size, indiv_imgs)
        gt_list = self.build_feature_list(batch_size, gt)
        # print(feat_list)
        local_com_mat = self.build_local_communication_matrix(feat_list)  # [2 5 13 256 256] [batch, agent, channel, height, width]
        local_com_mat_update = self.build_local_communication_matrix(feat_list)  # to avoid the inplace operation
        local_com_mat_gt = self.build_local_communication_matrix(gt_list)

        for b in range(batch_size):
            num_agent = num_agent_tensor[b, 0]
            for i in range(num_agent):
                # use gt for the ego
                self.tg_agent = local_com_mat_gt[b, i]
                # print("tg agent shape", self.tg_agent.shape) #[13,256,256] 
                self.neighbor_feat_list = []
                self.neighbor_feat_list.append(self.tg_agent)
                all_warp = trans_matrices[b, i]  # transformation [2 5 5 4 4]
                # print(all_warp.shape)[5,4,4]
                self.build_neighbors_feature_list(b, i, all_warp, num_agent, local_com_mat,
                                                    device, size)

                # feature update
                # torch.save(torch.stack(self.neighbor_feat_list).detach().cpu(), "/mnt/NAS/home/zjx/Masked-Multiagent-Autoencoder/debug/nbf-{}-{}.pt".format(b, i))
                local_com_mat_update[b, i] = self.fusion(mode="union") 
        
        # weighted feature maps is passed to decoder
        fused_images = self.agents_to_batch(local_com_mat_update)
        # print('feat fuse mat size', feat_fuse_mat.size())
        return fused_images

    def forward(self, bevs, trans_matrices=None, num_agent_tensor=None, batch_size=None):
        bevs = bevs.permute(0, 1, 4, 2, 3)  # (Batch, seq, z, h, w)

        x_8, x_7, x_6, x_5, x_3, x_2 = self.stpn(bevs)
        x = x_8

        # cls_preds, loc_preds, result = super().get_cls_loc_result(x)
        
        if self.train_completion:
            # fuse reconstructed BEVs
            ind_result = torch.argmax(torch.softmax(x, dim=1), dim=1)
            result = self.ego_late_fusion(ind_result, bevs.squeeze(1), trans_matrices, num_agent_tensor, batch_size)
        else:
            # do detection
            cls_preds, loc_preds, result = super().get_cls_loc_result(x)


        if self.kd_flag == 1:
            return result, x_8, x_7, x_6, x_5, x_3
        elif self.train_completion:
            return result, x_8
        else:
            return result


class Conv3D(nn.Module):
    """3D cnn used in the encoder."""

    def __init__(self, in_channel, out_channel, kernel_size, stride, padding):
        super(Conv3D, self).__init__()
        self.conv3d = nn.Conv3d(
            in_channel,
            out_channel,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
        )
        self.bn3d = nn.BatchNorm3d(out_channel)

    def forward(self, x):
        # input x: (batch, seq, c, h, w)
        x = x.permute(0, 2, 1, 3, 4).contiguous()  # (batch, c, seq_len, h, w)
        x = F.relu(self.bn3d(self.conv3d(x)))
        x = x.permute(0, 2, 1, 3, 4).contiguous()  # (batch, seq_len, c, h, w)

        return x

from . import *
from .VQVAE import VQVAENet
from .CNNNet import CNNNet

#mae base code

from functools import partial

import torch
import torch.nn as nn
import torch.nn.functional as F

from timm.models.vision_transformer import PatchEmbed, Block

# from coperception.utils.maeutil.pos_embed import get_2d_sincos_pos_embed
from ..utils.pos_embed import get_2d_sincos_pos_embed

from scipy.cluster.vq import kmeans2

class ConvPred(nn.Module):
    """
    a upsampling and deconve module in repalce of the original final linear projection,
    to obtain the prediction of the original image.
    for patch size 8x8, feature map size 32x32x512
    """
    def __init__(self, input_size=32, output_size=256, input_chans=512, output_chans=13):
        super().__init__()
        # 64x64x512 -> 64x64x256
        chans1 = input_chans//2
        self.conv1 = nn.Conv2d(input_chans, chans1, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(chans1)
        # 128x128x256 -> 128x128x128
        chans2 = chans1//2
        self.conv2 = nn.Conv2d(chans1, chans2, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(chans2)
        # 256x256x128 -> 2256x256x13
        self.conv3 = nn.Conv2d(chans2, output_chans, kernel_size=3, stride=1, padding=1)
        # self.conv3 = nn.Conv2d(chans2, output_chans, kernel_size=1, stride=1, padding=0)
        self.bn3 = nn.BatchNorm2d(output_chans)

        # self.cls_head = SegmentationHead(inplanes=1, planes=8, nbr_classes=2, dilations_conv_list=[1, 2, 3])

    def forward(self, x):
        # ---- patch size 8x8 --- will have 32x32x512 ----
        # upsampling 32x32x512 -> 64x64x512
        x = F.interpolate(x, scale_factor=2, mode='bilinear') #, align_corners=True)
        # channel compression 512 -> 256
        x = F.relu(self.bn1(self.conv1(x)))
        # upsamping 64x64x256 -> 128x128x256
        x = F.interpolate(x, scale_factor=2, mode='bilinear') #, align_corners=True)
        # channel compression 256 -> 128
        x = F.relu(self.bn2(self.conv2(x)))
        # upsamping 128x128x128 -> 256x256x128
        x = F.interpolate(x, scale_factor=2, mode='bilinear') #, align_corners=True)
        # channel compression 128 -> 13
        x = self.bn3(self.conv3(x))
        # x = torch.sigmoid(self.bn3(self.conv3(x)))
        # x = self.cls_head(x) #[B, 2, C, H, W]
        return x

class ConvPred16(nn.Module):
    """
    a upsampling and deconve module in repalce of the original final linear projection,
    to obtain the prediction of the original image.
    for patch size 16x16, feature map size 16x16x512
    """
    def __init__(self, input_size=16, output_size=256, input_chans=512, output_chans=13):
        super().__init__()
        # 32x32x512 -> 32x32x256
        chans1 = input_chans//2
        self.conv1 = nn.Conv2d(input_chans, chans1, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(chans1)
        # 64x64x256 -> 64x64x128
        chans2 = chans1//2
        self.conv2 = nn.Conv2d(chans1, chans2, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(chans2)
        # 128x128x128 -> 128x128x64
        chans3 = chans2//2
        self.conv3 = nn.Conv2d(chans2, chans3, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(chans3)
        # 256x256x64 -> 256x256x13
        self.conv4 = nn.Conv2d(chans3, output_chans, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(output_chans)

    def forward(self, x):
        # ---- 16x16x512 ------
        # upsampling 16x16x512 -> 32x32x512
        x = F.interpolate(x, scale_factor=2, mode='bilinear') #, align_corners=True)
        # print("up 1", x.size())
        # channel compression 512 -> 256
        x = F.relu(self.bn1(self.conv1(x)))
        # upsamping 32x32x256 -> 64x64x256
        x = F.interpolate(x, scale_factor=2, mode='bilinear') #, align_corners=True)
        # print("up 2", x.size())
        # channel compression 256->128
        x = F.relu(self.bn2(self.conv2(x)))
        # upsamping 64x64x128 -> 128x128x128
        x = F.interpolate(x, scale_factor=2, mode='bilinear') #, align_corners=True)
        # print("up 3", x.size())
        # channel compression 128->64
        x = F.relu(self.bn3(self.conv3(x)))
        # upsamping 128x128x64 -> 256x256x64
        x = F.interpolate(x, scale_factor=2, mode='bilinear') #, align_corners=True)
        # print("up 4", x.size())
        # channel compression 64->13
        # x = F.relu(self.bn4(self.conv4(x)))
        x = self.bn4(self.conv4(x))
        # ----------------------------
        return x

class STARVectorQuantizer(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, commitment_cost):
        super(STARVectorQuantizer, self).__init__()
        
        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings
        
        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)
        # self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)
        # pre_kmeans = torch.load("kmeans-centers-8192.pt")
        # print("initializing the vq embedding with pre trained kmeans cluster", pre_kmeans.size())
        # self._embedding.weight.data.copy_(pre_kmeans)
        self._commitment_cost = commitment_cost
        self.data_initialized = 1

    def forward(self, inputs):
        # convert inputs from BCHW -> BHWC
        # inputs = inputs.permute(0, 2, 3, 1).contiguous()
        # print("vq inputs", inputs.size())
        input_shape = inputs.shape
        # Flatten input
        flat_input = inputs.view(-1, self._embedding_dim)

        # # L2 normalization before calculating the distances:
        # normed_flat_input = flat_input / torch.linalg.norm(flat_input, dim=1, keepdim=True)
        # normed_emb = self._embedding.weight / torch.linalg.norm(self._embedding.weight, dim=1, keepdim=True)

        # Following Andrej Karpathy's attempt to fix index collapse:
        # https://github.com/karpathy/deep-vector-quantization/blob/main/dvq/model/quantize.py
        if self.training and self.data_initialized ==0:
            print("run kmeans")
            rp = torch.randperm(flat_input.size(0))
            kd = kmeans2(flat_input[rp].data.cpu().numpy(), self._num_embeddings, minit="points")
            self._embedding.weight.data.copy_(torch.from_numpy(kd[0]))
            self.data_initialized = 1
        
        # Calculate distances
        # print(flat_input.size(), self._embedding.weight.size())
        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) 
                     + torch.sum(self._embedding.weight**2, dim=1, keepdim=True).t()
                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))
        # distances = (torch.sum(normed_flat_input**2, dim=1, keepdim=True) 
        #             + torch.sum(normed_emb**2, dim=1)
        #             - 2 * torch.matmul(normed_flat_input, normed_emb.t()))
            
        # Encoding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)
        # print("encoding_indices", encoding_indices)
        # print("encoding_indices", encoding_indices.size())
        # Quantize and unflatten
        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)
        # print("quantized shape", quantized.size())
        # Loss
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        q_latent_loss = F.mse_loss(quantized, inputs.detach())
        loss = q_latent_loss + self._commitment_cost * e_latent_loss
        
        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        # print(avg_probs)
        # print("avg_probs", avg_probs.size())
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))
        
        # convert quantized from BHWC -> BCHW
        return loss, quantized, perplexity, encodings


class STARVectorQuantizerEMA(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):
        super(STARVectorQuantizerEMA, self).__init__()
        
        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings
        
        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)
        self._embedding.weight.data.normal_()
        self._commitment_cost = commitment_cost
        
        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))
        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))
        # pre_kmeans = torch.load("kmeans-centers-8192.pt")
        # print("initializing the vq embedding with pre trained kmeans cluster", pre_kmeans.size())
        # self._ema_w.data.copy_(pre_kmeans)
        # self._ema_w.data.normal_()
        self.data_initialized = 0
        self._decay = decay
        self._epsilon = epsilon

    def forward(self, inputs):
        # convert inputs from BCHW -> BHWC
        # inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape
        
        # Flatten input
        flat_input = inputs.view(-1, self._embedding_dim)

        # Following Andrej Karpathy's attempt to fix index collapse:
        # https://github.com/karpathy/deep-vector-quantization/blob/main/dvq/model/quantize.py
        if self.training and self.data_initialized ==0:
            print("run kmeans")
            rp = torch.randperm(flat_input.size(0))
            kd = kmeans2(flat_input[rp].data.cpu().numpy(), self._num_embeddings, minit="points")
            self._ema_w.data.copy_(torch.from_numpy(kd[0]))
            self.data_initialized = 1
        
        # Calculate distances
        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) 
                    + torch.sum(self._embedding.weight**2, dim=1)
                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))
            
        # Encoding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)
        
        # Quantize and unflatten
        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)
        
        # Use EMA to update the embedding vectors
        if self.training:
            self._ema_cluster_size = self._ema_cluster_size * self._decay + \
                                     (1 - self._decay) * torch.sum(encodings, 0)
            
            # Laplace smoothing of the cluster size
            n = torch.sum(self._ema_cluster_size.data)
            self._ema_cluster_size = (
                (self._ema_cluster_size + self._epsilon)
                / (n + self._num_embeddings * self._epsilon) * n)
            
            dw = torch.matmul(encodings.t(), flat_input)
            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)
            
            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))
        
        # Loss
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        loss = self._commitment_cost * e_latent_loss
        
        # Straight Through Estimator
        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))
        
        # convert quantized from BHWC -> BCHW
        return loss, quantized, perplexity, encodings



class MultiAgentMaskedAutoencoderViT(nn.Module):
    """ Masked Autoencoder with VisionTransformer backbone
    """
    def __init__(self, img_size=224, patch_size=16, in_chans=3,
                 embed_dim=1024, depth=24, num_heads=16,
                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
                 mlp_ratio=4., norm_layer=nn.LayerNorm, decoder_head="mlp", norm_pix_loss=False, time_stamp=1, mask_method="random",
                 inter_emb_dim=32):
        super().__init__()

        # --------------------------------------------------------------------------
        # MAE encoder specifics
        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding
        self.time_stamp = time_stamp
        # temporal embeddings
        self.temp_embed = nn.Parameter(torch.zeros(1, self.time_stamp, embed_dim)) # learnable temporal embeddings for 2 time stamp
        self.decoder_temp_embed = nn.Parameter(torch.zeros(1, self.time_stamp, 1, decoder_embed_dim)) # learnable temporal embeddings for 2 time stamp

        self.blocks = nn.ModuleList([
            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)
            for i in range(depth)])
        self.norm = norm_layer(embed_dim)
        
        self.compressor = nn.Sequential(
            nn.Linear(embed_dim, inter_emb_dim),
            nn.ReLU(),
        )
        # --------------------------------------------------------------------------

        # --------------------------------------------------------------------------
        # MAE decoder specifics
        self.decompressor = nn.Linear(inter_emb_dim, embed_dim)
        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)

        # try option1:
        # self.decoder_embed = nn.Linear(inter_emb_dim, decoder_embed_dim, bias=True)
        # try option2:
        # self.decoder_embed = nn.Sequential(
        #     nn.Linear(inter_emb_dim, embed_dim),
        #     nn.ReLU(),
        #     nn.Linear(embed_dim, decoder_embed_dim, bias=True)
        # )

        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))

        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding

        self.decoder_blocks = nn.ModuleList([
            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)
            for i in range(decoder_depth)])

        self.decoder_norm = norm_layer(decoder_embed_dim)

        if decoder_head == "mlp":
            # self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch
            self.decoder_pred_occ = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True)
            self.decoder_pred_free = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True)
        # conv decoder pred
        elif decoder_head == "conv3":
            if patch_size == 8:
                self.decoder_pred_occ = ConvPred(input_size=32, output_size=256, input_chans=512, output_chans=13)
                self.decoder_pred_free = ConvPred(input_size=32, output_size=256, input_chans=512, output_chans=13)
            elif patch_size == 16:
                self.decoder_pred_occ = ConvPred16(input_size=16, output_size=256, input_chans=512, output_chans=13)
                self.decoder_pred_free = ConvPred16(input_size=16, output_size=256, input_chans=512, output_chans=13)
            else:
                raise NotImplementedError("not supported conv head for patch size", patch_size)
        else:
            raise NotImplementedError("decoder head", decoder_head)
        # --------------------------------------------------------------------------

        self.norm_pix_loss = norm_pix_loss
        self.in_chans = in_chans
        self.L1_Loss = nn.L1Loss(reduction="none")
        

        self.initialize_weights()

        # ----- fushion support ------
        self.num_agent = 0
        self.neighbor_feat_list = []
        self.tg_agent = None

    def initialize_weights(self):
        # initialization
        # initialize (and freeze) pos_embed by sin-cos embedding
        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)
        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))

        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)
        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))

        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)
        w = self.patch_embed.proj.weight.data
        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))

        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)
        torch.nn.init.normal_(self.cls_token, std=.02)
        torch.nn.init.normal_(self.mask_token, std=.02)

        # temporal embedding initialized likewise
        torch.nn.init.normal_(self.temp_embed, std=.02)

        # initialize nn.Linear and nn.LayerNorm
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            # we use xavier_uniform following official JAX ViT:
            torch.nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def patchify(self, imgs):
        """
        imgs: (N, chs, H, W)
        x: (N, L, patch_size**2 *chns)
        """
        p = self.patch_embed.patch_size[0]
        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0
        num_chans = imgs.size(1)

        h = w = imgs.shape[2] // p
        x = imgs.reshape(shape=(imgs.shape[0], num_chans, h, p, w, p))
        x = torch.einsum('nchpwq->nhwpqc', x)
        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * num_chans))
        return x

    def unpatchify(self, x):
        """
        x: (N, L, patch_size**2 *chans)
        imgs: (N, chans, H, W)
        """
        p = self.patch_embed.patch_size[0]
        h = w = int(x.shape[1]**.5)
        assert h * w == x.shape[1]
        num_chans = x.size(-1)//(p**2)
        assert p**2 * num_chans == x.size(-1)
        
        x = x.reshape(shape=(x.shape[0], h, w, p, p, num_chans))
        x = torch.einsum('nhwpqc->nchpwq', x)
        imgs = x.reshape(shape=(x.shape[0], num_chans, h * p, h * p))
        return imgs

    def random_masking(self, x, mask_ratio):
        """
        Perform per-sample random masking by per-sample shuffling.
        Per-sample shuffling is done by argsort random noise.
        x: [N, L, D], sequence
        """
        N, L, D = x.shape  # batch, length, dim
        len_keep = int(L * (1 - mask_ratio))
        
        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]
        
        # sort noise for each sample
        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove
        ids_restore = torch.argsort(ids_shuffle, dim=1)

        # keep the first subset
        ids_keep = ids_shuffle[:, :len_keep]
        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))

        # generate the binary mask: 0 is keep, 1 is remove
        mask = torch.ones([N, L], device=x.device)
        mask[:, :len_keep] = 0
        # unshuffle to get the binary mask
        mask = torch.gather(mask, dim=1, index=ids_restore)

        return x_masked, mask, ids_restore

    # x_masked, x1len, mask1, ids_restore1 = self.more_random_masking(x1, xs, mask_ratio)
    def more_random_masking(self, x1, xs, mask_ratio):
        """
        random masking each time stamp's data and cat
        """
        assert isinstance(xs, list)
        xnum = len(xs)+1
        x_masked = []

        x1_masked, mask1, ids_restore1 = self.random_masking(x1, mask_ratio)
        x1len = x1_masked.shape[1]
        for idx in range(xnum-1):
            xt_masked, _, _ = self.random_masking(xs[idx], mask_ratio)
            x_masked.append(xt_masked)

        # x2, mask2, ids_restore2 = self.random_masking(x2, mask_ratio)
        x_masked = torch.cat(x_masked, dim=1)
        return x_masked, x1len, mask1, ids_restore1

    def complement_masking(self, x1, x2, mask_ratio):
        """
        mask x1 with mask_ratio
        mask x2 with 1 - mask ratio
        so that they are complementary
        x1, x2: same size [N, D, L] 
        """
        N, L, D = x1.shape  # batch, length, dim
        len_keep1 = int(L * (1 - mask_ratio))
        len_keep2 = L - len_keep1
        noise = torch.rand(N, L, device=x1.device)  # noise in [0, 1]
        
        # sort noise for each sample
        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove
        ids_restore = torch.argsort(ids_shuffle, dim=1)

        # keep the first subset
        ids_keep1 = ids_shuffle[:, :len_keep1]
        ids_keep2 = ids_shuffle[:, len_keep1:]
        assert ids_keep2.size(1) == len_keep2

        x1_masked = torch.gather(x1, dim=1, index=ids_keep1.unsqueeze(-1).repeat(1, 1, D))
        x2_masked = torch.gather(x2, dim=1, index=ids_keep2.unsqueeze(-1).repeat(1, 1, D))

        # generate the binary mask: 0 is keep, 1 is remove
        mask1 = torch.ones([N, L], device=x1.device)
        mask1[:, :len_keep1] = 0
        mask2 = torch.ones([N, L], device=x2.device)
        mask2[:, len_keep1:] = 0
        # unshuffle to get the binary mask
        mask1 = torch.gather(mask1, dim=1, index=ids_restore)
        mask2 = torch.gather(mask2, dim=1, index=ids_restore)

        return x1_masked, x2_masked, mask1, mask2, ids_restore

    def more_complement_masking(self, x1, xs, mask_ratio):
        """
        masking is conducted complementary
        x1 is the current time stamp image
        xs is a list of images for the other time stamps
        x1: [N, D, L] 
        xs: a list (ts-1) x [N, D, L]
        """
        assert isinstance(xs, list)
        xnum = len(xs)+1
        N, L, D = x1.shape  # batch, length, dim
        device = x1.device
        
        noise = torch.rand(N, L, device=x1.device)  # noise in [0, 1]
        
        # sort noise for each sample
        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove
        ids_restore = torch.argsort(ids_shuffle, dim=1)

        # keep the corresponding subset
        len_keep1 = int(L * (1 - mask_ratio))
        ids_keep1 = ids_shuffle[:, :len_keep1]
        mask1 = torch.ones([N, L], device=device)
        mask1[:, :len_keep1] = 0
        mask1 = torch.gather(mask1, dim=1, index=ids_restore)
        # for the rest time stamps
        ids_keeps = []
        masks = [] # binray masks, 0 is keep, 1 is remove
        if xnum==2:
            len_keep2 = L - len_keep1
            ids_keep2 = ids_shuffle[:, len_keep1:]
            assert ids_keep2.size(1) == len_keep2
            # ids_keeps.append(ids_keep1)
            ids_keeps.append(ids_keep2)
            # # generate the binary mask: 0 is keep, 1 is remove
            # mask2 = torch.ones([N, L], device=device)
            # mask2[:, len_keep1:] = 0
            # # unshuffle to get the binary mask
            # mask2 = torch.gather(mask2, dim=1, index=ids_restore)
            # masks.append(mask2)
        elif xnum == 3:
            len_keep2 = int(L * (1 - mask_ratio))
            len_keep3 = L - len_keep1 - len_keep2
            ids_keep2 = ids_shuffle[:, len_keep1: len_keep1+len_keep2]
            ids_keep3 = ids_shuffle[:, len_keep1+len_keep2 :]
            assert ids_keep2.size(1) == len_keep2
            assert ids_keep3.size(1) == len_keep3
            ids_keeps.append(ids_keep2)
            ids_keeps.append(ids_keep3)
            # generate the binary mask: 0 is keep, 1 is remove
            # mask2 = torch.ones([N, L], device=device)
            # mask2[:, len_keep1: len_keep1+len_keep2] = 0
            # mask3 = torch.ones([N, L], device=device)
            # mask3[:, len_keep1+len_keep2 :] = 0
            # # unshuffle to get the binary mask
            # mask2 = torch.gather(mask2, dim=1, index=ids_restore)
            # mask3 = torch.gather(mask3, dim=1, index=ids_restore)
            # masks.append(mask2)
            # masks.append(mask3)
        else:
            raise NotImplementedError

        x_masked = []
        x1_masked = torch.gather(x1, dim=1, index=ids_keep1.unsqueeze(-1).repeat(1, 1, D))
        x1_len = x1_masked.size(1)
        x_masked.append(x1_masked)
        # masks = []
        for idx in range(xnum-1):
            x_masked.append(torch.gather(xs[idx], dim=1, index=ids_keeps[idx].unsqueeze(-1).repeat(1, 1, D)))
            # mask_per = torch.ones([N, L], device=x1.device)
            # mask_per[:, :len_keep1] = 0

        x_masked = torch.cat(x_masked, dim=1)
        return x_masked, x1_len, mask1, ids_restore

    def get_feature_maps_size(self, feature_maps: tuple):
        size = list(feature_maps.shape)
        # NOTE: batch size will change the shape[0]. We need to manually set it to 1.
        size[0] = 1
        size = tuple(size)
        return size

    # get feat maps for each agent [10 512 32 32] -> [2 5 512 32 32]
    def build_feature_list(self, batch_size: int, feat_maps: dict) -> list:
        feature_map = {}
        # [5,256,32,32]
        feature_list = []

        for i in range(self.num_agent):
            feature_map[i] = torch.unsqueeze(feat_maps[batch_size * i:batch_size * (i + 1)], 1)
            # feature_map[i]: [B,1,256,32,32]
            feature_list.append(feature_map[i])

        return feature_list

    # [2 5 512 16 16] [batch, agent, channel, height, width]
    @staticmethod
    def build_local_communication_matrix(feature_list: list):
        return torch.cat(tuple(feature_list), 1)

    @staticmethod
    # FIXME: rename 'j'
    def feature_transformation(b, nb_agent_idx, local_com_mat, all_warp, device, size):
        nb_agent = torch.unsqueeze(local_com_mat[b, nb_agent_idx], 0)  # [1 512 16 16]
        nb_warp = all_warp[nb_agent_idx]  # [4 4]
        # normalize the translation vector
        x_trans = (4 * nb_warp[0, 3]) / 128
        y_trans = -(4 * nb_warp[1, 3]) / 128

        theta_rot = torch.tensor(
            [[nb_warp[0, 0], nb_warp[0, 1], 0.0], [nb_warp[1, 0], nb_warp[1, 1], 0.0]]).type(
            dtype=torch.float).to(device)
        theta_rot = torch.unsqueeze(theta_rot, 0)
        grid_rot = F.affine_grid(theta_rot, size=torch.Size(size))  # get grid for grid sample

        theta_trans = torch.tensor([[1.0, 0.0, x_trans], [0.0, 1.0, y_trans]]).type(dtype=torch.float).to(
            device)
        theta_trans = torch.unsqueeze(theta_trans, 0)
        grid_trans = F.affine_grid(theta_trans, size=torch.Size(size))  # get grid for grid sample

        # first rotate the feature map, then translate it
        warp_feat_rot = F.grid_sample(nb_agent, grid_rot, mode='nearest')
        warp_feat_trans = F.grid_sample(warp_feat_rot, grid_trans, mode='nearest')
        # FIXME: check dim
        # print("warp feature size", warp_feat_trans.size())
        return torch.squeeze(warp_feat_trans, dim=0) # [512, 16, 16]


    def build_neighbors_feature_list(self, b, agent_idx, all_warp, num_agent, local_com_mat, device,
                                     size) -> None:
        for j in range(num_agent):
            if j != agent_idx:
                warp_feat = self.feature_transformation(b, j, local_com_mat, all_warp, device, size)
                self.neighbor_feat_list.append(warp_feat)
        
    def fusion(self, mode="sum"):
        # add all the features point wise
        # print("len of nb feat list", len(self.neighbor_feat_list))
        fused = torch.sum(torch.stack(self.neighbor_feat_list), dim=0)
        if mode == "union":
            fused[fused>0.] = 1.0 
        if mode == "sum":
            fused[fused>=0.5] = 1.0
            fused[fused<0.5] = 0.0
        return fused 

    # Question: this can be done by a view or reshape?
    # shaped like: [ (batch_agent1, batch_agent2, ...)  , channel, h, w]
    @staticmethod
    def agents_to_batch(feats):
        agent_num = feats.shape[1]
        feat_list = []
        for i in range(agent_num):
            feat_list.append(feats[:, i, :, :, :])
        feat_mat = torch.cat(tuple(feat_list), 0)
        return feat_mat

    def forward_encoder(self, x1, x_next, mask_ratio):
        # embed patches
        x1 = self.patch_embed(x1)
        # x2 = self.patch_embed(x2)
        

        # add pos embed w/o cls token
        x1 = x1 + self.pos_embed[:, 1:, :] + self.temp_embed[:, 0, :]

        # handles other time stamps
        xs = []
        for ts in range(self.time_stamp-1):
            xt = x_next[:, ts, :, :, :] # [Bxa, C, H, W]
            # print("xt size", xt.size())
            xt = self.patch_embed(xt) + self.pos_embed[:, 1:, :] + self.temp_embed[:, ts+1, :]
            xs.append(xt)
        # x2 = x2 + self.pos_embed[:, 1:, :] + self.temp_embed[:, 1, :]

        # masking: length -> length * mask_ratio
        # x1, mask1, ids_restore1 = self.random_masking(x1, mask_ratio)
        # x2, mask2, ids_restore2 = self.random_masking(x2, mask_ratio)
        # x_masked, x1len, mask1, ids_restore1 = self.more_random_masking(x1, xs, mask_ratio)
        # complement masking
        # x_masked, x1len, mask1, ids_restore1 = self.complement_masking(x1, xs, mask_ratio)
        x_masked, x1len, mask1, ids_restore1 = self.masking_handle(x1, xs, mask_ratio)
        # print(x_masked.size())
        # print(x1len)

        # append cls token
        cls_token = self.cls_token + self.pos_embed[:, :1, :]
        cls_tokens = cls_token.expand(x_masked.shape[0], -1, -1)
        x = torch.cat((cls_tokens, x_masked), dim=1)

        # apply Transformer blocks
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        # compress for communication
        x = self.compressor(x)

        return x, mask1, ids_restore1, x1len

    def forward_decoder(self, x, ids_restore):
        # decompress
        x = self.decompressor(x)
        # embed tokens
        x = self.decoder_embed(x)

        # append mask tokens to sequence
        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)
        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token
        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle
        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token

        # if want to reconstruct the second time stemp
        # cat mask_tokens to the front and then unshuffle

        # add pos embed
        x = x + self.decoder_pos_embed

        # apply Transformer blocks
        for blk in self.decoder_blocks:
            x = blk(x)
        x = self.decoder_norm(x)

        # predictor projection
        x = self.decoder_pred(x)

        # remove cls token
        x = x[:, 1:, :]

        return x

    def forward_loss(self, imgs, pred, mask):
        """
        imgs: [N, 3, H, W]
        pred: [N, L, p*p*3]
        mask: [N, L], 0 is keep, 1 is remove, 
        """
        target = self.patchify(imgs)
        if self.norm_pix_loss:
            mean = target.mean(dim=-1, keepdim=True)
            var = target.var(dim=-1, keepdim=True)
            target = (target - mean) / (var + 1.e-6)**.5

        # # binarize pred, because target is binary
        # pred[pred>=0.5] = 1.0
        # pred[pred<0.5] = 0.0
        loss = (pred - target) ** 2 ## L2 loss
        # loss = self.L1_Loss(pred, target) ## L1 loss
        # loss = loss.mean(dim=-1)  # [N, L], mean loss per patch
        loss = loss.sum(dim=-1) # [N, L], sum loss per patch

        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches
        return loss

    def forward(self, imgs1, imgs2, mask_ratio=0.75):
        latent, mask1, ids_restore, size1 = self.forward_encoder(imgs1, imgs2, mask_ratio)
        # now we only consider reconstruct the first frame
        # this can be extended to reconstruct both
        latent_to_decode = latent[:, :1+size1, :]
        pred = self.forward_decoder(latent_to_decode, ids_restore)  # [N, L, p*p*3]
        loss = self.forward_loss(imgs1, pred, mask1)
        result = self.unpatchify(pred)
        return loss, pred, mask1, result

# modified MAE to be multiagent MAE
# Compared to the MAE and spatial temporal MAE, three modifications are made:
# 1) a compression auto encoder module is added at the end of the encoder (compress) and 
#    at the begnining of the decoder (decompress)
# 2) enable multiagent style
# 3) Loss changed from mean to sum
from difflib import restore
from functools import partial

from requests import PreparedRequest

import torch
import torch.nn as nn
import torch.nn.functional as F

from timm.models.vision_transformer import PatchEmbed, Block
from .mae_base import *
from ..utils.pos_embed import get_2d_sincos_pos_embed
import math

from ..utils.softmax_focal_loss import SoftmaxFocalLoss


class FusionMultiAgentMAEViT(MultiAgentMaskedAutoencoderViT):
    """
    joint reconstruction
    Serves as the DetModel, handles fusion/communication between encoder and decoder
    """
    def __init__(self, img_size=224, patch_size=16, in_chans=3,
                 embed_dim=1024, depth=24, num_heads=16,
                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
                 mlp_ratio=4., norm_layer=nn.LayerNorm, decoder_head="conv3", norm_pix_loss=False, time_stamp=1, mask_method="random"):
        super(FusionMultiAgentMAEViT, self).__init__(
                img_size=img_size, 
                patch_size=patch_size, 
                in_chans=in_chans,
                embed_dim=embed_dim, 
                depth=depth, 
                num_heads=num_heads,
                decoder_embed_dim=decoder_embed_dim, 
                decoder_depth=decoder_depth, 
                decoder_num_heads=decoder_num_heads,
                mlp_ratio=mlp_ratio, 
                norm_layer=norm_layer, 
                decoder_head = decoder_head,
                norm_pix_loss=norm_pix_loss,
                time_stamp = time_stamp,
                mask_method = mask_method
                )
        # for neighbor agents' features
        self.patch_h = 0
        self.patch_w = 0
        # self.num_agent = 0
        # self.neighbor_feat_list = []
        # self.tg_agent = None 
        if mask_method == "random":
            print("do random masking")
            self.masking_handle = self.more_random_masking
        elif mask_method == "complement":
            print("do complement masking")
            self.masking_handle = self.more_complement_masking
        else:
            raise NotImplementedError(mask_method)    

    def forward_fusion(self, x, ids_restore, trans_matrices, num_agent_tensor, batch_size):
        """
        as the pre process of the decoder, handles:
        1) add mask, restore feature maps shape like (b x agent x channel x h x w)
        2) communication/fushion/aggregation
        3) ready for decoder
        """
        device = x.device
        # # decompress
        x = self.decompressor(x)
        # # embed tokens
        x = self.decoder_embed(x)
        # append mask tokens to sequence
        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)
        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token
        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle
        # print("x_", x_.size())
        ## --- check fusion (communication) --- 
        # x_ = self.patchify(x) # here x is input image, directly
        
        # x_: (B, seq, chns)
        ## -- reshape back to 256x256 ---
        # feature_maps = self.unpatchify(x_)
        ## -------------
        ## --- reshape into B C H W ---
        feature_maps = x_.reshape(x_.shape[0], self.patch_h, self.patch_w, x_.shape[-1]) # (B, h, w, chns)
        feature_maps = feature_maps.permute(0, 3, 1, 2) # (B, chns, h, w)
        # print("feature map", feature_maps.size())
        # -------
        ## --- do fusion ---
        size = self.get_feature_maps_size(feature_maps)
        # print(size)
        assert feature_maps.size(0) % batch_size == 0, (feature_maps.size(), batch_size)
        self.num_agent = feature_maps.size(0) // batch_size
        feat_list = self.build_feature_list(batch_size, feature_maps)
        # [[1,1,256,32,32]x5] NOTE should it be [[B, 1, 256, 32, 32]x5]?
        # print(feat_list)
        local_com_mat = self.build_local_communication_matrix(
            feat_list)  # [2 5 512 32 32] [batch, agent, channel, height, width]
        # # FIXME: check size
        # print("local com mat size", local_com_mat.shape) #[2,5,256,32,32]
        local_com_mat_update = self.build_local_communication_matrix(feat_list)  # to avoid the inplace operation

        for b in range(batch_size):
            self.num_agent = num_agent_tensor[b, 0]
            for i in range(self.num_agent):
                self.tg_agent = local_com_mat[b, i]
                # print("tg agent shape", self.tg_agent.shape) #[256,32,32] 
                self.neighbor_feat_list = []
                self.neighbor_feat_list.append(self.tg_agent)
                all_warp = trans_matrices[b, i]  # transformation [2 5 5 4 4]
                # print(all_warp.shape)[5,4,4]
                self.build_neighbors_feature_list(b, i, all_warp, self.num_agent, local_com_mat,
                                                    device, size)

                # feature update
                # torch.save(torch.stack(self.neighbor_feat_list).detach().cpu(), "/mnt/NAS/home/zjx/Masked-Multiagent-Autoencoder/debug/nbf-{}-{}.pt".format(b, i))
                local_com_mat_update[b, i] = self.fusion() 
        
        # weighted feature maps is passed to decoder
        feat_fuse_mat = self.agents_to_batch(local_com_mat_update)
        # print('feat fuse mat size', feat_fuse_mat.size())
        
        
        # shape: [B, chns, h, w], B = batch_size x num_agent
        # reshape back to NxLxD
        real_bs = feat_fuse_mat.size(0)
        chns = feat_fuse_mat.size(1)
        fused_latent = feat_fuse_mat.permute(0,2,3,1).reshape(real_bs, self.patch_h*self.patch_w, chns)
        # --- re patchify ---
        # fused_latent = self.patchify(feat_fuse_mat)
        # --------------------
        # print("fused latent size", fused_latent.size()) 
        # NxLxD
        fused_latent = torch.cat([x[:, :1, :], fused_latent], dim=1)  # append cls token
        return fused_latent

    def forward_encoder(self, x1, x_next, mask_ratio):
        """
        x_next: [bxa, ts-1, C, H, W] beq_next_frames
        """
        # embed patches
        x1 = self.patch_embed(x1)      
        # add pos embed w/o cls token
        x1 = x1 + self.pos_embed[:, 1:, :] + self.temp_embed[:, 0, :]

        # handles other time stamps
        xs = []
        for ts in range(self.time_stamp-1):
            xt = x_next[:, ts, :, :, :] # [Bxa, C, H, W]
            # print("xt size", xt.size())
            xt = self.patch_embed(xt) + self.pos_embed[:, 1:, :] + self.temp_embed[:, ts+1, :]
            xs.append(xt)

        # masking: length -> length * mask_ratio
        # x_masked, x1len, mask1, ids_restore1 = self.more_random_masking(x1, xs, mask_ratio)
        # complement masking
        # x_masked, x1len, mask1, ids_restore1 = self.complement_masking(x1, xs, mask_ratio)
        x_masked, x1len, mask1, ids_restore1 = self.masking_handle(x1, xs, mask_ratio)
        # print(x_masked.size())

        # append cls token
        cls_token = self.cls_token + self.pos_embed[:, :1, :]
        cls_tokens = cls_token.expand(x_masked.shape[0], -1, -1)
        x = torch.cat((cls_tokens, x_masked), dim=1)

        # apply Transformer blocks
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        # compress for communication
        x = self.compressor(x)

        return x, mask1, ids_restore1, x1len

    def forward_decoder(self, x):
        """
        overwrite the original forward_decoder, now input latent are fused
        """
        # add pos embed
        x = x + self.decoder_pos_embed[:, 1:, :]

        # apply Transformer blocks
        for blk in self.decoder_blocks:
            x = blk(x)
        x = self.decoder_norm(x)

        # # predictor projection
        # x = self.decoder_pred(x)

        # # remove cls token
        # x = x[:, 1:, :]

        # # VERSION immediate input ---
        # x = x + self.decoder_pos_embed
        # x = self.decoder_pred(x)
        # x = x[:, 1:, :]
        # # ---------------------------

        # VERSION conv -----
        x = x[:, 1:, :]
        x = x.reshape(x.shape[0], self.patch_h, self.patch_w, x.shape[-1]) # (B, h, w, chns)
        x = x.permute(0, 3, 1, 2).contiguous() # (B, chns, h, w)
        # print("before pred size", x.size())
        x = self.decoder_pred(x)
        # patchify back to accomodate
        x = self.patchify(x)
        # print("pred size", x.size())
        # --------------------
        return x

    def forward_loss(self, teacher, pred, mask):
        """
        overwrite the original forward_loss, now calculate loss on the entire image

        """
        target = self.patchify(teacher)
        if self.norm_pix_loss:
            mean = target.mean(dim=-1, keepdim=True)
            var = target.var(dim=-1, keepdim=True)
            target = (target - mean) / (var + 1.e-6)**.5

        loss = (pred - target) ** 2 ## L2 loss
        # loss = self.L1_Loss(pred, target) ## L1 loss
        # loss = loss.mean(dim=-1)  # [N, L], mean loss per patch
        loss = loss.sum(dim=-1) # [N, L], sum loss per patch
        loss = loss.mean()
        # loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches
        return loss

    def forward(self, imgs1, imgs_next, teacher, trans_matrices, num_agent_tensor, batch_size, mask_ratio=0.75):
        """
        Modified from the original forward, make fusion happen
        """
        p = self.patch_embed.patch_size[0]
        self.patch_h = self.patch_w = imgs1.shape[2]//p
        
        latent, mask1, ids_restore, size1 = self.forward_encoder(imgs1, imgs_next, mask_ratio)
        # latent: [Bxa, L, D]
        # now we only consider reconstruct the first frame
        # this can be extended to reconstruct both
        latent_to_decode = latent[:, :1+size1, :] # CLS(0) + timesamp t (size1)
        fused_latent = self.forward_fusion(latent_to_decode, ids_restore, trans_matrices, num_agent_tensor, batch_size)
        pred = self.forward_decoder(fused_latent)  # [N, L, p*p*3]

        # --- check communication ---
        # fused_latent = self.forward_fusion(imgs1, ids_restore, trans_matrices, num_agent_tensor, batch_size)
        # pred = fused_latent # cls token is not included in forward fusion
        #----------------------------
        loss = self.forward_loss(teacher, pred, mask1)
        # loss = self.forward_loss(imgs1, pred, mask1) # use single view as supervision
        # what to do with the masking? now it is fused. So not exactly mask1
        # ONE solution: just calculate the loss over the entire input
        return loss, pred, mask1, fused_latent[:,1:,:] # remove cls
    

class IndivMultiAgentMAEViT(MultiAgentMaskedAutoencoderViT):
    """
    individual reconstruction
    Serves as the DetModel, handles fusion/communication after encoder and decoder
    """
    def __init__(self, img_size=224, patch_size=16, in_chans=3,
                 embed_dim=1024, depth=24, num_heads=16,
                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
                 mlp_ratio=4., norm_layer=nn.LayerNorm, decoder_head="mlp", norm_pix_loss=False, time_stamp=1, mask_method="random"):
        super(IndivMultiAgentMAEViT, self).__init__(
                img_size=img_size, 
                patch_size=patch_size, 
                in_chans=in_chans,
                embed_dim=embed_dim, 
                depth=depth, 
                num_heads=num_heads,
                decoder_embed_dim=decoder_embed_dim, 
                decoder_depth=decoder_depth, 
                decoder_num_heads=decoder_num_heads,
                mlp_ratio=mlp_ratio, 
                norm_layer=norm_layer, 
                decoder_head = decoder_head,
                norm_pix_loss=norm_pix_loss,
                time_stamp = time_stamp,
                mask_method = mask_method)
        # for neighbor agents' features
        self.patch_h = 0
        self.patch_w = 0
        if mask_method == "random":
            print("do random masking")
            self.masking_handle = self.more_random_masking
        elif mask_method == "complement":
            print("do complement masking")
            self.masking_handle = self.more_complement_masking
        else:
            raise NotImplementedError(mask_method) 

    def late_fusion(self, pred, trans_matrices, num_agent_tensor, batch_size):
        """ reshape the model's predictions back to 256x256x13, do aggregation on this"""
        device = pred.device
        indiv_imgs = self.unpatchify(pred) # [B C H W]
        ## --- do fusion ---
        size = self.get_feature_maps_size(indiv_imgs)
        # print(size)
        assert indiv_imgs.size(0) % batch_size == 0, (indiv_imgs.size(), batch_size)
        self.num_agent = indiv_imgs.size(0) // batch_size
        feat_list = self.build_feature_list(batch_size, indiv_imgs)
        # [[1,1,256,32,32]x5] NOTE should it be [[B, 1, 256, 32, 32]x5]?
        # print(feat_list)
        local_com_mat = self.build_local_communication_matrix(feat_list)  # [2 5 13 256 256] [batch, agent, channel, height, width]
        local_com_mat_update = self.build_local_communication_matrix(feat_list)  # to avoid the inplace operation

        for b in range(batch_size):
            num_agent = num_agent_tensor[b, 0]
            for i in range(num_agent):
                self.tg_agent = local_com_mat[b, i]
                # print("tg agent shape", self.tg_agent.shape) #[13,256,256] 
                self.neighbor_feat_list = []
                self.neighbor_feat_list.append(self.tg_agent)
                all_warp = trans_matrices[b, i]  # transformation [2 5 5 4 4]
                # print(all_warp.shape)[5,4,4]
                self.build_neighbors_feature_list(b, i, all_warp, num_agent, local_com_mat,
                                                    device, size)

                # feature update
                # torch.save(torch.stack(self.neighbor_feat_list).detach().cpu(), "/mnt/NAS/home/zjx/Masked-Multiagent-Autoencoder/debug/nbf-{}-{}.pt".format(b, i))
                local_com_mat_update[b, i] = self.fusion() 
        
        # weighted feature maps is passed to decoder
        fused_images = self.agents_to_batch(local_com_mat_update)
        # print('feat fuse mat size', feat_fuse_mat.size())
        return fused_images


    def forward(self, imgs1, imgs_next, teacher, trans_matrices, num_agent_tensor, batch_size, mask_ratio=0.7):
        latent, mask1, ids_restore, size1 = self.forward_encoder(imgs1, imgs_next, mask_ratio)
        # now we only consider reconstruct the first frame
        # this can be extended to reconstruct both
        latent_to_decode = latent[:, :1+size1, :]
        pred = self.forward_decoder(latent_to_decode, ids_restore)  # [N, L, p*p*3]
        loss = self.forward_loss(imgs1, pred, mask1)
        # result = self.unpatchify(pred)
        result = self.late_fusion(pred, trans_matrices, num_agent_tensor, batch_size)
        return loss, result, mask1, pred


class AmortizedFusionMMAEViT(MultiAgentMaskedAutoencoderViT):
    """
    Temporal amorized version
    """
    def __init__(self, img_size=224, patch_size=16, in_chans=3,
                 embed_dim=1024, depth=24, num_heads=16,
                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
                 mlp_ratio=4., norm_layer=nn.LayerNorm, decoder_head="conv3", norm_pix_loss=False, time_stamp=1, 
                 mask_method="random"):
        super(AmortizedFusionMMAEViT, self).__init__(
                img_size=img_size, 
                patch_size=patch_size, 
                in_chans=in_chans,
                embed_dim=embed_dim, 
                depth=depth, 
                num_heads=num_heads,
                decoder_embed_dim=decoder_embed_dim, 
                decoder_depth=decoder_depth, 
                decoder_num_heads=decoder_num_heads,
                mlp_ratio=mlp_ratio, 
                norm_layer=norm_layer, 
                decoder_head = decoder_head,
                norm_pix_loss=norm_pix_loss,
                time_stamp = time_stamp,
                mask_method = mask_method)
        # for neighbor agents' features
        self.patch_h = 0
        self.patch_w = 0
        if mask_method == "random":
            print("do random masking")
            self.masking_handle = self.amortized_random_masking
            self.unmasking_handle = self.amortized_random_unmasking
        elif mask_method == "complement":
            print("do complement masking")
            self.masking_handle = self.amortized_complement_masking
            self.unmasking_handle = self.amortized_complement_unmasking
        else:
            raise NotImplementedError(mask_method)
        # send radio w.r.t to already masked sequence
        # self.send_ratio = send_ratio
        # self.send_num = 0
        self.cls_loss = nn.CrossEntropyLoss()

    def amortized_random_masking(self, x_ts, mask_ratio):
        """
        Random masking for each data of each timestamp.
        Maintain indices for decoder to fuse and possibly fill in mask tokens
        x_ts: [bxa x time_stamp, L, D]
        """
        xts_masked, mask, ids_restore = self.random_masking(x_ts, mask_ratio)
        return xts_masked, mask, ids_restore

    def amortized_random_unmasking(self, x_ts, mask, ids_restore):
        """
        Reverse masking with fillings from other timestamp, using mask and torch where
        x_ts: [BxAxts, L, D]
        mask: [BxAxts, L]
        ids_restore: [BxAxts, L]
        """
        # fill with mask tokens
        mask_tokens = self.mask_token.repeat(x_ts.shape[0], ids_restore.shape[1] + 1 - x_ts.shape[1], 1)
        x_ts_ = torch.cat([x_ts, mask_tokens], dim=1)  # no cls token
        x_ts_ = torch.gather(x_ts_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_ts.shape[2]))  # unshuffle
        BAT, L, D = x_ts_.size()
        assert BAT%self.time_stamp == 0, (BAT, self.time_stamp)
        BA = BAT // self.time_stamp
        # add temporal embedding
        x_ts_ = x_ts_.reshape(BA, self.time_stamp, L, D)
        x_ts_ = x_ts_ + self.decoder_temp_embed #check dim

        # # prepare CLS token, now we use CLS of the current timestamp
        # x_ts_cls = x_ts[:,:1,:].reshape(BA, self.time_stamp, 1, D)
        # x_cls = x_ts_cls[:, 0, :, :] # [BA, 1, D]

        # figure out how to fill in the other timestamp
        x_ts_unp = x_ts_.reshape(BA, self.time_stamp, self.patch_h, self.patch_w, x_ts_.shape[-1])
        
        mask_unp = mask.reshape(mask.shape[0], self.patch_h, self.patch_w) #self.unpatchify(mask.unsqueeze(-1))
        # assert mask_unp.size(1) == 1, (mask_unp.size())
        mask_unp = mask_unp.reshape(BA, self.time_stamp, self.patch_h, self.patch_w) # [BxA, ts, H, W], 0 is kept, 1 is removed
        reverse_mask = 1. - mask_unp # 0 is removed, >0 is kept
        mask_filled = reverse_mask[:,0,:,:] # current timestamp, 
        x_curr = x_ts_unp[:,0,:,:,:] #[BxA, H, W, C]
        for ti in range(self.time_stamp-1):
            # not filled previously but can be filled by ti+1
            mask_curr = reverse_mask[:,ti+1, :, :]
            mask_select = ((mask_filled==0.) & (mask_curr>0.)).unsqueeze(-1)
            x_curr = torch.where(mask_select, x_ts_unp[:, ti+1, :, :, :], x_curr)
            mask_filled = mask_filled + mask_curr # update the mask
        # x_curr = x_curr.permute(0,3,1,2) #[BxA, C, H, W]
     
        return x_curr

    def amortized_complement_masking(self, x_ts, mask_ratio):
        """
        x_ts: [bxa x time_stamp, L, D]
        NOTE: the returned mask and ids_restore has a different dimensionality from the random masking
        """
        mask_ratio = 1.0 - 1.0 / self.time_stamp # e.g. times_stamp = 1, mask ratio ==0, 2, 0.5
        BAT, L, D = x_ts.shape
        assert BAT % self.time_stamp == 0
        BA = BAT // self.time_stamp
        # len_keep_each = int(L * (1 - mask_ratio))
        len_keep_each = math.ceil(L * (1 - mask_ratio)) # make sure every patch is covered
        noise = torch.rand(BA, L, device=x_ts.device)
        # sort noise for each sample
        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove
        ids_restore = torch.argsort(ids_shuffle, dim=1)

        x_ts = x_ts.reshape(BA, self.time_stamp, L, D)

        mask = torch.ones([BA, self.time_stamp, L], device=x_ts.device)
        x_masked = []
        # keep the subset for each timestamp
        for ti in range(self.time_stamp):
            ids_keep = ids_shuffle[:, len_keep_each*ti : min(len_keep_each*(ti+1), L)]
            x_ts_masked = torch.gather(x_ts[:,ti,:,:], dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))
            if ids_keep.size(1) < len_keep_each:
                # padding
                x_padded = torch.zeros(ids_keep.size(0), len_keep_each - ids_keep.size(1), D, device=x_ts.device)
                # print("use padded", x_padded.size())
                x_ts_masked = torch.cat((x_ts_masked, x_padded), dim=1)
            x_masked.append(x_ts_masked)
            mask[:, ti, len_keep_each*ti : min(len_keep_each*(ti+1), L)] = 0
            mask[:, ti, :] = torch.gather(mask[:, ti, :], dim=1, index=ids_restore)
        x_masked = torch.stack(x_masked, dim=0).permute(1,0,2,3) #[BA, timestamp, mL, D]
        x_masked = x_masked.reshape(BAT, -1, D)
        # print(x_masked.size())

        return x_masked, mask, ids_restore

    def amortized_complement_unmasking(self, x_ts, mask, ids_restore):
        """
        x_ts: [BxAxts, L, D]
        mask: [BxA, ts, L] -> this is real L
        ids_restore: [BxA, L] -> this is real L
        restore, add temporal embedding
        NOTE: subtlety: because of complementary masking, this L can be padded at the end thus >= real L
        """
        x_ts_ = x_ts# remove cls token
        BAT, L, D = x_ts_.shape
        realL = ids_restore.size(1)
        BA = BAT // self.time_stamp
        x_ts_ = x_ts_.reshape(BA, self.time_stamp, L, D)
        x_ts_ = x_ts_ + self.decoder_temp_embed #check dim
        # # prepare CLS token, now we use CLS of the current timestamp
        # x_ts_cls = x_ts[:,:1,:].reshape(BA, self.time_stamp, 1, D)
        # x_cls = x_ts_cls[:, 0, :, :] # [BA, 1, D]
        x_restore = []
        for ti in range(self.time_stamp):
            x_restore.append(x_ts_[:,ti, :, :]) #[BA, L, D]
        x_restore = torch.cat(x_restore, dim=1)
        x_restore = x_restore[:, :realL, :]
        x_restore = torch.gather(x_restore, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_ts.shape[2]))

        feature_maps = x_restore.reshape(BA, self.patch_h, self.patch_w, D)
        return feature_maps

    def forward_fusion(self, x, mask, ids_restore, trans_matrices, num_agent_tensor, batch_size):
        """
        as the pre process of the decoder, handles:
        1) add mask, restore feature maps shape like (b x agent x channel x h x w)
        2) communication/fushion/aggregation
        3) ready for decoder
        """
        device = x.device
        # # decompress
        x = self.decompressor(x)
        # # embed tokens
        x = self.decoder_embed(x)
        # -------
        feature_maps = self.unmasking_handle(x, mask, ids_restore)
        # print("feature map size", feature_maps.size()) # [BAT, H, W, D]
        feature_maps = feature_maps.permute(0,3,1,2)
        ## --- do fusion ---
        size = self.get_feature_maps_size(feature_maps)
        # print(size)
        assert feature_maps.size(0) % batch_size == 0, (feature_maps.size(), batch_size)
        self.num_agent = feature_maps.size(0) // batch_size
        feat_list = self.build_feature_list(batch_size, feature_maps)
        # print(feat_list)
        local_com_mat = self.build_local_communication_matrix(
            feat_list)  # [2 5 512 32 32] [batch, agent, channel, height, width]
       
        local_com_mat_update = self.build_local_communication_matrix(feat_list)  # to avoid the inplace operation

        for b in range(batch_size):
            self.num_agent = num_agent_tensor[b, 0]
            for i in range(self.num_agent):
                self.tg_agent = local_com_mat[b, i]
                # print("tg agent shape", self.tg_agent.shape) #[256,32,32] 
                self.neighbor_feat_list = []
                self.neighbor_feat_list.append(self.tg_agent)
                all_warp = trans_matrices[b, i]  # transformation [2 5 5 4 4]
                # print(all_warp.shape)[5,4,4]
                self.build_neighbors_feature_list(b, i, all_warp, self.num_agent, local_com_mat,
                                                    device, size)

                # feature update
                # torch.save(torch.stack(self.neighbor_feat_list).detach().cpu(), "/mnt/NAS/home/zjx/Masked-Multiagent-Autoencoder/debug/nbf-{}-{}.pt".format(b, i))
                local_com_mat_update[b, i] = self.fusion() 
        
        # weighted feature maps is passed to decoder
        feat_fuse_mat = self.agents_to_batch(local_com_mat_update)
        # print('feat fuse mat size', feat_fuse_mat.size())       
        # shape: [B, chns, h, w], B = batch_size x num_agent
        # reshape back to NxLxD
        real_bs = feat_fuse_mat.size(0)
        chns = feat_fuse_mat.size(1)
        fused_latent = feat_fuse_mat.permute(0,2,3,1).reshape(real_bs, self.patch_h*self.patch_w, chns)
        # print("fused latent size", fused_latent.size()) 
        # NxLxD
        # fused_latent = torch.cat([x_cls, fused_latent], dim=1)  # append cls token
        return fused_latent


    def forward_encoder(self, x1, x_next, mask_ratio):
        """
        x1: [bxa, C, H, W]
        x_next: [bxa, ts-1, C, H, W] beq_next_frames
        """
        # cat x1 and x_next to encoder independently
        BA, C, H, W = x1.size()
        x1 = x1.unsqueeze(1)
        if self.time_stamp>1:
            x_ind = torch.cat((x1, x_next), dim=1) # [bxa, ts, C, H, W]
        else:
            x_ind = x1
        # print(x_ind.size())
        assert x_ind.size(1) == self.time_stamp
        x_ind = x_ind.reshape(BA*self.time_stamp, C, H, W)
        # embed patches
        x_ind = self.patch_embed(x_ind)
        x_ind = x_ind + self.pos_embed[:, 1:, :]

        
        # amortized masking, complement and random
        # x_masked, mask, ids_restore = self.masking_handle(x_ind, mask_ratio)
        # print(x_masked.size())
        # print(mask.size())

        # append cls token
        # cls_token = self.cls_token + self.pos_embed[:, :1, :]
        # cls_tokens = cls_token.expand(x_ind.shape[0], -1, -1)
        # x = torch.cat((cls_tokens, x_ind), dim=1)

        # apply Transformer blocks
        for blk in self.blocks:
            x_ind = blk(x_ind)
        x = self.norm(x_ind) # [BAT, L, D]
        # compress for communication
        # TODO: mask ONLY for transmission, encode the complete sequence
        x_masked, mask, ids_restore = self.masking_handle(x, mask_ratio)
        # x = self.compressor(x)
        # x_send = torch.cat((x[:,:1,:], x_masked), dim=1) # put [CLS] token back
        x_send = self.compressor(x_masked)
        
        return x_send, mask, ids_restore

    def forward_decoder(self, x):
        """
        overwrite the original forward_decoder, now input latent are fused
        """
        # add pos embed
        x = x + self.decoder_pos_embed[:, 1:, :]

        # apply Transformer blocks
        for blk in self.decoder_blocks:
            x = blk(x)
        x = self.decoder_norm(x)

        # VERSION conv -----
        # x = x[:, 1:, :] # NO CLS anymore
        x = x.reshape(x.shape[0], self.patch_h, self.patch_w, x.shape[-1]) # (B, h, w, chns)
        x = x.permute(0, 3, 1, 2).contiguous() # (B, chns, h, w)
        # print("before pred size", x.size())
        # x = self.decoder_pred(x)
        # --------------------
        # two head classification
        x_occ = self.decoder_pred_occ(x) 
        # print("occ, ", x_occ.size())
        x_free = self.decoder_pred_free(x)
        # print("free,", x_free.size())
        x_pred = torch.stack((x_free, x_occ), dim=1)
        # print(x_pred.size())
        return x_pred

    def forward_loss(self, teacher, pred):
        """
        overwrite the original forward_loss, now calculate loss on the entire image

        """
        target = self.patchify(teacher)
        # print("after patchify, teacher, pred size", target.size(), pred.size())
        if self.norm_pix_loss:
            mean = target.mean(dim=-1, keepdim=True)
            var = target.var(dim=-1, keepdim=True)
            target = (target - mean) / (var + 1.e-6)**.5

        loss = (pred - target) ** 2 ## L2 loss
        loss = loss.sum(dim=-1) # [N, L], sum loss per patch
        loss = loss.mean()
        return loss

    def forward_bce_loss(self, target, pred):
        # target [B, C, H, W]
        # pred [B, class, C, H, W]
        # print(target.size())
        # print(pred.size())
        target = target.type(torch.LongTensor).to(pred.device)
        loss = self.cls_loss(pred, target)
        return loss

    def forward(self, imgs1, imgs_next, teacher, trans_matrices, num_agent_tensor, batch_size, mask_ratio=0.75):
        """
        Encoder encodes each timestamp alone
        Decoder fuses multi timestamp together
        """
        p = self.patch_embed.patch_size[0]
        self.patch_h = self.patch_w = imgs1.shape[2]//p
        # TODO: -----new model----
        # encoder handles each timestamp seperately, decoder fuse them together. 
        # temporal embedding add at the decoder side
        # special take care of the mask ids
        # implement a new model to do so.
        # a) complement masking: no mask tokens
        # b) random masking: fuse timestamps, [mask] fills the rest
        # ------------------------
        latent, mask, ids_restore = self.forward_encoder(imgs1, imgs_next, mask_ratio)
        fused_latent = self.forward_fusion(latent, mask, ids_restore, trans_matrices, num_agent_tensor, batch_size)
        pred = self.forward_decoder(fused_latent) # [B, 2, C, H, W]
        loss = self.forward_bce_loss(teacher, pred)
        # result = self.unpatchify(pred)
        result = torch.argmax(torch.softmax(pred, dim=1), dim=1)
        return loss, result, mask, pred


class AmortizedIndivMMAEViT(MultiAgentMaskedAutoencoderViT):
    """
    Temporal amorized version
    """
    def __init__(self, img_size=224, patch_size=16, in_chans=3,
                 embed_dim=1024, depth=24, num_heads=16,
                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
                 mlp_ratio=4., norm_layer=nn.LayerNorm, decoder_head="mlp", norm_pix_loss=False, time_stamp=1, 
                 mask_method="random", encode_partial=False, no_temp_emb=False, decode_singletemp=False):
        super(AmortizedIndivMMAEViT, self).__init__(
                img_size=img_size, 
                patch_size=patch_size, 
                in_chans=in_chans,
                embed_dim=embed_dim, 
                depth=depth, 
                num_heads=num_heads,
                decoder_embed_dim=decoder_embed_dim, 
                decoder_depth=decoder_depth, 
                decoder_num_heads=decoder_num_heads,
                mlp_ratio=mlp_ratio, 
                norm_layer=norm_layer, 
                decoder_head = decoder_head,
                norm_pix_loss=norm_pix_loss,
                time_stamp = time_stamp,
                mask_method = mask_method)
        # for neighbor agents' features
        self.patch_h = 0
        self.patch_w = 0
        # class_weights = torch.FloatTensor([1.0, 20.0]) # [free, occ]
        # print("Using weighted cross entropy loss with weights", class_weights)
        # self.cls_loss = nn.CrossEntropyLoss(weight = class_weights)
        self.cls_loss = nn.CrossEntropyLoss()
        # self.focal_loss = SoftmaxFocalLoss(alpha=0.75, gamma=3)
        
        if mask_method == "random":
            print("do random masking")
            self.masking_handle = self.amortized_random_masking
            self.unmasking_handle = self.amortized_random_unmasking
        elif mask_method == "complement":
            print("do complement masking")
            self.masking_handle = self.amortized_complement_masking
            self.unmasking_handle = self.amortized_complement_unmasking
        else:
            raise NotImplementedError(mask_method)

        # ---- ablation study for encoder ------
        if encode_partial:
            print("encode after masking")
            self.forward_encoder = self.forward_encoder_partial
        else:
            print("encoder BEFORE masking")
            self.forward_encoder = self.forward_encoder_all

        # ---- ablation study for decoder ------
        self.decode_singletemp = decode_singletemp
        self.no_temp_emb = no_temp_emb

        

    def amortized_random_masking(self, x_ts, mask_ratio):
        """
        Random masking for each data of each timestamp.
        Maintain indices for decoder to fuse and possibly fill in mask tokens
        x_ts: [bxa x time_stamp, L, D]
        """
        xts_masked, mask, ids_restore = self.random_masking(x_ts, mask_ratio)
        return xts_masked, mask, ids_restore

    def amortized_random_unmasking(self, x_ts, mask, ids_restore):
        """
        Reverse masking with fillings from other timestamp, using mask and torch where
        x_ts: [BxAxts, L, D]
        mask: [BxAxts, L] -> full length L
        ids_restore: [BxAxts, L] -> full length L
        """
        # print(x_ts.size(), mask.size(), ids_restore.size())
        # fill with mask tokens
        mask_tokens = self.mask_token.repeat(x_ts.shape[0], ids_restore.shape[1] + 1 - x_ts.shape[1], 1)
        x_ts_ = torch.cat([x_ts, mask_tokens], dim=1)  # no cls token
        x_ts_ = torch.gather(x_ts_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_ts.shape[2]))  # unshuffle
        BAT, L, D = x_ts_.size()
        assert BAT%self.time_stamp == 0, (BAT, self.time_stamp)
        BA = BAT // self.time_stamp
        # add temporal embedding
        x_ts_ = x_ts_.reshape(BA, self.time_stamp, L, D)

        # ---- ablation: use time emb -----
        if not self.no_temp_emb:
            # print("use temporal embedding")
            x_ts_ = x_ts_ + self.decoder_temp_embed #check dim
        # ----------------------------------

        # figure out how to fill in the other timestamp
        x_ts_unp = x_ts_.reshape(BA, self.time_stamp, self.patch_h, self.patch_w, x_ts_.shape[-1])
        # print("x_ts_unp size", x_ts_unp.size())
        mask_unp = mask.reshape(mask.shape[0], self.patch_h, self.patch_w) #self.unpatchify(mask.unsqueeze(-1))
        # assert mask_unp.size(1) == 1, (mask_unp.size())
        mask_unp = mask_unp.reshape(BA, self.time_stamp, self.patch_h, self.patch_w) # [BxA, ts, H, W], 0 is kept, 1 is removed
        reverse_mask = 1. - mask_unp # 0 is removed, >0 is kept
        mask_filled = reverse_mask[:,0,:,:] # current timestamp, 
        x_curr = x_ts_unp[:,0,:,:,:] #[BxA, h, w, D]
        # print("x_curr size", x_curr.size())
        if self.decode_singletemp:
            print("decode single time stamp")
            x_curr = x_curr.permute(0,3,1,2)
            return x_curr
        else:
            for ti in range(self.time_stamp-1):
                # not filled previously but can be filled by ti+1
                mask_curr = reverse_mask[:,ti+1, :, :]
                mask_select = ((mask_filled==0.) & (mask_curr>0.)).unsqueeze(-1)
                x_curr = torch.where(mask_select, x_ts_unp[:, ti+1, :, :, :], x_curr)
                mask_filled = mask_filled + mask_curr # update the mask
            x_curr = x_curr.permute(0,3,1,2) #[BxA, C, H, W]
            return x_curr

    def amortized_complement_masking(self, x_ts, mask_ratio):
        """
        x_ts: [bxa x time_stamp, L, D]
        NOTE: the returned mask and ids_restore has a different dimensionality from the random masking
        """
        mask_ratio = 1.0 - 1.0 / self.time_stamp # e.g. times_stamp = 1, mask ratio ==0, 2, 0.5
        BAT, L, D = x_ts.shape
        assert BAT % self.time_stamp == 0
        BA = BAT // self.time_stamp
        # len_keep_each = int(L * (1 - mask_ratio))
        len_keep_each = math.ceil(L * (1 - mask_ratio)) # make sure every patch is covered
        noise = torch.rand(BA, L, device=x_ts.device)
        # sort noise for each sample
        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove
        ids_restore = torch.argsort(ids_shuffle, dim=1)

        x_ts = x_ts.reshape(BA, self.time_stamp, L, D)

        mask = torch.ones([BA, self.time_stamp, L], device=x_ts.device)
        x_masked = []
        # keep the subset for each timestamp
        for ti in range(self.time_stamp):
            ids_keep = ids_shuffle[:, len_keep_each*ti : min(len_keep_each*(ti+1), L)]
            x_ts_masked = torch.gather(x_ts[:,ti,:,:], dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))
            if ids_keep.size(1) < len_keep_each:
                # padding
                x_padded = torch.zeros(ids_keep.size(0), len_keep_each - ids_keep.size(1), D, device=x_ts.device)
                x_ts_masked = torch.cat((x_ts_masked, x_padded), dim=1)
            x_masked.append(x_ts_masked)
            mask[:, ti, len_keep_each*ti : min(len_keep_each*(ti+1), L)] = 0
            mask[:, ti, :] = torch.gather(mask[:, ti, :], dim=1, index=ids_restore)
        x_masked = torch.stack(x_masked, dim=0).permute(1,0,2,3) #[BA, timestamp, L, D]
        x_masked = x_masked.reshape(BAT, -1, D) # because the seq len is not L after masking

        return x_masked, mask, ids_restore

    def amortized_complement_unmasking(self, x_ts, mask, ids_restore):
        """
        x_ts: [BxAxts, L, D]
        mask: [BxA, ts, L] -> this is real L
        ids_restore: [BxA, L] -> this is real L
        restore, add temporal embedding
        NOTE: subtlety: because of complementary masking, this L can be padded at the end thus >= real L
        """
        x_ts_ = x_ts
        BAT, L, D = x_ts_.shape
        realL = ids_restore.size(1)
        BA = BAT // self.time_stamp
        x_ts_ = x_ts_.reshape(BA, self.time_stamp, L, D)
        if not self.no_temp_emb:
            # print("use temporal embedding")
            x_ts_ = x_ts_ + self.decoder_temp_embed #check dim
        if self.decode_singletemp:
            # print("decode single timestamp")
            x_curr = x_ts_[:,0, :realL, :] # [BA, realL, D]
            mask_tokens = self.mask_token.repeat(x_curr.shape[0], ids_restore.shape[1] + 1 - x_curr.shape[1], 1)
            x_curr = torch.cat([x_curr, mask_tokens], dim=1)  # no cls token
            x_restore = torch.gather(x_curr, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_curr.shape[2]))  # unshuffle
        else:
            x_restore = []
            for ti in range(self.time_stamp):
                x_restore.append(x_ts_[:,ti, :, :]) #[BA, L, D]
            x_restore = torch.cat(x_restore, dim=1)
            x_restore = x_restore[:, :realL, :]
            x_restore = torch.gather(x_restore, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_ts.shape[2]))

        feature_maps = x_restore.reshape(BA, self.patch_h, self.patch_w, D)
        return feature_maps


    def late_fusion(self, pred, trans_matrices, num_agent_tensor, batch_size):
        """ reshape the model's predictions back to 256x256x13, do aggregation on this"""
        device = pred.device
        # indiv_imgs = self.unpatchify(pred) # [B C H W]
        indiv_imgs = pred.type(torch.FloatTensor).to(device)
        ## --- do fusion ---
        size = self.get_feature_maps_size(indiv_imgs)
        # print(size)
        assert indiv_imgs.size(0) % batch_size == 0, (indiv_imgs.size(), batch_size)
        self.num_agent = indiv_imgs.size(0) // batch_size
        feat_list = self.build_feature_list(batch_size, indiv_imgs)
        # [[1,1,256,32,32]x5] NOTE should it be [[B, 1, 256, 32, 32]x5]?
        # print(feat_list)
        local_com_mat = self.build_local_communication_matrix(feat_list)  # [2 5 13 256 256] [batch, agent, channel, height, width]
        local_com_mat_update = self.build_local_communication_matrix(feat_list)  # to avoid the inplace operation

        for b in range(batch_size):
            num_agent = num_agent_tensor[b, 0]
            for i in range(num_agent):
                self.tg_agent = local_com_mat[b, i]
                # print("tg agent shape", self.tg_agent.shape) #[13,256,256] 
                self.neighbor_feat_list = []
                self.neighbor_feat_list.append(self.tg_agent)
                all_warp = trans_matrices[b, i]  # transformation [2 5 5 4 4]
                # print(all_warp.shape)[5,4,4]
                self.build_neighbors_feature_list(b, i, all_warp, num_agent, local_com_mat,
                                                    device, size)

                # feature update
                # torch.save(torch.stack(self.neighbor_feat_list).detach().cpu(), "/mnt/NAS/home/zjx/Masked-Multiagent-Autoencoder/debug/nbf-{}-{}.pt".format(b, i))
                local_com_mat_update[b, i] = self.fusion(mode="union") 
        
        # weighted feature maps is passed to decoder
        fused_images = self.agents_to_batch(local_com_mat_update)
        # print('feat fuse mat size', feat_fuse_mat.size())
        return fused_images

    def ego_late_fusion(self, pred, gt, trans_matrices, num_agent_tensor, batch_size):
        """ aggregation, ego use ground truth input, used specifically in inference time"""
        device = pred.device
        # indiv_imgs = self.unpatchify(pred) # [B C H W]
        indiv_imgs = pred.type(torch.FloatTensor).to(device)
        ## --- do fusion ---
        size = self.get_feature_maps_size(indiv_imgs)
        # print(size)
        assert indiv_imgs.size(0) % batch_size == 0, (indiv_imgs.size(), batch_size)
        self.num_agent = indiv_imgs.size(0) // batch_size
        feat_list = self.build_feature_list(batch_size, indiv_imgs)
        gt_list = self.build_feature_list(batch_size, gt)
        # print(feat_list)
        local_com_mat = self.build_local_communication_matrix(feat_list)  # [2 5 13 256 256] [batch, agent, channel, height, width]
        local_com_mat_update = self.build_local_communication_matrix(feat_list)  # to avoid the inplace operation
        local_com_mat_gt = self.build_local_communication_matrix(gt_list)

        for b in range(batch_size):
            num_agent = num_agent_tensor[b, 0]
            for i in range(num_agent):
                # use gt for the ego
                self.tg_agent = local_com_mat_gt[b, i]
                # print("tg agent shape", self.tg_agent.shape) #[13,256,256] 
                self.neighbor_feat_list = []
                self.neighbor_feat_list.append(self.tg_agent)
                all_warp = trans_matrices[b, i]  # transformation [2 5 5 4 4]
                # print(all_warp.shape)[5,4,4]
                self.build_neighbors_feature_list(b, i, all_warp, num_agent, local_com_mat,
                                                    device, size)

                # feature update
                # torch.save(torch.stack(self.neighbor_feat_list).detach().cpu(), "/mnt/NAS/home/zjx/Masked-Multiagent-Autoencoder/debug/nbf-{}-{}.pt".format(b, i))
                local_com_mat_update[b, i] = self.fusion(mode="union") 
        
        # weighted feature maps is passed to decoder
        fused_images = self.agents_to_batch(local_com_mat_update)
        # print('feat fuse mat size', feat_fuse_mat.size())
        return fused_images

    def forward_encoder_all(self, x1, x_next, mask_ratio):
        """
        x1: [bxa, C, H, W]
        x_next: [bxa, ts-1, C, H, W] beq_next_frames
        """
        # cat x1 and x_next to encoder independently
        BA, C, H, W = x1.size()
        x1 = x1.unsqueeze(1)
        if self.time_stamp>1:
            x_ind = torch.cat((x1, x_next), dim=1) # [bxa, ts, C, H, W]
        else:
            x_ind = x1
        # print(x_ind.size())
        assert x_ind.size(1) == self.time_stamp
        x_ind = x_ind.reshape(BA*self.time_stamp, C, H, W)
        # embed patches
        x_ind = self.patch_embed(x_ind)
        x_ind = x_ind + self.pos_embed[:, 1:, :]

        # amortized masking, complement and random
        # x_masked, mask, ids_restore = self.masking_handle(x_ind, mask_ratio)
        # print(x_masked.size())
        # print(mask.size())

        # # append cls token
        # cls_token = self.cls_token + self.pos_embed[:, :1, :]
        # cls_tokens = cls_token.expand(x_masked.shape[0], -1, -1)
        # x = torch.cat((cls_tokens, x_masked), dim=1)

        # apply Transformer blocks
        for blk in self.blocks:
            x_ind = blk(x_ind)
        x = self.norm(x_ind)
        # compress for communication
        # TODO: mask ONLY for transmission, encode the complete sequence
        x_masked, mask, ids_restore = self.masking_handle(x, mask_ratio)
        x = self.compressor(x_masked)

        return x, mask, ids_restore

    def forward_encoder_partial(self, x1, x_next, mask_ratio):
        """
        x1: [bxa, C, H, W]
        x_next: [bxa, ts-1, C, H, W] beq_next_frames
        """
        # cat x1 and x_next to encoder independently
        BA, C, H, W = x1.size()
        x1 = x1.unsqueeze(1)
        if self.time_stamp>1:
            x_ind = torch.cat((x1, x_next), dim=1) # [bxa, ts, C, H, W]
        else:
            x_ind = x1
        # print(x_ind.size())
        assert x_ind.size(1) == self.time_stamp
        x_ind = x_ind.reshape(BA*self.time_stamp, C, H, W)
        # embed patches
        x_ind = self.patch_embed(x_ind)
        x_ind = x_ind + self.pos_embed[:, 1:, :]

        # mask before transformer encoding
        # amortized masking, complement and random
        x_masked, mask, ids_restore = self.masking_handle(x_ind, mask_ratio)
        # print(x_masked.size())
        # print(mask.size())
        for blk in self.blocks:
            x_masked = blk(x_masked)
        x = self.norm(x_masked)
        x = self.compressor(x)

        # # -------- encoder then mask ----------
        # # apply Transformer blocks
        # for blk in self.blocks:
        #     x_ind = blk(x_ind)
        # x = self.norm(x_ind)
        # # compress for communication
        # # mask ONLY for transmission, encode the complete sequence
        # x_masked, mask, ids_restore = self.masking_handle(x, mask_ratio)
        # x = self.compressor(x_masked)
        # # --------------------------------------

        return x, mask, ids_restore

    def forward_decoder(self, latent, mask, ids_restore):
        """
        overwrite the original forward_decoder, now input latent are fused
        """
        latent = self.decompressor(latent)
        # # embed tokens
        latent = self.decoder_embed(latent)

        restored_latent = self.unmasking_handle(latent, mask, ids_restore)
        restored_latent = restored_latent.reshape(restored_latent.size(0), self.patch_h*self.patch_w, -1)
        # x = torch.cat([latent_cls, restored_latent], dim=1)

        # add pos embed
        x = restored_latent + self.decoder_pos_embed[:, 1:, :]

        # apply Transformer blocks
        for blk in self.decoder_blocks:
            x = blk(x)
        x = self.decoder_norm(x)

        # VERSION mlp -----
        x_occ = self.decoder_pred_occ(x) 
        x_free = self.decoder_pred_free(x)
        x_occ = self.unpatchify(x_occ)
        x_free = self.unpatchify(x_free)
        # remove cls token
        # x = x[:, 1:, :]
        # print("pred size", x.size())
        # --------------------
        x_pred = torch.stack((x_free, x_occ), dim=1) # [B, class, C, H, W]
        return x_pred

    def forward_loss(self, teacher, pred):
        """
        overwrite the original forward_loss, now calculate loss on the entire image

        """
        target = self.patchify(teacher)
        if self.norm_pix_loss:
            mean = target.mean(dim=-1, keepdim=True)
            var = target.var(dim=-1, keepdim=True)
            target = (target - mean) / (var + 1.e-6)**.5

        loss = (pred - target) ** 2 ## L2 loss
        loss = loss.sum(dim=-1) # [N, L], sum loss per patch
        loss = loss.mean()
        return loss

    def forward_bce_loss(self, target, pred):
        target = target.type(torch.LongTensor).to(pred.device)
        loss = self.cls_loss(pred, target)
        return loss

    def forward_focal_loss(self, target, pred):
        return self.focal_loss(pred, target)

    def forward(self, imgs1, imgs_next, teacher, trans_matrices, num_agent_tensor, batch_size, mask_ratio=0.75):
        """
        Encoder encodes each timestamp alone
        Decoder fuses multi timestamp together
        """
        p = self.patch_embed.patch_size[0]
        self.patch_h = self.patch_w = imgs1.shape[2]//p
        # print("image, patch h, w", imgs1.shape, self.patch_h, self.patch_w)
        latent, mask, ids_restore = self.forward_encoder(imgs1, imgs_next, mask_ratio)
        # latent: [BAT, L, D]
        pred = self.forward_decoder(latent, mask, ids_restore)
        # loss = self.forward_loss(imgs1, pred)
        loss = self.forward_bce_loss(imgs1, pred)
        # loss = self.forward_focal_loss(imgs1, pred)
        # result = self.unpatchify(pred)
        ind_result = torch.argmax(torch.softmax(pred, dim=1), dim=1)
        result = self.late_fusion(ind_result, trans_matrices, num_agent_tensor, batch_size)
        # print(result.size())
        # ind_result = self.unpatchify(pred)
        return loss, result, latent, ind_result

    def inference(self, imgs1, imgs_next, teacher, trans_matrices, num_agent_tensor, batch_size, mask_ratio=0.75):
        """
        Encoder encodes each timestamp alone
        Decoder fuses multi timestamp together
        """
        p = self.patch_embed.patch_size[0]
        self.patch_h = self.patch_w = imgs1.shape[2]//p
        # print("image, patch h, w", imgs1.shape, self.patch_h, self.patch_w)
        latent, mask, ids_restore = self.forward_encoder(imgs1, imgs_next, mask_ratio)
        # latent: [BAT, L, D]
        pred = self.forward_decoder(latent, mask, ids_restore)
        # loss = self.forward_loss(imgs1, pred)
        loss = self.forward_bce_loss(imgs1, pred)
        # loss = self.forward_focal_loss(imgs1, pred)
        # print(imgs1.size(), imgs1.type())
        ind_result = torch.argmax(torch.softmax(pred, dim=1), dim=1)
        result = self.ego_late_fusion(ind_result, imgs1, trans_matrices, num_agent_tensor, batch_size)
        # result = self.ego_late_fusion(imgs1, imgs1, trans_matrices, num_agent_tensor, batch_size)
        # print(result.size())
        # ind_result = self.unpatchify(pred)
        return loss, result, latent, ind_result



def amo_individual_bev_multi_mae_vit_base_patch8_dec512d4b(**kwargs):
    model = AmortizedIndivMMAEViT(
        img_size=256, patch_size=8, in_chans=13, embed_dim=768, depth=6, num_heads=12,
        decoder_embed_dim=512, decoder_depth=4, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="mlp", **kwargs)
    return model

def amo_individual_bev_multi_mae_vit_base_patch8_dec256d4b(**kwargs):
    model = AmortizedIndivMMAEViT(
        img_size=256, patch_size=8, in_chans=13, embed_dim=384, depth=6, num_heads=12,
        decoder_embed_dim=256, decoder_depth=4, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="mlp", **kwargs)
    return model

def amo_individual_bev_multi_mae_vit_base_patch16_dec256d4b(**kwargs):
    model = AmortizedIndivMMAEViT(
        img_size=256, patch_size=16, in_chans=13, embed_dim=384, depth=6, num_heads=12,
        decoder_embed_dim=256, decoder_depth=4, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="mlp", **kwargs)
    return model

def amo_individual_bev_multi_mae_vit_base_patch32_dec256d4b(**kwargs):
    model = AmortizedIndivMMAEViT(
        img_size=256, patch_size=32, in_chans=13, embed_dim=384, depth=6, num_heads=12,
        decoder_embed_dim=256, decoder_depth=4, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="mlp", **kwargs)
    return model


def amo_individual_bev_multi_mae_vit_base_patch8_dec512d8b(**kwargs):
    model = AmortizedIndivMMAEViT(
        img_size=256, patch_size=8, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="mlp", **kwargs)
    return model

def amo_individual_bev_multi_mae_vit_base_patch16_dec512d8b(**kwargs):
    model = AmortizedIndivMMAEViT(
        img_size=256, patch_size=16, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="mlp", **kwargs)
    return model

def amo_individual_bev_multi_mae_vit_base_patch32_dec512d8b(**kwargs):
    model = AmortizedIndivMMAEViT(
        img_size=256, patch_size=32, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="mlp", **kwargs)
    return model

def amo_individual_bev_multi_mae_vit_base_patch4_dec512d8b(**kwargs):
    model = AmortizedIndivMMAEViT(
        img_size=256, patch_size=4, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="mlp", **kwargs)
    return model

def amo_fusion_bev_multi_mae_vit_base_patch8_dec512d6b(**kwargs):
    model = AmortizedFusionMMAEViT(
        img_size=256, patch_size=8, in_chans=13, embed_dim=768, depth=6, num_heads=12,
        decoder_embed_dim=512, decoder_depth=6, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="conv3", **kwargs)
    return model
# NOTE: depth changed.
def amo_fusion_bev_multi_mae_vit_base_patch16_dec512d6b(**kwargs):
    model = AmortizedFusionMMAEViT(
        img_size=256, patch_size=16, in_chans=13, embed_dim=768, depth=6, num_heads=12,
        decoder_embed_dim=512, decoder_depth=6, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="conv3", **kwargs)
    return model

def amo_fusion_bev_multi_mae_vit_base_patch8_dec512d8b(**kwargs):
    model = AmortizedFusionMMAEViT(
        img_size=256, patch_size=8, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="conv3", **kwargs)
    return model

def amo_fusion_bev_multi_mae_vit_base_patch16_dec512d8b(**kwargs):
    model = AmortizedFusionMMAEViT(
        img_size=256, patch_size=16, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="conv3", **kwargs)
    return model

    
def individual_bev_multi_mae_vit_base_patch8_dec512d8b(**kwargs):
    model = IndivMultiAgentMAEViT(
        img_size=256, patch_size=8, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="mlp", **kwargs)
    return model

def fusion_bev_multi_mae_vit_base_patch16_dec512d8b(**kwargs):
    model = FusionMultiAgentMAEViT(
        img_size=256, patch_size=16, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="conv3", **kwargs)
    return model

def fusion_bev_multi_mae_vit_base_patch8_dec512d8b(**kwargs):
    model = FusionMultiAgentMAEViT(
        img_size=256, patch_size=8, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="conv3", **kwargs)
    return model

def fusion_bev_multi_mae_vit_base_patch1_dec512d8b(**kwargs):
    model = FusionMultiAgentMAEViT(
        img_size=256, patch_size=1, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model

def fusion_bev_multi_mae_vit_base_patch32_dec512d8b(**kwargs):
    model = FusionMultiAgentMAEViT(
        img_size=256, patch_size=32, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="conv3", **kwargs)
    return model

def bev_multi_mae_vit_base_patch16_dec512d8b(**kwargs):
    model = MultiAgentMaskedAutoencoderViT(
        img_size=256, patch_size=16, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model

def fusion_bev_multi_mae_vit_base_patch16_dec1024d8b(**kwargs):
    model = FusionMultiAgentMAEViT(
        img_size=256, patch_size=16, in_chans=13, embed_dim=768, depth=12, num_heads=12,
        decoder_embed_dim=1024, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), decoder_head="conv3", **kwargs)
    return model


# Juexiao add
bev_mae_vit_base_patch16 = bev_multi_mae_vit_base_patch16_dec512d8b # decoder: 512 dim, 8 blocks
# joint reconstruction
joint_bev_mae_vit_base_patch16 = fusion_bev_multi_mae_vit_base_patch16_dec512d8b
joint_bev_mae_vit_base_patch8 = fusion_bev_multi_mae_vit_base_patch8_dec512d8b
joint_bev_mae_vit_base_patch1 = fusion_bev_multi_mae_vit_base_patch1_dec512d8b
joint_bev_mae_vit_base_patch32 = fusion_bev_multi_mae_vit_base_patch32_dec512d8b
joint_bev_mae_vit_base_patch16_dec1024 = fusion_bev_multi_mae_vit_base_patch16_dec1024d8b
# individual reconstruction
ind_bev_mae_vit_base_patch8 = individual_bev_multi_mae_vit_base_patch8_dec512d8b
# temporal amortized reconstruction
amortized_ind_patch8 = amo_individual_bev_multi_mae_vit_base_patch8_dec512d8b
amortized_ind_patch8_shallow = amo_individual_bev_multi_mae_vit_base_patch8_dec512d4b
amortized_ind_patch8_light = amo_individual_bev_multi_mae_vit_base_patch8_dec256d4b
amortized_ind_patch16 = amo_individual_bev_multi_mae_vit_base_patch16_dec512d8b
amortized_ind_patch32 = amo_individual_bev_multi_mae_vit_base_patch32_dec512d8b
amortized_ind_patch4 = amo_individual_bev_multi_mae_vit_base_patch4_dec512d8b
amortized_joint_patch8_shallow = amo_fusion_bev_multi_mae_vit_base_patch8_dec512d6b
amortized_joint_patch16_shallow = amo_fusion_bev_multi_mae_vit_base_patch16_dec512d6b
amortized_joint_patch8 = amo_fusion_bev_multi_mae_vit_base_patch8_dec512d8b
amortized_joint_patch16 = amo_fusion_bev_multi_mae_vit_base_patch8_dec512d8b
amortized_ind_patch16_light = amo_individual_bev_multi_mae_vit_base_patch16_dec256d4b
amortized_ind_patch32_light = amo_individual_bev_multi_mae_vit_base_patch32_dec256d4b

import os
from multiprocessing import Manager

import cv2
import numpy as np
import torch
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
from torch.utils.data import Dataset
from coperception.datasets.V2XSimSeg import V2XSimSeg


class Transform:
    def __init__(self, split):
        self.totensor = transforms.ToTensor()
        self.resize = transforms.Resize((256, 256))
        self.split = split

    def __call__(self, img, label):
        img = self.totensor(img.copy())
        label = self.totensor(label.copy())

        if self.split != "train":
            return img.permute(1, 2, 0).float(), label.squeeze(0).int()

        crop = transforms.RandomResizedCrop(256)
        params = crop.get_params(img, scale=(0.08, 1.0), ratio=(0.75, 1.33))
        img = TF.crop(img, *params)
        label = TF.crop(label, *params)

        if np.random.random() > 0.5:
            img = TF.hflip(img)
            label = TF.hflip(label)

        if np.random.random() > 0.5:
            img = TF.vflip(img)
            label = TF.vflip(label)

        img = self.resize(img)
        label = cv2.resize(
            label.squeeze(0).numpy(), dsize=(256, 256), interpolation=cv2.INTER_NEAREST
        )  # Resize provided by pytorch will have some random noise
        # return img.permute(1, 2, 0).float(), label.squeeze(0).int()
        return img.permute(1, 2, 0).float(), label


class MultiTempV2XSimSeg(V2XSimSeg):
    def __init__(
        self,
        dataset_roots=None,
        config=None,
        split=None,
        cache_size=1000,
        val=False,
        com=False,
        bound=None,
        kd_flag=False,
        no_cross_road=False,
        time_stamp = 2,
    ):
        """
        Inherited from the V2XSimSeg dataset, now support multi timestamp loading
        output padded_voxel_points_next are for other timestamp
        it is an empty list if timestamp = 1
        """
        super().__init__(dataset_roots, config, split, cache_size, val, com, bound, kd_flag, no_cross_road)
        self.time_stamp = config.time_stamp if hasattr(config, 'time_stamp') else time_stamp
        print("use time stamp", self.time_stamp)

    def get_seginfo_from_single_agent(self, agent_id, idx):
        empty_flag = False
        if idx in self.cache[agent_id]:
            gt_dict = self.cache[agent_id][idx]
        else:
            seq_file = self.seq_files[agent_id][idx]
            gt_data_handle = np.load(seq_file, allow_pickle=True)
            if gt_data_handle == 0:
                empty_flag = True
                if self.time_stamp > 1:
                    padded_voxel_next = torch.zeros((self.time_stamp-1, 256, 256, 13)).bool()
                else:
                    padded_voxel_next = torch.zeros(0).bool()
                if self.com:
                    return (
                        torch.zeros((256, 256, 13)).bool(),
                        padded_voxel_next,
                        torch.zeros((256, 256, 13)).bool(),
                        torch.zeros((256, 256)).int(),
                        torch.zeros((self.num_agent, 4, 4)),
                        0,
                        0,
                    )
                else:
                    return (
                        torch.zeros((256, 256, 13)).bool(),
                        padded_voxel_next,
                        torch.zeros((256, 256, 13)).bool(),
                        torch.zeros((256, 256)).int(),
                    )
            else:
                gt_dict = gt_data_handle.item()
                if len(self.cache[agent_id]) < self.cache_size:
                    self.cache[agent_id][idx] = gt_dict

        if not empty_flag:
            bev_seg = gt_dict["bev_seg"].astype(np.int32)

            padded_voxel_points = list()

            # if self.bound == 'lowerbound':
            for i in range(self.num_past_pcs):
                indices = gt_dict["voxel_indices_" + str(i)]
                curr_voxels = np.zeros(self.dims, dtype=bool)
                curr_voxels[indices[:, 0], indices[:, 1], indices[:, 2]] = 1

                curr_voxels = np.rot90(curr_voxels, 3)
                # curr_voxels = np.rot90(np.fliplr(curr_voxels), 3)
                bev_seg = np.rot90(bev_seg, 1)  # to align with voxel

                padded_voxel_points.append(curr_voxels)
            padded_voxel_points = np.stack(padded_voxel_points, 0)
            padded_voxel_points = np.squeeze(padded_voxel_points, 0)

            padded_voxel_points_teacher = list()
            # if self.bound == 'upperbound' or self.kd_flag:
            if self.no_cross_road:
                indices_teacher = gt_dict["voxel_indices_teacher_no_cross_road"]
            else:
                indices_teacher = gt_dict["voxel_indices_teacher"]

            curr_voxels_teacher = np.zeros(self.dims, dtype=bool)
            curr_voxels_teacher[
                indices_teacher[:, 0], indices_teacher[:, 1], indices_teacher[:, 2]
            ] = 1
            curr_voxels_teacher = np.rot90(curr_voxels_teacher, 3)
            padded_voxel_points_teacher.append(curr_voxels_teacher)
            padded_voxel_points_teacher = np.stack(padded_voxel_points_teacher, 0)
            padded_voxel_points_teacher = np.squeeze(padded_voxel_points_teacher, 0)

            # temporal
            padded_voxel_points_next_list = []
            end_frame_idx = - self.time_stamp + 1 + idx
            current_scene = self.seq_scenes[agent_id][idx]
            for nextframe_idx in range(idx-1, end_frame_idx-1, -1):
                while ( nextframe_idx < 0 or self.seq_scenes[agent_id][nextframe_idx] != current_scene):
                    nextframe_idx = nextframe_idx + 1 
                next_seq_file = self.seq_files[agent_id][nextframe_idx]
                next_gt_data_handle = np.load(next_seq_file, allow_pickle=True)
                if next_gt_data_handle == 0:
                    padded_voxel_points_next_list.append(padded_voxel_points[0].copy())
                else:
                    next_gt_dict = next_gt_data_handle.item()
                    indices_next = next_gt_dict['voxel_indices_0'] # FIXME: is it always 0.
                    next_voxels = np.zeros(self.dims, dtype=bool)
                    next_voxels[indices_next[:, 0], indices_next[:, 1], indices_next[:, 2]] = 1
                    next_voxels = np.rot90(next_voxels, 3)
                    padded_voxel_points_next_list.append(next_voxels)
            if len(padded_voxel_points_next_list)>0:
                padded_voxel_points_next_list = np.stack(padded_voxel_points_next_list, 0).astype(np.int32)
            else:
                padded_voxel_points_next_list = np.asarray(padded_voxel_points_next_list)

            if self.com:
                if self.no_cross_road:
                    trans_matrices = gt_dict["trans_matrices_no_cross_road"]
                else:
                    trans_matrices = gt_dict["trans_matrices"]

                target_agent_id = gt_dict["target_agent_id"]
                num_sensor = gt_dict["num_sensor"]

                return (
                    torch.from_numpy(padded_voxel_points),
                    torch.from_numpy(padded_voxel_points_next_list),
                    torch.from_numpy(padded_voxel_points_teacher),
                    torch.from_numpy(bev_seg.copy()),
                    torch.from_numpy(trans_matrices.copy()),
                    target_agent_id,
                    num_sensor,
                )
            else:
                return (
                    torch.from_numpy(padded_voxel_points),
                    torch.from_numpy(padded_voxel_points_next_list),
                    torch.from_numpy(padded_voxel_points_teacher),
                    torch.from_numpy(bev_seg.copy()),
                )


import os
import math
from multiprocessing import Manager

import numpy as np
from coperception.utils.obj_util import *
from coperception.datasets.V2XSimDet import V2XSimDet


class MultiTempV2XSimDet(V2XSimDet):
    def __init__(
        self,
        dataset_roots=None,
        config=None,
        config_global=None,
        split=None,
        cache_size=10000,
        val=False,
        bound=None,
        kd_flag=False,
        no_cross_road=False,
        time_stamp = 2,
    ):
        """
        Inherited from the V2XSimDet dataset, now support multi timestamp training
        output padded_voxel_points_next are for other timestamp
        it is an empty list if timestamp = 1
        """
        super().__init__(dataset_roots, config, config_global, split, cache_size, val, bound, kd_flag, no_cross_road)
        self.time_stamp = config.time_stamp if hasattr(config, 'time_stamp') else time_stamp
        print("use time stamp", self.time_stamp)

    def pick_single_agent(self, agent_id, idx):
        empty_flag = False
        if idx in self.cache[agent_id]:
            gt_dict = self.cache[agent_id][idx]
        else:
            seq_file = self.seq_files[agent_id][idx]
            gt_data_handle = np.load(seq_file, allow_pickle=True)
            if gt_data_handle == 0:
                empty_flag = True
                padded_voxel_points = []
                padded_voxel_points_next_list = []
                padded_voxel_points_teacher = []
                label_one_hot = np.zeros_like(self.label_one_hot_meta)
                reg_target = np.zeros_like(self.reg_target_meta)
                anchors_map = np.zeros_like(self.anchors_map_meta)
                vis_maps = np.zeros_like(self.vis_maps_meta)
                reg_loss_mask = np.zeros_like(self.reg_loss_mask_meta)

                if self.bound == "lowerbound" or self.bound == 'both':
                    padded_voxel_points = np.zeros_like(self.padded_voxel_points_meta)
                    # TODO: multi time stamp
                    if self.time_stamp>1:
                        for i in range(self.time_stamp-1):
                            padded_voxel_points_next_list.append(np.zeros_like(self.padded_voxel_points_meta))
                        padded_voxel_points_next_list = np.concatenate(padded_voxel_points_next_list, 0).astype(np.float32)
                    else:
                        padded_voxel_points_next_list = np.asarray(padded_voxel_points_next_list)
                    # print("padded other time stamp shape", padded_voxel_points_next_list.shape)

                if self.kd_flag or self.bound == "upperbound" or self.bound == 'both':
                    padded_voxel_points_teacher = np.zeros_like(
                        self.padded_voxel_points_meta
                    )

                if self.val:
                    return (
                        padded_voxel_points,
                        padded_voxel_points_next_list, # added
                        padded_voxel_points_teacher,
                        label_one_hot,
                        reg_target,
                        reg_loss_mask,
                        anchors_map,
                        vis_maps,
                        [{"gt_box": []}],
                        [seq_file],
                        0,
                        0,
                        np.zeros((self.num_agent, 4, 4)),
                    )
                else:
                    return (
                        padded_voxel_points,
                        padded_voxel_points_next_list, # added
                        padded_voxel_points_teacher,
                        label_one_hot,
                        reg_target,
                        reg_loss_mask,
                        anchors_map,
                        vis_maps,
                        0,
                        0,
                        np.zeros((self.num_agent, 4, 4)),
                    )
            else:
                gt_dict = gt_data_handle.item()
                if len(self.cache[agent_id]) < self.cache_size:
                    self.cache[agent_id][idx] = gt_dict

        if not empty_flag:
            allocation_mask = gt_dict["allocation_mask"].astype(bool)
            reg_loss_mask = gt_dict["reg_loss_mask"].astype(bool)
            gt_max_iou = gt_dict["gt_max_iou"]

            # load regression target
            reg_target_sparse = gt_dict["reg_target_sparse"]
            # need to be modified Yiqi , only use reg_target and allocation_map
            reg_target = np.zeros(self.reg_target_shape).astype(reg_target_sparse.dtype)

            reg_target[allocation_mask] = reg_target_sparse
            reg_target[np.bitwise_not(reg_loss_mask)] = 0
            label_sparse = gt_dict["label_sparse"]

            one_hot_label_sparse = self.get_one_hot(label_sparse, self.category_num)
            label_one_hot = np.zeros(self.label_one_hot_shape)
            label_one_hot[:, :, :, 0] = 1
            label_one_hot[allocation_mask] = one_hot_label_sparse

            if self.only_det:
                reg_target = reg_target[:, :, :, :1]
                reg_loss_mask = reg_loss_mask[:, :, :, :1]

            # only center for pred
            elif self.config.pred_type in ["motion", "center"]:
                reg_loss_mask = np.expand_dims(reg_loss_mask, axis=-1)
                reg_loss_mask = np.repeat(reg_loss_mask, self.box_code_size, axis=-1)
                reg_loss_mask[:, :, :, 1:, 2:] = False

            # Prepare padded_voxel_points
            padded_voxel_points = []
            if self.bound == "lowerbound" or self.bound == "both":
                for i in range(self.num_past_pcs):
                    indices = gt_dict["voxel_indices_" + str(i)]
                    curr_voxels = np.zeros(self.dims, dtype=bool)
                    curr_voxels[indices[:, 0], indices[:, 1], indices[:, 2]] = 1
                    curr_voxels = np.rot90(curr_voxels, 3)
                    padded_voxel_points.append(curr_voxels)
                padded_voxel_points = np.stack(padded_voxel_points, 0).astype(
                    np.float32
                )
                padded_voxel_points = padded_voxel_points.astype(np.float32)

            anchors_map = self.anchors_map

            if self.config.use_vis:
                vis_maps = np.zeros(
                    (
                        self.num_past_pcs,
                        self.config.map_dims[-1],
                        self.config.map_dims[0],
                        self.config.map_dims[1],
                    )
                )
                vis_free_indices = gt_dict["vis_free_indices"]
                vis_occupy_indices = gt_dict["vis_occupy_indices"]
                vis_maps[
                    vis_occupy_indices[0, :],
                    vis_occupy_indices[1, :],
                    vis_occupy_indices[2, :],
                    vis_occupy_indices[3, :],
                ] = math.log(0.7 / (1 - 0.7))
                vis_maps[
                    vis_free_indices[0, :],
                    vis_free_indices[1, :],
                    vis_free_indices[2, :],
                    vis_free_indices[3, :],
                ] = math.log(0.4 / (1 - 0.4))
                vis_maps = np.swapaxes(vis_maps, 2, 3)
                vis_maps = np.transpose(vis_maps, (0, 2, 3, 1))
                for v_id in range(vis_maps.shape[0]):
                    vis_maps[v_id] = np.rot90(vis_maps[v_id], 3)
                vis_maps = vis_maps[-1]
            else:
                vis_maps = np.zeros(0)

            if self.no_cross_road:
                trans_matrices = gt_dict["trans_matrices_no_cross_road"]
            else:
                trans_matrices = gt_dict["trans_matrices"]

            label_one_hot = label_one_hot.astype(np.float32)
            reg_target = reg_target.astype(np.float32)
            anchors_map = anchors_map.astype(np.float32)
            vis_maps = vis_maps.astype(np.float32)

            target_agent_id = gt_dict["target_agent_id"]
            num_sensor = gt_dict["num_sensor"]

            # Prepare padded_voxel_points_teacher
            padded_voxel_points_teacher = []
            if "voxel_indices_teacher" in gt_dict and (
                self.kd_flag or self.bound == "upperbound" or self.bound == "both"
            ):
                if self.no_cross_road:
                    indices_teacher = gt_dict["voxel_indices_teacher_no_cross_road"]
                else:
                    indices_teacher = gt_dict["voxel_indices_teacher"]

                curr_voxels_teacher = np.zeros(self.dims, dtype=bool)
                curr_voxels_teacher[
                    indices_teacher[:, 0], indices_teacher[:, 1], indices_teacher[:, 2]
                ] = 1
                curr_voxels_teacher = np.rot90(curr_voxels_teacher, 3)
                padded_voxel_points_teacher.append(curr_voxels_teacher)
                padded_voxel_points_teacher = np.stack(
                    padded_voxel_points_teacher, 0
                ).astype(np.float32)
                padded_voxel_points_teacher = padded_voxel_points_teacher.astype(
                    np.float32
                )

            # Temporal, added by Juexiao--------------------------------
            # main modification here: -----------------
            # use next time stamp image
            # if no next time stamp, use the same voxel (a plausible pseudo next)
            # -----------------------------------------
            # TODO: future -> past
            padded_voxel_points_next_list = [] # a list holding every padded_voxel_points_next for all the multi timestamp
            # end_frame_idx = self.time_stamp - 1 + idx
            # current_scene = self.seq_scenes[agent_id][idx]
            # for nextframe_idx in range(idx+1, end_frame_idx+1):
            #     while ((nextframe_idx) > (len(self.seq_scenes[agent_id])-1) or self.seq_scenes[agent_id][nextframe_idx] != current_scene):
            #         # padded_voxel_points_next = padded_voxel_points
            #         nextframe_idx = nextframe_idx - 1 # still use last frame as the next frame
            # use past features
            end_frame_idx = - self.time_stamp + 1 + idx
            current_scene = self.seq_scenes[agent_id][idx]
            for nextframe_idx in range(idx-1, end_frame_idx-1, -1):
                while ( nextframe_idx < 0 or self.seq_scenes[agent_id][nextframe_idx] != current_scene):
                    # padded_voxel_points_next = padded_voxel_points
                    nextframe_idx = nextframe_idx + 1 # still use last frame as the next frame

                next_seq_file = self.seq_files[agent_id][nextframe_idx]
                next_gt_data_handle = np.load(next_seq_file, allow_pickle=True)
                if next_gt_data_handle == 0:
                    padded_voxel_points_next_list.append(padded_voxel_points[0].copy())
                    # next_trans_matrices = trans_matrices
                else:
                    next_gt_dict = next_gt_data_handle.item()
                    indices_next = next_gt_dict['voxel_indices_0'] # FIXME: is it always 0.
                    # next_trans_matrices = next_gt_dict['trans_matrices']
                    next_voxels = np.zeros(self.dims, dtype=bool)
                    next_voxels[indices_next[:, 0], indices_next[:, 1], indices_next[:, 2]] = 1
                    next_voxels = np.rot90(next_voxels, 3)
                    padded_voxel_points_next_list.append(next_voxels)
                    # padded_voxel_points_next = np.stack(padded_voxel_points_next, 0).astype(np.float32)
                    # padded_voxel_points_next = padded_voxel_points_next.astype(np.float32)
                    # print(nextframe_idx, "voxels shape", padded_voxel_points_next.shape)
                # padded_voxel_points_next_list.append(padded_voxel_points_next)
            if len(padded_voxel_points_next_list)>0:
                padded_voxel_points_next_list = np.stack(padded_voxel_points_next_list, 0).astype(np.float32)
            else:
                padded_voxel_points_next_list = np.asarray(padded_voxel_points_next_list)
            # print("all other timestamp voxels shape", padded_voxel_points_next_list.shape)
            ## ----------------------------------------

            if self.val:
                return (
                    padded_voxel_points,
                    padded_voxel_points_next_list,
                    padded_voxel_points_teacher,
                    label_one_hot,
                    reg_target,
                    reg_loss_mask,
                    anchors_map,
                    vis_maps,
                    [{"gt_box": gt_max_iou}],
                    [seq_file],
                    target_agent_id,
                    num_sensor,
                    trans_matrices,
                )

            else:
                return (
                    padded_voxel_points,
                    padded_voxel_points_next_list,
                    padded_voxel_points_teacher,
                    label_one_hot,
                    reg_target,
                    reg_loss_mask,
                    anchors_map,
                    vis_maps,
                    target_agent_id,
                    num_sensor,
                    trans_matrices,
                )

    


from .MultiTempDet import MultiTempV2XSimDet
from .MultiTempSeg import MultiTempV2XSimSeg


import torch.nn as nn
import numpy as np
import torch
import torch.nn.functional as F

# Juexiao add for mae -----
from .misc import NativeScalerWithGradNormCount as NativeScaler
import math
from star.utils.move_optim import optimizer_to
# -------------------------


class CoModule(object):
    def __init__(self, model, optimizer, com):
        self.mae_loss_scaler = NativeScaler()
        self.model = model
        self.optimizer = optimizer
        self.scheduler = None
        if com=="late" or com=="vqvae":
            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
                    optimizer, milestones=[50, 100, 150, 200], gamma=0.5
            )

    def resume_from_cpu(self, checkpoint, device, trainable=True):
        """
        This function load state dict to model and optimizer on cpu, and move it back to device.
        This avoids a GPU memory surge issue.
        NOTE: assume checkpoint is loaded in cpu
        """
        # handles model
        self.model = self.model.cpu()
        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.model = self.model.to(device)
        if trainable:
            # handles optimizer
            self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
            optimizer_to(self.optimizer, device)
            # possible extension: reinitialize scheduler based on this new optimizer
            self.scheduler = self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
                    self.optimizer, milestones=[50, 100, 150, 200], gamma=0.5
            )

    # used by scene completion task
    def step_completion(self, data, batch_size, loss_fn='ce', trainable=False):
        bev_seq = data['bev_seq']
        trans_matrices = data['trans_matrices']
        num_agent = data['num_agent']

        result, ind_pred = self.model(bev_seq, trans_matrices, num_agent, batch_size=batch_size)

        loss_fn_dict = {
            'mse': nn.MSELoss(),
            'bce': nn.BCELoss(),
            'ce': nn.CrossEntropyLoss(),
            'l1': nn.L1Loss(),
            'smooth_l1': nn.SmoothL1Loss(),
        }

        loss = -1
        if trainable:
            # labels = data['bev_seq_teacher']
            # labels = labels.permute(0, 1, 4, 2, 3).squeeze()  # (Batch, seq, z, h, w)
            # loss = 10000 * loss_fn_dict[loss_fn](result, labels)
            target = bev_seq.permute(0, 1, 4, 2, 3).squeeze(1)
            target = target.type(torch.LongTensor).to(ind_pred.device)
            loss = loss_fn_dict[loss_fn](ind_pred, target)

            if self.MGDA:
                self.optimizer_encoder.zero_grad()
                self.optimizer_head.zero_grad()
                loss.backward()
                self.optimizer_encoder.step()
                self.optimizer_head.step()
            else:
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

        return loss, result

    def infer_completion(self, data, batch_size):
        bev_seq = data['bev_seq']
        trans_matrices = data['trans_matrices']
        num_agent = data['num_agent']

        result, ind_pred = self.model(bev_seq, trans_matrices, num_agent, batch_size=batch_size)

        return result

    # Used my MAE model in scene completion, added by Juexiao
    def step_mae_completion(self, data, batch_size, mask_ratio, loss_fn='mse', trainable = False):
        # figure out the dimensions: squeeze, permute etc
        bev_seq = data['bev_seq'].squeeze(1).permute(0,3,1,2)
        bev_seq_next = data['bev_seq_next'] # [bxa, ts-1, H, W, C]
        if bev_seq_next.dim()>2:
            bev_seq_next = bev_seq_next.permute(0,1,4,2,3) # [bxa, ts-1, C, H, W]
        # print(data['bev_seq_teacher'].size())
        bev_teacher = data['bev_seq_teacher'].squeeze(1).permute(0,3,1,2)
        # print("teacher size", bev_teacher.size())
        num_agent_tensor = data['num_agent']
        trans_matrices = data['trans_matrices']
        
        with torch.cuda.amp.autocast(enabled=False):
            loss, result, _, ind_pred = self.model(bev_seq, 
                                                    bev_seq_next, 
                                                    bev_teacher, 
                                                    trans_matrices, 
                                                    num_agent_tensor,
                                                    batch_size,
                                                    mask_ratio=mask_ratio)
        # print("result size", result.size())
        # exit(1)
        
        loss_value = loss.item()

        if not math.isfinite(loss_value):
            print("Loss is {}, stopping training".format(loss_value))
            # torch.save(bev_seq.cpu(), "lossnan-bev.pt")
            # torch.save(self.model.cpu(), "lossnan-mae.pt")
            # torch.save(result.cpu(), "lossnan-pred.pt")
            # torch.save(mask.cpu(), "lossnan-mask.pt")
            sys.exit(1)

        if trainable:
            self.mae_loss_scaler(loss, self.optimizer, parameters=self.model.parameters(),
                        update_grad=True)
            self.optimizer.zero_grad()
            # loss.backward()
            # self.optimizer.step()

        return loss_value, result, ind_pred.detach()

    # inference mae, ego use groundtruth
    def infer_mae_completion(self, data, batch_size, mask_ratio, loss_fn='mse'):
        # figure out the dimensions: squeeze, permute etc
        bev_seq = data['bev_seq'].squeeze(1).permute(0,3,1,2)
        bev_seq_next = data['bev_seq_next'] # [bxa, ts-1, H, W, C]
        if bev_seq_next.dim()>2:
            bev_seq_next = bev_seq_next.permute(0,1,4,2,3) # [bxa, ts-1, C, H, W]
        # print(data['bev_seq_teacher'].size())
        bev_teacher = data['bev_seq_teacher'].squeeze(1).permute(0,3,1,2)
        # print("teacher size", bev_teacher.size())
        num_agent_tensor = data['num_agent']
        trans_matrices = data['trans_matrices']
        
        with torch.cuda.amp.autocast(enabled=False):
            loss, result, latent, ind_pred = self.model.inference(bev_seq, 
                                                    bev_seq_next, 
                                                    bev_teacher, 
                                                    trans_matrices, 
                                                    num_agent_tensor,
                                                    batch_size,
                                                    mask_ratio=mask_ratio)
        
        loss_value = loss.item()
        return loss_value, result, ind_pred.detach(), latent.detach()

    def step_vae_completion(self, data, batch_size, loss_fn='ce', trainable=False):
        bev_seq = data['bev_seq']
        trans_matrices = data['trans_matrices']
        num_agent = data['num_agent']

        vq_loss, result, ind_recon, perplexity = self.model(bev_seq, trans_matrices, num_agent, batch_size=batch_size)
        target = bev_seq.squeeze(1).permute(0,3,1,2).detach()
        # print("target", target.size())
        # recon_error = F.mse_loss(ind_recon, target)
        ## classification task ##
        # target = bev_seq.permute(0, 1, 4, 2, 3).squeeze(1)
        target = target.type(torch.LongTensor).to(ind_recon.device)
        # print("target", target.size())
        # print("ind_recon", ind_recon.size(), ind_recon.type())
        loss_fn = nn.CrossEntropyLoss()
        recon_error = loss_fn(ind_recon, target)
        loss = recon_error + vq_loss

        if trainable:
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
        
        return loss.item(), result, ind_recon, perplexity.detach()

    def infer_vae_completion(self, data, batch_size, loss_fn='ce', trainable=False):
        bev_seq = data['bev_seq']
        trans_matrices = data['trans_matrices']
        num_agent = data['num_agent']

        vq_loss, result, ind_recon = self.model(bev_seq, trans_matrices, num_agent, batch_size=batch_size)

        return result

    # Used by VQ STAR model in scene completion, added by Juexiao
    def step_vqstar_completion(self, data, batch_size, mask_ratio, loss_fn='mse', trainable = False):
        # figure out the dimensions: squeeze, permute etc
        bev_seq = data['bev_seq'].squeeze(1).permute(0,3,1,2)
        bev_seq_next = data['bev_seq_next'] # [bxa, ts-1, H, W, C]
        if bev_seq_next.dim()>2:
            bev_seq_next = bev_seq_next.permute(0,1,4,2,3) # [bxa, ts-1, C, H, W]
        # print(data['bev_seq_teacher'].size())
        bev_teacher = data['bev_seq_teacher'].squeeze(1).permute(0,3,1,2)
        # print("teacher size", bev_teacher.size())
        num_agent_tensor = data['num_agent']
        trans_matrices = data['trans_matrices']
        
        with torch.cuda.amp.autocast(enabled=False):
            loss, result, _, ind_pred, perplexity = self.model(bev_seq, 
                                                    bev_seq_next, 
                                                    bev_teacher, 
                                                    trans_matrices, 
                                                    num_agent_tensor,
                                                    batch_size,
                                                    mask_ratio=mask_ratio)
        # print("result size", result.size())
        # exit(1)
        
        loss_value = loss.item()

        if not math.isfinite(loss_value):
            print("Loss is {}, stopping training".format(loss_value))
            sys.exit(1)

        if trainable:
            self.mae_loss_scaler(loss, self.optimizer, parameters=self.model.parameters(),
                        update_grad=True)
            self.optimizer.zero_grad()
            # loss.backward()
            # self.optimizer.step()

        return loss_value, result, ind_pred.detach(), perplexity.detach()

    def infer_vqstar_completion(self, data, batch_size, mask_ratio, loss_fn='mse'):
        # figure out the dimensions: squeeze, permute etc
        bev_seq = data['bev_seq'].squeeze(1).permute(0,3,1,2)
        bev_seq_next = data['bev_seq_next'] # [bxa, ts-1, H, W, C]
        if bev_seq_next.dim()>2:
            bev_seq_next = bev_seq_next.permute(0,1,4,2,3) # [bxa, ts-1, C, H, W]
        # print(data['bev_seq_teacher'].size())
        bev_teacher = data['bev_seq_teacher'].squeeze(1).permute(0,3,1,2)
        # print("teacher size", bev_teacher.size())
        num_agent_tensor = data['num_agent']
        trans_matrices = data['trans_matrices']
        
        with torch.cuda.amp.autocast(enabled=False):
            loss, result, ind_pred, perplexity, encodings = self.model.inference(bev_seq, 
                                                    bev_seq_next, 
                                                    bev_teacher, 
                                                    trans_matrices, 
                                                    num_agent_tensor,
                                                    batch_size,
                                                    mask_ratio=mask_ratio)
        
        loss_value = loss.item()
        return loss_value, result, ind_pred.detach(), perplexity.detach(), encodings.detach()


"""
IoU Evaluator for our BEV scene reconstruction
remove the keys for scales, only one 1_1 scale is used 
"""
# Some sections of this code reused code from SemanticKITTI development kit
# https://github.com/PRBonn/semantic-kitti-api

import numpy as np
import torch
import copy
import torch.nn as nn


class iouEval:
  def __init__(self, n_classes, ignore=None):
    # classes
    self.n_classes = n_classes

    # What to include and ignore from the means
    self.ignore = np.array(ignore, dtype=np.int64)
    self.include = np.array(
        [n for n in range(self.n_classes) if n not in self.ignore], dtype=np.int64)

    # reset the class counters
    self.reset()

  def num_classes(self):
    return self.n_classes

  def reset(self):
    self.conf_matrix = np.zeros((self.n_classes,
                                 self.n_classes),
                                dtype=np.int64)

  def addBatch(self, x, y):  # x=preds, y=targets

    assert x.shape == y.shape
    # print("x y shapes", x.shape)
    # sizes should be matching
    x_row = x.reshape(-1)  # de-batchify
    y_row = y.reshape(-1)  # de-batchify
    # print("x, y row shapes", x_row.shape)


    # check
    assert(x_row.shape == x_row.shape)

    # create indexes
    idxs = tuple(np.stack((x_row, y_row), axis=0))

    # make confusion matrix (cols = gt, rows = pred)
    np.add.at(self.conf_matrix, idxs, 1)

  def getStats(self):
    # remove fp from confusion on the ignore classes cols
    conf = self.conf_matrix.copy()
    conf[:, self.ignore] = 0

    # get the clean stats
    tp = np.diag(conf)
    fp = conf.sum(axis=1) - tp
    fn = conf.sum(axis=0) - tp
    return tp, fp, fn

  def getIoU(self):
    tp, fp, fn = self.getStats()
    intersection = tp
    union = tp + fp + fn + 1e-15
    iou = intersection / union
    iou_mean = (intersection[self.include] / union[self.include]).mean()
    return iou_mean, iou  # returns "iou mean", "iou per class" ALL CLASSES

  def getacc(self):
    tp, fp, fn = self.getStats()
    total_tp = tp.sum()
    total = tp[self.include].sum() + fp[self.include].sum() + 1e-15
    acc_mean = total_tp / total
    return acc_mean  # returns "acc mean"

  def get_confusion(self):
    return self.conf_matrix.copy()


class LossesTrackEpoch:
  def __init__(self, num_iterations):
    # classes
    self.num_iterations = num_iterations
    self.validation_losses = {}
    self.train_losses = {}
    self.train_iteration_counts = 0
    self.validation_iteration_counts = 0

  def set_validation_losses(self, keys):
    for key in keys:
      self.validation_losses[key] = 0
    return

  def set_train_losses(self, keys):
    for key in keys:
      self.train_losses[key] = 0
    return

  def update_train_losses(self, loss):
    for key in loss:
      self.train_losses[key] += loss[key]
    self.train_iteration_counts += 1
    return

  def update_validaiton_losses(self, loss):
    for key in loss:
      self.validation_losses[key] += loss[key]
    self.validation_iteration_counts += 1
    return

  def restart_train_losses(self):
    for key in self.train_losses.keys():
      self.train_losses[key] = 0
    self.train_iteration_counts = 0
    return

  def restart_validation_losses(self):
    for key in self.validation_losses.keys():
      self.validation_losses[key] = 0
    self.validation_iteration_counts = 0
    return


class Metrics:

  def __init__(self, nbr_classes, num_iterations_epoch):

    self.nbr_classes = nbr_classes # should be 2
    self.scales = ['1', '2', '4']
    self.evaluator = dict()
    self.evaluator['1'] = iouEval(self.nbr_classes, [])
    self.down_scale = nn.MaxPool3d((2, 2, 2), stride=(2, 2, 2), padding=(1,0,0))
    self.evaluator['2'] = iouEval(self.nbr_classes, []) # maxpooled 2,2,2
    self.evaluator['4'] = iouEval(self.nbr_classes, [])
    # self.evaluator = iouEval(self.nbr_classes, [])
    self.losses_track = LossesTrackEpoch(num_iterations_epoch)
    self.every_batch_IoU = []
    self.best_metric_record = {'mIoU': 0, 'IoU':0, 'epoch': 0, 'loss': 99999999}

    return

  def add_batch(self, prediction, target):

    # binarize and passing to cpu
    # for key in prediction:
    #   prediction[key] = torch.argmax(prediction[key], dim=1).data.cpu().numpy()
    # print("prediction", prediction.shape)
    prediction = prediction.detach().cpu()
    scale2_pred = self.down_scale(prediction.unsqueeze(1))
    scale2_target = self.down_scale(target.unsqueeze(1))
    scale4_pred = self.down_scale(scale2_pred)
    scale4_target = self.down_scale(scale2_target)
    # print(down_pred.size())
    
    prediction[prediction>=0.5] = 1.0
    prediction[prediction<0.5] = 0.0
    prediction = prediction.numpy()

    scale2_pred[scale2_pred>=0.5] = 1.0
    scale2_pred[scale2_pred<0.5] = 0.0
    scale2_pred = scale2_pred.numpy()

    scale4_pred[scale4_pred>=0.5] = 1.0
    scale4_pred[scale4_pred<0.5] = 0.0
    scale4_pred = scale4_pred.numpy()
    ## juexiao
    # print("prediction", prediction.shape)
    # target[target>0.5] = 1.0
    # target[target<=0.5] = 0.0
    target = target.cpu().numpy()
    scale2_target = scale2_target.cpu().numpy() #[6, 1, 7, 128, 128]
    scale4_target = scale4_target.cpu().numpy() #[6, 1, 4, 64, 64]
    # print("target", target.shape)
    # print("scale 2 target", scale2_target.shape)
    # print("scale 4 target", scale4_target.shape)

    prediction = prediction.reshape(-1).astype('int64')
    target = target.reshape(-1).astype('int64')
    scale2_pred = scale2_pred.reshape(-1).astype('int64')
    scale2_target = scale2_target.reshape(-1).astype('int64')
    scale4_pred = scale4_pred.reshape(-1).astype('int64')
    scale4_target = scale4_target.reshape(-1).astype('int64')
    self.evaluator['1'].addBatch(prediction, target)
    self.evaluator['2'].addBatch(scale2_pred, scale2_target)
    self.evaluator['4'].addBatch(scale4_pred, scale4_target)
    return

  def get_eval_mask_Lidar(self, target):
    '''
    eval_mask_lidar is only to ingore unknown voxels in groundtruth
    '''
    mask = (target != 255)
    return mask

  def get_occupancy_IoU(self, scale):
    conf = self.evaluator[scale].get_confusion()
    tp_occupancy = np.sum(conf[1:, 1:])
    fp_occupancy = np.sum(conf[1:, 0])
    fn_occupancy = np.sum(conf[0, 1:])
    intersection = tp_occupancy
    union = tp_occupancy + fp_occupancy + fn_occupancy + 1e-15
    iou_occupancy = intersection / union
    return iou_occupancy  # returns iou occupancy

  # Juexiao
  def update_IoU(self):
    self.every_batch_IoU.append(np.asarray([self.get_occupancy_IoU('1'), self.get_occupancy_IoU('2'), self.get_occupancy_IoU('4')]))
    self.evaluator['1'].reset()
    self.evaluator['2'].reset()
    self.evaluator['4'].reset()
  
  def get_average_IoU(self):
    IoUs = np.asarray(self.every_batch_IoU)
    return np.mean(IoUs, axis=0)

  def get_occupancy_Precision(self):
    conf = self.evaluator.get_confusion()
    tp_occupancy = np.sum(conf[1:, 1:])
    fp_occupancy = np.sum(conf[1:, 0])
    precision = tp_occupancy / (tp_occupancy + fp_occupancy + 1e-15)
    return precision  # returns precision occupancy

  def get_occupancy_Recall(self):
    conf = self.evaluator.get_confusion()
    tp_occupancy = np.sum(conf[1:, 1:])
    fn_occupancy = np.sum(conf[0, 1:])
    recall = tp_occupancy/(tp_occupancy + fn_occupancy + 1e-15)
    return recall  # returns recall occupancy

  def get_occupancy_F1(self):
    conf = self.evaluator.get_confusion()
    tp_occupancy = np.sum(conf[1:, 1:])
    fn_occupancy = np.sum(conf[0, 1:])
    fp_occupancy = np.sum(conf[1:, 0])
    precision = tp_occupancy/(tp_occupancy + fp_occupancy + 1e-15)
    recall = tp_occupancy/(tp_occupancy + fn_occupancy + 1e-15)
    F1 = 2 * (precision * recall) / (precision + recall + 1e-15)
    return F1  # returns recall occupancy

  def get_semantics_mIoU(self):
    _, class_jaccard = self.evaluator.getIoU()
    mIoU_semantics = class_jaccard[1:].mean()  # Ignore on free voxels (0 excluded)
    return mIoU_semantics  # returns mIoU semantics

  def reset_evaluator(self):
    for key in self.evaluator:
      self.evaluator[key].reset()

  def update_best_metric_record(self, mIoU, IoU, loss, epoch):
    self.best_metric_record['mIoU'] = mIoU
    self.best_metric_record['IoU'] = IoU
    self.best_metric_record['loss'] = loss
    self.best_metric_record['epoch'] = epoch
    return



# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import math

def adjust_learning_rate(optimizer, epoch, args):
    """Decay the learning rate with half-cycle cosine after warmup"""
    if epoch < args.warmup_epochs:
        lr = args.lr * epoch / args.warmup_epochs 
    else:
        lr = args.min_lr + (args.lr - args.min_lr) * 0.5 * \
            (1. + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.nepoch - args.warmup_epochs)))
    for param_group in optimizer.param_groups:
        if "lr_scale" in param_group:
            param_group["lr"] = lr * param_group["lr_scale"]
        else:
            param_group["lr"] = lr
    return lr


import torch
import torch.nn as nn
import torch.nn.functional as F

class SoftmaxFocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2, reduction="mean"):
        super(SoftmaxFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, target):
        # print(inputs.size())
        # print(target.size())
        prob = F.softmax(inputs, dim=1)
        reverse_target = 1 - target
        one_hot_target = torch.stack([reverse_target, target], dim=1)
        # print(one_hot_target.size())
        pt = (prob*one_hot_target).sum(dim=1) 
        ce_loss = - pt.log() # [B, C, H, W] C is the pc height channel
        alpha_t = (self.alpha * one_hot_target).sum(dim=1)

        loss = alpha_t * torch.pow((1-pt), self.gamma) * ce_loss
        # print("loss", loss.mean())

        if self.reduction=="mean":
            return loss.mean()
        elif self.reduction == "sum":
            return loss.sum()

        return loss





# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# --------------------------------------------------------
# References:
# DeiT: https://github.com/facebookresearch/deit
# BEiT: https://github.com/microsoft/unilm/tree/master/beit
# --------------------------------------------------------

import builtins
import datetime
import os
import time
from collections import defaultdict, deque
from pathlib import Path

import torch
import torch.distributed as dist
from torch._six import inf


class SmoothedValue(object):
    """Track a series of values and provide access to smoothed values over a
    window or the global series average.
    """

    def __init__(self, window_size=20, fmt=None):
        if fmt is None:
            fmt = "{median:.4f} ({global_avg:.4f})"
        self.deque = deque(maxlen=window_size)
        self.total = 0.0
        self.count = 0
        self.fmt = fmt

    def update(self, value, n=1):
        self.deque.append(value)
        self.count += n
        self.total += value * n

    def synchronize_between_processes(self):
        """
        Warning: does not synchronize the deque!
        """
        if not is_dist_avail_and_initialized():
            return
        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')
        dist.barrier()
        dist.all_reduce(t)
        t = t.tolist()
        self.count = int(t[0])
        self.total = t[1]

    @property
    def median(self):
        d = torch.tensor(list(self.deque))
        return d.median().item()

    @property
    def avg(self):
        d = torch.tensor(list(self.deque), dtype=torch.float32)
        return d.mean().item()

    @property
    def global_avg(self):
        return self.total / self.count

    @property
    def max(self):
        return max(self.deque)

    @property
    def value(self):
        return self.deque[-1]

    def __str__(self):
        return self.fmt.format(
            median=self.median,
            avg=self.avg,
            global_avg=self.global_avg,
            max=self.max,
            value=self.value)


class MetricLogger(object):
    def __init__(self, delimiter="\t"):
        self.meters = defaultdict(SmoothedValue)
        self.delimiter = delimiter

    def update(self, **kwargs):
        for k, v in kwargs.items():
            if v is None:
                continue
            if isinstance(v, torch.Tensor):
                v = v.item()
            assert isinstance(v, (float, int))
            self.meters[k].update(v)

    def __getattr__(self, attr):
        if attr in self.meters:
            return self.meters[attr]
        if attr in self.__dict__:
            return self.__dict__[attr]
        raise AttributeError("'{}' object has no attribute '{}'".format(
            type(self).__name__, attr))

    def __str__(self):
        loss_str = []
        for name, meter in self.meters.items():
            loss_str.append(
                "{}: {}".format(name, str(meter))
            )
        return self.delimiter.join(loss_str)

    def synchronize_between_processes(self):
        for meter in self.meters.values():
            meter.synchronize_between_processes()

    def add_meter(self, name, meter):
        self.meters[name] = meter

    def log_every(self, iterable, print_freq, header=None):
        i = 0
        if not header:
            header = ''
        start_time = time.time()
        end = time.time()
        iter_time = SmoothedValue(fmt='{avg:.4f}')
        data_time = SmoothedValue(fmt='{avg:.4f}')
        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'
        log_msg = [
            header,
            '[{0' + space_fmt + '}/{1}]',
            'eta: {eta}',
            '{meters}',
            'time: {time}',
            'data: {data}'
        ]
        if torch.cuda.is_available():
            log_msg.append('max mem: {memory:.0f}')
        log_msg = self.delimiter.join(log_msg)
        MB = 1024.0 * 1024.0
        for obj in iterable:
            data_time.update(time.time() - end)
            yield obj
            iter_time.update(time.time() - end)
            if i % print_freq == 0 or i == len(iterable) - 1:
                eta_seconds = iter_time.global_avg * (len(iterable) - i)
                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
                if torch.cuda.is_available():
                    print(log_msg.format(
                        i, len(iterable), eta=eta_string,
                        meters=str(self),
                        time=str(iter_time), data=str(data_time),
                        memory=torch.cuda.max_memory_allocated() / MB))
                else:
                    print(log_msg.format(
                        i, len(iterable), eta=eta_string,
                        meters=str(self),
                        time=str(iter_time), data=str(data_time)))
            i += 1
            end = time.time()
        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print('{} Total time: {} ({:.4f} s / it)'.format(
            header, total_time_str, total_time / len(iterable)))


def setup_for_distributed(is_master):
    """
    This function disables printing when not in master process
    """
    builtin_print = builtins.print

    def print(*args, **kwargs):
        force = kwargs.pop('force', False)
        force = force or (get_world_size() > 8)
        if is_master or force:
            now = datetime.datetime.now().time()
            builtin_print('[{}] '.format(now), end='')  # print with time stamp
            builtin_print(*args, **kwargs)

    builtins.print = print


def is_dist_avail_and_initialized():
    if not dist.is_available():
        return False
    if not dist.is_initialized():
        return False
    return True


def get_world_size():
    if not is_dist_avail_and_initialized():
        return 1
    return dist.get_world_size()


def get_rank():
    if not is_dist_avail_and_initialized():
        return 0
    return dist.get_rank()


def is_main_process():
    return get_rank() == 0


def save_on_master(*args, **kwargs):
    if is_main_process():
        torch.save(*args, **kwargs)


def init_distributed_mode(args):
    if args.dist_on_itp:
        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])
        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])
        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])
        args.dist_url = "tcp://%s:%s" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])
        os.environ['LOCAL_RANK'] = str(args.gpu)
        os.environ['RANK'] = str(args.rank)
        os.environ['WORLD_SIZE'] = str(args.world_size)
        # ["RANK", "WORLD_SIZE", "MASTER_ADDR", "MASTER_PORT", "LOCAL_RANK"]
    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        args.rank = int(os.environ["RANK"])
        args.world_size = int(os.environ['WORLD_SIZE'])
        args.gpu = int(os.environ['LOCAL_RANK'])
    elif 'SLURM_PROCID' in os.environ:
        args.rank = int(os.environ['SLURM_PROCID'])
        args.gpu = args.rank % torch.cuda.device_count()
    else:
        print('Not using distributed mode')
        setup_for_distributed(is_master=True)  # hack
        args.distributed = False
        return

    args.distributed = True

    torch.cuda.set_device(args.gpu)
    args.dist_backend = 'nccl'
    print('| distributed init (rank {}): {}, gpu {}'.format(
        args.rank, args.dist_url, args.gpu), flush=True)
    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                         world_size=args.world_size, rank=args.rank)
    torch.distributed.barrier()
    setup_for_distributed(args.rank == 0)


class NativeScalerWithGradNormCount:
    state_dict_key = "amp_scaler"

    def __init__(self):
        self._scaler = torch.cuda.amp.GradScaler()

    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):
        self._scaler.scale(loss).backward(create_graph=create_graph)
        if update_grad:
            if clip_grad is not None:
                assert parameters is not None
                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place
                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)
            else:
                self._scaler.unscale_(optimizer)
                norm = get_grad_norm_(parameters)
            self._scaler.step(optimizer)
            self._scaler.update()
        else:
            norm = None
        return norm

    def state_dict(self):
        return self._scaler.state_dict()

    def load_state_dict(self, state_dict):
        self._scaler.load_state_dict(state_dict)


def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    parameters = [p for p in parameters if p.grad is not None]
    norm_type = float(norm_type)
    if len(parameters) == 0:
        return torch.tensor(0.)
    device = parameters[0].grad.device
    if norm_type == inf:
        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)
    else:
        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
    return total_norm


def save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler):
    output_dir = Path(args.output_dir)
    epoch_name = str(epoch)
    if loss_scaler is not None:
        checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]
        for checkpoint_path in checkpoint_paths:
            to_save = {
                'model': model_without_ddp.state_dict(),
                'optimizer': optimizer.state_dict(),
                'epoch': epoch,
                'scaler': loss_scaler.state_dict(),
                'args': args,
            }

            save_on_master(to_save, checkpoint_path)
    else:
        client_state = {'epoch': epoch}
        model.save_checkpoint(save_dir=args.output_dir, tag="checkpoint-%s" % epoch_name, client_state=client_state)


def load_model(args, model_without_ddp, optimizer, loss_scaler):
    if args.resume:
        if args.resume.startswith('https'):
            checkpoint = torch.hub.load_state_dict_from_url(
                args.resume, map_location='cpu', check_hash=True)
        else:
            checkpoint = torch.load(args.resume, map_location='cpu')
        model_without_ddp.load_state_dict(checkpoint['model'])
        print("Resume checkpoint %s" % args.resume)
        if 'optimizer' in checkpoint and 'epoch' in checkpoint and not (hasattr(args, 'eval') and args.eval):
            optimizer.load_state_dict(checkpoint['optimizer'])
            args.start_epoch = checkpoint['epoch'] + 1
            if 'scaler' in checkpoint:
                loss_scaler.load_state_dict(checkpoint['scaler'])
            print("With optim & sched!")


def all_reduce_mean(x):
    world_size = get_world_size()
    if world_size > 1:
        x_reduce = torch.tensor(x).cuda()
        dist.all_reduce(x_reduce)
        x_reduce /= world_size
        return x_reduce.item()
    else:
        return x

# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# --------------------------------------------------------
# Position embedding utils
# --------------------------------------------------------

import numpy as np

import torch

# --------------------------------------------------------
# 2D sine-cosine position embedding
# References:
# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py
# MoCo v3: https://github.com/facebookresearch/moco-v3
# --------------------------------------------------------
def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: int of the grid height and width
    return:
    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)
    """
    grid_h = np.arange(grid_size, dtype=np.float32)
    grid_w = np.arange(grid_size, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)  # here w goes first
    grid = np.stack(grid, axis=0)

    grid = grid.reshape([2, 1, grid_size, grid_size])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    assert embed_dim % 2 == 0

    # use half of dimensions to encode grid_h
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)

    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)
    return emb


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float)
    omega /= embed_dim / 2.
    omega = 1. / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product

    emb_sin = np.sin(out) # (M, D/2)
    emb_cos = np.cos(out) # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb


# --------------------------------------------------------
# Interpolate position embeddings for high-resolution
# References:
# DeiT: https://github.com/facebookresearch/deit
# --------------------------------------------------------
def interpolate_pos_embed(model, checkpoint_model):
    if 'pos_embed' in checkpoint_model:
        pos_embed_checkpoint = checkpoint_model['pos_embed']
        embedding_size = pos_embed_checkpoint.shape[-1]
        num_patches = model.patch_embed.num_patches
        num_extra_tokens = model.pos_embed.shape[-2] - num_patches
        # height (== width) for the checkpoint position embedding
        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)
        # height (== width) for the new position embedding
        new_size = int(num_patches ** 0.5)
        # class_token and dist_token are kept unchanged
        if orig_size != new_size:
            print("Position interpolate from %dx%d to %dx%d" % (orig_size, orig_size, new_size, new_size))
            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
            # only the position tokens are interpolated
            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)
            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
            checkpoint_model['pos_embed'] = new_pos_embed


from . import *

import torch.nn.functional as F
import torch.nn as nn
import torch
from coperception.utils.detection_util import *


class SegModule(object):
    def __init__(self, model, teacher, config, optimizer, kd_flag):
        self.config = config
        self.model = model
        self.optimizer = optimizer
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.config.nepoch
        )
        # self.scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150, 200], gamma=0.5)
        self.criterion = nn.CrossEntropyLoss()
        self.teacher = teacher
        if kd_flag:
            for k, v in self.teacher.named_parameters():
                v.requires_grad = False  # fix parameters

        self.kd_flag = kd_flag

        self.com = config.com

    def resume(self, path):
        def map_func(storage, location):
            return storage.cuda()

        if os.path.isfile(path):
            if rank == 0:
                print("=> loading checkpoint '{}'".format(path))

            checkpoint = torch.load(path, map_location=map_func)
            self.model.load_state_dict(checkpoint["state_dict"], strict=False)

            ckpt_keys = set(checkpoint["state_dict"].keys())
            own_keys = set(model.state_dict().keys())
            missing_keys = own_keys - ckpt_keys
            for k in missing_keys:
                print("caution: missing keys from checkpoint {}: {}".format(path, k))
        else:
            print("=> no checkpoint found at '{}'".format(path))

    def step(self, data, num_agent, batch_size, loss=True):
        bev = data["bev_seq"]
        labels = data["labels"]
        self.optimizer.zero_grad()
        bev = bev.permute(0, 3, 1, 2).contiguous()

        if not self.com:
            filtered_bev = []
            filtered_label = []
            for i in range(bev.size(0)):
                if torch.sum(bev[i]) > 1e-4:
                    filtered_bev.append(bev[i])
                    filtered_label.append(labels[i])
            bev = torch.stack(filtered_bev, 0)
            labels = torch.stack(filtered_label, 0)

        if self.kd_flag:
            data["bev_seq_teacher"] = (
                data["bev_seq_teacher"].permute(0, 3, 1, 2).contiguous()
            )

        if self.com:
            print("communication happening...")
            if self.kd_flag:
                pred, x9, x8, x7, x6, x5, fused_layer = self.model(
                    bev, data["trans_matrices"], data["num_sensor"]
                )
            elif self.config.flag.startswith("when2com") or self.config.flag.startswith(
                "who2com"
            ):
                if self.config.split == "train":
                    pred = self.model(
                        bev, data["trans_matrices"], data["num_sensor"], training=True
                    )
                else:
                    pred = self.model(
                        bev,
                        data["trans_matrices"],
                        data["num_sensor"],
                        inference=self.config.inference,
                        training=False,
                    )
            else:
                pred = self.model(bev, data["trans_matrices"], data["num_sensor"])
        else:
            pred = self.model(bev)

        if self.com:
            filtered_pred = []
            filtered_label = []
            for i in range(bev.size(0)):
                if torch.sum(bev[i]) > 1e-4:
                    filtered_pred.append(pred[i])
                    filtered_label.append(labels[i])
            pred = torch.stack(filtered_pred, 0)
            labels = torch.stack(filtered_label, 0)
        if not loss:
            return pred, labels

        kd_loss = (
            self.get_kd_loss(batch_size, data, fused_layer, num_agent, x5, x6, x7)
            if self.kd_flag
            else 0
        )
        loss = self.criterion(pred, labels.long()) + kd_loss

        if isinstance(self.criterion, nn.DataParallel):
            loss = loss.mean()

        loss_data = loss.data.item()
        if np.isnan(loss_data):
            raise ValueError("loss is nan while training")

        loss.backward()
        self.optimizer.step()

        return pred, loss_data

    def get_kd_loss(self, batch_size, data, fused_layer, num_agent, x5, x6, x7):
        if not self.kd_flag:
            return 0

        bev_seq_teacher = data["bev_seq_teacher"].type(torch.cuda.FloatTensor)
        kd_weight = data["kd_weight"]
        (
            logit_teacher,
            x9_teacher,
            x8_teacher,
            x7_teacher,
            x6_teacher,
            x5_teacher,
            x4_teacher,
        ) = self.teacher(bev_seq_teacher)
        kl_loss_mean = nn.KLDivLoss(size_average=True, reduce=True)

        target_x5 = x5_teacher.permute(0, 2, 3, 1).reshape(
            num_agent * batch_size * 16 * 16, -1
        )
        student_x5 = x5.permute(0, 2, 3, 1).reshape(
            num_agent * batch_size * 16 * 16, -1
        )
        kd_loss_x5 = kl_loss_mean(
            F.log_softmax(student_x5, dim=1), F.softmax(target_x5, dim=1)
        )

        target_x6 = x6_teacher.permute(0, 2, 3, 1).reshape(
            num_agent * batch_size * 32 * 32, -1
        )
        student_x6 = x6.permute(0, 2, 3, 1).reshape(
            num_agent * batch_size * 32 * 32, -1
        )
        kd_loss_x6 = kl_loss_mean(
            F.log_softmax(student_x6, dim=1), F.softmax(target_x6, dim=1)
        )

        target_x7 = x7_teacher.permute(0, 2, 3, 1).reshape(
            num_agent * batch_size * 64 * 64, -1
        )
        student_x7 = x7.permute(0, 2, 3, 1).reshape(
            num_agent * batch_size * 64 * 64, -1
        )
        kd_loss_x7 = kl_loss_mean(
            F.log_softmax(student_x7, dim=1), F.softmax(target_x7, dim=1)
        )

        target_x4 = x4_teacher.permute(0, 2, 3, 1).reshape(
            num_agent * batch_size * 32 * 32, -1
        )
        student_x4 = fused_layer.permute(0, 2, 3, 1).reshape(
            num_agent * batch_size * 32 * 32, -1
        )
        kd_loss_fused_layer = kl_loss_mean(
            F.log_softmax(student_x4, dim=1), F.softmax(target_x4, dim=1)
        )

        return kd_weight * (kd_loss_x5 + kd_loss_x6 + kd_loss_x7 + kd_loss_fused_layer)


import torch

def optimizer_to(optim, device):
    for param in optim.state.values():
        # Not sure there are any global tensors in the state dict
        if isinstance(param, torch.Tensor):
            param.data = param.data.to(device)
            if param._grad is not None:
                param._grad.data = param._grad.data.to(device)
        elif isinstance(param, dict):
            for subparam in param.values():
                if isinstance(subparam, torch.Tensor):
                    subparam.data = subparam.data.to(device)
                    if subparam._grad is not None:
                        subparam._grad.data = subparam._grad.data.to(device)